text,#label
"Abstract An overarching goal of natural language pro- cessing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an in- teractive process known as repair: asking ques- tions and seeking clarification until their uncer- tainty is resolved. We propose a framework for building a visually grounded question- asking model capable of producing polar (yes- no) clarification questions to resolve misunder- standings in dialogue. Our model uses an ex- pected information gain objective to derive in- formative questions from an off-the-shelf im- age captioner without requiring any supervised question-answer data. We demonstrate our model’s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human an- swerers. Introduction Human-machine interaction relies on accurate transfer of knowledge from users. However, nat- ural language input can be ambiguous or unclear, giving rise to uncertainty. A fundamental aspect of human communication is collaborative grounding, or seeking and providing incremental evidence of mutual understanding through dialog. Specifically, humans can correct for uncertainty through cooper- ative repair (Clark, 1996; Purver et al., 2002; Arkel et al., 2020) which involves interactively asking questions and seeking clarification. Making and recovering from mistakes collaboratively through question-asking is a key ingredient in grounding meaning and therefore an important feature in di- alog systems (Benotti and Blackburn, 2021). In this work, we focus on the computational chal- lenge of generating clarification questions in visu- ally grounded human-machine interactions. One popular approach is to train an end-to-end model to map visual and linguistic inputs directly to questions (Yao et al., 2018; Das et al., 2017). This approach is heavily data-driven, requiring large annotated training sets of questions under different goals and contexts. Another approach has drawn from work on active learning and Optimal Experiment Design (OED) in cognitive science to search for questions that are likely to maximize ex- pected information gain from an imagined answerer (Wang and Lake, 2019; Lee et al., 2018; Misra et al., 2018; Rao and Daumé III, 2018; Rothe et al., 2017;Kovashka and Grauman, 2013). Much of this work has relied on large-scale question-answer datasets (Kumar and Black, 2020; de Vries et al., 2017) for training or retrieval to propose candidate ques- tions or evaluate their expected utility. Others, like (Yu et al., 2020), derive questions from attribute annotations for domain-specific systems. In this paper, we address an open-domain setting where one cannot rely on an immediate grounding of the meaning of questions in the target domain (in contrast to end-to-end approaches, which assume examples of questions to train on, or semantic pars- ing approaches, which assume a logical form for questions). Our key contribution is a lightweight method to ground question semantics in the open image domain without observing question exam- ples. Instead, our framework builds a visually grounded question-asking model from image cap- tioning data, deriving question selection and belief updating without existing semantics. Our model generates candidate polar questions, arguably the most common form of clarification in dialogue (Stivers, 2010), by applying rule-based linguistic transformations to the outputs of a pretrained image captioner. We then use self-supervision to train a re- sponse model that predicts the likelihood of differ- ent answers. Given these predictions, we estimate the expected information gain of each question and select the question with the highest utility. We demonstrate our method’s ability to pose questions that improve communicative success in a question- driven communication game with synthetic and human answerers. Conclusions We introduce a question generation framework ca- pable of producing open-domain clarification ques- tions. Instead of relying on specialized question- answer training data or pre-specified question meanings, our model uses a pretrained image cap- tioner in conjunction with expected information gain to produce informative questions for unseen images. We demonstrate the effectiveness of this method in a question-driven communication game with synthetic and human answerers. We found it important to generate questions varying in speci- ficity by decomposing captioner utterances into component noun phrases. Having generated this set of potential questions, selecting based on esti- mated information gain yielded useful questions. Without seeing question examples, our framework demonstrates a capacity for generating effective clarification questions. Future research should aim to generate more diverse question sets, allow for more expressive answers, and address abstract properties of objects within images. One approach, as demonstrated by our preliminary work with ‘what’-questions, would be to extend our framework to incorporate additional types of wh-questions. Integrating this clarification capacity more fully into collaborative, goal-directed dialog agents will allow them to en- gage in cooperative repair.",1
"Abstract An overarching goal of natural language pro- cessing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an in- teractive process known as repair: asking ques- tions and seeking clarification until their uncer- tainty is resolved. We propose a framework for building a visually grounded question- asking model capable of producing polar (yes- no) clarification questions to resolve misunder- standings in dialogue. Our model uses an ex- pected information gain objective to derive in- formative questions from an off-the-shelf im- age captioner without requiring any supervised question-answer data. We demonstrate our modelÃ¢ÂÂs ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human an- swerers. Introduction Human-machine interaction relies on accurate transfer of knowledge from users. However, nat- ural language input can be ambiguous or unclear, giving rise to uncertainty. A fundamental aspect of human communication is collaborative grounding, or seeking and providing incremental evidence of mutual understanding through dialog. Specifically, humans can correct for uncertainty through cooper- ative repair (Clark, 1996; Purver et al., 2002; Arkel et al., 2020) which involves interactively asking questions and seeking clarification. Making and recovering from mistakes collaboratively through question-asking is a key ingredient in grounding meaning and therefore an important feature in di- alog systems (Benotti and Blackburn, 2021). In this work, we focus on the computational chal- lenge of generating clarification questions in visu- ally grounded human-machine interactions. One popular approach is to train an end-to-end model to map visual and linguistic inputs directly to questions (Yao et al., 2018; Das et al., 2017). This approach is heavily data-driven, requiring large annotated training sets of questions under different goals and contexts. Another approach has drawn from work on active learning and Optimal Experiment Design (OED) in cognitive science to search for questions that are likely to maximize ex- pected information gain from an imagined answerer (Wang and Lake, 2019; Lee et al., 2018; Misra et al., 2018; Rao and DaumeÃÂ III, 2018; Rothe et al., 2017;Kovashka and Grauman, 2013). Much of this work has relied on large-scale question-answer datasets (Kumar and Black, 2020; de Vries et al., 2017) for training or retrieval to propose candidate ques- tions or evaluate their expected utility. Others, like (Yu et al., 2020), derive questions from attribute annotations for domain-specific systems. In this paper, we address an open-domain setting where one cannot rely on an immediate grounding of the meaning of questions in the target domain (in contrast to end-to-end approaches, which assume examples of questions to train on, or semantic pars- ing approaches, which assume a logical form for questions). Our key contribution is a lightweight method to ground question semantics in the open image domain without observing question exam- ples. Instead, our framework builds a visually grounded question-asking model from image cap- tioning data, deriving question selection and belief updating without existing semantics. Our model generates candidate polar questions, arguably the most common form of clarification in dialogue (Stivers, 2010), by applying rule-based linguistic transformations to the outputs of a pretrained image captioner. We then use self-supervision to train a re- sponse model that predicts the likelihood of differ- ent answers. Given these predictions, we estimate the expected information gain of each question and select the question with the highest utility. We demonstrate our methodÃ¢ÂÂs ability to pose questions that improve communicative success in a question- driven communication game with synthetic and human answerers. Conclusions We introduce a question generation framework ca- pable of producing open-domain clarification ques- tions. Instead of relying on specialized question- answer training data or pre-specified question meanings, our model uses a pretrained image cap- tioner in conjunction with expected information gain to produce informative questions for unseen images. We demonstrate the effectiveness of this method in a question-driven communication game with synthetic and human answerers. We found it important to generate questions varying in speci- ficity by decomposing captioner utterances into component noun phrases. Having generated this set of potential questions, selecting based on esti- mated information gain yielded useful questions. Without seeing question examples, our framework demonstrates a capacity for generating effective clarification questions. Future research should aim to generate more diversesets, allow for more expressive answers, and address abstract properties of objects within images. One approach, as demonstrated by our preliminary work with what-questions, would be to extend our framework to incorporate additional types of wh-questions.",0
"Abstract While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest “I enjoyed the game this weekend”: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker’s broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive languagemodeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines. Introduction Language models are at the very heart of many modern NLP systems and applications (Young et al., 2018). Representations derived from largescale language models are used widely in many downstream NLP models (Peters et al., 2018; Devlin et al., 2019). However, an implicit assumption made in most modern NLP systems (including language models) is that language is independent of extra-linguistic context such as speaker/author identity and their social setting. While this simplifying assumption has undoubtedly encouraged remarkable progress in modeling language, there is overwhelming evidence in socio-linguistics that language understanding is influenced by the social context in which language is grounded (Nguyen et al., 2016; Hovy, 2018; Mishra et al., 2018; Garten et al., 2019; Flek, 2020; Bender and Koller, 2020). In fact, language use on social media where every utterance is grounded in a specific social context (like time, geography, social groups, communities) reinforces this often ignored aspect of language. When NLP applications ignore this social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021). Prior attempts to better leverage the social context surrounding language while learning language representations have mostly focused on learning social context dependent word embeddings and have been primarily used to characterize language variation across many dimensions (time, geography, and demographics). These methods learn word embeddings for each specific social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b). However, word embedding based approaches in general suffer from two fundamental limitations: (a) word embeddings are not linguistically contextualized as noted by Peters et al. (2018) (b) word embedding learning is transductive – they can only generate embeddings for words observed during training and usually assume a finite word vocabulary and a set of social contexts all of which need to be seen during training. Recent approaches have addressed the first limitation by learning word representations that are contextualized by their tokenspecific usage context (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b,a). The second limitation has been addressed by WordPiece tokenization methods (Schuster and Nakajima, 2012; Devlin et al., 2019; Liu et al., 2019). While these approaches have successfully captured linguistic context, they still do not capture social context in language representations.2 “How can we learn linguistically contextualized and socially contextualized language representations?” is the question we seek to answer in this paper. We propose LMSOC to (a) learn representations of tokens that are both linguistically contextualized and socially sensitive and (b) enable the language model to inductively generate representations for language grounded in social contexts it has never observed during the language model pre-training process. As an example, our model can enable NLP systems to associate the right entity being referred to based on the broader user/social context in which an utterance like “Our Prime Minister visited the UK last week.” is grounded.  Conclusion We proposed a method to learn socially sensitive contextualized representations from large-scale language models. Our method embeds social context in continuous space using graph representation algorithms and proposes a simple but effective socially sensitive pre-training approach. Our approach thus enables language models to leverage correlations between social contexts and thus generalize better to social contexts not observed in training. More broadly, our method sets the stage for future research on incorporating new types of social contexts and enabling NLP systems like personalized predictive typing systems and entity-linking systems to better accommodate language variation.",1
"Abstract While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest I enjoyed the game this weekend: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speakerÃÂs broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive languagemodeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines. Introduction Language models are at the very heart of many modern NLP systems and applications (Young et al., 2018). Representations derived from largescale language models are used widely in many downstream NLP models (Peters et al., 2018; Devlin et al., 2019). However, an implicit assumption made in most modern NLP systems (including language models) is that language is independent of extra-linguistic context such as speaker/author identity and their social setting. While this simplifying assumption has undoubtedly encouraged remarkable progress in modeling language, there is overwhelming evidence in socio-linguistics that language understanding is influenced by the social context in which language is grounded (Nguyen et al., 2016; Hovy, 2018; Mishra et al., 2018; Garten et al., 2019; Flek, 2020; Bender and Koller, 2020). In fact, language use on social media where every utterance is grounded in a specific social context (like time, geography, social groups, communities) reinforces this often ignored aspect of language. When NLP applications ignore this social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021). Prior attempts to better leverage the social context surrounding language while learning language representations have mostly focused on learning social context dependent word embeddings and have been primarily used to characterize language variation across many dimensions (time, geography, and demographics). These methods learn word embeddings for each specific social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b). However, word embedding based approaches in general suffer from two fundamental limitations: (a) word embeddings are not linguistically contextualized as noted by Peters et al. (2018) (b) word embedding learning is transductive Ã¢ÂÂ they can only generate embeddings for words observed during training and usually assume a finite word vocabulary and a set of social contexts all of which need to be seen during training. Recent approaches have addressed the first limitation by learning word representations that are contextualized by their tokenspecific usage context (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b,a). The second limitation has been addressed by WordPiece tokenization methods (Schuster and Nakajima, 2012; Devlin et al., 2019; Liu et al., 2019). While these approaches have successfully captured linguistic context, they still do not capture social context in language representations.2 Ã¢ÂÂHow can we learn linguistically contextualized and socially contextualized language representations?Ã¢ÂÂ is the question we seek to answer in this paper. We propose LMSOC to (a) learn representations of tokens that are both linguistically contextualized and socially sensitive and (b) enable the language model to inductively generate representations for language grounded in social contexts it has never observed during the language model pre-training process. As an example, our model can enable NLP systems to associate the right entity being referred to based on the broader user/social context in which an utterance like the city is grounded. Conclusion We proposed a method to learn socially sensitive contextualized representations from large-scale language models. Our method embeds social context in continuous spacegraph representation algorithms and proposes a simple but effective socially sensitive pre-training approach. Our approach thus enables language models to leverage correlations between social contexts and thus generalize better to social contexts not observed in training. More broadly, our method sets the stage for future research on incorporating new types of representations and enabling artists to better leverage the social context surrounding a specific work. ",0
"ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in this work that this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2020) trained with the SpanBERT objective for text (Joshi et al., 2020) and the w2v-BERT (Chung et al., 2021) objective for speech. We show that a model using only self-supervised objectives leads to good performance on both modalities, but is outperformed by mono-modal pre-trained models, suffering from significant transfer interference. To reduce the gap, we leverage supervised alignment losses, specifically a translation language model (Conneau & Lample, 2019; Zheng et al., 2021) and speech-text matching (Li et al., 2021) loss. We train our model in a multi-task fashion with the self-supervised and alignment losses. This leads to performance competitive with the state-of-the-art on SpeechStew and LibriSpeech ASR and on CoVoST 2 speech translation tasks. On speech translation, we demonstrate further quality improvements by continuing pre-training on speech-only, outperforming previous approaches by 1 BLEU on average. On text tasks, our joint model loses quality compared to equivalent mono-modal pre-trained models, but remains competitive with initial BERT results (Devlin et al., 2019), demonstrating the capacity limitations with modeling two high-resource modalities simultaneously. To the best of our knowledge, our work is the first to study and underline the benefits and limitations of speech-text unsupervised pre-training over mono-modal models, on various speech and text downstream tasks. Our initial results set a new challenge in multimodal self-supervised language understanding. DISCUSSION In this work, we demonstrate that a single encoder model can be pre-trained to learn strong contextualized representations of speech and text simultaneously. We combine self-supervised learning objectives for text (BERT) and self-supervised approaches for speech (w2v-BERT) to learn a joint Speech and LAnguage Model (SLAM). Downstream evaluations on speech and language understanding tasks, including LibriSpeech and SpeechStew ASR, CoVoST 2 speech translation, four GLUE tasks, and text-normalization uncover significant interference challenges when pre-training simultaneously on high-resource modalities. Using alignment losses such as translation language modeling and speech-text matching which leverage speech-text supervised aligned data, we show that we can improve the cross-modal representation alignment and improve over mono-modal models on the speech translation tasks, while maintaining state-of-the-art performance on speech recognition. We hope that this work would motivate further research on extending the universality of self-supervised learning of language representations to the multimodal speech-text setting.",1
"ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in thisthat this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2019) and a SuperGLUE (Wang et al., 2020; Xue et al., 2021b) that makes use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2020), SuperGLUE (Wang et al., 2021b)) and multilingual (e.g., XTREME (Hu et al., 2021), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020).",0
"Abstract This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, named S-APIR, where outlier detection models are investigated to properly handle various genres of news articles. Moreover, we propose a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 years’ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time. Keywords: business sentiment, sentiment analysis, deep learning, text analytics Introduction In Japan, there exist business sentiment indices, such as Economy Watchers Survey1 and Short-term Economic Survey of Principal Enterprise2 conducted by the Government and the Bank of Japan, respectively. These diffusion indices (DI) play a crucial role in decision-making for governmental/monetary policies, industrial production planning, institutional/private investment, and so on. However, these DIs rely on traditional surveys, which are costly and time-consuming to conduct. For example, Economy Watchers Survey is carried out in 12 regions of Japan, where 2,050 preselected respondents who can observe the regional business/economic conditions (e.g., store owners and taxi drivers) fill out a questionnaire and then an investigative organization in each region aggregates the surveys and calculates a DI. As the survey and subsequent processes take time, the DI is published only monthly. On the other hand, so-called alternative data, including merchandise sales, news, micro-blogs, query logs, credit card transactions, GPS location information, and satellite images, are constantly generated and accumulated. The availability of such data has accelerated the development of data-driven artificial intelligence (AI) models and techniques represented by deep learning. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys (Chen et al., 2019; Jain, 2019). For example, point of sales (POS) data have been used for estimating consumer price index (CPI) (Watanabe & Watan-abe, 2014); financial and economic reports for business sentiment (Yamamoto & Matsuo, 2016); newspaper for stock prices (Li et al., 2020; Picasso et al., 2019; Yoshihara et al., 2014, 2016), socio-economic indicators (Chakraborty et al., 2016), consumer sentiment (Shapiro et al., 2020); and social media for stock prices (Bollen et al., 2011; Derakhshan & Beigy, 2019; Levenberg et al., 2014). This work focuses on textual data and uses daily newspaper articles to develop a new business sentiment index, named the S-APIR index. In addition, using the computed index, we propose an approach to temporally analyzing the influence of an arbitrary event on business sentiment. The remainder of the paper is structured as follows: Section 2 introduces the related work on sentiment analysis in general and its applications to market sentiment and business sentiment prediction. Section 3 states the research objectives pursued in the present work. Section 4 details our proposed approach to forecasting business sentiment index and describes how to temporally analyze the contribution of a given event to business sentiment index based on predicted business sentiment scores. Section 5 conducts evaluative experiments using over 12 years’ worth of newspaper articles and discusses the properties of S-APIR, in addition to word-level temporal analysis. Section 6 discusses the implications and findings of this work. Section 7 concludes with a brief summary and possible future directions. Conclusions This paper reported our work to develop a new business sentiment index, called S-APIR. The main contribution of this work is threefold: Firstly, we proposed an approach to capturing business sentiment based on news texts and empirically validated it in comparison with an existing survey-based index. Secondly, we thoroughly studied the properties of the proposed index. Lastly, we illustrated how the predicted business sentiment can be used by policymakers and economists when it was broken down into individual events. The following describes, more specifically, the contribution from methodological, theoretical, and practical viewpoints. The methodological contribution is that we devised an effective framework composed of outlier detection and prediction models. The former used one-class SVM to identify news texts related to the economy and the latter was a BERT model fine-tuned on Economy Watchers Survey to predict the sentiment score of input news text. Another contribution is that we proposed an approach to analyzing the effect of an event represented by an individual/compound word on business sentiment. Next, the theoretical contribution is that business sentiment was shown to be accurately measured by news articles instead of a traditional, large-scale survey. Our evaluation using the Nikkei Newspaper demonstrated that S-APIR had a strong positive correlation with an existing business sentiment index, EWDI, up to 0.937. Also, the result suggested that the S-APIR index more accurately reflects business sentiment in industries. Then, the practical contribution is that S-APIR does not require a costly survey and can be computed much more frequently than monthly EWDI with almost no time lag. From the comparison with other business conditions indicators, it was revealed that S-APIR is useful as a leading index of actual consumption activity especially during major economic events such as the global financial crisis and the COVID-19 recession. Also, it could help us examine what factors cause economic fluctuations for specific periods. With several example events, such as “Tokyo Olympics”, it was demonstrated that S-APIR can be useful for economists or policymakers to measure the impact of any event of their interest on business sentiment over time to promptly respond to, if any, their negative effects when necessary. The findings of this study, however, should be considered in the light of the following limitations: • Both the outlier detection and sentiment analysis models are learned from the past Economy Watchers Survey responses and thus potentially suffered from new words unknown to the models. To keep up with the latest events, these models need to be regularly updated every time new survey data are available. • Similarly, while the usefulness of the S-APIR index was demonstrated, the evaluation was done retrospectively on the historical data. Ideally, it should be evaluated by prospective users on ongoing events with a real-time system where the S-APIR index is dynamically updated as breaking news comes in. • Predicted values of the S-APIR index depend on news texts we feed to the model. Currently, we use the Nikkei newspaper, which resulted in a strong correlation with EWDI, but feeding a different newspaper yields a different result as witnessed in Section 5.5. The difference may come from the different coverage of different newspapers but we do not know exactly if that is the case. For example, it might be caused by different political stances or tones of different newspapers. We plan to investigate it further in future work. We are currently working on automatically collecting online news and applying our models to nowcast daily business sentiment and are planning to provide a temporal analysis tool to be used by economists.",1
"Abstract This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, named S-APIR, where outlier detection models are investigated to properly handle various genres of news articles. Moreover, we propose a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 yearsÃ¢ÂÂ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time. Keywords: business sentiment, sentiment analysis, deep learning, text analytics Introduction In Japan, there exist business sentiment indices, such as Economy Watchers Survey1 and Short-term Economic Survey of Principal Enterprise2 conducted by the Government and the Bank of Japan, respectively. These diffusion indices (DI) play a crucial role in decision-making for governmental/monetary policies, industrial production planning, institutional/private investment, and so on. However, these DIs rely on traditional surveys, which are costly and time-consuming to conduct. For example, Economy Watchers Survey is carried out in 12 regions of Japan, where 2,050 preselected respondents who can observe the regional business/economic conditions (e.g., store owners and taxi drivers) fill out a questionnaire and then an investigative organization in each region aggregates the surveys and calculates a DI. As the survey and subsequent processes take time, the DI is published only monthly. On the other hand, so-called alternative data, including merchandise sales, news, micro-blogs, query logs, credit card transactions, GPS location information, and satellite images, are constantly generated and accumulated. The availability of such data has accelerated the development of data-driven artificial intelligence (AI) models and techniques represented by deep learning. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys (Chen et al., 2019; Jain, 2019). For example, point of sales (POS) data have been used for estimating consumer price index (CPI) (Watanabe & Watan-abe, 2014); financial and economic reports for business sentiment (Yamamoto & Matsuo, 2016); newspaper for stock prices (Li et al., 2020; Picasso et al., 2019; Yoshihara et al., 2014, 2016), socio-economic indicators (Chakraborty et al., 2016), consumer sentiment (Shapiro et al., 2020); and social media for stock prices (Bollen et al., 2011; Derakhshan & Beigy, 2019; Levenberg et al., 2014). This work focuses on textual data and uses daily newspaper articles to develop a new business sentiment index, named the S-APIR index. In addition, using the computed index, we propose an approach to temporally analyzing the influence of an arbitrary event on business sentiment. The remainder of the paper is structured as follows: Section 2 introduces the related work on sentiment analysis in general and its applications to market sentiment and business sentiment prediction. Section 3 states the research objectives pursued in the present work. Section 4 details our proposed approach to forecasting business sentiment index and describes how to temporally analyze the contribution of a given event to business sentiment index based on predicted business sentiment scores. Section 5 conducts evaluative experiments using over 12 yearsÃ¢ÂÂ worth of newspaper articles and discusses the properties of S-APIR, in addition to word-level temporal analysis. Section 6 discusses the implications and findings of this work. Section 7 concludes with a brief summary and possible future directions. Conclusions This paper reported our work to develop a new business sentiment index, called S-APIR. The main contribution of this work is threefold: Firstly, we proposed an approach to capturing business sentiment based on news texts and empirically validated it in comparison with an existing survey-based index. Secondly, we thoroughly studied the properties of the proposed index.we illustrated how the predicted business sentiment can be used by policymakers and economists when it was broken down into individual events. The following describes, more specifically, the contribution from methodological, theoretical, and practical viewpoints. The methodological contribution is that we devised an effective framework composed of outlier detection and prediction models. The former used one-class model and the latter was a different approach based on traditional surveys. Also, the former used the survey-based model and the latter was a different approach based on traditional surveys. The latter was a little more work than the former. The difference is that the outlier detection is more effective than the traditional survey-based model. Also, the difference is that the outlier detection is much more effective than the traditional one-class one-issue-at-res. Finally, the difference is that the outlier detection is much more effective than the traditional one-class one-issue-at-res. We hope this help shows us how S-APIR can benefit economists and policymakers, and how S-APIR could benefit economists and policymakers, several billion lines of text. More specifically, we hope that this help shows us how S-APIR can be useful as a data-driven artificial intelligence (AI) model and how S-APIR could benefit economists and policymakers, several billion lines of text. Section 2 shows how to properly handle various genres of news articles. Section 3 states the research objectives pursued in the present work. Section 4 details our proposed approach and describes how to properly handle various genres of news articles. Section 5 conducts evaluative experiments using over 12 yearsÃ¢ÂÂ worth of newspaper articles and discusses the properties of S-APIR, in addition to word-level temporal analysis. Section 6 discusses the implications and findings of this work. Section 7 concludes with a brief summary and possible future directions. Conclusions This paper reported our work to develop a new approach to capturing the sentiment of online news articles. The main contribution of this work is threefold: Firstly, we proposed an approach to capturing the sentiment of news articles and empirically validated it in comparison with an existing survey-based model. Secondly, we thoroughly studied the properties of the proposed approach. Lastly, we illustrated how the predicted business sentiment can be used by policymakers and economists when it was broken down into individual events. The following describes, more specifically, the contribution from methodological, theoretical, and practical viewpoints. The former was a little more work than the latter. The difference is that the outlier detection is much more effective than the traditional one-class one-issue-at-res. Also, the difference is that the outlier detection is much more effective than the traditional one-class one-issue-at. Also, the difference is that the outlier detection is much more effective than the traditional one-class one-issue. Finally, the difference is that the outlier detection is much more effective than the traditional one-class one-issue.. The computed business sentiment has been measured by traditional surveys and its accuracy has been measured by traditional surveys only. Our approach is in addition to looking at the properties of the proposed approach. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys.",0
"Abstract Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online. Introduction Web sources, such as social networks, internet forums, and customer reviews from online shops, provide large amounts of unstructured text data. Along with the steady development of new platforms and the increasing number of internet users, the interest in methods that automatically extract the expressed opinions along with the corresponding topics and sentiments in text data has increased in recent years. Scholars and organizations from different fields can utilize such methods to identify patterns and generate new insights. Examples are opinion researchers investigating current opinions on political and societal issues, consumer researchers interested in consumers’ beliefs about the consumption and production of goods (Danner et al., 2020), and marketing managers curious about the public perception of their products and services (Berger et al., 2020; Murphy et al., 2014). (Kirchhoff, 2019) These domain-specific use cases are of interest for research disciplines which taken by itself are not directly related to natural language processing (NLP). Consequentially, there is a constant need to provide state-of-the-art NLP methods such that domain researchers from other fields can take advantage of them. The requirements therefore are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017). This is improved recently (Karamanolakis et al., 2019; Angelidis and Lapata, 2018; Luo et al., 2019), which underlines the recent impact and potential of related techniques. However, these have not been utilized for visualizations based on correlated topic modeling (Blei and Lafferty, 2006), where all pairs of topics ”are” analyzed to determine if two topics generally tend to occur in the same texts of a given dataset. Thus, the similarity between topics can be defined. This is successfully used to connect topics (nodes) among each other based on their correlations (edges) leading to more abstract and more meaningful meta topics (graph-clusters) which additionally improves topic coherence. Consequentially, these meta topics, e.g., company-related events or research subdisciplines (Liu et al., 2014; Maiya and Rolfe, 2014), can be successfully identified by graphbased visualization techniques. However, there is a lack of related prototypes on texts discussing consumption related issues in product reviews or social media. To the best of our knowledge, there is also no related integration of sentiment analysis into a system available for potential end users, i.e., domain experts. As according text data from customers is available on a large scale in social media, this can be considered as a shortcoming in the field. To address all denoted issues, we propose the SocialVisTUM toolkit, a new visualization and labeling tool to give users a comprehensible overview of the topics discussed in social media texts. It integrates a neural method for unsupervised sentence and comment clustering based on word vectors and attention. We denote the respective clusters as topics hereafter. In addition, we provide a graphbased visualization showing the topics as labeled nodes and the correlation between them as edges. A force-directed graph layout maintains readability even while many relevant topics and topic relations are displayed. (Kirchhoff, 2019) In our interactive graphical user interface, the number of topics displayed and the correlation threshold required to display a connection between two topics can be dynamically adjusted. Further, contextual topic information is provided, such as the number of respective topic occurrences in the social media texts as node diameter, the correlation between the topic occurrences as edge width, example sentences from the data for each topic, a list of representative words for each topic, and the regarding sentiment distribution of a topic. It is a common practice to represent topics merely by word lists (Blei et al., 2003; Chen et al., 2014), which tend to be insufficient to comprehensively express a topic on our given dataset. (Kirchhoff, 2019) To avoid manual labeling and to give users an immediate impression of each topic, topic labels are generated automatically based on a custom algorithm utilizing the most common WordNet hypernym in a topic’s top words. Furthermore, we find that topic hypernym statistics can serve as a metric for automatic hyperparameter optimization, which in our case gives practical advantages over widely used coherence scoring metrics. In addition to a more detailed description of our SocialVisTUM toolkit, we show the results of a case study based on social media texts from online commenters debating about organic food consumption. We demonstrate that the correlated topics give a meaningful graph representation of the social media discussions supporting the understanding of the concerns of consumers. In this regard, we also show how the combined illustration of different types of relevant topic and sentiment information and automatic labeling of clusters are a contribution.  Conclusion In this paper, a case of the proposed SocialVisTUM demonstrates the visualization of coherent topics on a given corpus of social media texts about organic food. The graph-based visualization with topics as nodes and topic correlations as edges reflects the topics and patterns found in a related qualitative content analysis (Danner and Menapace, 2020). The presentation of additional topic information, such as word lists, representative sentences, topic importance, and meaningful predefined labels, provide a basis for the understanding and interpretation of a topic for domain experts. The integrated hyperparameter optimization automatically yields interpretable topics and helps tailoring the model to the given data set. For future work, we plan to evaluate the correlated topics on other corpora and in other use cases. In addition to Pearson correlation, other correlations could improve the approach. We plan to integrate multi-lingual word features, such as BERT (Devlin et al., 2018), for cross-cultural comparisons.",1
"Abstract Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online. Introduction Web sources, such as social networks, internet forums, and customer reviews from online shops, provide large amounts of unstructured text data. Along with the steady development of new platforms and the increasing number of internet users, the interest in methods that automatically extract the expressed opinions along with the corresponding topics and sentiments in text data has increased in recent years. Scholars and organizations from different fields can utilize such methods to identify patterns and generate new insights. Examples are opinion researchers investigating current opinions on political and societal issues, consumer researchers interested in consumersÃ¢ÂÂ beliefs about the consumption and production of goods (Danner et al., 2020), and marketing managers curious about the public perception of their products and services (Berger et al., 2020; Murphy et al., 2014). (Kirchhoff, 2019) These domain-specific use cases are of interest for research disciplines which taken by itself are not directly related to natural language processing (NLP). Consequentially, there is a constant need to provide state-of-the-art NLP methods such that domain researchers from other fields can take advantage of them. The requirements therefore are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017). This is improved recently (Karamanolakis et al., 2019; Angelidis and Lapata, 2018; Luo et al., 2019), which underlines the recent impact and potential of related techniques. However, these have not been utilized for visualizations based on correlated topic modeling (Blei and Lafferty, 2006), where all pairs of topics Ã¢ÂÂareÃ¢ÂÂ analyzed to determine if two topics generally tend to occur in the same texts of a given dataset. Thus, the similarity between topics can be defined. This is successfully used to connect topics (nodes) among each other based on their correlations (edges) leading to more abstract and more meaningful meta topics (graph-clusters) which additionally improves topic coherence. Consequentially, these meta topics, e.g., company-related events or research subdisciplines (Liu et al., 2014; Maiya and Rolfe, 2014), can be successfully identified by graphbased visualization techniques. However, there is a lack of related prototypes on texts discussing consumption related issues in product reviews or social media. To the best of our knowledge, there is also no related integration of sentiment analysis into a system available for potential end users, i.e., domain experts. As according text data from customers is available on a large scale in social media, this can be considered as a shortcoming in the field. To address all denoted issues, we propose the SocialVisTUM toolkit, a new visualization and labeling tool to give users a comprehensible overview of the topics discussed in social media texts. It integrates a neural method for unsupervised sentence and comment clustering based on word vectors and attention. We denote the respective clusters as topics hereafter. In addition, we provide a graphbased visualization showing the topics as labeled nodes and the correlation between them as edges. A force-directed graph layout maintains readability even while many relevant topics and topic relations are displayed. (Kirchhoff, 2019) In our interactive graphical user interface, the number of topics displayed and the correlation threshold required to display a connection between two topics can be dynamically adjusted. Further, contextual topic informationprovided, such as the number of respective topic occurrences in the social media texts as node diameter, the correlation between the topic occurrences as edge width, example sentences from the data for each topic, a list of representative words for each topic, and the regarding sentiment distribution of a topic. The regarding sentiment distribution method optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online. Introduction Web sources, such as social networks, internet forums, and customer reviews from online shops, provide large amounts of unstructured text data. Along with the steady development of new platforms and the increasing number of internet users, the interest in methods that automatically extract the expressed opinions along with the corresponding topics and sentiments in text data has increased in recent years. Scholars and organizations from different fields can utilize such methods to identify patterns and generate new insights. Examples are opinion researchers investigating current opinions on political and societal issues, consumer researchers interested in consumersÃ¢ÂÂ beliefs about the consumption and production of goods (Danner et al., 2020), and marketing managers curious about the public perception of their products and services (Berger et al., 2020; Murphy et al., 2014). (Kirchhoff, 2019) These domain-specific use cases are of interest for research disciplines which taken by itself are not directly related to natural language processing (NLP). Consequentially, there is a constant need to provide state-of-the-art NLP methods such that domain researchers from other fields can take advantage of them. The requirements therefore are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017).",0
"Abstract We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021). Given a potentially large collection of relatively short, opinionated texts focused on a topic of interest, the goal of KPA is to produce a succinct list of the most prominent key-points (KPs) in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle. Successful solutions to KPA can be used to gain better insights from public opinions as expressed in social media, surveys, and so forth, giving rise to a new form of a communication channel between decision makers and people that might be impacted by the decision. Various requirements govern the value of the KPA output. KPs are expected to be succinct, nonredundant, capturing points that are central to the topic of interest, and reflecting a clear stance towards that topic. Ideally, they should be at the right granularity for summarising the input data – not too specific and yet still informative and not overly general. In addition, accurate mapping of input texts to KPs is obviously essential. First, to ensure a reliable estimate of the prevalence of each key point. And second, to enable the user to drill-down, to gain a deeper understanding of the issues underlying each key point, as expressed by the input texts mapped to that key point. The goal of the KPA-2021 shared task was to further increase the attention of the NLP community to this emerging task, while enriching the space of existing KPA solutions. Since providing a complete KPA solution is challenging, we divided the task into two tracks, enabling teams to participate only in the first, relatively simpler track. Specifically, in the first track, referred to as Matching Track, KPs are given as part of the input, and the task is focused on mapping input text to these KPs. In contrast, in the second track, referred to as Generation Track, no KPs are provided and the task requires to further generate the KPs. The data being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. Discussion We presented KPA-2021, the first shared task focused on key point analysis. The shared task received 22 submissions from 17 teams, covering both the Matching Track and the Generation Track. We presented the submitted models, their performance on different evaluation measures, and further analyzed the results. As expected by its simpler nature, Matching Track received more submissions. However, evidently, success in this intermediate matching task, does not guarantee success in the overall KPA task, which also requires generating the KPs. Future work should determine whether future KPA shared tasks should focus on the Generation Track, or perhaps modify the evaluation measures of the intermediate Matching Track, such that they better reflect the model performance in the full KPA task. Given the inherent subjective nature of matching arguments to KPs, we opted for a ternary label, allowing argument-KP pairs to receive an ""undecided"" label, if the annotators votes were inconclusive. These undecided pairs are eventually considered either as positive or as negative matches, resulting with two gold standards, one potentially too strict, while the other perhaps too lenient. In future tasks, it may be valuable to consider a binary labeling scheme that will give rise to a single gold standard. Such labeling may be created conditionally, such that an undecided pair is marked as a positive match if and only if a minimum number of annotators marked it as positive and no other KP was labeled as matching to that argument. A further point for future consideration, is the choice of a pre-defined threshold of 50%, that guided our evaluation. Although, this has the advantage of not requiring submissions to tune a con- fidence threshold, it has the limitation that it complicates the evaluation, since the ground truth coverage depends on the arguments, topic and stance. A possible alternative would be to require each model to set its own minimum confidence threshold to determine if an input argument should not be matched to any KP. Finally, in this task, we used comparative evaluations to determine the quality of the sets of generated KPs. Obviously, this results with rankingbased evaluation. Establishing an absolute evaluation metric in this context merits further investigation. We thank all participating teams for taking the time to participate in this challenging new shared task. We believe that these contributions, along with the data collected and shared in this report, will be valuable to further advance the research around KPA and related tasks.",1
"Abstract We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021). Given a potentially large collection of relatively short, opinionated texts focused on a topic of interest, the goal of KPA is to produce a succinct list of the most prominent key-points (KPs) in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle. Successful solutions to KPA can be used to gain better insights from public opinions as expressed in social media, surveys, and so forth, giving rise to a new form of a communication channel between decision makers and people that might be impacted by the decision. Various requirements govern the value of the KPA output. KPs are expected to be succinct, nonredundant, capturing points that are central to the topic of interest, and reflecting a clear stance towards that topic. Ideally, they should be at the right granularity for summarising the input data Ã¢ÂÂ not too specific and yet still informative and not overly general. In addition, accurate mapping of input texts to KPs is obviously essential. First, to ensure a reliable estimate of the prevalence of each key point. And second, to enable the user to drill-down, to gain a deeper understanding of the issues underlying each key point, as expressed by the input texts mapped to that key point. The goal of the KPA-2021 shared task was to further increase the attention of the NLP community to this emerging task, while enriching the space of existing KPA solutions. Since providing a complete KPA solution is challenging, we divided the task into two tracks, enabling teams to participate only in the first, relatively simpler track. Specifically, in the first track, referred to as Matching Track, KPs are given as part of the input, and the task is focused on mapping input text to these KPs. In contrast, in the second track, referred to as Generation Track, no KPs are provided and the task requires to further generate the KPs. The data being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. Discussion We presented KPA-2021, the first shared task focused on key point analysis. The shared task received 22 submissions from 17 teams, covering both the Matching Track and the Generation Track. We presented the submitted models, their performance on different evaluation measures, and further analyzed the results. As expected by its simpler nature, Matching Track received more submissions. However, evidently, success in this intermediate matching task, does not guarantee success in the overall KPA task, which also requires generating the KPs. Future work should determine whether future KPA shared tasks should focus on the Generation Track, or perhaps modify the evaluation measures of the intermediate Matching Track, such that they better reflect the model performance in the full KPA task. Given the inherent subjective nature of matching arguments to KPs, we opted for a ternary label, allowing argument-KP pairs to receive an ""undecided"" label, if the annotators votes were inconclusive. These undecided pairs are eventually considered either as positive or as negative matches, resulting with two gold standards, one potentially too strict, while the other perhaps too lenient. In future tasks, it may be valuable to consider a binary labeling scheme that will give rise to a single gold standard. Such labeling may be created conditionally, such that an undecided pair is marked as a positive match if and only if a minimum number of annotators marked it as positive and no other KP was labeled as matching to that argument. A further point for future consideration, is the choice of a pre-defined threshold ofthat guided our evaluation. Although, this has the advantage of not requiring submissions to tune a con- fidence threshold, it has the limitation that it complicates the evaluation, since the ground truth coverage depends on the arguments, topic and stance. A possible alternative would be to require each model to set its own minimum confidence threshold to determine if an argument should not be considered a valid input. Finally, in this task, we used comparative evaluations to estimate the quality of the sets of generated texts. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021).",0
"Abstract Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic selfattention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text. Representation from large language models can partially mitigate the data scarcity issue due to pretraining on a large amounts of unlabeled data. However, those models mostly consider token-level information and their pretraining tasks are not directly targeting long document representations. Another issue of directly applying transformer-based models is the limit of the input text length. Due to the quadratic complexity of self-attention, most of the pretrained transformers models can only handle a relatively short text. A wide spectrum of efficient, fast transformer models (collectively called “Xformers”) have been proposed to tackle this problem; e.g., Longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2020) use sparse attention to improve the computational and memory efficiency for long sequence text. Nevertheless, these models still focus on token-level interactions without considering high-level semantic structure of the document. Recently, there is a resurgence of interest in Contrastive Learning (CL) due to its success in selfsupervised representation learning in computer vision (Chen et al., 2020; He et al., 2020). Contrastive Learning offers a simple method to learn disentangled representation that encodes invariance to small and local changes in the input data without using any labeled data. In NLP domain, contrastive learning has been employed to learn sentence representation (Wu et al., 2020; Qu et al., 2020) under either self-supervised or supervised settings. In this work, we propose a Graph Attention Network (GAT) based model that explicitly utilizes the high-level semantic structure of the documents to learn document embeddings. We model the document as not just a sequence of text, but a collection of passages or sentences. Specifically, the proposed model introduces a graph on top of the document passages (Fig. 1) to utilize multi-granularity information. First, passages are encoded using RoBERTa (Liu et al., 2019) to collect word-level knowledge. Then passages are connected to leverage the higher-level structured information. At last, a graph attention network (Velickovi ˇ c et al. ´ , 2017) is applied to obtain the multi-granularity document representation. To better learn the document embedding, we propose a document-level contrastive learning strategy to pretrain our models. In our contrastive learning framework, we split the document into random sub-documents and train the model to maximize the agreement over the representations of the sub-documents that come from the same document. This simple strategy allows us to pretrain our models on a large unlabelled corpus without any additional priors. As we will see, this simple pretraining task indeed helps the model on the downstream tasks. The contributions of this paper can be summarized as follows. • We propose a graph document model with graph attention networks that can not only explicitly utilize the high-level structure of the document but also leverage pretrained Transformer encoders to obtain low-level contextual information. • We propose a simple document-level contrastive learning strategy, which does not require any handcrafted transformations and is suitable for large-scale pretraining. • We conduct empirical evaluations on our models and contrastive pretraining strategy. We show that our graph-roberta models achieve great performance on both document classi- fication and retrieval tasks. Specifically we demonstrate that our contrastive pretraining helps the model learn a meaningful document representation even without fine-tuning, and improve both the training convergence speed and final performance during end-to-end finetuning on downstream classification tasks. For document retrieval tasks, we demonstrate that our graph-roberta models have great semantic matching performance, compensating the typical lexical matching system. Conclusions In this work, we propose a simple graph attention network model to learn document embeddings. Our model not only can leverage the recent advancement of pretrained Transformer models as building blocks, but also explicitly utilize the high-level structure of the documents. In addition, we propose a simple document-level contrastive learning strategy that does not require handcraft transformations. With this strategy, we conduct large scale contrastive pretraining on a large corpus. Empirically we demonstrate our methods achieve great performance on both document classification and document retrieval tasks.",1
"Abstract Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic selfattention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text. Representation from large language models can partially mitigate the data scarcity issue due to pretraining on a large amounts of unlabeled data. However, those models mostly consider token-level information and their pretraining tasks are not directly targeting long document representations. Another issue of directly applying transformer-based models is the limit of the input text length. Due to the quadratic complexity of self-attention, most of the pretrained transformers models can only handle a relatively short text. A wide spectrum of efficient, fast transformer models (collectively called Ã¢ÂÂXformersÃ¢ÂÂ) have been proposed to tackle this problem; e.g., Longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2020) use sparse attention to improve the computational and memory efficiency for long sequence text. Nevertheless, these models still focus on token-level interactions without considering high-level semantic structure of the document. Recently, there is a resurgence of interest in Contrastive Learning (CL) due to its success in selfsupervised representation learning in computer vision (Chen et al., 2020; He et al., 2020). Contrastive Learning offers a simple method to learn disentangled representation that encodes invariance to small and local changes in the input data without using any labeled data. In NLP domain, contrastive learning has been employed to learn sentence representation (Wu et al., 2020; Qu et al., 2020) under either self-supervised or supervised settings. In this work, we propose a Graph Attention Network (GAT) based model that explicitly utilizes the high-level semantic structure of the documents to learn document embeddings. We model the document as not just a sequence of text, but a collection of passages or sentences. Specifically, the proposed model introduces a graph on top of the document passages (Fig. 1) to utilize multi-granularity information. First, passages are encoded using RoBERTa (Liu et al., 2019) to collect word-level knowledge. Then passages are connected to leverage the higher-level structured information. At last, a graph attention network (Velickovi ÃÂ c et al. ÃÂ´ , 2017) is applied to obtain the multi-granularity document representation. To better learn the document embedding, we propose a document-level contrastive learning strategy to pretrain our models. In our contrastive learning framework, we split the document into random sub-documents and train the model to maximize the agreement over the representations of the sub-documents that come from the same document. This simple strategy allows us to pretrain our models on a large unlabelled corpus without any additional priors. As we will see, this simple pretraining task indeed helps the model on the downstream tasks. The contributions of this paper can be summarized as follows. Ã¢ÂÂ¢ We propose a graph document modelgraph attention networks that can not only explicitly utilize the high-level structure of the document but also leverage pretrained Transformer encoders to obtain low-level contextual information. ÃÂ¢ÃÂÃÂ¢ We propose a simple document-level contrastive learning strategy, which does not require any handcrafted transformations and is suitable for large-scale pretraining.  We conduct empirical evaluations on our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text.",0
"Abstract We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from two major U.S. journals to forecast the CBOE Volatility Index. Keywords: Aggregation, Penalized Regression, Prediction, R, sentometrics, Textual Senti- ment, Time Series. Introduction Individuals, companies, and governments continuously consume written material from various sources to improve their decisions. The corpus of texts is typically of a high-dimensional longitudinal nature requiring statistical tools to extract the relevant information. A key source of information is the sentiment transmitted through texts, called textual sentiment. Algaba, Ardia, Bluteau, Borms, and Boudt (2020) review the notion of sentiment and its applications, mainly in economics and finance. They define sentiment as “the disposition of an entity towards an entity, expressed via a certain medium.” The medium in this case is texts. The sentiment expressed through texts may provide valuable insights on the future dynamics of variables related to firms, the economy, political agendas, product satisfaction, and marketing campaigns, for instance. Still, textual sentiment does not live by the premise to be equally useful across all applications. Deciphering when, to what degree, and which layers of the sentiment add value is needed to consistently study the full information potential present within qualitative communications. The econometric approach of constructing time series of sentiment by means of optimized selection and weighting of textual sentiment is referred to as sentometrics by Algaba et al. (2020) and Ardia, Bluteau, and Boudt (2019). The term sentometrics is a composition of (textual) sentiment analysis and (time series) econometrics. The release of the R (R Core Team 2021) text mining infrastructure tm (Feinerer, Hornik, and Meyer 2008) over a decade ago can be considered the starting point of the development and popularization of textual analysis tools in R. A number of successful follow-up attempts at improving the speed and interface of the comprehensive natural language processing capabil- ities provided by tm have been delivered by the packages openNLP (Hornik 2019), cleanNLP (Arnold 2017), quanteda (Benoit, Watanabe, Wang, Nulty, Obeng, Müller, and Matsuo 2018), tidytext (Silge and Robinson 2016), and qdap (Rinker 2020). The notable tailor-made packages for sentiment analysis in R are meanr (Schmidt 2019), SentimentAnalysis (Feuerriegel and Proellochs 2021), sentimentr (Rinker 2019b), and syuzhet (Jockers 2020). Many of these packages rely on one of the larger above-mentioned textual analysis infrastructures. The meanr package computes net sentiment scores fastest, but offers no flexibility.1 The SentimentAnalysis package relies on a similar calculation as used in tm’s sentiment scoring function. The package can additionally be used to generate and evaluate sentiment dictionaries. The sentimentr package extends the polarity scoring function from the qdap package to handle more difficult linguistic edge cases, but is therefore slower than packages which do not attempt this. The SentimentAnalysis and syuzhet packages also become comparatively slower for large input corpora. The quanteda and tidytext packages have no explicit sentiment computation function but their toolsets can be used to construct one. Our R package sentometrics proposes a well-defined modeling workflow, specifically targeted at studying the evolution of textual sentiment and its impact on other quantities. It can be used (i) to compute textual sentiment, (ii) to aggregate fine-grained textual sentiment into various sentiment time series, and (iii) to predict other variables with these sentiment measures. The combination of these three facilities leads to a flexible and computationally efficient framework to exploit the information value of sentiment in texts. The package presented in this paper therefore addresses the present lack of analytical capability to extract time series intelligence about the sentiment transmitted through a large panel of texts. Furthermore, the sentometrics package positions itself as both integrative and supplementary to the powerful text mining and data science toolboxes in the R universe. It is integrative, as it combines the strengths of quanteda and stringi (Gagolewski 2021) for corpus construction and manipulation. It uses data.table (Dowle and Srinivasan 2021) for fast aggregation of textual sentiment into time series, and glmnet (Friedman, Hastie, and Tibshirani 2010) and caret (Kuhn 2021) for (sparse) model estimation. It is supplementary, given that it easily extends any text mining workflow to compute, aggregate and predict with textual sentiment.  The remainder of the paper is structured as follows. Section 2 introduces the methodology behind the R package sentometrics. Section 3 describes the main control functions and illustrates the package’s typical workflow. Section 4 applies the entire framework to forecast the Chicago Board Options Exchange (CBOE) Volatility Index. Section 5 concludes. Conclusion and future development The R package sentometrics provides a framework to calculate sentiment for texts, to aggregate textual sentiment scores into many time series at a desired frequency, and to use these in a flexible prediction modeling setup. It can be deployed to quantify a qualitative corpus of texts, relate it to a target variable, and retrieve which type of sentiment is most informative through visualization and attribution analysis. The main priorities for further development are integrating better prediction tools, enhancing the complexity of the sentiment engine, allowing user-defined weighting schemes, and adding intra-day aggregation.",1
"Abstract We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from two major U.S. journals to forecast the CBOE Volatility Index. Keywords: Aggregation, Penalized Regression, Prediction, R, sentometrics, Textual Senti- ment, Time Series. Introduction Individuals, companies, and governments continuously consume written material from various sources to improve their decisions. The corpus of texts is typically of a high-dimensional longitudinal nature requiring statistical tools to extract the relevant information. A key source of information is the sentiment transmitted through texts, called textual sentiment. Algaba, Ardia, Bluteau, Borms, and Boudt (2020) review the notion of sentiment and its applications, mainly in economics and finance. They define sentiment as Ã¢ÂÂthe disposition of an entity towards an entity, expressed via a certain medium.Ã¢ÂÂ The medium in this case is texts. The sentiment expressed through texts may provide valuable insights on the future dynamics of variables related to firms, the economy, political agendas, product satisfaction, and marketing campaigns, for instance. Still, textual sentiment does not live by the premise to be equally useful across all applications. Deciphering when, to what degree, and which layers of the sentiment add value is needed to consistently study the full information potential present within qualitative communications. The econometric approach of constructing time series of sentiment by means of optimized selection and weighting of textual sentiment is referred to as sentometrics by Algaba et al. (2020) and Ardia, Bluteau, and Boudt (2019). The term sentometrics is a composition of (textual) sentiment analysis and (time series) econometrics. The release of the R (R Core Team 2021) text mining infrastructure tm (Feinerer, Hornik, and Meyer 2008) over a decade ago can be considered the starting point of the development and popularization of textual analysis tools in R. A number of successful follow-up attempts at improving the speed and interface of the comprehensive natural language processing capabil- ities provided by tm have been delivered by the packages openNLP (Hornik 2019), cleanNLP (Arnold 2017), quanteda (Benoit, Watanabe, Wang, Nulty, Obeng, MuÃÂller, and Matsuo 2018), tidytext (Silge and Robinson 2016), and qdap (Rinker 2020). The notable tailor-made packages for sentiment analysis in R are meanr (Schmidt 2019), SentimentAnalysis (Feuerriegel and Proellochs 2021), sentimentr (Rinker 2019b), and syuzhet (Jockers 2020). Many of these packages rely on one of the larger above-mentioned textual analysis infrastructures. The meanr package computes net sentiment scores fastest, but offers no flexibility.1 The SentimentAnalysis package relies on a similar calculation as used in tmÃ¢ÂÂs sentiment scoring function. The package can additionally be used to generate and evaluate sentiment dictionaries. The sentimentr package extends the polarity scoring function from the qdap package to handle more difficult linguistic edge cases, but is therefore slower than packages which do not attempt this. The SentimentAnalysis and syuzhet packages also become comparatively slower for large input corpora. The quanteda and tidytext packages have no explicit sentiment computation function but their toolsets can be used to construct one. Our R package sentometrics proposes a well-defined modeling workflow, specifically targeted at studying the evolution of textual sentiment and its impact on other quantities. It can be used (i) to compute textual sentiment, (ii) to aggregate fine-grained textual sentiment into various sentiment time series, and (iii) to predict other variables with these sentiment measures. The combination of these three facilities leads to a flexible and computationally efficient framework to exploit the information value of sentiment in texts. The package presented in this paper therefore addresses the present lack of analytical capability to extract time series intelligence about the sentiment transmitted through a large panel of texts. Furthermore, the sentometrics package positions itself as both integrative and supplementary to the powerful textand data science toolboxes in the R universe. It is integrative, as it combines the strengths of quanteda and stringi (Gagolewski 2021) for corpus construction and manipulation. It uses data.table (Dowle and Srinivasan 2021) for fast aggregation of textual sentiment into time series, and glmnet (Friedman, Hastie, and Tibshirani 2010) and caret (Kuhn 2021) for (sparse) model estimation. The time series and time series packages also become available. The time series and time series packages also become available. The time series and time series packages also become available. The time series and time series packages also become available. The packages time series (Feinerer, Hornik, and Meyer 2008) and time series (Gagolewski and Srinivasan 2021). The release of the R (R Core Team 2021) text mining infrastructure tm (Feinerer, Hornik, and Meyer 2008) over a decade ago can be considered the starting point of the development and popularization of textual analysis tools in R. A number of successful follow-up attempts at improving the speed and interface of the comprehensive natural language processing capabil- ities provided by tm have been delivered by the packages openNLP (Hornik 2021), cleanNLP (Arnold 2017), quanteda (Benoit, Watanabe, Wang, Nulty, Obeng, MuÃÂller, and Matsuo 2018), tidytext (Silge and Robinson 2016), and qdap (Rinker 2020).",0
"ABSTRACT While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks. Introduction Deep neural networks are widely used in various applications ranging from computer vision to language processing. While deep learning has surpassed human-level performance in numerous tasks, neural networks are extremely vulnerable to overfitting to spurious correlations and therefore fail to generalize even under slight perturbations of the test distribution [Arjovsky et al., 2019]. This observation motivated the research community to tackle the problem of domain generalization (see [Ribeiro et al., 2020] for a detailed literature review). Recent benchmark datasets, such as Rotated MNIST [Arjovsky et al., 2019], Colored MNIST [Arjovsky et al., 2019], PACS [Li et al., 2017], VLCS [Fang et al., 2013], Office-Home [Venkateswara et al., 2017], Terra Incognita [Beery et al., 2018] and DomainNet [Peng et al., 2019], have shown difficulties for the generalization of deep neural network models under distribution shifts, and have sparked invention of many new algorithmic frameworks to address domain generalization. A standard approach for improving out-of-distribution performance is to guarantee that learned representations are invariant to certain transformations. For example, image representations and trained models for computer vision should generally be invariant to rotations, changes in color, or background. There are two main directions for promoting such invariance to transformations, namely data augmentation and geometric deep learning. Data augmentation promotes invariances in learned representations by curating synthetic examples that exhibit the desired invariances. Zhang et al. [2017] introduced mixup to train a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures. Volpi et al. [2018] proposed an adaptive data augmentation method where adversarial examples are generated at every iteration, offering performance gain over unseen domains. Kuznichov et al. [2019] showed data augmentation can be applied to leaf segmentation by proposing a method that preserves the geometric structure of the data objects and keep the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field. Tellez et al. [2019] showed stain color augmentation and stain color normalization could be used in computational pathology applications. Goel et al. [2020] proposed an approach to patch a model that fails due to spurious features on a real-world skin cancer dataset by data augmentation. Zhou et al. [2020] showed data augmentation with adversarial images could make the label classifier more robust to unknown domain shifts. Nam et al. [2021] improved domain generalization by reducing the intrinsic style bias of CNNs. This is achieved by training a separate network for randomizing the style of images and generating augmented data during training. Geometric deep learning bakes such invariances into the neural network architecture. For example, convolutional layers [Lecun et al., 1998] are fundamentally preserving translations. There are other specifically designed networks to maintain invariances: Zaheer et al. [2017] studied the problem of designing models for machine learning tasks defined on sets and characterized the permutation invariant functions. Bloem-Reddy and Teh [2020] obtained generative functional representations of probability distributions that are invariant under the action of a compact group. Finzi et al. [2021] provided an algorithm for solving for the equivariant layers of matrix groups. Besides two main directions mentioned above, researchers have proposed numerous algorithmic solutions to impose invariance and improve domain generalization: Ganin et al. [2016] introduced a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. [Arjovsky et al., 2019] proposed invariant risk minimization (IRM) — a novel learning paradigm that estimates nonlinear, invariant, causal predictors from multiple training environments, to enable out-of-distribution generalization. The idea is to learn representations that perform equally well across different environments. Sagawa et al. [2019] introduced distributionally robust optimization (DRO) framework to learn models by minimizing the worst-case training loss over a set of pre-defined groups/environments. Li et al. [2018a] proposed a novel meta-learning method for domain generalization (MLDG), which simulates domain shift during training by synthesizing virtual testing domains within each mini-batch. Correlation alignment for deep domain adaptation [Sun and Saenko, 2016] (Deep CORAL) learns a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Li et al. [2018b] extended adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to a prior distribution via adversarial feature learning. Li et al. [2018c] proposed a conditional invariant neural network that minimizes the discrepancy in conditional distribution of images given the class labels across different domains. The approaches listed above are more complex than simple training mechanisms such as empirical risk minimization (ERM) and hence they cannot be easily applied to involved tasks such as training generative models. In contrast, Gulrajani and Lopez-Paz [2020] demonstrated that the simple ERM method can achieve state of the art performance after fine-tuning in various datasets/applications. However, there are still problem instances that ERM performs very poorly. For example, in learning end-to-end dialogue models, Qian et al. [2021] showed 29% performance drop on MultiWOZ [Budzianowski et al., 2018] due to the memorization of named entities. In this paper, we propose a regularization technique, called data augmented invariant regularization (DAIR) that penalizes the inconsistency of loss on augmented samples with respect to the original ones. DAIR is applicable whenever data augmentation is used to promote invariances. DAIR only requires marginal additional cost on top of data augmentation, and is simple and applicable to a wide host of supervised and unsupervised learning tasks, including generative models. We introduce the DAIR formulation, motivate it, and theoretically prove some of its properties in Section 2. We empirically evaluate DAIR on a variety of problem setups ranging from defense against adversarial attacks to domain generalization in the presence of environment shift in Section 3. Our experimental results show that DAIR is competitive with or even outperforms state-of-the-art algorithms specifically designed for imposing invariance in these problems. Conclusion. In this paper, we proposed a simple yet effective regularizer that can be used wherever data augmentation is used to promote invariance. We rigorously showed that our proposed regularizer can recover the optimal solution in certain regression task where simple data augmentation can not. We also compare our DAIR-SQ regularizer with the existing off-the-shelf regularizers such as L1, L2 and KL divergence. We empirically showed that the DAIR-SQ regularizer is competitive with or outperforms state-of-the-art problem-specific baselines in a variety of problem setups. We evaluated DAIR in four different categories of machine learning tasks including regression, visual question answering, and training robust deep neural networks, and task-oriented dialog modeling. This is a major benefit of DAIR-SQ as some of other regularizers cannot be applied to regression tasks. Empirically the proposed algorithm outperforms well in all tasks. Better understanding of the tuning of the corresponding hyperparameter also remains as an open area for research. Finally, a more in-depth theoretical understanding of the properties of DAIR-SQ regularizer that lead to its superior empirical performance are also important questions for future work. While we showed that DAIR-SQ boosts existing performance metrics, such as average accuracy, the interplay of DAIR-SQ with other metrics, especially group fairness is also another important area for future research.",1
"ABSTRACT While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks. Introduction Deep neural networks are widely used in various applications ranging from computer vision to language processing. While deep learning has surpassed human-level performance in numerous tasks, neural networks are extremely vulnerable to overfitting to spurious correlations and therefore fail to generalize even under slight perturbations of the test distribution [Arjovsky et al., 2019]. This observation motivated the research community to tackle the problem of domain generalization (see [Ribeiro et al., 2020] for a detailed literature review). Recent benchmark datasets, such as Rotated MNIST [Arjovsky et al., 2019], Colored MNIST [Arjovsky et al., 2019], PACS [Li et al., 2017], VLCS [Fang et al., 2013], Office-Home [Venkateswara et al., 2017], Terra Incognita [Beery et al., 2018] and DomainNet [Peng et al., 2019], have shown difficulties for the generalization of deep neural network models under distribution shifts, and have sparked invention of many new algorithmic frameworks to address domain generalization. A standard approach for improving out-of-distribution performance is to guarantee that learned representations are invariant to certain transformations. For example, image representations and trained models for computer vision should generally be invariant to rotations, changes in color, or background. There are two main directions for promoting such invariance to transformations, namely data augmentation and geometric deep learning. Data augmentation promotes invariances in learned representations by curating synthetic examples that exhibit the desired invariances. Zhang et al. [2017] introduced mixup to train a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures. Volpi et al. [2018] proposed an adaptive data augmentation method where adversarial examples are generated at every iteration, offering performance gain over unseen domains. Kuznichov et al. [2019] showed data augmentation can be applied to leaf segmentation by proposing a method that preserves the geometric structure of the data objects and keep the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field. Tellez et al. [2019] showed stain color augmentation and stain color normalization could be used in computational pathology applications. Goel et al. [2020] proposed an approach to patch a model that fails due to spurious features on a real-world skin cancer dataset by data augmentation. Zhou et al. [2020] showed data augmentation with adversarial images could make the label classifier more robust to unknown domain shifts. Nam et al. [2021] improved domain generalization by reducing the intrinsic style bias of CNNs. This is achieved by training a separate network for randomizing the style of images and generating augmented data during training. Geometric deep learning bakes such invariances into the neural network architecture. For example, convolutional layers [Lecun et al., 1998] are fundamentally preserving translations. There are other specifically designed networks to maintain invariances: Zaheer et al. [2017] studied the problem of designing models for machine learning tasks defined on sets and characterized the permutation invariant functions. Bloem-Reddy and Teh [2020] obtained generative functional representationsprobability distributions that are invariant under the action of a compact group. Finzi et al. [2021] provided an algorithm for solving for the equivariant layers of matrix groups. Besides two main directions mentioned above, researchers have proposed numerous algorithmic solutions to impose invariance and improve domain generalization: Ganin et al. [2016] introduced a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. [Arjovsky et al., 2019] proposed invariant risk minimization (IRM) Ã¢ÂÂ a novel learning paradigm that estimates nonlinear, invariant, causal predictors from multiple training environments, to enable out-of-distribution generalization. The idea is to learn representations that perform equally well across different environments. Sagawa et al. [2019] introduced distributionally robust optimization (DRO) framework to learn models by minimizing the worst-case training loss over a set of pre-defined groups/environments. Li et al. [2018a] proposed a novel meta-learning method for domain generalization (MLDG), which simulates domain shift during training by synthesizing virtual testing domains within each mini-batch. Correlation alignment for deep domain adaptation [Sun and Saenko, 2016] (Deep CORAL) learns a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Li et al. [2018b] extended adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to a prior distribution via adversarial feature learning. Li et al. [2018c] proposed a conditional invariant neural network that minimizes the discrepancy in conditional distribution of images given the class labels across different domains. The approaches listed above are more complex than simple training mechanisms such as empirical risk minimization (ERM) and hence they cannot be easily applied to involved tasks such as training generative models. In contrast, Gulrajani and Lopez-Paz [2020] demonstrated that the simple ERM method can achieve state of the art performance after fine-tuning in various datasets/applications. However, there are still problem instances that ERM performs very poorly. For example, in learning end-to-end dialogue models, Qian et al. [2021] showed 29% performance drop on MultiWOZ [Budzianowski et al., 2018] due to the memorization of named entities. In this paper, we propose a regularization technique, called data augmented invariant regularization (DAIR) that penalizes the inconsistency of loss on augmented samples with respect to the original ones. DAIR is applicable whenever data augmentation is used to promote invariances. DAIR only requires marginal additional cost on top of data augmentation, and is simple and applicable to a wide host of supervised and unsupervised learning tasks, including generative models. We introduce the DAIR formulation, motivate it, and theoretically prove some of its properties in Section 2. We empirically evaluate DAIR on a variety of problem setups ranging from defense against adversarial attacks to domain generalization in the presence of environment shift in Section 3. Our experimental results show that DAIR is competitive with or even outperforms state-of-the-art algorithms specifically designed for imposing invariance in these problems. Conclusion. In this paper, we proposed a simple yet effective regularizer that can be used wherever data augmentation is used to promote invariance. We rigorously showed that our proposed regularizer can recover the optimal solution in certain regression task where simple data augmentation can not. We also compare our DAIR-SQ regularizer with the existing off-the-shelf regularizers such as L1, L2 and KL divergence. We empirically showed that the DAIR-SQ regularizer is competitive with or outperforms state-of-the-art problem-specific baselines in a variety of problem setups. We evaluated DAIR in four different categories of machine learning tasks including regression, visual question answering, and training robust deep neural networks, and task-oriented dialog modeling. This is a major benefit of DAIR-SQ as some of other regularizers cannot be applied to regression tasks. Empirically the proposed algorithm outperforms well in all tasks. Better understanding of the tuning of the corresponding hyperparameter also remains as an open area for research. Finally, a more in-depth theoretical understanding of the properties of DAIR-SQ regularizer that lead to its superior empirical performance are also important questions for future work. While wethat DAIR-SQ boosts existing performance metrics, such as average accuracy, the interplay of DAIR-SQ with other metrics, especially group fairness is also another important area for future research.",0
"Abstract A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that “summarizes” texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words(Lin and Ng, 2019). Recently, with the development of representation learning for NLP (Vaswani et al., 2017; Devlin et al., 2018) and large-scale datasets (Fabbri et al., 2019), some studies have achieved promising results on abstractive MDS (Liu and Lapata, 2019; Jin et al., 2020). Nevertheless, we found there are two limitations that have not been addressed by previous studies. First, some works simply concatenate multiple documents into a flat sequence and then apply single-document summarization approaches (Liu et al., 2018; Fabbri et al., 2019). However, this paradigm fails to consider the hierarchical document structures, which plays a key role in MDS task (Jin et al., 2020). Also, the concatenation operation inevitably produces a lengthy sequence, and encoding long texts for summarization is a challenge (Cohan et al., 2018). Second, when dealing with multiple documents, a critical point is to learn the cross-document relations. Some studies address this problem by mining the co-occurrence words or entities (Wang et al., 2020a), which can hardly capture implicit associations due to the diverse language expressions. Some other studies (Jin et al., 2020; Liu and Lapata, 2019) first generate low-dimensional vectors in sentence- or paragraph-level and then build interaction based on these highly compressed representations. These methods inevitably result in the loss of large amounts of fine-grained interaction features and would damage the interpretability of models. Therefore, how to learn the relation across documents effectively remains an open question. To shed lights on these missing points, this paper proposes a novel abstractive MDS model that marries topic modeling into abstractive summary generation. The motivation is that both tasks aim to distil salient information from massive text and therefore could provide complementary features for each other. Concretely, we jointly optimize a neural topic model (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017) that learns topic distribution of source documents and corpus-level topic representations, and an abstractive summarizer that incorporates latent topics to summary generation process. In the encoding process, we represent multiple documents as a heterogeneous graph consisting of word, topic, and document nodes and encode it with a graph neural network to capture the interactions among different semantic units. In the decoding process, we devise a topic-aware decoder that leverages learned topics to guide the summary generation. We train the two modules with a multitask learning framework, where an inconsistency loss is applied to penalize the difference between the topic distribution of source documents and that of generated summaries. It encourages the summarizer to generate a summary that is thematically consistent with its source documents and also helps the two modules learn from each other. In this manner, our model is learned such that better topics can yield better summaries and vice versa. We conduct throughout experiments on the recently released Multi-News dataset (Fabbri et al., 2019). The results demonstrate the effectiveness and superiority of our model. To sum up, the contributions of this paper are threefolds: 1) To the best of our knowledge, we carry out the first systematic study on jointly modeling topic inference and abstractive MDS and demonstrate the positive mutual effect between the two tasks. 2) We propose a novel MDS model that joint optimizes a neural topic model and an abstractive summarizer. We propose an inconsistency loss to penalize the disagreement between the two modules and help them learn from each other. 3) Experimental results on the Multi-News dataset demonstrate that our model achieves the state-of-the-art performance on both Rouge scores and human evaluation, meanwhile learns highquality topics. Conclusion and Future Work This study proposes a novel abstractive MDS model that integrates a joint NTM to discover latent topics. Experimental results demonstrate that our model not only achieves the-state-of-the art results on summarization but also produce high-quality topics. Further discussions show that topic inference and summary generation can promote each other. In the future, we will explore how to apply latent topics in controllable summarization.",1
"Abstract A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that Ã¢ÂÂsummarizesÃ¢ÂÂ texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words(Lin and Ng, 2019). Recently, with the development of representation learning for NLP (Vaswani et al., 2017; Devlin et al., 2018) and large-scale datasets (Fabbri et al., 2019), some studies have achieved promising results on abstractive MDS (Liu and Lapata, 2019; Jin et al., 2020). Nevertheless, we found there are two limitations that have not been addressed by previous studies. First, some works simply concatenate multiple documents into a flat sequence and then apply single-document summarization approaches (Liu et al., 2018; Fabbri et al., 2019). However, this paradigm fails to consider the hierarchical document structures, which plays a key role in MDS task (Jin et al., 2020). Also, the concatenation operation inevitably produces a lengthy sequence, and encoding long texts for summarization is a challenge (Cohan et al., 2018). Second, when dealing with multiple documents, a critical point is to learn the cross-document relations. Some studies address this problem by mining the co-occurrence words or entities (Wang et al., 2020a), which can hardly capture implicit associations due to the diverse language expressions. Some other studies (Jin et al., 2020; Liu and Lapata, 2019) first generate low-dimensional vectors in sentence- or paragraph-level and then build interaction based on these highly compressed representations. These methods inevitably result in the loss of large amounts of fine-grained interaction features and would damage the interpretability of models. Therefore, how to learn the relation across documents effectively remains an open question. To shed lights on these missing points, this paper proposes a novel abstractive MDS model that marries topic modeling into abstractive summary generation. The motivation is that both tasks aim to distil salient information from massive text and therefore could provide complementary features for each other. Concretely, we jointly optimize a neural topic model (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017) that learns topic distribution of source documents and corpus-level topic representations, and an abstractive summarizer that incorporates latent topics to summary generation process. In the encoding process, we represent multiple documents as a heterogeneous graph consisting of word, topic, and document nodes and encode it with a graph neural network to capture the interactions among different semantic units. In the decoding process, we devise a topic-aware decoder that leverages learned topics to guide the summary generation. We train the two modules with a multitask learning framework, where an inconsistency loss is applied to penalize the difference between the topic distribution of source documents and that of generated summaries. It encourages theto generate a summary that is thematically consistent with its source documents and also helps the two modules learn from each other. In this manner, our model is learned such that better topics can yield better summaries and vice versa. We conduct throughout experiments on the recently released Multi-News dataset, which demonstrates the effectiveness of our model. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary.",0
"Abstract We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent whitebox and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging — the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP models’ classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the model’s behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification model’s decision for financial texts. Related, textual adversaries also aim to change the model prediction (with modifications resembling natural text). The difference is that adversaries further aim to escape human detection (not changing a human’s classification), whereas counterfactuals do not have such requirement. Another line of related work is style transfer (Sudhakar et al., 2019; Wang et al., 2019; Hu et al., 2017), which aim to modify a given text according to a target style. It differs from adversary or counterfactual generation in that it seeks to fully change all style-related phrases, as opposed to minimally perturbing a text to change a classifier’s decision. White-box approaches have been widely used to generate adversaries or counterfactuals for vision tasks where the continuous inputs can be optimized to alter model predictions (Goodfellow et al., 2014; Carlini and Wagner, 2017; Neal et al., 2018). Such optimization based approaches are difficult to apply to language due to the discrete nature of text. We circumvent this difficulty by directly optimizing in the latent space of the input towards the desired classification. We then exploit the language generation capability of pre-trained language models, available for most state-of-the-art NLP models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), to generate semantically plausible substitutions from the optimized latent representations. We further introduce Shapley values to estimate the combinatoric effect of multiple simultaneous changes, which are then used to guide a beam search to generate the final counterfactual. Leveraging pre-trained language models to generate alternative texts has been a popular black-box approach in the recent literature on text adversaries (Li et al., 2020b; Garg and Ramakrishnan, 2020; Li et al., 2020a). Our work presents a first attempt to combine the strength of white-box optimization and the power of pre-trained language models. While Shapley values have been widely studied for the problem of feature importance (Lundberg and Lee, 2017; Sundararajan and Najmi, 2020) and data valuation (Jia et al., 2020), this is the first effort demonstrating their usefulness for text generation. We compare our method to several white-box and black-box baselines on two different text classification tasks. Automatic and human evaluation results show that our method significantly improves the success rate of counterfactual generation, while reducing the fraction of input tokens modified and enhancing the semantic plausibility of generated counterfactuals. We also show through ablation studies that both counterfactual optimization of the latent representations and Shapley value estimates contribute to our method’s strong performance. We are motivated by how humans use counterfactuals to explain the concept of a class and seek to automatically generate counterfactual text input as a means to understand a deep NLP model and its definition of class. We assume full white-box access to the given model and perform optimization in the latent space to maximize the probability of predicting a target class. We then map from the optimized latent representation to candidate token substitutions using a language model. A key novelty of CLOSS is using Shapley values to estimate the potential of a token substitution in changing the model’s prediction when used in combination with other substitutions. The Shapley value is then used to guide a breadth-first beam search to generate the final counterfactual. Through both automatic and human evaluations, we show that CLOSS achieves highly competitive performance both in terms of the success rate of generating counterfactuals as well as the quality of the generated counterfactuals. Our approach has several limitations. As a whitebox approach, we require full access to the model, which can be restrictive in practical applications. Our approach currently only considers substitutions, excluding deletions and insertions. Finally, our method is only applicable to models that are based on pre-trained language models. Future work will adapt CLOSS to adversarial and black box settings. We also hope to improve the efficiency of CLOSS via more efficient Shapley value estimation (Chen et al., 2018; Jia et al., 2020).",1
"Abstract We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the modelÃÂ¢ÃÂÃÂs prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent whitebox and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging Ã¢ÂÂ the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP modelsÃ¢ÂÂ classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the modelÃ¢ÂÂs behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification modelÃ¢ÂÂs decision for financial texts. Related, textual adversaries also aim to change the model prediction (with modifications resembling natural text). The difference is that adversaries further aim to escape human detection (not changing a humanÃ¢ÂÂs classification), whereas counterfactuals do not have such requirement. Another line of related work is style transfer (Sudhakar et al., 2019; Wang et al., 2019; Hu et al., 2017), which aim to modify a given text according to a target style. It differs from adversary or counterfactual generation in that it seeks to fully change all style-related phrases, as opposed to minimally perturbing a text to change a classifierÃ¢ÂÂs decision. White-box approaches have been widely used to generate adversaries or counterfactuals for vision tasks where the continuous inputs can be optimized to alter model predictions (Goodfellow et al., 2014; Carlini and Wagner, 2017; Neal et al., 2018). Such optimization based approaches are difficult to apply to language due to the discrete nature of text. We circumvent this difficulty by directly optimizing in the latent space of the input towards the desired classification. We then exploit the language generation capability of pre-trained language models, available for most state-of-the-art NLP models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), to generate semantically plausible substitutions from the optimized latent representations. We further introduce Shapley values to estimate the combinatoric effect of multiple simultaneous changes, which are then used to guide a beam search to generate the final counterfactual. Leveraging pre-trained language models to generate alternative texts has been a popular black-box approach in the recent literature on text adversaries (Li et al., 2020b; Garg and Ramakrishnan, 2020; Li et al., 2020a). Our work presents a first attempt to combine the strength of white-box optimization and the power of pre-trained language models. While Shapley values have been widely studied for the problemfeature importance (Lundberg and Lee, 2017; Sundararajan and Najmi, 2020) and data valuation (Jia et al., 2020), this is the first effort demonstrating their usefulness for text generation. We compare our method to several white-box and black-box baselines on two different text classification tasks. Automatic and human evaluation results show that our method significantly improves the success rate of text generation. We also show through ablation studies that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging Ã¢ÂÂ the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP modelsÃ¢ÂÂ classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the modelÃ¢ÂÂs behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification modelÃ¢ÂÂs decision for financial texts.",0
"Abstract. A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information. Almost everybody prefers to spend a short time reading a summary of a topic before deciding whether to engage in further reading. Presently, the majority of people still check documents by hand because the current technology is mainly based on users providing keywords to filter articles. The user would save much time if it were possible to perform automatic checks on the document. Consequently, we have an idea to implement an automated topic extraction method for research papers. Consequently, we have an idea to implement an automated topic extraction method for research papers. This will facilitate the reading process and give the readers a good sense of what the papers are about. This allows them to quickly and easily decide which documents to read. A proven technique for achieving this is topic extraction (also known as key phrase extraction). Research on this topic has been carried out by a large number of researchers. They have published some insightful articles about models and frameworks. For example, in Zhiyuan et al (2010)’s work, the authors propose two approaches for topic extraction: supervised and unsupervised approaches. Turney's work illustrates the supervised principle by providing a model for determining if a topic should be considered a key topic. An online digital library search engine offers this type of extraction. Human labeling is an inconvenience with this approach. It means that the users are required to come up with their own keywords, which are then compared to the whole database to see if any labels are found. However, if the purpose is to collect a large number of articles, the user will have to spend considerable time ensuring that the results are actually focused on his keywords instead of just briefly mentioning them several times. This requires a supervised approach because of the time-consuming nature of the unsupervised principle. The graph-based ranking algorithms, as suggested by Mihalcca and Tarau, have gained a lot of attention and have proven successful in the supervised setting. Based on recursively computing information from the entire graph, this approach can be used to determine the relative importance of vertexes within a graph. The most common uses of these algorithms are to analyze citations, social networks, and web links. Therefore, topic extraction cannot be used to summarize the text's concept, which is its major limitation. Here, we propose a method based on n-gram analysis to automate the topic extraction process and make it more efficient and reliable. To filter out irrelevant results, we create semi-customized blacklists and whitelists to make the process more efficient. after ngram analysis. We also take an adequate number of training samples to ensure the quality of our blacklist and whitelist. Our automated approach will also be evaluated with the aid of a manual process to determine its effectiveness and accuracy. The objective of our study is to help educators properly label selected documents so that they can decide if this is the right topic for them. The topic extraction process should also be fully automated. It means the reader should be able to identify the topic in the shortest amount of time, and the topic should be delivered automatically to the reader. 1.2 Research Objective As a result, we propose an integrated strategy to automate topic extraction automation by analyzing N-grams with the help of blacklists and whitelists, and subsequent labeling of each document based on the results. The aim of this is to develop a practical solution for automating article topic extraction from a large number of articles. The application works well in assisting scholars or any person interested in finding an article they desire from an abundance of articles. By analyzing the results of automatic and manual topic extraction methods, we will have a deeper understanding of the accuracy and efficiency of the N-gram based method. RQ1: “Is N-gram analysis useful in automating topic extraction from research papers?” RQ2: “How can we evaluate our automated topic extraction approach?”. Conclusion In light of the results we obtained, the text-based topic extraction approach is powerful and time-saving to use if the evaluation conditions are set up right. We have concluded that text-based topics can be very effective which provided the evaluation conditions are properly set up. We are able to efficiently perform topic extraction by automating our process while still maintaining an acceptable level of accuracy. Even though an automated approach does not always result in highly accurate labels, our approach can serve as a valuable starting point for topic extraction. As a result of continuous development over time, identifying more accurate labels can be achieved by regularly updating and carefully selecting the black- and whitelists. In order to accomplish the automated topic extraction, our approach excels at processing a variety of PDF documents and presenting the reader with the potential topics of interest. It saved the reader's time to read articles that they are unsure if the articles are relevant to their interests. With a more reasonable set of conditions, it will be more likely that our topic extraction will become more accurate for more topics.",1
"Abstract. A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information. Almost everybody prefers to spend a short time reading a summary of a topic before deciding whether to engage in further reading. Presently, the majority of people still check documents by hand because the current technology is mainly based on users providing keywords to filter articles. The user would save much time if it were possible to perform automatic checks on the document. Consequently, we have an idea to implement an automated topic extraction method for research papers. Consequently, we have an idea to implement an automated topic extraction method for research papers. This will facilitate the reading process and give the readers a good sense of what the papers are about. This allows them to quickly and easily decide which documents to read. A proven technique for achieving this is topic extraction (also known as key phrase extraction). Research on this topic has been carried out by a large number of researchers. They have published some insightful articles about models and frameworks. For example, in Zhiyuan et al (2010)Ã¢ÂÂs work, the authors propose two approaches for topic extraction: supervised and unsupervised approaches. Turney's work illustrates the supervised principle by providing a model for determining if a topic should be considered a key topic. An online digital library search engine offers this type of extraction. Human labeling is an inconvenience with this approach. It means that the users are required to come up with their own keywords, which are then compared to the whole database to see if any labels are found. However, if the purpose is to collect a large number of articles, the user will have to spend considerable time ensuring that the results are actually focused on his keywords instead of just briefly mentioning them several times. This requires a supervised approach because of the time-consuming nature of the unsupervised principle. The graph-based ranking algorithms, as suggested by Mihalcca and Tarau, have gained a lot of attention and have proven successful in the supervised setting. Based on recursively computing information from the entire graph, this approach can be used to determine the relative importance of vertexes within a graph. The most common uses of these algorithms are to analyze citations, social networks, and web links. Therefore, topic extraction cannot be used to summarize the text's concept, which is its major limitation. Here, we propose a method based on n-gram analysis to automate the topic extraction process and make it more efficient and reliable. To filter out irrelevant results, we create semi-customized blacklists and whitelists to make the process more efficient. after ngram analysis. We also take an adequate number of training samples to ensure the quality of our blacklist and whitelist. Our automated approach will also be evaluated with the aid of a manual process to determine its effectiveness and accuracy. The objective of our study is to help educators properly label selected documents so that they can decide if this is the right topic for them. The topic extraction process should also be fully automated. It means the reader should be able to identify the topic in the shortest amount of time, and the topic should be delivered automatically to the reader. 1.2 Research Objective As a result, we propose an integrated strategy to automate topic extraction automation by analyzing N-grams with the help of blacklists and whitelists, and subsequent labeling of each document based on the results. The aimthis is to develop a practical solution for automating article topic extraction from a large number of articles. The application works well in assisting scholars or any person interested in finding an article they desire from an abundance of articles. By analyzing the results of automatic and manual topic extraction methods, researchers can ensure they are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information. Almost everybody prefers to spend a short time reading a summary of a topic before deciding whether to engage in further reading. Presently, the majority of people still check documents by hand because the current technology is mainly based on users providing keywords to filter articles. The user would save much time if it were possible to perform automatic checks on the document.",0
"Abstract Over the recent years, large pretrained lan- guage models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general lan- guage has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include specific language that common LMs can not repre- sent accurately. We argue that this short- coming of today’s LMs limits the applica- bility of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., Grüning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018). Deep learning tech- niques that promise higher accuracy are grad- ually replacing these approaches (e.g., Kölbel et al., 2020; Luccioni et al., 2020; Bingler et al., 2021; Callaghan et al., 2021; Wang et al., 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP al- lows for impressive results, outperforming tradi- tional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretrain- ing and leads to the model learning representa- tions of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Represen- tations from Transformers) (Devlin et al., 2018) with its successors ROBERTA (Liu et al., 2019), Transformer-XL (Dai et al., 2019) and ELEC- TRA (Clark et al., 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and lan- guage patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of sam- ples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task- specific text exhibits better performance, com- pared to a model that has been pretrained solely on general text (Araci, 2019; Lee et al., 2020). Hence, a straightforward extension to the stan- dard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al., 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times — in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) + domain-adaptive pretraining (downstream domain) + training (downstream task). To date, regardless of the increase in using NLP for climate change related research, a model with climate domain-adaptive pretraining has not been publicly available, yet. Research so far rather re- lied on models pretrained on general language, and fine-tuned on the downstream task. To fill this gap, our contribution is threefold. First, we intro- duce CLIMATEBERT, a state-of-the-art language model that is specifically pretrained on climate- related text corpora of various sources, namely news, corporate disclosures, and scientific articles. This language model is designed to supports re- searchers of various disciplines in obtaining better performing NLP models for a manifold of down- stream tasks in the climate change domain. Sec- ond, to illustrate the strength of CLIMATEBERT, we highlight the performance improvements using CLIMATEBERT on three standard climate-related NLP downstream tasks. Third, to further promote research at the intersection of climate change and NLP, we make the weights of all trained language models publicly available at climatebert.ai. Conclusion We propose CLIMATEBERT, the first language model that was pretrained on a large scale dataset of over 1.6 million climate-related paragraphs. We study various selection strategies to find samples from our corpus which are most helpful for later tasks. Our experiments reveal that our domain- adaptive pretraining leads to considerably lower masked language modeling loss on our climate corpus. We further find that this improvement is also reflected in predictive performance across three essential downstream climate-related NLP tasks: text classification, the analysis of risk and opportunity statements by corporations, and fact- checking climate-related claims.",1
"Abstract Over the recent years, large pretrained lan- guage models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general lan- guage has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include specific language that common LMs can not repre- sent accurately. We argue that this short- coming of todayÃ¢ÂÂs LMs limits the applica- bility of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., GruÃÂning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018). Deep learning tech- niques that promise higher accuracy are grad- ually replacing these approaches (e.g., KoÃÂlbel et al., 2020; Luccioni et al., 2020; Bingler et al., 2021; Callaghan et al., 2021; Wang et al., 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP al- lows for impressive results, outperforming tradi- tional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretrain- ing and leads to the model learning representa- tions of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Represen- tations from Transformers) (Devlin et al., 2018) with its successors ROBERTA (Liu et al., 2019), Transformer-XL (Dai et al., 2019) and ELEC- TRA (Clark et al., 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and lan- guage patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of sam- ples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task- specific text exhibits better performance, com- pared to a model that has been pretrained solely on general text (Araci, 2019; Lee et al., 2020). Hence, a straightforward extension to the stan- dard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al., 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times Ã¢ÂÂ in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) + domain-adaptive pretraining (downstream domain) + training (downstream task). To date, regardless of the increase in using NLP for climate change related research, a model with climate domain-adaptive pretraining has not been publicly available, yet. Research so far rather re- lied on models pretrained on general language, and fine-tuned on the downstream task. To fill this gap, our contribution is threefold. First, we intro- duce CLIMATEBERT, a state-of-the-art language model that is specifically pretrained on climate- related text corporavarious sources, namely news, corporate disclosures, and scientific articles. This language model is designed to supports re- searchers of various disciplines in obtaining better performing NLP models for a manifold of down- stream tasks in the climate change domain. Sec- ond, to illustrate the strength of CLIMATEBERT, we highlight the performance improvements using CLIMATEBERT for a selection of down- stream tasks. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., GruÃÂning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018).",0
"Abstract We present Harmonic Memory Networks (HMem), a neural architecture for knowl- edge base completion that models entities as weighted sums of pairwise bindings between an entity’s neighbors and corresponding rela- tions. Since entities are modeled as aggregated neighborhoods, representations of unseen enti- ties can be generated on the fly. We demon- strate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methods—knowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a fact—which in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)—are com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces. Since representations must be learned from training-set instances of each component, this creates problems when such databases are to be scaled, and there- fore these methods have difficulty accommodating an open-world setting in which knowledge graphs evolve in time, since new facts inserted into the database after model training cannot be used for inference without model retraining. Furthermore, databases may be augmented in time not only with new facts about known entities, but also with new entities. In embedding-based models, new repre- sentation for such entities must be trained. We present Harmonic Memories (HMem), a neu- ral network which models entities by aggregating information about their neighborhoods using a su- perposition memory architecture, achieving gener- alization to new entities without retraining.1 The network combines two ideas. First, a represen- tation of entities as memory states consisting of superposed vector associations between learned en- tity and relation embeddings. Second, completion of memory states using a learned transformation based on Harmony-optimization methods (Smolen- sky and Legendre, 2006) (see §4). We refer to vector associations as bindings in the sense of the ”variable-binding problem” in the philosophy of cognitive science: in neural net models of cogni- tion, how are representations of the elements of a structure bound together into structures? In this work, we investigate two solutions prominent in the cognitive science literature—tensor product bind- ing (Smolensky, 1990) and circular convolution (Plate, 1994)—which have also both been effec- tively applied in KBC (Nickel et al., 2011, 2016). The approach is inspired by computational mod- eling of biological neural architectures for knowl- edge representation (Crawford et al., 2015), and is related to KBC methods based on convolution of graph neighborhoods (Schlichtkrull et al., 2017; Dettmers et al., 2018; Nguyen et al., 2018), in which inference is performed over representations of aggregated entity neighborhoods. Recent work has extended this idea using Graph Attention Net- works (GANs) (Nathani et al., 2018), which assign attention weights to entries in a graph neighbor- hood, these being later combined. For instance, Velicˇkovic ́ et al. (2018) use Graph Attention to gen- erate weights for triplet representations obtained by transforming concatenated entity and relation vectors, combining the results by averaging. This is similar to our approach, with the key difference that formulating the model—as we do—in terms of binding allows for clear formal analysis of certain scaling results (§7). We therefore gain in inter- pretability. HMem scales well in three respects. First, it allows a database with a fixed set of entities and relations to incorporate new facts into the model without parameter re-estimation. Empirically, per- formance improves in nearly every case when the neighborhoods are thus expanded. Second, it permits the addition of entities unseen in train- ing, whose representations are useless in a vec- tor embedding framework. For our model, infer- ences about these entities are possible when a sub- graph including them becomes available. Third, our model effectively handles nodes with high in-degree. We show that, whereas embedding- based approaches show decreased performance with highly connected nodes, our model exhibits improved performance on nodes with many neigh- bors. §2, §3 and §4 introduce the Harmonic Memory architecture, and §5 shows that our model achieves state-of-the-art results on benchmark KBC datasets. After evaluation on standard benchmarks, §6 introduces WNGen and FBGen, datasets based on Word- Net and FREEBASE that evaluate the network’s ability to abstract from node identity and make in- ferences exclusively on the basis of information about nodes in its neighborhood, and §7 examines in detail how the model scales with the size of en- tity neighborhoods and the addition of new input facts. §8 concludes. Conclusion This article presents a neural model for knowledge base completion that is powerful enough to achieve state of the art results on large databases, and flexi- ble enough to evolve with knowledge base content sans retraining. The approach complements ex- isting neighborhood-aggregation techniques (e.g. graph convolution), with the advantage of inter- pretable mechanisms: vector binding and memory completion. The results indicate that the model operates well at scale and in an open-world setting.",1
"Abstract We present Harmonic Memory Networks (HMem), a neural architecture for knowl- edge base completion that models entities as weighted sums of pairwise bindings between an entityÃÂ¢ÃÂÃÂs neighbors and corresponding rela- tions. Since entities are modeled as aggregated neighborhoods, representations of unseen enti- ties can be generated on the fly. We demon- strate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methodsÃ¢ÂÂknowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a factÃ¢ÂÂwhich in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)Ã¢ÂÂare com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces. Since representations must be learned from training-set instances of each component, this creates problems when such databases are to be scaled, and there- fore these methods have difficulty accommodating an open-world setting in which knowledge graphs evolve in time, since new facts inserted into the database after model training cannot be used for inference without model retraining. Furthermore, databases may be augmented in time not only with new facts about known entities, but also with new entities. In embedding-based models, new repre- sentation for such entities must be trained. We present Harmonic Memories (HMem), a neu- ral network which models entities by aggregating information about their neighborhoods using a su- perposition memory architecture, achieving gener- alization to new entities without retraining.1 The network combines two ideas. First, a represen- tation of entities as memory states consisting of superposed vector associations between learned en- tity and relation embeddings. Second, completion of memory states using a learned transformation based on Harmony-optimization methods (Smolen- sky and Legendre, 2006) (see ÃÂ§4). We refer to vector associations as bindings in the sense of the Ã¢ÂÂvariable-binding problemÃ¢ÂÂ in the philosophy of cognitive science: in neural net models of cogni- tion, how are representations of the elements of a structure bound together into structures? In this work, we investigate two solutions prominent in the cognitive science literatureÃ¢ÂÂtensor product bind- ing (Smolensky, 1990) and circular convolution (Plate, 1994)Ã¢ÂÂwhich have also both been effec- tively applied in KBC (Nickel et al., 2011, 2016). The approach is inspired by computational mod- eling of biological neural architectures for knowl- edge representation (Crawford et al., 2015), and is related to KBC methods based on convolution of graph neighborhoods (Schlichtkrull et al., 2017; Dettmers et al., 2018; Nguyen et al., 2018), in which inference is performed over representations of aggregated entity neighborhoods. Recent work has extended this idea using Graph Attention Net- works (GANs) (Nathani et al., 2018), which assign attention weights to entries in a graph neighbor- hood, these being later combined. For instance, VelicÃÂkovic ÃÂ et al. (2018) use Graph Attention to gen- erate weights for triplet representations obtained by transforming concatenated entity and relation vectors, combining the results by averaging. This is similar to our approach, with the key difference that formulating the modelÃ¢ÂÂas we doÃ¢ÂÂin terms of binding allows for clear formal analysis of certain scaling results (ÃÂ§7). We therefore gain in inter- pretability. HMem scales well in three respects. First, it allows a database with a fixed set of entities and relations to incorporate new facts into the model without parameter re-estimation. Empirically, per- formance improves in nearly every case when the neighborhoods are thus expanded. Second, it permits the addition of entities unseen in train- ing, whose representations are useless in a vec- tor embedding framework. For our model, infer- ences about these entities are possible when a sub- graph including them becomesThird, our model effectively handles nodes with high in-degree. We show that, whereas embedding- based approaches show decreased performance with highly connected nodes, our model exhibits improved performance on nodes with many neigh- bors. ÃÂÃÂ§2, ÃÂÃÂ§3 and ÃÂÃÂ§4 introduce the Harmonic Memory architecture, and ÃÂÃÂ§5 shows that our model achieves state-of-the-art results on benchmark KBCs. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methodsÃ¢ÂÂknowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a factÃ¢ÂÂwhich in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)Ã¢ÂÂare com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces.",0
"Abstract We present the task of Automated Punish- ment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Ad- dressing APE will enable the identification of sentencing patterns and constitute an im- portant stepping stone for many follow up le- gal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evalua- tion dataset, and implement rule-based and su- pervised models. We find that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models’ errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions. Much of the legal data, however, comes in unstructured formats that pose critical challenges for extracting and analyzing it in sys- tematic ways. In addition, different countries vary in their legal systems, norms and conventions, fur- ther compounding the challenges in developing multilingual approaches (Peruginelli, 2009). While legal NLP is gaining traction in recent years (Van Gog and Van Engers, 2001; Dale, 2019; Zhong et al., 2020), relatively little attention has been given to low-resource settings outside of the English language, where the availability of tools such as large pretrained language models, syntactic parsers, or named entity recognizers is limited. In this work, conducted as part of an on-going collaboration with The Association of Rape Crisis Centers in Israel (ARCCI), we focus specifically on the task of Automated Punishment Extraction (APE) in sexual assault cases in Hebrew within Israeli court sentencing decisions (see formal task definition in Section 2). Punishment decisions are of special importance as they constitute a prerequi- site for many other downstream tasks in legal NLP and digital humanities, such as legal prediction of judicial decisions (Aletras et al., 2016; Branting et al., 2021) and detecting biases in court deci- sions (Pinto et al., 2020). APE is difficult in the Israeli court system. This is due to the fact that sen- tencing decisions for criminal offences are reported, in natural language idiomatic to the legal field, in the written sentencing decision. We focus on sex- ual assault cases due to the legal and public debate around claims of lenient punishments (Phillips and Chagnon, 2020), that in the absence of system- atic rigorous data collection cannot be empirically examined and assessed. This worldwide debate requires legal NLP methods in multiple languages and legal systems. To address this challenge, we begin by curating a dataset of sexual assault sentencing decisions from the years 1990-2021 and manually annotate pun- ishment in a subset of 100 cases with the use of legal experts in our team and in collaboration with ARCCI (Section 3). Following, in Section 4, we use this data to build several models for the APE task, including rule-based and supervised methods, based on linguistically and semantically informed features, setting first benchmark results on the APE task in Hebrew. We thoroughly analyze our models’ performance in Section 5, finding that they are ca- pable of extracting the correct punishment in 68% of the cases,while the best model’s average error is roughly 5 months, attesting to the difficulty of the task. Based on our models, we find that in our data the median predicted punishment is 3 years, while more than a third of the punishments are below 15 months. Although these figures are obtained on a medium-size corpus, using automatic measures which do not account for the type of offense, we note that they are well below the maximum pun- ishments for sexual offenses as determined by the Israeli legislator, which range between 2-7 years for indecent acts and sodomy and up to 20 years for aggravated rape. We conclude by analyzing common error pat- terns in our models. For example, we find that models often tend to erroneously extract a proba- tion imprisonment punishment instead of the actual imprisonment punishment. Distinguishing between the two is left as an interesting avenue for future work. To the best of our knowledge, this is the first examination of automatic punishment extraction in the Hebrew language. It includes data collection, annotation, and benchmark models. We hope it will spur further research into this important task. Conclusions In this work we created the first annotated corpus of Hebrew language sentencing decisions, focus- ing on sexual assaults. We compared a rule-based approach with supervised learners using the unique attributes of the legal language for representing sentences. We found that the rule-based approach achieved best results with an average error rate of 5 months and accuracy of 68% in extracting the punishment sentence. Our analysis shows that such research could focus on fine tuning of the super- vised models. While supervised learning models help us narrow down a full legal document to 2 - 5 sentences that include the punishment, further research can contribute in reaching a single target sentence, which could also benefit from our error analysis, especially regarding the probation sen- tences, perhaps targeting them separately in a prior task and using them as features.",1
"Abstract We present the task of Automated Punish- ment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Ad- dressing APE will enable the identification of sentencing patterns and constitute an im- portant stepping stone for many follow up le- gal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evalua- tion dataset, and implement rule-based and su- pervised models. We find that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common modelsÃ¢ÂÂ errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions. Much of the legal data, however, comes in unstructured formats that pose critical challenges for extracting and analyzing it in sys- tematic ways. In addition, different countries vary in their legal systems, norms and conventions, fur- ther compounding the challenges in developing multilingual approaches (Peruginelli, 2009). While legal NLP is gaining traction in recent years (Van Gog and Van Engers, 2001; Dale, 2019; Zhong et al., 2020), relatively little attention has been given to low-resource settings outside of the English language, where the availability of tools such as large pretrained language models, syntactic parsers, or named entity recognizers is limited. In this work, conducted as part of an on-going collaboration with The Association of Rape Crisis Centers in Israel (ARCCI), we focus specifically on the task of Automated Punishment Extraction (APE) in sexual assault cases in Hebrew within Israeli court sentencing decisions (see formal task definition in Section 2). Punishment decisions are of special importance as they constitute a prerequi- site for many other downstream tasks in legal NLP and digital humanities, such as legal prediction of judicial decisions (Aletras et al., 2016; Branting et al., 2021) and detecting biases in court deci- sions (Pinto et al., 2020). APE is difficult in the Israeli court system. This is due to the fact that sen- tencing decisions for criminal offences are reported, in natural language idiomatic to the legal field, in the written sentencing decision. We focus on sex- ual assault cases due to the legal and public debate around claims of lenient punishments (Phillips and Chagnon, 2020), that in the absence of system- atic rigorous data collection cannot be empirically examined and assessed. This worldwide debate requires legal NLP methods in multiple languages and legal systems. To address this challenge, we begin by curating a dataset of sexual assault sentencing decisions from the years 1990-2021 and manually annotate pun- ishment in a subset of 100 cases with the use of legal experts in our team and in collaboration with ARCCI (Section 3). Following, in Section 4, we use this data to build several models for the APE task, including rule-based and supervised methods, based on linguistically and semantically informed features, setting first benchmark results on the APE task in Hebrew. We thoroughly analyze our modelsÃ¢ÂÂ performance in Section 5, finding that they are ca- pable of extracting the correct punishment in 68% of the cases,while the best modelÃ¢ÂÂs average error is roughly 5 months, attesting to the difficulty of the task. Based on our models, we find that in our data the median predicted punishment is 3 years, while more than a third of the punishments are below 15 months. Although these figures are obtained on a medium-size corpus, using automatic measures which do not account for the type of offense, we note that they are well below the maximum pun- ishments for sexual offenses as determined by the Israeli legislator, which range between 2-7 years for indecent acts and sodomy and up to 20 years for aggravated rape. We conclude by analyzing common error pat- terns in our models. For example, we find that models often tend to erroneously extract a proba- tion imprisonment punishment instead of the actual imprisonment punishment. Distinguishing between the two is left as an interesting avenue for future work. To the best of our knowledge, this is the first examination of automatic punishment extraction in the Hebrew language.includes data collection, annotation, and benchmark models. We hope it will spur further research into this important task. Conclusions In this work we created the first annotated corpus of Hebrew language sentencing decisions, focus- ing on sexual assaults. We compared a rule-based approach with supervised learners using the unique attributes of the legal language for representing sentences. We found that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common modelsÃ¢ÂÂ errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions.",0
"Abstract Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Dis- tant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstracti- fies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance. Introduction Relation extraction (RE) is a key facet of information extraction in large bodies of unstructured textual data. RE is particularly important in the biomedical domain where extracting relationships between pairs of biomedical entities, also known as “fact triples”, can produce new insights into complicated biological interactions. For instance, with the near-exponential growth of microbiome research [Sa’ed et al., 2019], advanced RE methods may help discover important links between gut microbiota and diseases. It is in this context that we motivate our work. RE within the biomedical domain comes with two inherent challenges: there are more than 30 million scientific articles, with hundreds of thousands of articles published every year, and there is a corresponding lack of labeled data. To resolve these challenges, many have leveraged distant supervision techniques which pair knowledge graphs with raw textual data to automatically generate labels to train deep-learning models [Gu et al., 2019, Su et al., 2019, Junge and Jensen, 2019]. We seek to improve distantly supervised biomedical RE methods in this work. We use the Unified Medical Language System (UMLS) Metathesaurus [Bodenreider, 2004] for our knowledge graph and pair it with raw textual data from PubMed [Canese and Weis, 2013]. To automatically generate labels, distantly supervised RE methods rely on a simple yet powerful assumption: any singular sentence that contains a pair of entities also expresses a relationship, as determined by the accompanying knowledge graph, between those entities [Mintz et al., 2009]. However, this assumption leads to a noisy training signal with many false positives as not all sentences express a relationship between an entity pair. To combat this, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping sentences into bags greatly reduces noise in the training signal since a bag of sentences is more likely to express a relationship than a single sentence. This enables the model to better classify relationships between unseen entity pairs. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences of evidence. After processing the PubMed corpus, we observe that a majority (∼ 52%) of extracted triples are supported by fewer than three sentences. Creating bags of sentences for such entity pairs requires heavy up-sampling. For example, if a pair of entities is only supported by one sentence and a bag size is equal to 16 sentences, the single sentence is duplicated 15 times to fill the bag. This erases the benefit of MIL. To counter this issue, we introduce abstractified multi-instance learning (AMIL) where, instead of grouping entity pairs by name, we group entities by the corresponding semantic type as determined by UMLS. UMLS categorizes each entity with a semantic type within the UMLS semantic network. The UMLS semantic network is curated by human experts for decades and provides a rich ontology of biomedical concepts which we leverage to group multiple different entity pairs within a single MIL bag, reducing the need to up-sample sentences. For example, consider two sentences: (1) a sentence containing the entity pair (fibula, tibia) and (2) a second sentence containing the entity pair (humerus, ulna). With distant supervision, we assume each sentence expresses the relationship linking both pairs, namely articulates with. Despite expressing the same relationship, without abstraction, these sentences are placed into separate MIL bags since bags are grouped by distinct entity pairs. By introducing abstractified multi-instance learning, the entities fibula, tibia, humerus, and ulna are grouped by their corresponding UMLS semantic type—“Body Part, Organ, or Organ Component.” This allows us to place the aforemen- tioned sentences into the same MIL bag based on their entity type, creating a heterogeneous bag of entity pairs that express the same relationship. With this reformulation, bags containing a single duplicated sentence are reduced by half. AMIL produces better overall performance for biomedical RE with significant performance gains for “rare” triples. Here, we define “rare” triples as triples that are supported by fewer than eight sentences. These triples make up roughly 80% of the long-tail distribution of triples. We also take inspiration from Soares et al.(2019) and conduct a suite of experiments with varia- tions of relationship embedding architectures. Such experiments are underexplored in the biomedical domain and many are novel to the general task of relationship classification. Soares et al. report the best RE performance using a relationship representation consisting of embedded entity start markers—special span tokens that denote the beginning of an entity. We test this RE architec- ture in the biomedical domain and also test the performance of entity end markers. Moreover, we introduce a novel relationship representation, namely the middle mention pool, which pools word pieces between head and tail entities. This embedding architecture is inspired by the observation that context between two biomedical entities in a sentence often contains the information-rich and relationship-relevant signal. Our best performing relationship embedding architecture results from the combination of both entity end markers and the middle mention pool. We observe that this architecture further increases the performance of our relation classification model. In this paper, we make the following contributions: • We introduce abstractified multiple-instance learning (AMIL), which achieves new state-of-the- art performance for biomedical relationship extraction. We also report significant performance gains for rare fact triples.  •  We propose an improved relationship representation for biomedical relation extraction. We show that concatenating embedding tokens from entity end markers with the middle mention pool produces the best performing model.  • We make all our code, saved models, and pre-processing scripts publicly available1 to facilitate future biomedical RE efforts. Pre-processing scripts can impact model performance and are important to prepare an up-to-date, ready-for-RE dataset from ever-growing PubMed and UMLS. Our results in Section 5 show that using updated pre-processing tools can improve model performance by ∼ 10%. Conclusion In this work, we propose abstractified multi-instance (AMIL), a novel denoising method that in- creases the efficacy of multi-instance learning in the biomedical domain. With it, we improve perfor- mance on biomedical relationship extraction and report significant performance gains on rare fact triples. We also propose a novel relationship embedding architecture which further increases model performance. For future work, we will explore combining AMIL with more advanced bag aggregation meth- ods. We will also explore applying our novel relationship embedding architectures to relationship extraction tasks using general-domain datasets.",1
"Abstract Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Dis- tant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstracti- fies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance. Introduction Relation extraction (RE) is a key facet of information extraction in large bodies of unstructured textual data. RE is particularly important in the biomedical domain where extracting relationships between pairs of biomedical entities, also known as Ã¢ÂÂfact triplesÃ¢ÂÂ, can produce new insights into complicated biological interactions. For instance, with the near-exponential growth of microbiome research [SaÃ¢ÂÂed et al., 2019], advanced RE methods may help discover important links between gut microbiota and diseases. It is in this context that we motivate our work. RE within the biomedical domain comes with two inherent challenges: there are more than 30 million scientific articles, with hundreds of thousands of articles published every year, and there is a corresponding lack of labeled data. To resolve these challenges, many have leveraged distant supervision techniques which pair knowledge graphs with raw textual data to automatically generate labels to train deep-learning models [Gu et al., 2019, Su et al., 2019, Junge and Jensen, 2019]. We seek to improve distantly supervised biomedical RE methods in this work. We use the Unified Medical Language System (UMLS) Metathesaurus [Bodenreider, 2004] for our knowledge graph and pair it with raw textual data from PubMed [Canese and Weis, 2013]. To automatically generate labels, distantly supervised RE methods rely on a simple yet powerful assumption: any singular sentence that contains a pair of entities also expresses a relationship, as determined by the accompanying knowledge graph, between those entities [Mintz et al., 2009]. However, this assumption leads to a noisy training signal with many false positives as not all sentences express a relationship between an entity pair. To combat this, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping sentences into bags greatly reduces noise in the training signal since a bag of sentences is more likely to express a relationship than a single sentence. This enables the model to better classify relationships between unseen entity pairs. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences of evidence. After processing the PubMed corpus, we observe that a majority (Ã¢ÂÂ¼ 52%) of extracted triples are supported by fewer than three sentences. Creating bags of sentences for such entity pairs requires heavy up-sampling. For example, if a pair of entities is only supported by one sentence and a bag size is equal to 16 sentences, the single sentence is duplicated 15 times to fill the bag. This erases the benefit of MIL. To counter this issue, we introduce abstractified multi-instance learning (AMIL) where, instead of grouping entity pairs by name, we group entities by the corresponding semantic type as determined by UMLS. UMLS categorizes each entity with a semantic type within the UMLS semantic network. The UMLS semantic network is curated by human experts for decades and provides a rich ontology of biomedical concepts which we leverage to group multiple different entity pairs within a single MIL bag, reducing the need to up-sample sentences. For example, consider two sentences: (1) a sentence containing the entity pair (fibula, tibia) and (2) a second sentence containing the entity pair (humerus, ulna). With distant supervision, we assume each sentence expresses the relationship linking both pairs, namely articulatesDespite expressing the same relationship, without abstraction, these sentences are placed into separate MIL bags since bags are grouped by distinct entity pairs. By introducing abstractified multi-instance learning, the entities fibula, tibia, humerus, and ulna are grouped by their corresponding UMLS semantic typeÃÂ¢ÃÂÃÂÃÂ¢ÃÂÃÂBody Part, Organ, or Organ Component.ÃÂ¢ÃÂÃÂ This allows the entity pair to automatically generate labels to train deep-learning models. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many a triples relationship is formed by interactions between an entity pair. For instance, with the near-exponential growth of microbiome research [SaÃ¢ÂÂed et al., 2019], advanced RE methods may help discover important links between gut microbiota and diseases. It is in this context that we motivate our work. RE within the biomedical domain comes with two inherent challenges: there are more than 30 million scientific articles, with hundreds of thousands of articles published every year, and there is a corresponding lack of labeled data. To resolve these challenges, many have leveraged distant supervision techniques which pair knowledge graphs with raw textual data to automatically generate labels to train deep-learning models [Gu et al., 2019, Su et al., 2019, Junge and Jensen, 2019]. We seek to improve distantly supervised biomedical RE methods in this work. We use the Unified Medical Language System (UMLS) Metathesaurus [Bodenreider, 2004] for our knowledge graph and pair it with raw textual data from PubMed [Canese and Weis, 2013]. To automatically generate labels, distantly supervised RE methods rely on a simple yet powerful assumption: any singular sentence that contains a pair of entities also expresses a relationship, as determined by the accompanying knowledge graph, between those entities [Mintz et al., 2009]. However, this assumption leads to a noisy training signal with many false positives as not all sentences express a relationship between an entity pair. To combat this, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping sentences into bags greatly reduces noise in the training signal since a bag of sentences is more likely to express a relationship than a single sentence. This enables the model to better classify relationships between unseen entity pairs. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many triples relationships are formed by interactions between an entity pair. These entities are often labeled as follows:Ã¢ÂÂBody Part, Organ, or Organ Component.Ã¢ÂÂ This entity pair is most often found in the gastrointestinal tract, where bacteria are found to produce various types of biomedical entities. These entities are grouped by relatively simple yet powerful assumptions: since*/() is the best-fitting entity pair, and (II) is the worst-fitting entity pair. Because these entities are grouped by their corresponding UMLS semantic type, we assume each entity pair expresses the relationship it contains, regardless of UMLS type.",0
"Abstract Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sen- tences one by one to compose a summary, which have two main drawbacks: (1) neglect- ing both the intra and cross-document relations between sentences; (2) neglecting the coher- ence and conciseness of the whole summary. In this paper, we propose a novel MDS frame- work (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly out- puts an integrate summary in the form of sub- graph which is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architec- ture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks. Introduction Currently, most extractive models treat summariza- tion as a sequence labeling task. They score and select sentences one by one (Zhong et al., 2020). These models (called sentence-level extractors) do not consider summary as a whole but a combina- tion of independent sentences. This may cause incoherent and redundant problem, and result in a poor summary even if the summary consists of high score sentences. Some works (Wan et al., 2015; Zhong et al., 2020) treat summary as a whole unit and try to solve the weakness of sentence- level extractors by using a summary-level extrac- tor. However, these models neglect the intra and cross-document relations between sentences which also have benefits for extracting salient sentences, detecting redundancy and generating overall coher- ent summaries. Relations become more necessary when input source documents are much longer and more complex such as multi-document input. In this paper, we propose a novel MDS frame- work called SgSum which formulates the MDS task as a sub-graph selection problem. In our frame- work, source documents are regarded as a relation graph of sentences (e.g., similarity graph or dis- course graph) and the candidate summaries are its sub-graphs. In this view, how to generate a good summary becomes how to select a proper sub- graph. In our framework, the whole graph structure is modeled to help extract salient information from source documents and the sub-graph structures are also modeled to help reflect the quality of candidate summaries. Moreover, the summary is considered as a whole unit, so SgSum directly outputs the final summary in the form of sub-graph. By capturing relations between sentences and evaluating sum- mary as a sub-graph, our framework can generate more informative and coherent summaries com- pared with traditional extractive MDS methods. We evaluate SgSum on two MDS datasets with several types of graphs which all significantly im- prove the MDS performance. Besides, the hu- man evaluation results demonstrate that SgSum can obtain more coherent and informative summaries compared with traditional MDS methods. Moreover, the experimental results also indicate that Sg- Sum has strong power on transfer ability when only trained on single-document data. It performs much better than several strong MDS baselines including supervised and unsupervised models. The contributions of our work are as follows: We propose a novel framework called SgSum which transforms MDS task into the problem of sub-graph selection. The framework lever- ages graph to capture relations between sen- tences, and generates more informative and coherent summaries by modeling sub-graph structures. Due to the graph-based multi-document en- coder, our framework unifies single and multi- document summarization and has strong trans- fer ability from SDS to MDS task without any parallel MDS training data. Thus, it can re- duce the resource bottleneck in MDS tasks. Our model is general to several well-known graph representations. We experiment with similarity graph, topic graph and discourse graph on two benchmark MDS datasets. Re- sults show that SgSum has achieved superior performance compared with strong baselines. Conclusion We propose a novel framework SgSum which trans- forms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by modelling both the graph structure of the whole document set and the candidate sub- graphs, then directly output an integrate summary in the form of sub-graph which is more informa- tive and coherent. Experimental results on two MDS datasets show that SgSum brings substantial improvements over several strong baselines. Moreover, the proposed architecture has strong transfer ability from single to multi-document, which can reduce the resource bottleneck in MDS tasks.",1
"Abstract Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sen- tences one by one to compose a summary, which have two main drawbacks: (1) neglect- ing both the intra and cross-document relations between sentences; (2) neglecting the coher- ence and conciseness of the whole summary. In this paper, we propose a novel MDS frame- work (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly out- puts an integrate summary in the form of sub- graph which is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architec- ture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks. Introduction Currently, most extractive models treat summariza- tion as a sequence labeling task. They score and select sentences one by one (Zhong et al., 2020). These models (called sentence-level extractors) do not consider summary as a whole but a combina- tion of independent sentences. This may cause incoherent and redundant problem, and result in a poor summary even if the summary consists of high score sentences. Some works (Wan et al., 2015; Zhong et al., 2020) treat summary as a whole unit and try to solve the weakness of sentence- level extractors by using a summary-level extrac- tor. However, these models neglect the intra and cross-document relations between sentences which also have benefits for extracting salient sentences, detecting redundancy and generating overall coher- ent summaries. Relations become more necessary when input source documents are much longer and more complex such as multi-document input. In this paper, we propose a novel MDS frame- work called SgSum which formulates the MDS task as a sub-graph selection problem. In our frame- work, source documents are regarded as a relation graph of sentences (e.g., similarity graph or dis- course graph) and the candidate summaries are its sub-graphs. In this view, how to generate a good summary becomes how to select a proper sub- graph. In our framework, the whole graph structure is modeled to help extract salient information from source documents and the sub-graph structures are also modeled to help reflect the quality of candidate summaries. Moreover, the summary is considered as a whole unit, so SgSum directly outputs the final summary in the form of sub-graph. By capturing relations between sentences and evaluating sum- mary as a sub-graph, our framework can generate more informative and coherent summaries com- pared with traditional extractive MDS methods. We evaluate SgSum on two MDS datasets with several types of graphs which all significantly im- prove the MDS performance. Besides, the hu- man evaluation results demonstrate that SgSum can obtain more coherent and informative summaries compared with traditional MDS methods. Moreover, the experimental results also indicate that Sg- Sum has strong power on transfer ability when only trained on single-document data. It performs much better than several strong MDS baselines including supervised and unsupervised models. The contributions of our work are as follows: We propose a novel framework called SgSum which transforms MDS task into the problem of sub-graph selection. The framework lever- ages graph to capture relations between sen- tences, and generates more informative and coherent summaries by modeling sub-graph structures. Due to the graph-based multi-document en- coder, our framework unifies single and multi- document summarization and has strong trans- fer ability from SDS to MDS task without any parallel MDS training data. Thus, it can re- duce the resource bottleneck in MDS tasks. Our model is general to several well-known graph representations. We experiment with similarity graph, topic graph and discourse graphtwo benchmark MDS datasets. Re- sults show that SgSum has achieved superior performance compared with strong baselines. Conclusion We propose a novel framework SgSum which trans- forms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by modelling both the graph structure of the whole document set and the candidate summaries. By capturing relations between sentences individually and extracting salient sen- tences from among them, SgSum can extract salient sen- tences from among the whole document set and compose a summary. Re- sults show that SgSum has achieved superior performance compared with strong baselines.",0
"Abstract Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Cre- ating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (∼40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience’s laughter. The normalized duration (laughter duration divided by the clip duration) of laugh- ter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by compar- ing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that pro- vides a “funniness” score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our “Open Mic” dataset is re- leased for further research along with the code.  Introduction Humour is one of the most important lubricants of communication between people. Humour is subjec- tive and, at times, also requires cultural knowledge as humour is often dependent on stereotypes in a culture or a country. At times, even cultural ap- propriation is used to convey humour, which can be offensive to minority cultures (Rosenthal et al., 2015; Kuipers, 2017). The factors listed above, along with the underlying subjectivity in humour render the task of rating humour, difficult for ma- chines (Meaney, 2020). The task of humour classi- fication suffers due to this subjectivity and the lack of datasets that rate the “funniness” of content. In this paper, we propose rating humour on a scale of zero to four. We create the first multi- modal dataset2 using standup comedy clips and compute the humour quotient of each clip using the audience laughter. The validity of our scoring criteria is verified by finding the overall agreement between human annotation and automated scores. We use the audio and text-based signals to process this multi-modal data to generate ‘humour ratings’. Since humour annotation is subjective, even the data annotated by humans might not provide an objective measure. We reduce this subjectivity by taking laughter feedback from a larger audience. To the best of our knowledge, no previous literature has proposed an automatically humour-rated multi- modal dataset and used it in ML model-building to automatically obtain the humor score. Standup comedy is an art form where the deliv- ery of humour has a much larger context, and there are multiple jokes and multiple related punchlines in the same story. The resulting laughter from the audience depends on various factors, including the understanding of the context, delivery, and tonality of the comic. Standup comedy seems to be an ideal choice for a humour rating dataset as it inherently contains some feedback in terms of the audience laughter. We believe a smaller context window re- stricts computational models, but we know this is not the case for the human audience. Hence, our approach utilises live audience laughter as a mea- sure to rate the humour quotient in the data created. We also believe that such an approach can gener- ate insights into what aspects of stories and their delivery make them funny. Our humour rating model is partly inspired by the character “TARS” from the movie “Interstel- lar”, which generates funny responses based on adjustable humour setting (Nolan, 2014). An es- sential step in developing such a machine that can adjust its “funniness” is to create a model that can recognize and rate the “funniness” of a joke. With this work, we aim to release a dataset that can help researchers shed light on the humour quotient of a particular text. The key contributions of this paper are: (a) Creation and public release of an automatically rated multi-modal dataset based on English standup comedy clips and (b) Manual eval- uation of this dataset along with humour-rating quotient defined on a Likert-scale (Likert, 1932). Conclusion and Future Work We propose a novel scoring mechanism to show that humour rating can be automated using audi- ence laughter, which concurs well with the humour perception of humans. We create a multi-modal (audio & text) dataset for the task of humour rating. With the help of three human annotators, we man- ually evaluate our scoring mechanism and show a substantial agreement in terms of QWK. Our eval- uation shows that our scoring mechanism can be emulated with the help of pre-existing language models and traditional audio features. Our neu- ral network-based experiments show that the out- put obtained using various language models like RoBERTa show an agreement with our scoring mechanism. Despite the inherent subjectivity in hu- mour and its different perceptions among humans, we propose a method to rate humour and release this dataset under the CC-BY-SA-NC 4.0 license for further research. In the future, we would like to evaluate this scor- ing mechanism with the help of more human an- notators. We aim to extend the dataset with the help of more standup comedy clips. Further experi- ments can be conducted to compare the contribu- tion of audio, video and text features with a more detailed analysis. We would also like to perform experiments by including more audio features like Line Spectral Frequencies, Zero-Crossing rate, and Delta Coefficients. With the release of this dataset, we hope that research in computational humour can be taken further.",1
"Abstract Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Cre- ating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (ÃÂ¢ÃÂÃÂ¼40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audienceÃ¢ÂÂs laughter. The normalized duration (laughter duration divided by the clip duration) of laugh- ter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by compar- ing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that pro- vides a Ã¢ÂÂfunninessÃ¢ÂÂ score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our Ã¢ÂÂOpen MicÃ¢ÂÂ dataset is re- leased for further research along with the code. Introduction Humour is one of the most important lubricants of communication between people. Humour is subjec- tive and, at times, also requires cultural knowledge as humour is often dependent on stereotypes in a culture or a country. At times, even cultural ap- propriation is used to convey humour, which can be offensive to minority cultures (Rosenthal et al., 2015; Kuipers, 2017). The factors listed above, along with the underlying subjectivity in humour render the task of rating humour, difficult for ma- chines (Meaney, 2020). The task of humour classi- fication suffers due to this subjectivity and the lack of datasets that rate the Ã¢ÂÂfunninessÃ¢ÂÂ of content. In this paper, we propose rating humour on a scale of zero to four. We create the first multi- modal dataset2 using standup comedy clips and compute the humour quotient of each clip using the audience laughter. The validity of our scoring criteria is verified by finding the overall agreement between human annotation and automated scores. We use the audio and text-based signals to process this multi-modal data to generate Ã¢ÂÂhumour ratingsÃ¢ÂÂ. Since humour annotation is subjective, even the data annotated by humans might not provide an objective measure. We reduce this subjectivity by taking laughter feedback from a larger audience. To the best of our knowledge, no previous literature has proposed an automatically humour-rated multi- modal dataset and used it in ML model-building to automatically obtain the humor score. Standup comedy is an art form where the deliv- ery of humour has a much larger context, and there are multiple jokes and multiple related punchlines in the same story. The resulting laughter from the audience depends on various factors, including the understanding of the context, delivery, and tonality of the comic. Standup comedy seems to be an ideal choice for a humour rating dataset as it inherently contains some feedback in terms of the audience laughter. We believe a smaller context window re- stricts computational models, but we know this is not the case for the human audience. Hence, our approach utilises live audience laughter as a mea- sure to rate the humour quotient in the data created. We also believe that such an approach can gener- ate insights into what aspects of stories and their delivery make them funny. Our humour rating model is partly inspired by the character Ã¢ÂÂTARSÃ¢ÂÂ from the movie Ã¢ÂÂInterstel- larÃ¢ÂÂ, which generates funny responses based on adjustable humour setting (Nolan, 2014). An es- sential step in developing such a machine that can adjust its Ã¢ÂÂfunninessÃ¢ÂÂ is to create a model that can recognize and rate the Ã¢ÂÂfunninessÃ¢ÂÂ of a joke. With this work, we aim to release a dataset that can help researchers shed light on the humour quotient of a particular text. The key contributions of this paper are: (a) Creation and public release of an automatically rated multi-modal dataset based on English standup comedy clips and (b) Manual eval- uation of this dataset along with humour-rating quotient defined on a Likert-scale (Likert, 1932). Conclusion and Future Work We propose a novel scoring mechanism to show that humour rating can be automated using audi- encewhich concurs well with the humour perception of humans. We create a multi-modal (audio & text) dataset for the task of humour rating. With the help of three human annotators, we man- ually evaluate our scoring mechanism and show a substantial agreement in terms of QWK. Our eval- uation shows that although the card-rating mechanism is robust, it is not smooth. We also show that the card-rating mechanism can be automated with the help of pre-existing language models and traditional audio features. Our approach utilises live audience laughter as a mea- sure to rate the humour quotient of a card and to annotate the training data with a humour quotient score using the audience laughter. We use this dataset to train a model that pro- vides a Ã¢ÂÂfunninessÃ¢ÂÂ score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our Ã¢ÂÂOpen MicÃ¢ÂÂ dataset is re- leased for further research along with the code. Introduction Humour is one of the most important lubricants of communication between people. Humour is subjec- tive and, at times, also requires cultural knowledge as humour is often dependent on stereotypes in a culture or a country.",0
"Abstract Neural machine learning models can success- fully model language that is similar to their training distribution, but they are highly sus- ceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Dis- tillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We apply our method to a GRU architecture, demonstrating improve- ments on multiple language modeling (LM) datasets. Introduction Neural language models have become the main component of modern natural language processing systems, with larger and larger models being used as feature extractors for downstream tasks (Devlin et al., 2019), as probability estimators for ranking and ensembling (Gulcehre et al., 2015) or as lan- guage generators (Bahdanau et al., 2015; Vaswani et al., 2017; Brown et al., 2020). Despite their success, neural machine learning models can suffer large performance degradation when they are applied to out-of-domain data which is substantially different than their training data (La- puschkin et al., 2019; Hupkes et al., 2019; Recht et al., 2019). Autoregressive language models used for generation also suffer from the related expo- sure bias problem (Ranzato et al., 2015): as the model is fed its own samples, deviations from the training distribution are amplified, and eventually for sufficiently long sequences the model generates completely abnormal text. Unlike the older statistical language models, Re- current LMs (RNNLMs) (Mikolov et al., 2010) and their successors Transformers LMs (Vaswani et al., 2017) can consider the entire prefix of a sentence when predicting or generating the next token. By being able to relate a very high-dimensional input to the output, these models can learn many sub- tle correlations which are highly useful as long as the input is in-distribution, unfortunately these correlations tend to be brittle to distribution shift, causing a model that depends on them to go astray. This phenomenon is known as ""shortcut learning"" (Geirhos et al., 2020) and it has been found to also occur in humans and animals, but it is especially prevalent in artificial neural networks. Research on this problem has explored models invariant or equivariant w.r.t. certain transforma- tions by means of compositional representations (Sabour et al., 2017; Soulos et al., 2019; Liu et al., 2020), causal modeling (Schölkopf et al., 2021), or both (Arjovsky et al., 2019; Krueger et al., 2020), but these works focus on classification tasks often on synthetic datasets and can’t be straightforwardly applied to black-box language models. Approaches specific to LMs have focused on robustness where the data domains are known and represented in the training data (Oren et al., 2019; Gerstenberger et al., 2020). In this work we propose a method that uses Random Network Distillation (RND) (Burda et al., 2018) to dynamically adapt the amount of context that the model relies upon during inference based on an estimate of how much this context is out-of- distribution (OOD). This way the model can still make use of all available context when operating within a familiar context space, exploiting long- distance weak correlations, but it reduces to a less expressive and more robust model when operating OOD, relying only on the strongest correlations. As a proof of concept we implement our ap- proach on a GRU recurrent language model (Cho et al., 2014). While Transformer decoders out- perform RNNs when trained on large training sets, RNNs remain competitive on smaller datasets (< 107 tokens) where OOD phenomena are easier to measure, furthermore they are easier to opti- mize, simplifying architecture and hyperparameter search. We evaluate our method on language modeling tasks on English datasets, obtaining improvements when evaluating on eight OOD domains.We re- port additional preliminary sequence-to-sequence results on Transformer-RNN models (Zhang et al., 2018) in appendix A. We leave extensions of our method to full Transformers as future research. Conclusions and future work We proposed a method to improve the robustness of language models to distribution shift caused by train/test domain mismatch. Our model con- tracts the RNN state based on an unsupervised out-of-distribution estimator in order to reduce the model dependency on weak long-distance correla- tions, which are useful in-distribution but tend to be spurious in out-of-distribution conditions. We obtain perplexity improvements on multiple out-of- domain test sets without substantial degradation on in-domain test sets. While our approach is based on Recurrent de- coders, its general principles may be applicable to other neural architectures. For instance, the self-attention heads of a Transformer might modulated by an OOD detector in order to avoid attending to out-of-distribution parts of a sentence. We antic- ipate that extending our method to these kind of models will be a promising research direction.",1
"Abstract Neural machine learning models can success- fully model language that is similar to their training distribution, but they are highly sus- ceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Dis- tillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We apply our method to a GRU architecture, demonstrating improve- ments on multiple language modeling (LM) datasets. Introduction Neural language models have become the main component of modern natural language processing systems, with larger and larger models being used as feature extractors for downstream tasks (Devlin et al., 2019), as probability estimators for ranking and ensembling (Gulcehre et al., 2015) or as lan- guage generators (Bahdanau et al., 2015; Vaswani et al., 2017; Brown et al., 2020). Despite their success, neural machine learning models can suffer large performance degradation when they are applied to out-of-domain data which is substantially different than their training data (La- puschkin et al., 2019; Hupkes et al., 2019; Recht et al., 2019). Autoregressive language models used for generation also suffer from the related expo- sure bias problem (Ranzato et al., 2015): as the model is fed its own samples, deviations from the training distribution are amplified, and eventually for sufficiently long sequences the model generates completely abnormal text. Unlike the older statistical language models, Re- current LMs (RNNLMs) (Mikolov et al., 2010) and their successors Transformers LMs (Vaswani et al., 2017) can consider the entire prefix of a sentence when predicting or generating the next token. By being able to relate a very high-dimensional input to the output, these models can learn many sub- tle correlations which are highly useful as long as the input is in-distribution, unfortunately these correlations tend to be brittle to distribution shift, causing a model that depends on them to go astray. This phenomenon is known as ""shortcut learning"" (Geirhos et al., 2020) and it has been found to also occur in humans and animals, but it is especially prevalent in artificial neural networks. Research on this problem has explored models invariant or equivariant w.r.t. certain transforma- tions by means of compositional representations (Sabour et al., 2017; Soulos et al., 2019; Liu et al., 2020), causal modeling (SchoÃÂlkopf et al., 2021), or both (Arjovsky et al., 2019; Krueger et al., 2020), but these works focus on classification tasks often on synthetic datasets and canÃ¢ÂÂt be straightforwardly applied to black-box language models. Approaches specific to LMs have focused on robustness where the data domains are known and represented in the training data (Oren et al., 2019; Gerstenberger et al., 2020). In this work we propose a method that uses Random Network Distillation (RND) (Burda et al., 2018) to dynamically adapt the amount of context that the model relies upon during inference based on an estimate of how much this context is out-of- distribution (OOD). This way the model can still make use of all available context when operating within a familiar context space, exploiting long- distance weak correlations, but it reduces to a less expressive and more robust model when operating OOD, relying only on the strongest correlations. As a proof of concept we implement our ap- proach on a GRU recurrent language model (Cho et al., 2014). While Transformer decoders out- perform RNNs when trained on large training sets, RNNs remain competitive on smaller datasets (< 107 tokens) where OOD phenomena are easier to measure, furthermore they are easier to opti- mize, simplifying architecture and hyperparameter search. We evaluate our method on language modeling tasks on English datasets, obtaining improvements when evaluating on eight OOD domains.We re- port additional preliminary sequence-to-sequence results on Transformer-RNN models (Zhang et al., 2018) in appendix A. We leave extensions of our method to fullas future research. Conclusions and future work We proposed a method to improve the robustness of language models to distribution shift caused by train/test domain mismatch. Our model con- tracts the RNN state based on an unsupervised out-of-distribution estimator in order to reduce the model dependency on weak long-distance correla- tions, which are useful in-distribution but are brittle to distribution shift, causing a model that depends on them to go astray. We propose a method based on OOD detection with Random Network Dis- tillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We apply our method to a GRU architecture, demonstrating improve- ments on multiple language modeling (LM) datasets.",0
"Abstract The fourth industrial revolution (4IR) is likely to have a substantial impact on the econ- omy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occu- pations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Re- searchers could validate our findings through further analyses with micro data, and our dataset can serve as a source for more complex labor market analyses. Further, we compared the 4IR ex- posure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. We show that a combination of 4IR exposure with other automation measures may provide additional in- sights. For example, near-term automation might be driven by non-4IR technologies. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations. These measures can provide valuable insights for future research in terms of overall automation potential. Our approach does not aspire to predict the share of automated jobs, but aims to reflect actual technology maturity (diffusion), which is not covered by the aforementioned indicators Arntz et al. (2020). For example, scores by Brynjolfsson & Mitchell (2017) are based on expert assessments of “what can machine learning do?” We use patent data as an indicator for technological progress; patents actually document what existing technology can currently do. The McKinsey Global Institute follows a similar objective and focuses on what actual automation might look like until 2030, acknowledging that there is a much higher automation potential in the long term (Manyika et al., 2017). They provide estimates of automation potentials per occupation, which they expect to be implemented until 2030. Linking patent data to occupation activities offers a direct indicator of the exposure of occu- pations to technology. There exist patent occupation mappings at an industry (Silverman, 2002) and occupation level (Kogan et al., 2020; Webb, 2019) which have been used for economic anal- yses (Mann & Pu ̈ttmann, 2017; Acemoglu et al., 2020). Webb (2019) found, for example, that exposure to previous automation technologies had a negative impact on employment at an occu- pation level, and Mann & Pu ̈ttmann (2017) identified an overall positive impact of automation patents on employment. We build on the approach of Kogan et al. (2020) and refine for improved accuracy and to account for task-level differences. Each occupation relates to several tasks, and technology ex- posure may vary among different tasks within an occupation (Brynjolfsson & Mitchell, 2017). The task level, as the “unit of work that produces output,” is a highly insightful level of detail for evaluating the impact of technologies on jobs (Acemoglu & Autor, 2011). Our approach has two main benefits. On the one hand, it allows accounting for a specific technology exposure for each task, which is ignored when looking at occupations as a whole. On the other hand, our task-level approach increases the accuracy of the mapping, as it identifies patents specific to each activity, rather than patents which have many words in common with the overall occupation description. For example, our approach might avoid associating a robot engineer mainly with robot patents in general (e.g., improved efficiency of assembly robots), but rather with patents which describe in- novations that help to “plan robot path”, “debug robot programs”, and “maintain robots.” Further, we introduce a measure of technology exposure; we therefore differentiate between technologies of the fourth industrial revolution (4IR patents) and other patents (non-4IR patents) for creating technology exposure scores. These scores indicate patent exposure at the task and occupation level. Our analysis includes patent data since 1970 and thus allows us to review developments over time, e.g., when 4IR technologies have been developed and how long it takes them to impact the labor market. Various researchers identify the lack of high-quality data on technological progress of key 4IR technologies as a key barrier to better understanding the impact of those technologies on the workforce (Frank et al., 2019b; Mitchell & Brynjolfsson, 2017). With this article, we address this issue by providing a mapping of patents to occupational tasks and introducing a 4IR technology exposure score per occupation. Conclusions and future work The aim of this paper is to better understand the exposure of occupations to technologies of the 4IR. Several existing indicators describe the theoretical automation potential or future exposure potential of occupations. We introduce an indicator reflecting actual technology diffusion, based on patent data. This paper presents a method for mapping patents to tasks and introduces an occupation and task-level indicator of exposure to patents of the 4IR (4IR exposure score). We refine existing approaches to better account for task-level differences in patent exposure and the context in which an activity is conducted (e.g., diagnose machine condition vs. diagnose patient condition). We therefore consider that this approach offers a highly valuable contribution towards mapping patents to tasks and occupations. Occupations with higher exposure scores may, for example, be more impacted by 4IR tech- nologies. The analysis shows that ratio of exposure to 4IR and non-4IR patents differs per oc- cupation. Occupations with many manual tasks, such as manufacturing and construction, have high non-4IR exposure and low 4IR exposure, whereas many non-manual occupations, such as finance and marketing occupations, have a higher ratio of 4IR exposure. The 4IR exposure score is also valuable as a complementary score to other technology or automation scores. For example, comparing theoretical and actual technology exposure can provide insights into which occupations might undergo changes through current technologies versus future diffusion. This direct measure of technological progress can provide highly valuable data for further ex- ploration of the impact of technological change on employment (Mitchell & Brynjolfsson, 2017) and may serve as a source for labor market analysis to explore impact patterns of technologies on jobs. We compared our 4IR exposure scores with labor market indicators and found that exposure to non-4IR patents is highest for medium-wage occupations, and that 4IR exposure is highest for high-wage occupations. Further, regression analysis showed a negative (concave) relation of 4IR exposure to job growth. Patent exposure 10 and 20 years ago showed higher coefficients on the impact on job growth than more recent patent exposure. The gap may reflect the time between invention and technology diffusion and is in line with findings of Kogan et al. (2020). Further analysis with micro data is required to confirm these findings. To estimate the overall impact on the labor market, more complex modeling is required, e.g., considering the effect of deepening of automation or capital accumulation. Acemoglu & Restrepo (2019) observed that different technologies may have different impact patterns on the labor markets. Therefore, differentiating between 4IR technologies may offer additional value for labor market analyses. Researchers can build on our mapping for technology- level analysis. On the one hand, we provide technology-specific exposure scores (e.g., CAD, augmented reality for surgery, and smart office technologies). On the other hand, our mapping of patents to tasks allows researchers to build any other exposure scores, such as robots, or breakthrough patents, as long as a patent technology mapping is available. Also, patent data is available at firm level and allows for time-varying measures. Our work provides an occupation (and task)-level indicator of 4IR patent exposure. Patents describe inventions, and not all inventions have an equal impact. Future work could thus fur- ther improve the indicator by accounting for a patent’s impact. The count of patent citations is frequently discussed as potential measure for novelty and social usefulness, but its validity is ambiguous (Strumsky & Lobo, 2015). Another approach is described by Kelly et al. (2020), who describe “breakthrough patents” which significantly differ in text content from previous patents and thus might have particularly high impact. Our approach builds on occupation and task description data provided by O*Net. We take advantage of its extensive and hierarchical descriptions of occupational activities and tasks. Fu- ture work could rely on additional information provided by O*Net. For example, at a task and occupation level, the dataset indicates which technologies and tools are used, such as word pro- cessing software or programmable logic controllers. Building on this information may provide information on inventions related specifically to labor augmentation. The O*Net database de- scribes occupations in the context of the US labor market. There exist concordance tables, which can help to use the patent occupation mapping in other contexts. These might provide additional accuracy to directly map patents to those regional occupation descriptions, if regional databases with similar hierarchical structures exist.",1
"Abstract The fourth industrial revolution (4IR) is likely to have a substantial impact on the econ- omy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occu- pations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Re- searchers could validate our findings through further analyses with micro data, and our dataset can serve as a source for more complex labor market analyses. Further, we compared the 4IR ex- posure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. We show that a combination of 4IR exposure with other automation measures may provide additional in- sights. For example, near-term automation might be driven by non-4IR technologies. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations. These measures can provide valuable insights for future research in terms of overall automation potential. Our approach does not aspire to predict the share of automated jobs, but aims to reflect actual technology maturity (diffusion), which is not covered by the aforementioned indicators Arntz et al. (2020). For example, scores by Brynjolfsson & Mitchell (2017) are based on expert assessments of Ã¢ÂÂwhat can machine learning do?Ã¢ÂÂ We use patent data as an indicator for technological progress; patents actually document what existing technology can currently do. The McKinsey Global Institute follows a similar objective and focuses on what actual automation might look like until 2030, acknowledging that there is a much higher automation potential in the long term (Manyika et al., 2017). They provide estimates of automation potentials per occupation, which they expect to be implemented until 2030. Linkingdata to occupation activities offers a direct indicator of the exposure of occu- pations to technology. There exist patent occupation mappings at an industry (Silverman, 2002) and occupation level (Kogan et al., 2020; Webb, 2019) which have been used for economic anal- yses (Mann & Pu ÃÂÃÂttmann, 2017; Acemoglu et al., 2020). Webb (2019) found, for example, that exposure to previous automation technologies had a negative impact on employment at an occu- pation level, and Mann & Pu (2017) identified an overall positive impact of automation patents on employment. We build on the approach of Kogan et al. (2020) and refine for improved accuracy and to account for task-level differences. Each occupation relates to several tasks, and technology ex- posure may vary among different tasks within an occupation (Brynjolfsson & Mitchell, 2017). The task level, as the Ã¢ÂÂunit of work that produces output,Ã¢ÂÂ is a highly insightful level of detail for evaluating the impact of technologies on jobs (Acemoglu & Autor, 2011). Our approach has two main benefits. On the one hand, it allows accounting for a specific technology exposure for each task, which is ignored when looking at occupations as a whole. On the other hand, our task-level approach increases the accuracy of the mapping, as it identifies patents specific to each activity, rather than patents which have many words in common with the overall occupation description. For example, our approach might avoid associating a robot engineer mainly with robot patents in general (e.g., improved efficiency of assembly robots), but rather with patents which describe in- novations that help to Ã¢ÂÂplan robot pathÃ¢ÂÂ, Ã¢ÂÂdebug robot programsÃ¢ÂÂ, and Ã¢ÂÂmaintain robots.Ã¢ÂÂ Further, we introduce a measure of technology exposure; we therefore differentiate between technologies of the fourth industrial revolution (4IR patents) and other patents (non-4IR patents) for creating technology exposure scores. These scores indicate patent exposure at the task and occupation level. Our analysis includes patent data since 1970 and thus allows us to review developments over time, e.g., when 4IR technologies have been developed and how long it takes them to impact the labor market. Various researchers identify the lack of high-quality data on technological progress of key 4IR technologies as a key barrier to better understanding the impact of those technologies on the workforce (Frank et al., 2019b; Mitchell & Brynjolfsson, 2017). With this article, we address this issue by providing a mapping of patents to occupational tasks and introducing a 4IR technology exposure score per occupation. Conclusions and future work The aim of this paper is to better understand the exposure of occupations to technologies of the 4IR. Several existing indicators describe the theoretical automation potential or future exposure potential of occupations. We introduce an indicator reflecting actual technology diffusion, based on patent data. This paper presents a method for mapping patents to tasks and introduces an occupation and task-level indicator of exposure to patents of the 4IR (4IR exposure score). We refine existing approaches to better account for task-level differences in patent exposure and the context in which an activity is conducted (e.g., diagnose machine condition vs. diagnose patient condition). We therefore consider that this approach offers a highly valuable contribution towards mapping patents to tasks and occupations. Occupations with higher exposure scores may, for example, be more impacted by 4IR tech- nologies. The analysis shows that ratio of exposure to 4IR and non-4IR patents differs per oc- cupation. Occupations with many manual tasks, such as manufacturing and construction, have high non-4IR exposure and low 4IR exposure, whereas many non-manual occupations, such as finance and marketing occupations, have a higher ratio of 4IR exposure. The 4IR exposure score is also valuable as a complementary score to other technology or automation scores. For example, comparing theoretical and actual technology exposure can provide insights into which occupations might undergo changes through current technologies versus future diffusion. This direct measure of technological progress can provide highly valuable data for further ex- ploration of the impact of technological change on employment (Mitchell & Brynjolfsson, 2017) and may serve as a source for labor market analysis to explore impact patterns of technologies on jobs. We compared our 4IR exposure scores with labor market indicators and found that exposure to non-4IR patents is highest for medium-wage occupations, and that 4IR exposure is highest for high-wage occupations. Further, regression analysis showed a negative (concave) relation of 4IR exposure to job growth. Patent exposure 10 and 20 years ago showed higher coefficients on the impact ongrowth than more recent patent exposure. The gap may reflect the time between invention and technology diffusion and is in line with findings of Kogan et al. (2020). Further analysis with micro data is required to confirm these findings. To estimate the overall impact on the labor market, more complex modeling is required, such as those described for robot patents. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations. These measures can provide valuable insights for future research in terms of overall automation potential.",0
"Abstract Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages re- mains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Ara- bic written using an extension of the Latin script, called NArabizi, found mostly on so- cial media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similari- ties with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African di- alectal Arabic. Its Latin transcription (Arabizi) dis- plays a high level of linguistic variability1, on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual mod- els(Mulleretal.,2020).2 To Tackle The Resource scarcity issue regarding Arabic dialects, Antoun et al. (2020) use BERT architecture (Devlin et al., 2019) to train a model on Arabic text to compare this approach to standard multilingual models. In- deed, Martin et al. (2020) show that fine-tuning a monolingual model leads to better results than fine-tuning a multilingual one, meaning that when fine-tuning is used there is no significant perfor- mance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single lan- guage and was not trained to handle the presence of multiple languages in the same sentence (code- switching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being lim- ited by a subword tokenization-based vocabu- lary when facing out-of-domain training data lan- guage, especially in high-variability noisy scenar- ios (El Boukkouri et al., 2020; Clark et al., 2021), even though Muller et al. (2020) demonstrated a positive effect for NArabizi when using target lan- guage data to fine-tune a multilingual language model on its own objective function before pre- training. Following a different approach, we investigate the use of a recently issued character-based lan- guage model (El Boukkouri et al., 2020) that was shown to display a remarkable robustness to lexi- cal variation and noise when facing a new distant domain, namely biomedical. The pipeline we de- veloped is simple and consists in fine-tuning this character-based model for several tasks in a noisy low-resource language scenario. We show that a character-based model trained on only 99k sen- tences of NArabizi and fined-tuned on a small tree- bank of the language leads to performance close to that obtained with the same architecture pre- trained on large multilingual and monolingual mod- els (mBERT and CamemBERT). Interestingly, we generalize this observation by using the same architecture on a much larger French user-generated Content treebank that ex- hibits similar language variability issues than NAra- bizi. In fact, pre-training a character-based model on 1% of the large-scale French instance of the multilingual corpus OSCAR leads to similar per- formance as a subword based model trained on the full corpus, showing that such character-based lan- guage model can reach similar performance levels and that the resulting models exhibit the same tol- erance to noise as their much larger BERT counter- parts. This demonstrates the value of such models in very scarce resource scenario. Our code and models are freely available. Discussion In this work, we evaluate the benefits of using a character-based model in low-resource scenarios. Our results show that training such a model from scratch on much fewer data gives similar perfor- mance to a multilingual BERT adapted to the lan- guage using the same amount of data. Overall, our observations confirm the findings of El Boukkouri et al. (2020) regarding the robust- ness to noise and misspellings of the Character- BERT model. We showed that the model has com- petitive performance on noisy French UGC data when trained on only a fraction of the OSCAR corpus compared to CamemBERT trained on the full corpus and when trained on corpora containing about 1M words in the extremely noisy and low- resource case of NArabizi. This is consistent with the findings of Martin et al. (2020) and Micheli et al. (2020), who showed that MLM could already learn a lot from pre-training on smaller data set. Extending this investigation by training on a larger amount of data could help to explore the ability of the model to handle highly variable noisy data. However, one could question the usefulness of such Character-BERT based models if small Bert- based models were available on the same domain. To build an answer to that question, we conducted a quick set of experiments comparing our char- acterBert model trained on 1% of Oscar with the off-the-shelf Camembert version trained on 4gb of the Oscar corpus French instance (2.38% of the full corpus) and which was shown to perform al- most as well as the full model (Martin et al., 2020) on many downstream tasks. Both models were fined-tuned according to our MODEL+Task archi- tecture on either the FSMB or the Sequoia treebank, allowing us to evaluate their in-domain and out-of- domain performance. Results on Table 8 confirm the effectiveness of our characterBert model with overall better results than CamemBERT4gb in the in-domain scenario and similar, if not slightly bet- ter in the out-of-domain scenario, except for the labeled attachment score (75.83 vs 75.39). The fact that CamemBERT4gb was trained on more than twice as much data and with 200k pre-training steps while the characterBert pre-training stopped below 20k steps probably explains this small dis- crepancy but further investigations are needed with a fully parallel setting where both characterBert and CamemBERT are pretrained on the same amount of data and the same hyper-parameters. The take- home message from this in-domain experiments is that CharacterBert seems to be able to better cap- ture at least some of the UGC idiosyncracies that are prevalent in the FSMB (Seddah et al., 2012b) than its Bert-based counterparts. This was also shown by Rosales Núñez et al. (2021) in the con- text of character-based neural machine translation. Interestingly, their results showed that transformer- based models with subword tokenization also ex- hibit strong robustness to a certain type of lex- ical noise. This behavior has been very recently demonstrated by (Itzhak and Levy, 2021) and could explain why the BERT-based models we tested per- formed so well in our experiments. The key seems to be relying on the ability of the subword distri- bution to model some forms of lexical variations. Much more experiments are needed to clearly in- vestigate in what circumstances, besides noisy and resource-scarce scenarios, characterBERT models bring in a decisive advantage. Our results are based on the evaluation of two low-level tasks. Therefore, it would be interesting to see if they can be generalized to other – e.g. more semantic – tasks, as additional experiments on model layers configuration showed that most of the important information is captured early in the layers of the model (cf. Appendix A). Regarding the specific case of Arabic dialects written in Arabizi, a recent BERT-based model have been pretrained on 7 millions Egyptian tweets and displayed effective results on a sentiment anal- ysis task (Baert et al., 2020). Another very recent model, at the date of writing, was pre-trained on 4 millions Algerian tweets and also demonstrated interesting results on sentiment analysis (Abdaoui et al., 2021). Unfortunately, the authors did not perform any experiments on the Narabizi data set, making thus the comparison with our work not straightforward. It would be of course interesting to evaluate the interoperability between these new data sets and the NArabizi resources we used to produce our models. Head to head comparisons between these models and ours could be of value of course but we believe that given the shortcomings of finding enough data to pretrain large models for dialects, it would be probably better to first consol- idate a large enough common pre-training data set and then work on model performance. We leave this for future work. We showed that CharacterBert models trained on very little data could provide an interesting alterna- tive to large multilingual and monolingual models in resource-scarce and noisy scenarios. This is why we release all the code, data and models to repro- duce our experiments, hoping our work will favor the rise of efficient robust NLP models for under- resourced languages, domains and dialects.",1
"Abstract Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages re- mains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Ara- bic written using an extension of the Latin script, called NArabizi, found mostly on so- cial media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similari- ties with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African di- alectal Arabic. Its Latin transcription (Arabizi) dis- plays a high level of linguistic variability1, on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual mod- els(Mulleretal.,2020).2 To Tackle The Resource scarcity issue regarding Arabic dialects, Antoun et al. (2020) use BERT architecture (Devlin et al., 2019) to train a model on Arabic text to compare this approach to standard multilingual models. In- deed, Martin et al. (2020) show that fine-tuning a monolingual model leads to better results than fine-tuning a multilingual one, meaning that when fine-tuning is used there is no significant perfor- mance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single lan- guage and was not trained to handle the presence of multiple languages in the same sentence (code- switching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being lim- ited by a subword tokenization-based vocabu- lary when facing out-of-domain training data lan- guage, especially in high-variability noisy scenar- ios (El Boukkouri et al., 2020; Clark et al., 2021), even though Muller et al. (2020) demonstrated a positive effect for NArabizi when using target lan- guage data to fine-tune a multilingual language model on its own objective function before pre- training. Following a different approach, we investigate the use of a recently issued character-based lan- guage model (El Boukkouri et al., 2020) that was shown to display a remarkable robustness to lexi- cal variation and noise when facing a new distant domain, namely biomedical. The pipeline we de- veloped is simpleconsists in fine-tuning this character-based model for several tasks in a noisy low-resource language scenario. We show that a character-based model trained on only 99k sen- tences of NArabizi and fined-tuned on a small tree- bank of the language leads to performance close to that obtained with the same architecture pre- trained on large multilingual and monolingual mod- els (mBERT and CamemBERT). Interestingly, we generalize this observation by using the same architecture on a much larger French user-generated Content treebank that ex- hibits similar language variability issues than NAra- bizi. In fact, pre-training a character-based model on 1% of the large-scale French instance of the multilingual corpus OSCAR leads to similar per- formance as a subword based model trained on the full corpus, showing that such character-based lan- guage model can reach similar performance levels and that the resulting models exhibit the same tol- erance to noise as their much larger BERT counter- parts. This demonstrates the value of such models in very scarce resource scenario. Our code and models are freely available. Discussion In this work, we evaluate the benefits of using a character-based model in low-resource scenarios. Our results show that training such a model from scratch on much fewer data gives similar perfor- mance to a multilingual BERT adapted to the lan- guage using the same amount of data. Overall, our observations confirm the findings of El Boukkouri et al. (2020) regarding the robust- ness to noise and misspellings of the Character- BERT model. We showed that the model has com- petitive performance on noisy French UGC data when trained on only a fraction of the OSCAR corpus compared to CamemBERT trained on the full corpus and when trained on corpora containing about 1M words in the extremely noisy and low- resource case of NArabizi. This is consistent with the findings of Martin et al. (2020) and Micheli et al. (2020), who showed that MLM could already learn a lot from pre-training on smaller data set. Extending this investigation by training on a larger amount of data could help to explore the ability of the model to handle highly variable noisy data. However, one could question the usefulness of such Character-BERT based models if small Bert- based models were available on the same domain. To build an answer to that question, we conducted a quick set of experiments comparing our char- acterBert model trained on 1% of Oscar with the off-the-shelf Camembert version trained on 4gb of the Oscar corpus French instance (2.38% of the full corpus) and which was shown to perform al- most as well as the full model (Martin et al., 2020) on many downstream tasks. Both models were fined-tuned according to our MODEL+Task archi- tecture on either the FSMB or the Sequoia treebank, allowing us to evaluate their in-domain and out-of- domain performance. Results on Table 8 confirm the effectiveness of our characterBert model with overall better results than CamemBERT4gb in the in-domain scenario and similar, if not slightly bet- ter in the out-of-domain scenario, except for the labeled attachment score (75.83 vs 75.39). The fact that CamemBERT4gb was trained on more than twice as much data and with 200k pre-training steps while the characterBert pre-training stopped below 20k steps probably explains this small dis- crepancy but further investigations are needed with a fully parallel setting where both characterBert and CamemBERT are pretrained on the same amount of data and the same hyper-parameters. The take- home message from this in-domain experiments is that CharacterBert seems to be able to better cap- ture at least some of the UGC idiosyncracies that are prevalent in the FSMB (Seddah et al., 2012b) than its Bert-based counterparts. This was also shown by Rosales NuÃÂnÃÂez et al. (2021) in the con- text of character-based neural machine translation. Interestingly, their results showed that transformer- based models with subword tokenization also ex- hibit strong robustness to a certain type of lex- ical noise. This behavior has been very recently demonstrated by (Itzhak and Levy, 2021) and could explain why the BERT-based models weper- formed so well in our experiments. The key seems to be relying on the ability of the subword distri- bution to model some forms of lexical variations. Much more experiments are needed to clearly in- vestigate in what circumstances, besides noisy and resource-scarce scenarios, characterBERT models bring in a decisive advantage. In this work, we fo- cus on North-African colloquial dialectal Ara- bic written using an extension of the Latin script, called NArabizi, found mostly on so- cial media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b).",0
"Abstract Self-supervised learning (SSL) achieves great suc- cess in speech recognition, while limited explo- ration has been attempted for other speech pro- cessing tasks. As speech signal contains multi- faceted information including speaker identity, paralinguistics, spoken content, etc., learning uni- versal representations for all speech tasks is chal- lenging. In this paper, we propose a new pre- trained model, WavLM, to solve full-stack down- stream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker iden- tity preservation. We first equip the Transformer structure with gated relative position bias to im- prove its capability on recognition tasks. For bet- ter speaker discrimination, we propose an utter- ance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extrac- tion. WavLM Large achieves state-of-the-art per- formance on the SUPERB benchmark, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index Terms— Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks. In the past, it has been infeasible to build such a general model, as dif- ferent tasks focus on different aspects of speech signals. For instance, speaker verification requires the network to learn the speaker characteristic regardless of the spoken con- tent, while speech recognition demands the network discard speaker characteristics and focus only on content informa- tion. Meanwhile, unlike verification and recognition tasks, speaker diarization and speech separation involve multiple speakers, which creates additional obstacles for learning general speech representations. Recent advances fueled by large-scale pre-trained models have changed the situation. Yang et al. (2021) proves the potential of pre-trained models on full-stack speech tasks by using the weighted sum of em- beddings from different layers.1 They find different layers contain information useful for different tasks. For instance, the hidden states of the top layers are useful for ASR, while the bottom layers are more effective for speaker verification. While exciting as a proof of concept, there are still some drawbacks in existing pre-trained models: 1) Current pre- trained models are unsatisfactory for multi-speaker tasks, such as speaker diarization and speech separation. Our ex- periments show that speech separation models trained on top of HuBERT (Hsu et al., 2021a), a top performed speech pre-trained model, achieve only marginal improvement compared with the models trained from scratch. This is mainly because the pre-training methods do not sufficiently enforce the speaker discrimination, and the training data contain only single-speaker audios. 2) Speech pre-training crucially relies on high quality and large quantities of unlabeled au- dios. The existing system utilizes Libri-Light (Kahn et al., 2020) as the main source, but the massive audiobook data mismatches the data in a real scenario and using it exclu- sively hurts the model performance when the acoustic char- acteristics of the downstream tasks are different from those of the audiobook. Hsu et al. (2021b) train wav2vec 2.0 (Baevski et al., 2020b) on larger and more diverse datasets, but there are still over 90% audio data derived from au- diobook. To eliminate the audiobook data bias, we try to gather data from different sources as much as possible in our experiments. In this paper, we present WavLM which learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks. WavLM is built based on the pre-training strategy of HuBERT, with three extensions for better speech char- acteristic modeling. 1) We add gated relative position bias (grep) (Chi et al., 2021) to the Transformer structure as the backbone, which improves model performance for ASR and keeps almost the same parameter number and training speed. Compared with the convolutional relative position embedding used in wav2vec 2.0 and HuBERT, the gates allow the relative position bias to be adjusted adaptively by conditioning on the current speech content. 2) To handle multi-speaker tasks, such as speaker diarization and speech separation, we propose an utterance-mixing training strat- egy, where partially overlapped signals are constructed to augment the training data, by mixing individual training samples with randomly selected speech pieces. 3) To fur- ther improve the model robustness and alleviate the data mismatch, we scale up unlabeled pre-training data to 94k hours of public audios. The dataset consists of 60k hours of Libri-Light, 10k hours of GigaSpeech (Chen et al., 2021a), and 24k hours of VoxPopuli (Wang et al., 2021a). The new dataset consists of training instances from different scenar- ios, such as podcasts, YouTube and European Parliament (EP) event recordings. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets.  WavLM achieves state-of-the-art performance on SU- PERB (Yang et al., 2021). The leaderboard screenshot is shown in Appendix A.4. WavLM Large outperforms HuBERT Large on all subtasks, and achieves an abso- lute 2.4 point improvement in overall evaluation. Even WavLM Base+, a 3 times smaller model, is better than HuBERT Large owing to our three modifications. Speaker verification is a task to verify the speaker identity from the voice characteristics. We select this task to evaluate the model capability of extracting speaker-related features. WavLM Large exceeds the well-known state-of-the-art (SOTA) system, ECAPA- TDNN (Desplanques et al., 2020), by a large mar- gin and achieves 0.431%, 0.538% and 1.154% EER (Equal Error Rate) on the three official trial lists of VoxCeleb1 (Nagrani et al., 2017). Speech separation is a classic multi-speaker task, which is the key to solving the cocktail party prob- lem. The task can evaluate the model capability of extracting multiple speech signals from a mixture of sounds. WavLM achieves SOTA performance on the speech separation LibriCSS benchmark (Chen et al., 2020), and significantly outperforms the previous Con- former model (Chen et al., 2021b) by a 27.7% relative word error rate (WER) reduction. Speakerdiarizationisatasktorecognize“whospoke when” from an input audio stream (Park et al., 2021). WavLM achieves SOTA performance on the CALL- HOME speaker diarization benchmark. Compared to the EEND-EDA clustering method (Horiguchi et al., 2021b), our model achieves a 12.6% diarization error rate reduction. Speech recognition requires the model to learn con- tent information, which is the main focus of the pre- vious SSL work. We evaluate our model in the Lib- riSpeech 960h setting. WavLM shows comparable performance to the wav2vec 2.0 and HuBERT, which achieves 1.8% and 3.2% WER on the test-clean and test-other sets, respectively. The contribution of the paper can be summarized as follows: 1) WavLM sheds light on a general pre-trained model for full stack speech processing tasks, in contrast to the pre- vious SSL works focusing on a group of similar tasks. 2) We propose simple but effective modifications to the exist- ing pre-trained models, which show general and consistent improvements across downstream tasks. 3) We scale-up self-supervised speech pre-training with more unlabeled data and longer training steps. 4) We achieve state-of-the- art results on the SUPERB benchmark, and significantly boost the performance for various speech processing tasks on their representative benchmarks, including speech separation, speaker verification, and speaker diarization. The models and code are released to facilitate future research. Conclusion We present WavLM, a large-scale pre-trained model with 94k hour audio as inputs, to solve full stack speech pro- cessing tasks. WavLM is built based on the pre-training strategy of the HuBERT with utterance mixing strategy and the grep structure change for the Transformer. WavLM updates state-of-the-art results on the SUPERB, as well as the representative testsets of speaker verification, speech separation, and speaker diarization. In contrast to previous SSL models, WavLM is not only effective for the ASR task, but also has the potential to become the next-generation backbone network for speaker related tasks. In the future, we would like to scale up the model size to in- crease the model capability, as previous work has shown the benefits from more parameters (Zhang et al., 2021). Mean- while, the model compression technique is also worth trying due to the time constraint and limited test time resource in real scenarios. It is also a promising direction to jointly learn text and speech representation in a self-supervised pre- training framework (Ao et al., 2021), as the huge amount of text data might increase the capability on speech content modeling.",1
"Abstract Self-supervised learning (SSL) achieves great suc- cess in speech recognition, while limited explo- ration has been attempted for other speech pro- cessing tasks. As speech signal contains multi- faceted information including speaker identity, paralinguistics, spoken content, etc., learning uni- versal representations for all speech tasks is chal- lenging. In this paper, we propose a new pre- trained model, WavLM, to solve full-stack down- stream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker iden- tity preservation. We first equip the Transformer structure with gated relative position bias to im- prove its capability on recognition tasks. For bet- ter speaker discrimination, we propose an utter- ance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extrac- tion. WavLM Large achieves state-of-the-art per- formance on the SUPERB benchmark, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index TermsÃ¢ÂÂ Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks. In the past, it has been infeasible to build such a general model, as dif- ferent tasks focus on different aspects of speech signals. For instance, speaker verification requires the network to learn the speaker characteristic regardless of the spoken con- tent, while speech recognition demands the network discard speaker characteristics and focus only on content informa- tion. Meanwhile, unlike verification and recognition tasks, speaker diarization and speech separation involve multiple speakers, which creates additional obstacles for learning general speech representations. Recent advances fueled by large-scale pre-trained models have changed the situation. Yang et al. (2021) proves the potential of pre-trained models on full-stack speech tasks by using the weighted sum of em- beddings from different layers.1 They find different layers contain information useful for different tasks. For instance, the hidden states of the top layers are useful for ASR, while the bottom layers are more effective for speaker verification. While exciting as a proof of concept, there are still some drawbacks in existing pre-trained models: 1) Current pre- trained models are unsatisfactory for multi-speaker tasks, such as speaker diarization and speech separation. Our ex- periments show that speech separation models trained on top of HuBERT (Hsu et al., 2021a), a top performed speech pre-trained model, achieve only marginal improvement compared with the models trained from scratch. This is mainly because the pre-training methods do not sufficiently enforce the speaker discrimination, and the training data contain only single-speaker audios. 2) Speech pre-training crucially relies on high quality and large quantities of unlabeled au- dios. The existing system utilizes Libri-Light (Kahn et al., 2020) as the main source, but the massive audiobook data mismatches the data in a real scenario and using it exclu- sively hurts the model performance when the acoustic char- acteristics of the downstream tasks are different from those of the audiobook. Hsu et al. (2021b) train wav2vec 2.0 (Baevski et al., 2020b) on larger and more diverse datasets, but there are still over 90%data derived from au- diobook. To eliminate the audiobook data bias, we try to gather data from different sources as much as possible in our experiments. In this paper, we present WavLM which learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks. WavLM is built based on the pre-training strategy of HuBERT, with three extensions for better speech char- acteristic modeling. 1) We add gated relative position bias (grep) (Chi et al., 2021) to the Transformer structure as the backbone, which improves model performance for ASR and keeps almost the same parameter number and training speed. Compared with the convolutional relative position embedding used in wav2vec 2.0 and HuBERT, the gates allow the relative position bias to be adjusted adaptively by conditioning on the current speech content. 2) To handle multi-speaker tasks, such as speaker diarization and speech separation, we propose an utterance-mixing training strat- egy, where partially overlapped signals are constructed to augment the training data, by mixing individual training samples with randomly selected speech pieces. 3) To fur- ther improve the model robustness and alleviate the data mismatch, we scale up unlabeled pre-training data to 94k hours of public audios. The dataset consists of 60k hours of Libri-Light, 10k hours of GigaSpeech (Chen et al., 2021a), and 24k hours of VoxPopuli (Wang et al., 2021a). The new dataset consists of training instances from different scenar- ios, such as podcasts, YouTube and European Parliament (EP) event recordings. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets. WavLM achieves state-of-the-art performance on SU- PERB (Yang et al., 2021). The leaderboard screenshot is shown in Appendix A.4. WavLM Large outperforms HuBERT Large on all subtasks, and achieves an abso- lute 2.4 point improvement in overall evaluation. Even WavLM Base+, a 3 times smaller model, is better than HuBERT Large owing to our three modifications. Speaker verification is a task to verify the speaker identity from the voice characteristics. We select this task to evaluate the model capability of extracting speaker-related features. WavLM Large exceeds the well-known state-of-the-art (SOTA) system, ECAPA- TDNN (Desplanques et al., 2020), by a large mar- gin and achieves 0.431%, 0.538% and 1.154% EER (Equal Error Rate) on the three official trial lists of VoxCeleb1 (Nagrani et al., 2017). Speech separation is a classic multi-speaker task, which is the key to solving the cocktail party prob- lem. The task can evaluate the model capability of extracting multiple speech signals from a mixture of sounds. WavLM achieves SOTA performance on the speech separation LibriCSS benchmark (Chen et al., 2020), and significantly outperforms the previous Con- former model (Chen et al., 2021b) by a 27.7% relative word error rate (WER) reduction. SpeakerdiarizationisatasktorecognizeÃ¢ÂÂwhospoke whenÃ¢ÂÂ from an input audio stream (Park et al., 2021). WavLM achieves SOTA performance on the CALL- HOME speaker diarization benchmark. Compared to the EEND-EDA clustering method (Horiguchi et al., 2021b), our model achieves a 12.6% diarization error rate reduction. Speech recognition requires the model to learn con- tent information, which is the main focus of the pre- vious SSL work. We evaluate our model in the Lib- riSpeech 960h setting. WavLM shows comparable performance to the wav2vec 2.0 and HuBERT, which achieves 1.8% and 3.2% WER on the test-clean and test-other sets, respectively. The contribution of the paper can be summarized as follows: 1) WavLM sheds light on a general pre-trained model for full stack speech processing tasks, in contrast to the pre- vious SSL works focusing on a group of similar tasks. 2) We propose simple but effective modifications to the exist- ing pre-trained models, which show general and consistentacross downstream tasks. 3) We scale-up self-supervised speech pre-training with more unlabeled data and longer training steps. 4) We achieve state-of-the- art results on the SUPERB benchmark, and significantly boost the performance for various speech processing tasks on their representative benchmarks, including speech separation, speaker verification, and speaker diarization. The pre-training methods used in the models are effective for various speech processing tasks, including speaker diarization and speech separation. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets. WavLM achieves state-of-the-art performance on SU- PERB (Supervised Learning Basecamp), and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index TermsÃ¢ÂÂ Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. ",0
"ABSTRACT Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi- automated fact-checking that can be used for either the general public but also useful for fact- checking organisations. FacTeR-Check enables retrieving fact-checked information, unchecked claims verification and tracking dangerous information over social media. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-checking tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI19-SP that is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering or context-aware sentences similarity evaluation. In this research, we leverage the most recent advances in Natural Language Processing to develop a semantic-aware multilingual Transformer-based architecture for semantic similarity evaluation, semi-automated fact-checking and tracking of information pieces in Online Social Networks. We present an architecture that, on the one hand, can help general public in checking the veracity of a claim (i.e. a tweet) through context-aware automated comparison against a databases of hoaxes. On the other hand, our proposal aims at providing useful tools for fact-checking organisations for tracking and monitoring hoaxes circulating in OSNs. To validate and to show the capabilities of the architecture proposed, we use the COVID-19 pandemic scenario in Spanish speaking countries. We manually selected 61 hoaxes related to Covid-19 and extracted related tweets using Twitter API. Our architecture allows to label the degree of entailment of these tweets with a hoax, providing a useful insight into the propagation of hoaxes in Spanish on Twitter throughout one year. In summary, this research presents the following contributions: A labelled dataset of Spanish tweets IDs with the degree of entailment against a list of 61 hoaxes. A context-aware multilingual semantic similarity method for searching hoaxes with high similarity to a given query. A Natural Language Inference model for semi-automated fact-checking. A deep insight of misinformation and disinformation circulating on Twitter related to Covid-19 in Spanish speaking countries. The remaining sections of this manuscript are organised as follows. Conclusion In this article we have proposed FacTr-Check to mitigate OSN misinformation. Our architecture proposes two pipelines, one for semi-automated verification of claims; another for tracking known hoaxes on social media. The pipelines share three modules: a semantic similarity module, a NLI module and a information retrieval module. Using semantic similarity to find related fact-checks and NLI to contrast the claim to reputable sources we are able to semi-automatically verify information. On the other hand, to track hoaxes, we retrieve tweets related to a hoax, filtering the most relevant tweets with semantic similarity and contrasting them with the original hoax, finding how this particular piece of misinformation has spread along a social media platform. While our validation has been limited to COVID-19 and Twitter we want to emphasise that our architecture is adaptable to other knowledge domains as well as other social networks. First we evaluate all three modules individually, then the modules are put together in both pipelines to test their joint performance. To begin with, the similarity module offers above average performance using multilingual models on the STS benchmark. The NLI module uses XLM-RoBERTa fine-tuned on XNLI and the SICK training dataset, which performs adequately on SICK test, offering similar results to state-of-the-art models, as well as offering multilingual capabilities. Finally, the information retrieval module is compared against KeyBERT and RAKE on a dataset of Spanish keywords from our gathered hoaxes. Using this architecture we build a dataset for misinformation detection using NLI in Spanish about COVID-19, as well as track a selection of hoaxes to analyse their spread. FacTr-Check proves to extract insightful information about the spread of many hoaxes, showing aggregate frequency peaks matching COVID-19 waves in Spain. Identified hoaxes have their own particular activity peaks, some have more longevity than others, others are used much more; they are extremely diverse in lifetime and popularity. FacTr-Check relies on external databases to operate, if a rumour reaches the verification pipeline, and there is no related fact-check retrievable on the topic, only similar articles will be retrieved. This means that the verification pipeline is as robust as the fact-check database. Alternatives may include composing a massive database of hoax embeddings, as well as a dynamic information retrieval process to detect new hoaxes and calculate their embeddings. The architecture has been tested on OSNs, meaning that it is blind to outside information such as news sites or other valuable sources of information. If a piece of disinformation is published outside of the OSN, it will be out of the scope of the tracking algorithm. Finally, information is varied, coming in many shapes and forms, including text but also audio, video or images; the verification and tracking pipeline can only work on textual data, meaning that there is room for building systems that support other formats.",1
"ABSTRACT Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi- automated fact-checking that can be used for either the general public but also useful for fact- checking organisations. FacTeR-Check enables retrieving fact-checked information, unchecked claims verification and tracking dangerous information over social media. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-checking tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI19-SP that is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering or context-aware sentences similarity evaluation. In this research, we leverage the most recent advances in Natural Language Processing to develop a semantic-aware multilingual Transformer-based architecture for semantic similarity evaluation, semi-automated fact-checking and tracking of information pieces in Online Social Networks. We present an architecture that, on the one hand, can help general public in checking the veracity of a claim (i.e. a tweet) through context-aware automated comparison against a databases of hoaxes. On the other hand, our proposal aims at providing useful tools for fact-checking organisations for tracking and monitoring hoaxes circulating in OSNs. To validate and to show the capabilities of the architecture proposed, we use the COVID-19 pandemic scenario in Spanish speaking countries. We manually selected 61 hoaxes related to Covid-19 and extracted related tweets using Twitter API. Our architecture allows to label the degree of entailment of these tweets with a hoax, providing a useful insight into the propagation of hoaxes in Spanish on Twitter throughout one year. In summary, this research presents the following contributions: A labelled dataset of Spanish tweets IDs with the degree of entailment against a list of 61 hoaxes. A context-aware multilingual semantic similarity method for searching hoaxes with high similarity to a given query. A Natural Language Inference model for semi-automated fact-checking. A deep insight of misinformation and disinformation circulating on Twitter related to Covid-19 in Spanish speaking countries. The remaining sections of this manuscriptorganised as follows. Conclusion In this article we have proposed FacTr-Check to mitigate OSN misinformation. Our architecture proposes two pipelines, one for semi-automated verification of claims; another for tracking known hoaxes on social media. The pipelines share three modules: a semantic similarity module, a NLI module and a information retrieval module. Using semantic similarity to find related fact pieces from Online Social Networks (OSNs), we are able to link the different modules in the semantic similarity module to finds related fact pieces from Online Social Networks. On the other hand, to track hoaxes, we retrieve hoaxes from Online Social Networks and finds related fact pieces from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering or context-aware sentences similarity evaluation. In this research, we leverage the most recent advances in Natural Language Processing to develop a semantic-aware multilingual Transformer-based architecture for semantic similarity evaluation, semi-automated fact-checking and tracking of information pieces in Online Social Networks. ",0
"Abstract Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One fac- tor that significantly affects the performance of NMT is the availability of high-quality paral- lel corpora. However, high-quality parallel cor- pora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently re- leased seven types of parallel corpora for Ko- rean. In this study, we conduct an in-depth ver- ification of the quality of corresponding par- allel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant ex- periments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictio- nary base. To the best of our knowledge, this study is the first to use LIWC to analyze par- allel corpora in the field of NMT. Our find- ings suggest the direction of further research toward obtaining the improved quality paral- lel corpora through our correlation analysis in LIWC and NMT performance. Introduction In recent years, the demand for machine translation (MT) systems has been continuously increasing and its importance is growing, especially for the industrial services. (Vieira et al., 2021; Zheng et al., 2019) Companies, such as Google, Facebook, Mi- crosoft, Amazon, and Unbabel continue to conduct research and formulate plans to commercialize ap- plications related to MT. From the late 1950s, numerous MT-related projects were proceeded by mainly focusing on rule-based and statistical-based approaches before the advent of deep learning technology. As deep learning based neural machine translation (NMT) was proposed and adopted to several researches, it has been gradually figured out that more supe- rior performance can be derived through NMT ap- proach (Bahdanau et al., 2014; Vaswani et al., 2017;Lample and Conneau, 2019; Song et al., 2019). Followed by the adoption of deep learning based technique, the improvements of computing power (e.g. GPU) and corresponding enhancement of par- allel processing accelerated the advancement of NMT. Recently, release of open source frameworks, such as Pytorch(Paszke et al., 2019), and lowered accessibility to the big data further facilitated vig- orous and diverse research. However, several issues considering the enhance- ment of the NMT system remain still. Represen- tatively, limitations in ensuring the quality of data is an unresolved issue. As have previously been studied, the quality of the training data is deeply related to the NMT performance (Park et al., 2020d, 2021b). The major problem is that the process of building a high-quality parallel corpus is time- consuming and expensive, and it is significantly difficult for low-resource languages, such as Ko- rean. Although data-augmentation techniques, such as back translation (Edunov et al., 2018) and copied translation (Currey et al., 2017) have been intro- duced, as the human supervision is generally mini- mized or excluded in the data generation process, the quality of such pseudo-generated parallel cor- pus cannot be guaranteed (Burlot and Yvon, 2019; Epaliyana et al., 2021). This restricted the usage of pseudo-generated parallel to complements of human-labeled gold parallel corpus, rather than its substitutes (Imankulova et al., 2017). For the alleviation of above limitations, numer- ous studies on the collection of high-quality train- ing data have been conducted, such as parallel cor- pus filtering (PCF) research and Data Dam project. PCF refers to a research field that aims to filter out low-quality noisy data (i.e. sentence pairs) residing in the parallel corpus, and improve the over- all quality of the corpus. PCF is currently being applied to various NMT studies and contributed to the advancement of the NMT systems (Koehn et al., 2019; Park et al., 2020c). While the amount of training data caused significant impact on the statistical-based MT approaches, the quality of data is treated as more important than the amount of data in general deep learning-based MT approaches (Khayrallah and Koehn, 2018; Koehn et al., 2020b). Moreover, Data Dam 1 projects for building high- quality parallel corpora nationally are in progress. In the Republic of Korea, a large number of paral- lel corpora is open to the public through AI-Hub 2, which is organized by the National Information Society Agency (NIA) (Park and Lim, 2020). Following these research trends, where the qual- ity is treated more importantly than the quantity in the data construction process, we analyzed the above Korean-English parallel corpus distributed by AI-Hub. Despite its sufficient amount of data, the quality of corresponding corpus has not been confirmed clearly. This may restrict the uncon- strained utilization of such corpus in adoption to the NMT model, as low quality data may degrade the overall performance. In this study, we conducted several quality verification experiments including Linguistic Inquiry and Word Count (LIWC) (Pen- nebaker et al., 2001; Tausczik and Pennebaker, 2010), and clarified the quality and characteris- tics of such corpus. By analyzing various factors that can affect NMT performance, we proposed a method that can be applied in future research using the analysis results. LIWC is a text-analysis tool that automatically analyzes the number of words in a sentence and classifies words with similar meanings and sen- timental characteristics. LIWC extracts various interpersonal variables related to clinical, social, physiological, cognitive, psychological, and de- velopmental contexts that cannot be detected us- ing previous text-analysis programs. Additionally, LIWC comprises a variety of features for analyzing text. LIWC generally used to recognize linguistic markers for mental health study in Psychopathol- ogy such as detecting Narcissism(Holtzman et al., 2019), schizophrenia(Bae et al., 2021), bipolar dis- order(Sekulic ́ et al., 2018). However, LIWC pro- vides various linguistic features, word count, gender bias and so on, so it can be used for various analyses. In this study, we use LIWC to analyze parallel corpora based on diverse properties. It is also first time to analyze corpus using LIWC. In addition, we conduct baseline translation ex- periments by training transformer-base model struc- ture (Vaswani et al., 2017) through all the parallel corpora given by AIhub. By analyzing MT per- formance of corresponding models, we propose further research directions on MT for the Korean language. The contributions of this study are as follows: For the first time, we conduct a deep data anal- ysis on AI-Hub data. To the best of our knowl- edge, this is the first time LIWC has been used to analyze corpora. This study acts as a mile- stone for further studies on NMT with respect to the Korean language. We conduct baseline translation experiments on all the data in the AI-Hub parallel corpus. Our experiments provide a foundation for fur- ther research on Korean-based NMT. We discovered that many factors might cause decreasing model performance, and we pro- vide the direction that those factors could be filtered through our correlation analysis be- tween LIWC and model performance. Conclusions In this work, we proceeded with a quality eval- uation of all the Korean-related parallel corpus, released by AI Hub. For the model-centric perfor- mance validation, we constructed a transformer based NMT model trained with each parallel cor- pus. Through quantitative and qualitative analysis of these NMT models, we point out some proba- ble limitations on constructing corpora. First, for learning NMT model well in specific field, the do- main corpora should contain various words and expressions in consideration of the excessive per- formance difference between domain and general corpora. Second, given the significant performance gap in terms of language direction, half of the paral- lel data to be built must be configured in the source language and the other half in the target language and then translated respectively. Away from the model-centric analysis, we en- couraged data-centric research through LIWC anal- ysis. We figured out the association between LIWC and model performance in terms of data filtering. Through this analysis, we suggested the direction of further work to improve model performance. The national level re-examination of the various standards and building processes should be made for the encouragement of AI data construction re- searches. In the future, we plan to investigate ef- ficient beam search strategies and new decoding methods by utilizing these AI Hub data. Also, to more accurately measure the model performance, we plan to build an official Korean-English test set.",1
"Abstract Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One fac- tor that significantly affects the performance of NMT is the availability of high-quality paral- lel corpora. However, high-quality parallel cor- pora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently re- leased seven types of parallel corpora for Ko- rean. In this study, we conduct an in-depth ver- ification of the quality of corresponding par- allel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant ex- periments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictio- nary base. To the best of our knowledge, this study is the first to use LIWC to analyze par- allel corpora in the field of NMT. Our find- ings suggest the direction of further research toward obtaining the improved quality paral- lel corpora through our correlation analysis in LIWC and NMT performance. Introduction In recent years, the demand for machine translation (MT) systems has been continuously increasing and its importance is growing, especially for the industrial services. (Vieira et al., 2021; Zheng et al., 2019) Companies, such as Google, Facebook, Mi- crosoft, Amazon, and Unbabel continue to conduct research and formulate plans to commercialize ap- plications related to MT. From the late 1950s, numerous MT-related projects were proceeded by mainly focusing on rule-based and statistical-based approaches before the advent of deep learning technology. As deep learning based neural machine translation (NMT) was proposed and adopted to several researches, it has been gradually figured out that more supe- rior performance can be derived through NMT ap- proach (Bahdanau et al., 2014; Vaswani et al., 2017;Lample and Conneau, 2019; Song et al., 2019). Followed by the adoption of deep learning based technique, the improvements of computing power (e.g. GPU) and corresponding enhancement of par- allel processing accelerated the advancement of NMT. Recently, release of open source frameworks, such as Pytorch(Paszke et al., 2019), and lowered accessibility to the big data further facilitated vig- orous and diverse research. However, several issues considering the enhance- ment of the NMT system remain still. Represen- tatively, limitations in ensuring the quality of data is an unresolved issue. As have previously been studied, the quality of the training data is deeply related to the NMT performance (Park et al., 2020d, 2021b). The major problem is that the process of building a high-quality parallel corpus is time- consuming and expensive, and it is significantly difficult for low-resource languages, such as Ko- rean. Although data-augmentation techniques, such as back translation (Edunov et al., 2018) and copied translation (Currey et al., 2017) have been intro- duced, as the human supervision is generally mini- mized or excluded in the data generation process, the quality of such pseudo-generated parallel cor- pus cannot be guaranteed (Burlot and Yvon, 2019; Epaliyana et al., 2021). This restricted the usage of pseudo-generated parallel to complements of human-labeled gold parallel corpus, rather than its substitutes (Imankulova et al., 2017). For the alleviation of above limitations, numer- ous studies on the collection of high-quality train- ing data have been conducted, such as parallel cor- pus filtering (PCF) research and Data Dam project. PCF refers to a research field that aims to filter out low-quality noisy data (i.e. sentence pairs) residing in the parallel corpus, and improve the over- all quality of the corpus. PCF is currently being applied to various NMT studies and contributed to the advancement of the NMT systems (Koehn et al., 2019; Park et al., 2020c). While the amount of training data caused significant impact on the statistical-based MT approaches, the quality of data is treated as more important than the amount of data in general deep learning-based MT approaches (Khayrallah and Koehn, 2018; Koehn et al., 2020b). Moreover, Data Dam 1 projectsbuilding high- quality parallel corpora nationally are in progress. In the Republic of Korea, a large number of paral- lel corpora is open to the public through AI-Hub 2, which is organized by the National Information Society Agency (NIA) (Park and Lim, 2020). Following these research trends, where the qual- ity is treated more importantly than the quantity in the process, we analyzed the above Korean-English parallel corpus distributed by AI-Hub. Despite its sufficient amount of data, the quality of corresponding corpus has not been confirmed clearly. This may restrict the uncon- strained utilization of such corpus in adoption to the NMT model, as low quality data may degrade the overall performance. In this study, we conducted several quality verification experiments including Linguistic Inquiry and Word Count (LIWC) (Pen- nebaker et al., 2001; Tausczik and Pennebaker, 2010), and clarified the quality and characteris- tics of such corpus. By analyzing various factors that can affect NMT performance, we proposed a method that can be applied in future research using the analysis results. LIWC is a text-analysis tool that automatically analyzes the number of words in a sentence and classifies words with similar meanings and sen- timental characteristics. LIWC extracts various interpersonal variables related to clinical, social, physiological, cognitive, psychological, and de- velopmental contexts that cannot be detected us- ing previous text-analysis programs. Additionally, LIWC comprises a variety of features for analyzing text. LIWC generally used to recognize linguistic markers for mental health study in Psychopathol- ogy such as detecting Narcissism(Holtzman et al., 2019), schizophrenia(Bae et al., 2021), bipolar dis- order(Sekulic ÃÂ et al., 2018). However, LIWC pro- vides various linguistic features, word count, gender bias and so on, so it can be used for various analyses. In this study, we use LIWC to analyze parallel corpora based on diverse properties. It is also first time to analyze corpus using LIWC. In addition, we conduct baseline translation ex- periments by training transformer-base model struc- ture (Vaswani et al., 2017) through all the parallel corpora given by AIhub. By analyzing MT per- formance of corresponding models, we propose further research directions on MT for the Korean language. The contributions of this study are as follows: For the first time, we conduct a deep data anal- ysis on AI-Hub data. To the best of our knowl- edge, this is the first time LIWC has been used to analyze corpora. This study acts as a mile- stone for further studies on NMT with respect to the Korean language. We conduct baseline translation experiments on all the data in the AI-Hub parallel corpus. Our experiments provide a foundation for fur- ther research on Korean-based NMT. We discovered that many factors might cause decreasing model performance, and we pro- vide the direction that those factors could be filtered through our correlation analysis be- tween LIWC and model performance. Conclusions In this work, we proceeded with a quality eval- uation of all the Korean-related parallel corpus, released by AI Hub. For the model-centric perfor- mance validation, we constructed a transformer based NMT model trained with each parallel cor- pus. Through quantitative and qualitative analysis of these NMT models, we point out some proba- ble limitations on constructing corpora. First, for learning NMT model well in specific field, the do- main corpora should contain various words and expressions in consideration of the excessive per- formance difference between domain and general corpora. Second, given the significant performance gap in terms of language direction, half of the paral- lel data to be built must be configured in the source language and the other half in the target language and then translated respectively. Away from the model-centric analysis, we en- couraged data-centric research through LIWC anal- ysis. We figured out the association between LIWC and model performance in terms of data filtering. Through this analysis, we suggested the direction of further work to improve model performance. The national level re-examination of the various standards and building processes should be made for the encouragement of AI data construction re- searches. In the future, we plan to investigate ef- ficient beam search strategies and new decoding methods by utilizing these AI Hub data. Also, to more accurately measure the model performance, we plan to build an official Korean-English test set.",0
"Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models have been fine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by performing transfer black-box adversarial attacks, where only the final decisions of victim models are accessi- ble, against three victim models on three bench- mark datasets. We use a local pre-trained language model to construct potential adversarial samples and then query the victim models for decisions. Ex- perimental results demonstrate the effectiveness of our framework and TPGD algorithm. Specifically, TPGD significantly outperforms all baseline meth- ods in terms of attack success rate, and produces more fluent and grammatical adversarial examples. To summarize, the main contributions of this paper are as follows: We propose a general gradient-based textual ad- versarial attack framework based on continuous perturbations, bridging the gap between CV and NLP on the study of adversarial attacks. Com- mon gradient-based attack methods in CV can be easily adapted to NLP within our framework. We propose a novel adversarial attack method called TPGD within our framework. We employ a local model to construct adversarial samples by iteratively perturbing its embedding layer through the gradient information, and accumu- lating these small perturbations to search for potential adversarial samples. We successfully handle the challenge of black- box attack where only the decisions of models are accessible, which is rarely investigated in NLP. Conclusion and Future Works In this paper, we propose a general framework to adapt gradient-based adversarial attack methods investigated in CV to NLP. In our framework, the problem of searching textual adversarial samples is transformed from the discrete text space to the embedding layer, where continuous gradient-based perturbations can be directly added to. The pertur- bations will be amplified in the forward propaga- tion process. Then a MLM head is employed to decode the final perturbed latent representations. With its extensive linguistic knowledge, the coher- ence and grammaticality of the adversary samples can be guaranteed. We instantiate our framework with TPGD, including the iterative perturbation process and the reconstruction process, to perform decision-based black-box attack. We conduct ex- haustive experiments to evaluate our framework and TPGD algorithm. Experimental results show the superiority of our method, especially in terms of attack success rate and adversarial samples quality. In the future, we will adapt other gradient-based methods in CV with our framework and explore to improve models’ robustness through adversarial training.",1
"Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models havefine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples.",0
"Abstract Predicting linearized Abstract Meaning Rep- resentation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Introduction The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the se- mantics of the sentence, with nodes representing concepts and edges representing their relations (Ba- narescu et al., 2013). Recent works utilizing pre- trained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These ap- proaches avoid explicit modeling of the graph struc- ture. Instead, they directly predict the linearized AMR graph treated as free text. While the use of pre-trained Transformer encoders is widely ex- tended in AMR parsing, the use of pre-trained Transformer decoders is recent and has shown to be very effective, maintaining current state-of-the-art results (Bevilacqua et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, pre- dicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapa- nipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, gen- erating the graph incrementally. They implicitly model graph structural constraints through transi- tions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequence- to-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to what degree they are complementary. To fully utilize the generation power of the pre-trained language models, we propose a transition system with a small set of basic actions – a generaliza- tion of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR gen- eration (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with linearized graphs, we modify the model structure to work with our transition system, and encode parser states in BART’s attention mechanism (Astudillo et al., 2020; Zhou et al., 2021). We also explore dif- ferent vocabulary strategies for action generation. These changes convert the pre-trained BART to a transition-based parser where graph constraints and alignments are internalized. We provide a detailed comparison with top- performing AMR parsers and perform ablation ex- periments showing that our proposed transition sys- tem and BART modifications are both necessary to achieve strong performance. Although BART has great language generation capacity, it still benefits from parser state encoding with hard attention, and can efficiently learn structural output. Our model establishes a new state of the art for AMR 2.0 while maintaining graph well-formedness guarantees and producing built-in alignments. Conclusion We explore the integration of pre-trained sequence- to-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the high performance of the former and structural advantages of the latter. We show that both approaches are complementary, establishing the new state of the art for AMR 2.0. Our results indicate that instead of simply converting the struc- tured data into unstructured sequences to fit the need of the pre-trained model, it is possible to ef- fectively re-purpose a generic pre-trained model to a structure-aware one achieving strong perfor- mance. Similar principles can be applied to adapt other powerful pre-trained models such as T5 (Raf- fel et al., 2019) and GPT-2 (Radford et al., 2019) for structured data predictions. It is worth exploring thoroughly the pros and cons of introducing struc- ture to the model compared to removing structure from the data (linearization) in various scenarios.",1
"Abstract Predicting linearized Abstract Meaning Rep- resentation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Introduction The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the se- mantics of the sentence, with nodes representing concepts and edges representing their relations (Ba- narescu et al., 2013). Recent works utilizing pre- trained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These ap- proaches avoid explicit modeling of the graph struc- ture. Instead, they directly predict the linearized AMR graph treated as free text. While the use of pre-trained Transformer encoders is widely ex- tended in AMR parsing, the use of pre-trained Transformer decoders is recent and has shown to be very effective, maintaining current state-of-the-art results (Bevilacqua et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, pre- dicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapa- nipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, gen- erating the graph incrementally. They implicitly model graph structural constraints through transi- tions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequence- to-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to what degree they are complementary. To fully utilize the generation power of the pre-trained language models, we propose a transition system with a small set of basic actions Ã¢ÂÂ a generaliza- tion of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR gen- eration (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with linearized graphs, we modify the model structure to work with our transition system, and encode parser states in BARTÃ¢ÂÂs attention mechanism (Astudillo et al., 2020; Zhou et al., 2021). We also explore dif- ferent vocabulary strategies for action generation. These changes convert the pre-trained BART to a transition-based parser where graph constraints and alignments are internalized. We provide a detailed comparison with top- performing AMR parsers and perform ablation ex- periments showing that our proposed transition sys- tem and BART modifications are both necessary to achieve strong performance. Although BART has great language generation capacity, it still benefits fromstate encoding with hard attention, and can efficiently learn structural output. Our model establishes a new state of the art for AMR 2.0 while maintaining graph well-formedness guarantees and producing built-in alignments. Conclusion We explore the integration of pre-trained sequence- to-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Introduction The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the se- mantics of the sentence, with nodes representing concepts and edges representing their relations (Ba- narescu et al., 2013). Recent works utilizing pre- trained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These ap- proaches avoid explicit modeling of the graph struc- ture. Instead, they directly predict the linearized AMR graph treated as free text.",0
"Abstract Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense prop- erties of everyday concepts to detailed factual knowledge about named entities. Among oth- ers, this makes it possible to distill high-quality word vectors from pre-trained language mod- els. However, it is currently unclear to what ex- tent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically gen- erated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and re- lation classification (supervised) benchmarks, even without any task-specific fine-tuning. Introduction One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word em- beddings is nonetheless important. For instance, understanding lexical relations is an important pre- requisite for understanding the meaning of com- pound nouns (Turney, 2012). Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema matching (Fernandez et al., 2018), completion and re- trieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Ar- guello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), le- gal reasoning (Ashley, 1988; Walton, 2010), on- tology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008). Given the recent success of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), we may wonder whether such mod- els are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relation- ship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above. Since it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that re- lation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as “The relation between Paris and France is <mask>"". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is repre- sented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judge- ments that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of a different nature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is en- coded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. Related Work Bommasani et al., 2020; Vulic et al., 2020), and var- ious forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in par- ticular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input “Dante was born in <mask>” and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. “the place where Dante is born” is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ̨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge. We address this con- cern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Unsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation be- tween two nouns as the dependency path connect- ing them. Their view is that two such depen- dency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which re- lations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embed- dings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts. The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on dis- tributional models have been explored that rely on similar intuitions but go beyond simple vec- tor operations of word embeddings.2 For instance, Jameel et al. (2018) introduced a variant of the GloVe word embedding model, in which relation vectors are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), rela- tion vectors are computed by averaging the embed- dings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facil- itate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Si- mon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation ex- traction. In contrast, our focus in this paper is on characterising the overall relationship between two words. Conclusion We have proposed a strategy for learning relation embeddings, i.e. vector representations of pairs of words which capture their relationship. The main idea is to fine-tune a pre-trained language model us- ing the relational similarity dataset from SemEval 2012 Task 2, which covers a broad range of seman- tic relations. In our experimental results, we found the resulting relation embeddings to be of high qual- ity, outperforming state-of-the-art methods on sev- eral analogy and relation classification benchmarks. Among the models tested, we obtained the best re- sults with RoBERTa, when using manually defined templates for encoding word pairs. Importantly, we found that high-quality relation embeddings can be obtained even for relations that are unlike those from the SemEval dataset, such as morphological and encyclopedic relations. This suggests that the knowledge captured by our relation embeddings is largely distilled from the pre-trained language model, rather than being acquired during training.",1
"Abstract Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense prop- erties of everyday concepts to detailed factual knowledge about named entities. Among oth- ers, this makes it possible to distill high-quality word vectors from pre-trained language mod- els. However, it is currently unclear to what ex- tent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically gen- erated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and re- lation classification (supervised) benchmarks, even without any task-specific fine-tuning. Introduction One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word em- beddings is nonetheless important. For instance, understanding lexical relations is an important pre- requisite for understanding the meaning of com- pound nouns (Turney, 2012). Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema matching (Fernandez et al., 2018), completion and re- trieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Ar- guello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), le- gal reasoning (Ashley, 1988; Walton, 2010), on- tology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008). Given the recent success of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), we may wonder whether such mod- els are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relation- ship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above. Since it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that re- lation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as Ã¢ÂÂThe relation between Paris and France is <mask>"". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is repre- sented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judge- ments that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of anature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is en- coded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. Related Work Bommasani et al., 2020; Vulic et al., 2020), and var- ious forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in par- ticular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input Ã¢ÂÂDante was born in <mask>Ã¢ÂÂ and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. Ã¢ÂÂthe place where Dante is bornÃ¢ÂÂ is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ÃÂ¨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge. We address this con- cern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Unsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation be- tween two nouns as the dependency path connect- ing them. Their view is that two such depen- dency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which re- lations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embed- dings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts. The aforementioned worksrepresent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on dis- tributional models have been explored that rely on similar intuitions but go beyond simple vec- tor operations of word embeddings.2 For instance, Jameel et al. (2018) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input Ã¢ÂÂDante was born in <mask>Ã¢ÂÂ and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. the place where Dante is bornÃ¢ÂÂ is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ÃÂ¨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge.",0
"Abstract Natural Language Processing (NLP) is a branch of artificial intelli- gence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level ac- curacy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words con- sideration. Introduction Artificial intelligence had shown a great progress in the recent years especially the deep learning branch where learning techniques have been improved very quickly. The combination of representation learning and deep learning have allowed the emerging of a new AI class called deep reinforcement learning. Deep Reinforcement learning tend to estimate value functions from exper- iments and simulations and using dynamic programming through Deep Re- inforcement learning is an efficient way to build reactive strategies acting on instantaneous control. An algorithm which approves its performance by experi- ence is an algorithm capable of avoiding his own mistakes through a combination of a strong memory fed by fresh helpful data and the ability to keep winning pre- dictions after a long-term performance (Barto, Bradtke, & Singh, 1995) (Mnih et al., 2015). Neural Network can be considered as a dynamic Reinforcement Learning scheme where the layers are putted in a parallel way to have a cascaded trans- mission of the treated signal (Fukushima & Miyake, 1982) (LeCun et al., 1989) and where a prior knowledge is important to predict the output state of new observations. Sequential modeling is a way to process data in natural language processing by maximizing awards after manipulating situation and producing resulting actions (Vithayathil Varghese & Mahmoud, 2020) (LeCun, Bengio, & Hinton, 2015). A sequential model representation is influenced by its data representation and how tensors are trained to produce an optimal control (Bengio, Courville, & Vincent, 2013) To improve the target learning task, transfer learning is used as a powerful technique to increase the value of the most probable cases inside a state matrix (Boutsioukis, Partalas, & Vlahavas, 2011). Transferring the knowledge helps us to reduce the amount of data consumed and rely on feature engineering to reduce the noise caused by annotation errors and other tag-set anomalies in a context of multi-agent system. Extracting ingredients automatically from a recipe text is an extremely use- ful activity especially when we want to analyze a massive data of text recipes. Rule-Based methods were implemented to extract information from unstruc- tured recipe data (Silva, Ribeiro, & Ferreira, 2019) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, Bagrow, & Barab ́asi, 2011) who introduced the notion of Flavor Network and tried to verify the Food Paring hypothesis introduced on the 90’s by Heston Blumenthal and Francois Benzi. Flavor network as described by Ahn is a graph where the nodes are the ingredients extracted from recipes and weights are shared flavors between nodes. Food paring hypothesis is an indicator calculated after forming the Flavor Network to show if in a country or in a geographical part of the word we have tasty recipes or the ingredients do not have similar molecules. Tiago Simas (Simas, Ficek, Diaz-Guilera, Obrador, & Rodriguez, 2017) introduced the notion of food bridging formed with semi-metric distances. A group of scientists in a recent publication (Van Erp et al., 2021) devel- oped a state of the art of the use of artificial intelligence and natural language processing in analyzing food recipes. In this article we can found collected ref- erences talking about the challenging part in collecting food and recipe data. For example, Ahnert (Ahnert, 2013) presented the emergence of computational gastronomy in food science and its effect on culinary practices. Aiello and al (Aiello, Schifanella, Quercia, & Del Prete, 2019) discovered what are the most important predictors in food responsible of three diseases in a population sit- uated in London. (Amato & Cozzolino, 2020) extracted ingredients from food text to alert readers from allergens presence in a recipe. I agree with (Van Erp et al., 2021) concerning how challenging to use IA in food domain and how it will resolve issues concerning the creation of a data driven analysis of nutrition. In our paper data extracted can be used in a phone application or a recommended system for people who want to take care of their health. All previously cited researches on cuisine recipes need information extrac- tion from text recipe to use it on graphical visualization and statistical analysis. Information extraction can be used manually by extracting ingredients indi- cated on recipes or automatically. The problem in automatic extraction is that information should be precise to have also precise analysis, for example some ingredients take only one word and others can take two or three words. Another problem on automatic information extraction is that some ingredients that take one word have in common some words with other ingredients that have more than one word which make automatic information extraction more difficult. I tried to develop a mathematical model dedicated to extract ingredients from text recipe written in Arabic language with precision higher than what tradi- tional methods could make. According to Cutting (Cutting, Kupiec, Pedersen, & Sibun, 1992), a Tagger must be robust that should deal with unknown words, efficient that can deal with large corpora, accurate that can tag with high accu- racy, tunable that can deal with different corpora and reusable that take small efforts to re-target a new corpus. There are three types of POS Tagger: Taggers based on stochastic models, Taggers based on rules and Taggers Based on neural networks. On this work we will use Taggers based on HMM models. The use of POS tags as external features to solve NER problems was experimented by Zhou (Zhou & Su, 2002) but it was discarded because it showed bad results but our methodology and experiments demonstrate that using POS tags as external features is not a bad idea. This could be explained by the difference between our tokens and Zhou’s tokens: tokens as defined by Zhou is a pair of word-feature and in our model, token is only a word from our corpus. Conclusion Our Ingredient Extractor algorithm showed great results. It is based on HMM methods. We realized it by training two layers: first we trained tokens by tagging POS tags and second, we trained tokens by extracting the ingredients. Our HMM model needed modifications in iteration step because we didn’t get a square transition probability matrix or a square lexical probability matrix after the training step. A detailed iterations of our method is illustrated in appendix A. We can ameliorate our model by calculating the probabilities in it differently not as simple as we deed. We can make our model more interesting by adding two layers, one for extracting quantities and the other for extracting unities.",1
"Abstract Natural Language Processing (NLP) is a branch of artificial intelli- gence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level ac- curacy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words con- sideration. Introduction Artificial intelligence had shown a great progress in the recent years especially the deep learning branch where learning techniques have been improved very quickly. The combination of representation learning and deep learning have allowed the emerging of a new AI class called deep reinforcement learning. Deep Reinforcement learning tend to estimate value functions from exper- iments and simulations and using dynamic programming through Deep Re- inforcement learning is an efficient way to build reactive strategies acting on instantaneous control. An algorithm which approves its performance by experi- ence is an algorithm capable of avoiding his own mistakes through a combination of a strong memory fed by fresh helpful data and the ability to keep winning pre- dictions after a long-term performance (Barto, Bradtke, & Singh, 1995) (Mnih et al., 2015). Neural Network can be considered as a dynamic Reinforcement Learning scheme where the layers are putted in a parallel way to have a cascaded trans- mission of the treated signal (Fukushima & Miyake, 1982) (LeCun et al., 1989) and where a prior knowledge is important to predict the output state of new observations. Sequential modeling is a way to process data in natural language processing by maximizing awards after manipulating situation and producing resulting actions (Vithayathil Varghese & Mahmoud, 2020) (LeCun, Bengio, & Hinton, 2015). A sequential model representation is influenced by its data representation and how tensors are trained to produce an optimal control (Bengio, Courville, & Vincent, 2013) To improve the target learning task, transfer learning is used as a powerful technique to increase the value of the most probable cases inside a state matrix (Boutsioukis, Partalas, & Vlahavas, 2011). Transferring the knowledge helps us to reduce the amount of data consumed and rely on feature engineering to reduce the noise caused by annotation errors and other tag-set anomalies in a context of multi-agent system. Extracting ingredients automatically from a recipe text is an extremely use- ful activity especially when we want to analyze a massive data of text recipes. Rule-Based methods were implemented to extract information from unstruc- tured recipe data (Silva, Ribeiro, & Ferreira, 2019) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, Bagrow, & Barab ÃÂasi, 2011) who introduced the notion of Flavor Network and tried to verify the Food Paring hypothesis introduced on the 90Ã¢ÂÂs by Heston Blumenthal and Francois Benzi. Flavor network as described by Ahn is a graph where the nodes are the ingredients extracted from recipes and weights are shared flavors between nodes. Food paring hypothesis is an indicator calculated after forming the Flavor Network to show if in a country or in a geographical part of the word we have tasty recipes or the ingredients do not have similar molecules. Tiago Simas (Simas, Ficek, Diaz-Guilera, Obrador, & Rodriguez, 2017) introduced the notion of food bridging formed with semi-metric distances. A group of scientists in a recent publication (Van Erp et al., 2021)oped a state of the art of the use of artificial intelligence and natural language processing in analyzing food recipes. In this article we can found collected ref- erences talking about the challenging part in collecting food and recipe data. For example, Ahnert (Ahnert, 2013) presented the emergence of computational gastronomy in food science and its effect on culinary practices. (Amato & Cozzolino, 2017) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, 2013) who introduced the notion of Flavor Network and tried to verify the Food Paring hypothesis introduced on the 90Ã¢ÂÂs by Heston Blumenthal and Francois Benzi. Flavor network as described by Ahn is a graph where the nodes are the ingredients extracted from recipes and weights are shared flavors between nodes. Food paring hypothesis is an indicator calculated after forming the Flavor Network to show if in a country or in a geographical part of the word we have tasty recipes or the ingredients do not have similar molecules. Tiago Simas (Simas, Ficek, Diaz-Guilera, Obrador, & Rodriguez, 2017) introduced the notion of food bridging formed with semi-metric distances. A group of scientists in a recent publication (Van Erp et al., 2021) devel- oped a state of the art of the use of artificial intelligence and natural language processing in analyzing food recipes. In this article we can found collected ref- erences talking about the challenging part in collecting food and recipe data. For example, Ahnert (Ahnert, 2013) presented the emergence of computational gastronomy in food science and its effect on culinary practices. Sina Sajadmanesh (Sajadmanesh et al., 2017) presented the emergence of artificial intelligence and natural language processing in food science. I agree with (Van Erp et al., 2021) concerning how challenging to use IA in food domain and how it will resolve issues concerning the creation of a data driven analysis of nutrition. In our paper data extracted can be used in a phone application or a recommended system for people who want to take care of their health. All previously cited researches on cuisine recipes need information extrac- tion from text recipe to use it on graphical visualization and statistical analysis. Information extraction can be used manually by extracting ingredients indi- cated on recipes or automatically. The problem in automatic extraction is that information should be precise to have also precise analysis, for example some ingredients take only one word and others can take two or three words. Another problem on automatic information extraction is that some ingredients that take one word have in common some words with other ingredients that have more than one word which make automatic information extraction more difficult.",0
"ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to “universal” loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ́ 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates. A lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the “similarity” function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals. If this “similarity” function expresses the dot-product between embeddings, the InfoNCE loss is equivalent to the N-pair loss up to some constants. It was also shown that InfoNCE is equivalent to the Mutual Information Neural Estimator (MINE) up to a constant (Belghazi et al., 2018), whose minimization maximizes a lower bound on MI. Deep InfoMax (DIM) (Hjelm et al., 2019) improves MINE, and can be modified to incorporate some autoregression as InfoNCE. However, Tschannen et al. (2020) pointed out that the effectiveness of loss functions such as DIM and InfoNCE might be primarily connected not to deep metric learning but rather to MI. The idea gained a lot of popularity in Computer Vision with the advent of SimCLR (a Simple frame- work for Contrastive Learning of visual Representations), which introduced NT-Xent (normalized temperature-scaled cross-entropy loss) (Chen et al., 2020). It uses self-supervised learning, where augmentations of the same image are considered as positive examples and augmentations of dif- ferent images are used as negative examples. Thus, the task is as follows: for each example in a batch, find its paired positive augmentation. Here, the N-pairs loss is modified with a temperature parameter and with an L2 normalization of embeddings to the unit hypersphere. The loss was further extended for supervised learning as SupCon loss (Khosla et al., 2020), which aggregates all positive examples (from the same class) in the softmax numerator. Subsequently, these losses were introduced to the field of Natural Language Processing (NLP). Gunel et al. (2020) combined the SupCon loss with the cross-entropy loss and obtained state-of-the- art results for several downstream NLP tasks using RoBERTa. Giorgi et al. (2020) and Fang & Xie (2020) used NT-Xent to pre-train Transformers, considering spans sampled from the same document and sentences augmented with back-translation as positive examples. Luo et al. (2020) proposed to use NT-Xent in a self-supervised setting to learn noise-invariant sequence representations, where sentences augmented with masking were considered as positive examples. Finally, Gao et al. (2021) introduced the SimCLR loss to NLP under the name SimCSE (Simple Contrastive Learning of Sentence Embeddings), where sentences processed by a neural network with dropout served as augmentations of the original sentences. Here, we explore various ways to use a similar loss function for pairwise sentence scoring tasks. While the above-described loss functions have different names, they are all based on similar ideas. Below, we will use the name Batch-Softmax Contrastive (BSC) loss, which we believe reflects the main idea best. In our experiments below, we will use the “modern” variant of the loss: with tem- perature, normalization, and symmetrization components (described in more detail in Section 3.1). These components were not used for NLP in combination before. We further introduce a number of novel and important modifications in the definition of the loss and in the training procedure, which make it more efficient, and we show that using the resulting loss yields better task-specific sentence embeddings for pairwise sentence scoring tasks. CONCLUSION AND FUTURE WORK We explored the idea of using a batch-softmax contrastive loss for fine-tuning large-scale pre-trained transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduced and studied a number of variations in the calculation of the loss as well as in the overall training procedure. Our experimental results have shown sizable improvements on a number of datasets and pairwise sentence scoring tasks including ranking, classification, and regression. In future work, we want to explore new variations of the loss, and to gain better understanding of when to use which variation. We further plan experiments with a larger set of NLP tasks.",1
"ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to Ã¢ÂÂuniversalÃ¢ÂÂ loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ÃÂ 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates.lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the ÃÂ¢ÃÂÃÂsimilarityÃÂ¢ÃÂÃÂ function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals and signals from temporally nearby signals. If this function expresses the dot-product between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the ÃÂ¢ÃÂÃÂsimilarityÃÂ¢ÃÂÃÂ function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals and signals from temporally nearby signals. If this function expresses the dot-product between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Contrastive Estimation, called InfoCSE. It models the ÃÂ»Ã¢ÂÂ similarity between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Contrastive Learning, called InfoCLS. It models the ÃÂ»Ã¢ÂÂ similarity between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Neural Information Systems, called Nys. It models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a loss function based on Contrastive Learning, called NysST. It models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a triplet loss function, which models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a loss function based on L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Contrastive Learning, called Noise-Contrastive Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning, called Noise-Contrastive Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Class-Based Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presentedloss function based on Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes.",0
"Abstract We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner sim- ilar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environ- ment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments. Introduction In collaborative human-robot environments, embodied agents must be capable of integrating language commands into behavior. Natural language instruction can range from feedback on the subtlest of movements to abstract, high- level plans. While humans are generally capable of giving and receiving language feedback regarding all aspects of a task, methods for integrating language understanding into behavior differ based on the level of behavioral abstrac- tion the instruction refers to. Following Patel, Rodriguez- Sanchez, and Konidaris (2020), who argued that the struc- ture of language closely relates to the structure of an agent’s decision process, we focus on grounding adverbs—words used to describe the quality of a verb (i.e. a skill)—to di- rectly modify skill execution. We adopt the framework of hierarchical reinforcement learning (Barto and Mahadevan 2003), wherein an agent’s behavior is mainly generated by skills responsible for low- level motor control, and learning is primarily concerned with sequencing given skills to solve a task. Much existing re- search on integrating language understanding into hierarchi- cal agents attempts to map language to sequences of abstract skill executions (Andreas, Klein, and Levine 2017; Mei, Bansal, and Walter 2016; Oh et al. 2017). However, agents must also be able to use language to modify their underly- ing skill policies. Commands like “lift the pallet higher” and “crack the egg gently” clearly request adjustments to a spe- cific skill execution. Therefore, a key question is how natural language understanding can ground to changes in the lowest levels of behavior. The existence of adverbs that modify discrete verbs calls for agents with a discrete set of skills, with behavior that can be modified by a parameter vector describing how the skill can be executed (Da Silva, Konidaris, and Barto 2012; Masson, Ranchod, and Konidaris 2016). We propose a novel method for grounding adverbs to adjustments in skill parameters—called adverb-skill groundings—which, when integrated into policy search, lead to greater sample- efficiency than traditional policy search methods that typi- cally depend on explicit reward from the environment. We demonstrate the effectiveness of adverb-skill groundings for policy search in a toy ball-throwing domain and a domain involving a simulated 7-DoF robot arm. We compare the sample efficiency of our approach to PI2-CMA (Stulp and Sigaud 2012), a state-of-the-art local policy search method. Related Work Natural Language in Reinforcement Learning Most existing research that has used natural language in re- inforcement learning problems can be categorized as either language-conditional (in which agents must interact with language to solve problems) or language-assisted (in which language can be used to facilitate learning) (Luketina et al. 2019). Although our setting is language-conditional, since the agent is presumed only to have access to natural lan- guage feedback, there are related works in both categories. Some previous research has attempted to map language instructions to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modular sub-policies in the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2019) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed us- ing a separate model. Tellex et al. (2011) generate high-level plans from the semantic structure of language instructions. Gopalan et al. (2020) derive symbol sketches from demon- strated navigation trajectories which they ground language instructions to. While all of these works ground language to agent behavior, ours is the first to integrate language feed- back at the level of modifying the low-level behavior of motor skills. Semantic Representation While earlier semantic space representations were mainly concerned with encoding individual words or n-grams into vector space (Lund and Burgess 1996; Landauer and Du- mais 1997), there has been recent discussion regarding how to capture phrases and sentences with similar machinery. Mitchell and Lapata (2010) explore this problem, which hinges on linguistic structures as being compositional, i.e. that the meaning of a language fragment is a function of the meanings of its composite parts. Compositionality itself has been accounted for in older logic-based formalisms (Mon- tague 1974), but incorporating compositionality into modern semantic space representations is still an unsolved problem. Baroni and Zamparelli (2010) proposed a candidate so- lution that accounts for compositionality in semantic space models by representing nouns as vectors and adjectives as matrices, and the meaning of their combinations to be their tensor products. Krishnamurthy and Mitchell (2013) ex- pand on this idea by using Combinatory Categorial Gram- mar (CCG) (Steedman 1996) to prescribe tensors of various modes to syntactic categories, whose weights they learn via a training process that utilizes a corpus. While the primary focus of this research is not on semantic models, we firmly believe that core linguistic principles—such as the principle of compositionality— should be considered when designing systems for ground- ing language to behavior. Accordingly, we utilized the syn- tax/semantics formulation laid out by Steedman (1996) and the intuition behind more recent compositional distribu- tional semantics research (Baroni and Zamparelli 2010; Kr- ishnamurthy and Mitchell 2013) in our strategy for ground- ing adverbs. Adverbs by the CCG account are functions from verbs to verbs, and adverbs by our account are simi- larly functions from skills to skills. Conclusion We have presented a novel method for efficiently integrating granular natural language feedback into low-level behavior. The method relies on learning adverb-skill groundings— mappings of adverbs to adjustments in skill parameters— which can be learned once using few training examples and do not require the agent to interact directly with environ- ment reward. Using adverb-skill groundings, an agent can integrate adverb feedback into a policy search—in place of sample-based direct policy search methods—and achieve an order of magnitude increase in sample efficiency. This work can be extended in several directions. First, the ability of our model to ground adverbs is limited by the pa- rameterization of the skill. If no variation in skill parame- ters could result in the desired effect of an adverb, a new skill parameterization should be learned with more expres- sive power. Second, humans typically do not need to learn how to ground adverbs anew each time they learn a new skill, but our model does. Future work might consider the ability for agents to transfer adverb groundings to new skills. Another important question to address is how to learn lan- guage embeddings which can exclusively capture the mean- ings of language commands as they relate to specific tasks. Our embedding procedure was designed by a human expert with knowledge of the tasks and which adverbs most apply to it. Future work should look to relax this constraint, per- haps by defining a broad and exhaustive set of adverbs of motion which can be applied to any task.",1
"Abstract We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner sim- ilar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environ- ment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments. Introduction In collaborative human-robot environments, embodied agents must be capable of integrating language commands into behavior. Natural language instruction can range from feedback on the subtlest of movements to abstract, high- level plans. While humans are generally capable of giving and receiving language feedback regarding all aspects of a task, methods for integrating language understanding into behavior differ based on the level of behavioral abstrac- tion the instruction refers to. Following Patel, Rodriguez- Sanchez, and Konidaris (2020), who argued that the struc- ture of language closely relates to the structure of an agentÃ¢ÂÂs decision process, we focus on grounding adverbsÃ¢ÂÂwords used to describe the quality of a verb (i.e. a skill)Ã¢ÂÂto di- rectly modify skill execution. We adopt the framework of hierarchical reinforcement learning (Barto and Mahadevan 2003), wherein an agentÃ¢ÂÂs behavior is mainly generated by skills responsible for low- level motor control, and learning is primarily concerned with sequencing given skills to solve a task. Much existing re- search on integrating language understanding into hierarchi- cal agents attempts to map language to sequences of abstract skill executions (Andreas, Klein, and Levine 2017; Mei, Bansal, and Walter 2016; Oh et al. 2017). However, agents must also be able to use language to modify their underly- ing skill policies. Commands like Ã¢ÂÂlift the pallet higherÃ¢ÂÂ and Ã¢ÂÂcrack the egg gentlyÃ¢ÂÂ clearly request adjustments to a spe- cific skill execution. Therefore, a key question is how natural language understanding can ground to changes in the lowest levels of behavior. The existence of adverbs that modify discrete verbs calls for agents with a discrete set of skills, with behavior that can be modified by a parameter vector describing how the skill can be executed (Da Silva, Konidaris, and Barto 2012; Masson, Ranchod, and Konidaris 2016). We propose a novel method for grounding adverbs to adjustments in skill parametersÃ¢ÂÂcalled adverb-skill groundingsÃ¢ÂÂwhich, when integrated into policy search, lead to greater sample- efficiency than traditional policy search methods that typi- cally depend on explicit reward from the environment. We demonstrate the effectiveness of adverb-skill groundings for policy search in a toy ball-throwing domain and a domain involving a simulated 7-DoF robot arm. We compare the sample efficiency of our approach to PI2-CMA (Stulp and Sigaud 2012), a state-of-the-art local policy search method. Related Work Natural Language in Reinforcement Learning Most existing research that has used natural language in re- inforcement learning problems can be categorized as either language-conditional (in which agents must interact with language to solve problems) or language-assisted (in which language can be used to facilitate learning) (Luketina et al. 2019). Although our setting is language-conditional, since the agent is presumed only to have access to natural lan- guage feedback, there are related works in both categories. Some previous research has attempted to map language instructions to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modularin the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2019) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed us- ing a separate model. Tellex et al. (2011) generate high-level plans from the semantic structure of language instructions. Gopalan et al. (2020) derive symbol sketches from demon- strated navigation trajectories which they ground language instructions to. While all of these works ground language to agent behavior, ours is the first to integrate language feed- back at the level of modifying the low-level behavior of motor skills. Semantic Representation While earlier semantic space representations were mainly concerned with encoding individual words or n-grams into vector space (Lund and Burgess 1996; Landauer and Du- mais 1997), there has been recent discussion regarding how to capture phrases and sentences with similar machinery. Mitchell and Lapata (2010) explore this problem, which hinges on linguistic structures as being compositional, i.e. that the meaning of a language fragment is a function of the meanings of its composite parts. Compositionality itself has been accounted for in older logic-based formalisms (Mon- tague 1974), but incorporating compositionality into modern semantic space representations is still an unsolved problem. Baroni and Zamparelli (2010) proposed a candidate so- lution that accounts for compositionality in semantic space models by representing nouns as vectors and adjectives as matrices, and the meaning of their combinations to be their tensor products. Krishnamurthy and Mitchell (2013) ex- pand on this idea by using Combinatory Categorial Gram- mar (CCG) (Steedman 1996) to prescribe tensors of various modes to syntactic categories, whose weights they learn via a training process that utilizes a corpus. While the primary focus of this research is not on semantic models, we firmly believe that core linguistic principlesÃ¢ÂÂsuch as the principle of compositionalityÃ¢ÂÂ should be considered when designing systems for ground- ing language to behavior. Accordingly, we utilized the syn- tax/semantics formulation laid out by Steedman (1996) and the intuition behind more recent compositional distribu- tional semantics research (Baroni and Zamparelli 2010; Kr- ishnamurthy and Mitchell 2013) in our strategy for ground- ing adverbs. Adverbs by the CCG account are functions from verbs to verbs, and adverbs by our account are simi- larly functions from skills to skills. Conclusion We have presented a novel method for efficiently integrating granular natural language feedback into low-level behavior. The method relies on learning adverb-skill groundingsÃ¢ÂÂ mappings of adverbs to adjustments in skill parametersÃ¢ÂÂ which can be learned once using few training examples and do not require the agent to interact directly with environ- ment reward. Using adverb-skill groundings, an agent can integrate adverb feedback into a policy searchÃ¢ÂÂin place of sample-based direct policy search methodsÃ¢ÂÂand achieve an order of magnitude increase in sample efficiency. This work can be extended in several directions. First, the ability of our model to ground adverbs is limited by the pa- rameterization of the skill. If no variation in skill parame- ters could result in the desired effect of an adverb, a new skill parameterization should be learned with more expres- sive power. Second, humans typically do not need to learn how to ground adverbs anew each time they learn a new skill, but our model does. Future work might consider the ability for agents to transfer adverb groundings to new skills. Another important question to address is how to learn lan- guage embeddings which can exclusively capture the mean- ings of language commands as they relate to specific tasks. Our embedding procedure was designed by a human expert with knowledge of the tasks and which adverbs most apply to it. Future work should look to relax this constraint, per- haps by defining a broad and exhaustive set of adverbs of motion which can be applied to any task.",0
"ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67× and 1.15× faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowing these networks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these networks in resource-constrained environments like drones, mobile computers and IoT devices. As we introduce skip connections in the architecture to obtain a complex feature representation, the gradient flow during the backward pass improves. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network.Therefore, we expect BERMo architecture to be better candidate for compression and hence, we believe the proposed model could be ideal for resource-constrained settings. Contributions: The Contributions Of This Work Can Be Summarized As Follows: i  We propose BERMo, which generates complex feature maps using linear combination of features from different depths. We evaluate the proposed model on the probing task from SentEval dataset (Conneau et al. (2018)) and find our model performs 2.67% better than the baseline on the semantic tasks (Tense, Subjnum, Objnum, Somo, Coordinv) on average. ii  We observe our approach is stable when pruning with smaller datasets like SST-2 (Wang et al. (2018)), where BERT commonly diverges. iii  We show our model supports higher pruning rates when compressing and converges 1.67× and 1.15× faster than BERT on MNLI and QQP (Wang et al. (2018)), respectively. iv  Forlosspenaltybasedpruningmethodourapproachcanobtainbetterparameterefficiency, 1.35× for QQP, than BERT model for comparable performance. v  Our approach produces comparable results to the baseline for Knowledge Distillation with marginal improvements on SQuAD dataset. Outline: The rest of the paper is organised as follows: Section 2 de- scribes ELMo (Peters et al. (2018)), BERT (Devlin et al. (2019)) and Pruning methods. Section 3 elab- orates the proposed model. The experimental setup and the results are presented in Section 4. In Section 5 we summarize our work and discuss the future possibilities. Section 6 reports the related work and we conclude our paper with Broader Impact in Section 7. BROADER IMPACT Research presented in this work improves the baseline performance on semantic tasks. This work highlights the importance of adding skip connections to the network in improving the training con- vergence and stability. We believe this work would act as a stepping stone and motivate further research in this direction, reducing the training time for these models. These improved training speeds also make room for enlarging the dataset size generally correlated with improvements in generalization performance. Further, as this work deals with reducing the training time for prun- ing, a possible application would be online pruning on resource constrained setup. Moreover, from an environmental perspective reducing training time will reduce the carbon footprint of these large language models.",1
"ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67ÃÂ and 1.15ÃÂ faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowingnetworks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these enormous resources on demand. As we introduce skip connections in the BERT architecture, the result is a large memory footprint and enormous computational resources. These enormous resources make the BERT ideal for distributing resource bruntings across different network depths. We observe that our approach converges 1.67ÃÂ and 1.15ÃÂ faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation.",0
"Abstract Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources. Introduction The cost of language model training increases exponentially. Among different models, Transformerbased language models [Vaswani et al., 2017, Devlin et al., 2019, Liu et al., 2019, Lewis et al., 2020] are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity (O(n) where n is the input sequence length). Consequently, Transformers cannot support long sequence processing and large batch size with limited resources. The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020, Kitaev et al., 2020] or low-rank approximation [Choromanski et al., 2020, Wang et al., 2020]. However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible. It is also unclear in theory how to set the hyper-parameters of those methods to attain a desired level of approximation accuracy. Another issue of Transformers is the training instability that small perturbations in parameter updates tend to be amplified, resulting in significant disturbances in the model output [Liu et al., 2020a]. Transformers on some NLP tasks have shown to be sensitive to hyper-parameters, learning schedulers, or even random seeds, which usually demands a time-costly grid search for the best configuration in real-world applications. It has also been observed in our experiments that a slight change in the learning rate may cause the failure of convergence for some models. We conjecture that the instability in Transformer training comes from the softmax structure, as the un-normalized attention score matrices before softmax tend to have extremely large condition numbers due to its fast singular value decay. To alleviate the instability issue, an extra factor of 1/ √p in the softmax kernel SM is suggested by Vaswani et al. [2017] to restrain the scale variation; Liu et al. [2020a] proposes a new scheme to control the magnitude of output change and stabilize the training in early stages. In practice, we also need to consider the lower numerical precision of GPU implementation in model training, which further deteriorates the stability. Kernel methods may be the answer to both challenges. As pointed out by Choromanski et al. [2020], the softmax structure is closely related to Gaussian kernels up to diagonal matrix multiplications, as the pairwise dot products naturally appear when expanding the squared `2 distance. We further notice some important connections between self-attention and Gaussian kernels. First, the un-normalized attention score matrix can be formed via basic matrix operations on an empirical Gaussian kernel matrix. Moreover, the form of Gaussian kernels has the natural interpretation of assigning “attention” to different tokens. Compared to the softmax function, Gaussian kernels automatically perform the normalization as softmax does (c.f. Section 4.1). These observations motivate us to replace the softmax structure with Gaussian kernels. As we demonstrated in this paper, the new attention model, Kernelized Attention, empirically stabilizes the model training while being comparable to self-attention in model accuracy. To further improve the efficiency, we propose Skyformer (Symmetrization of Kernelized attention for NYström method) to accelerate kernelized attention. Skyformer adapts the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005] to the non-PSD empirical Gaussian kernel matrix (as query matrices in general do not equal to key matrices), by instead lifting the kernelized attention score matrix into a large PSD matrix that contains the un-normalized attention score matrix as the off-diagonal block. We further conduct theoretical analysis by showing that Skyformer has a small matrix approximation error on kernelized attention in the spectral norm. Our experiments on the LRA benchmark show that Skyformer consistently uses less space and time while achieving better accuracy than other baseline methods. In summary, our main contributions are: (1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers. (2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the Nyström method to a non-PSD matrix. We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. (3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs.  Related Work Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads [Voita et al., 2019, Michel et al., 2019] and model size reduction with knowledge distillation [Jiao et al., 2020, Tang et al., 2019, Liu et al., 2020b], we focus on attention approximation models, which are closely related to kernel methods. To reduce the time and space complexity by avoiding exhaustive computation over the attention metric, recent studies propose to apply sparse attention patterns to limit the numbers of elements participating in matrix multiplications [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020]. Beyond limiting the attention to fixed patterns, some approaches learn the patterns by determining token assignments to relevant groups [Kitaev et al., 2020, Roy et al., 2021]. Those models utilize local and global information in the attention score matrix to perform approximation, which coincides with the attempt to accelerate the computation in Gaussian processes [Snelson and Ghahramani, 2007]. The attention score matrix is known to exhibit a very fast rate of singular value decay [Bhojanapalli et al., 2020, Dong et al., 2021], similar to that of an empirical kernel matrix [Yang et al., 2017]. This near singular property motivates many low-rank attention approximation methods to skillfully leverage the computation techniques in kernel methods. Among them, Linformer [Wang et al., 2020] compresses the size of the key and value matrix with random projections based on the Johnson–Lindenstrauss transform, a common randomized sketching method in Gaussian processes [Yang et al., 2017]; Reformer [Kitaev et al., 2020] applies locality-sensitive hashing (LSH) [Har-Peled et al., 2012] to simplify the computation of the attention score matrix, which is widely used in kernel density estimation [Charikar and Siminelakis, 2017, Backurs et al., 2019]; Performer [Choromanski et al., 2020] projects both query and key matrix through random Fourier features [Rahimi et al., 2007], heavily exploiting Bochner Theorem for stationary kernels. The most related papers to ours are linear attention [Katharopoulos et al., 2020], Synthesizer [Tay et al., 2020a], and Nyströmformer [Xiong et al., 2021]. Linear attention takes the softmax structure in self-attention as a measure of similarity and replaces it with the dot product of separately activated query and key matrices; Synthesizer aims to modify the original self-attention by replacing the dot product before softmax with Synthetic Attention, which generates the alignment matrix independent of token-token dependencies. Their attempts indicate that the softmax structure in self-attention is not the only feasible choice, and justify our usage of kernelized attention. Rather than remodeling self-attention, Nyströmformer applies the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005], a powerful and effective method for large-scale kernel machines acceleration, to approximate the attention score matrix. However, Nyströmformer applies the Nyström method to a non-PSD matrix, and thus fails to utilize the full potential of the Nyström method. This issue is resolved in our proposed Skyformer by instead lifting the kernelized attention score matrix into a large PSD matrix which contains the target non-PSD matrix as its off-diagonal block. For more details on attention approximation methods, we refer readers to a survey paper on efficient transformers [Tay et al., 2020c]. Conclusions and future work Motivated by the connection between kernel methods and self-attention, we introduce Kernelized Attention, which replaces the softmax structure in self-attention with a Gaussian kernel. We also propose Skyformer, which adapts the Nyström method to Kernelized Attention to improve its efficiency. We expect the new model can enjoy more stable training while inheriting the strong performance from self-attention. Extensive experiments verify our intuitions and show that both Kernelized Attention and its Nyström approximation variant have comparable accuracy to the original Transformer on the LRA benchmark. Direct development of this work is the incorporation of further computation tricks in kernel methods, such as the local and global approximation for gram matrix [Snelson and Ghahramani, 2007] and the importance sampling in Nyström methods [Musco and Musco, 2017, Chen and Yang, 2021b,a]. Other related questions include the choice of the kernel other than the Gaussian kernel in our kernelized attention model. It is expected that for different tasks there will be specific kernels more proper than the original self-attention. The results in this work also shed new light on the design of the attention mechanism, which may benefit board downstream NLP tasks.",1
"Abstract Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the NystrÃÂ¶m method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources. Introduction The cost of language model training increases exponentially. Among different models, Transformerbased language models [Vaswani et al., 2017, Devlin et al., 2019, Liu et al., 2019, Lewis et al., 2020] are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity (O(n) where n is the input sequence length). Consequently, Transformers cannot support long sequence processing and large batch size with limited resources. The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020, Kitaev et al., 2020] or low-rank approximation [Choromanski et al., 2020, Wang et al., 2020]. However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible. It is also unclear in theory how to set the hyper-parameters of those methods to attain a desired level of approximation accuracy. Another issue of Transformers is the training instability that small perturbations in parameter updates tend to be amplified, resulting in significant disturbances in the model output [Liu et al., 2020a]. Transformers on some NLP tasks have shown to be sensitive to hyper-parameters, learning schedulers, or even random seeds, which usually demands a time-costly grid search for the best configuration in real-world applications. It has also been observed in our experiments that a slight change in the learning rate may cause the failure of convergence for some models. We conjecture that the instability in Transformer training comes from the softmax structure, as the un-normalized attention score matrices before softmax tend to have extremely large condition numbers due to its fast singular value decay. To alleviate the instability issue, an extra factor of 1/ Ã¢ÂÂp in the softmax kernel SM is suggested by Vaswani et al. [2017] to restrain the scale variation; Liu et al. [2020a] proposes a new scheme to control the magnitude of output change and stabilize the training in early stages. In practice, we also need to consider the lower numerical precision of GPU implementation in model training, which further deteriorates the stability. Kernel methods may be the answer to both challenges. As pointed out by Choromanski et al. [2020], the softmax structure is closely related to Gaussian kernels up to diagonal matrix multiplications, as the pairwise dot products naturally appear when expanding the squared `2 distance. We further notice some important connections between self-attention and Gaussian kernels. First, the un-normalized attention score matrix can be formed via basic matrix operations on an empirical Gaussian kernel matrix. Moreover, the form of Gaussian kernels has the natural interpretation of assigning Ã¢ÂÂattentionÃ¢ÂÂ to different tokens. Compared to the softmax function, Gaussian kernels automatically perform the normalization as softmax does (c.f. Section 4.1). These observations motivate us to replace the softmax structure with Gaussian kernels. As we demonstrated in this paper, the new attention model, Kernelized Attention, empirically stabilizes the model training while being comparable to self-attention in model accuracy. To further improve the efficiency, we propose Skyformer (Symmetrization ofattention for NYstrÃÂÃÂ¶m method) to accelerate kernelized attention. Skyformer adapts the NystrÃÂÃÂ¶m method [Williams and Seeger, 2001, Drineas et al., 2005] to the non-PSD empirical Gaussian kernel matrix (as query matrices in general do not equal to key matrices), by instead lifting the kernelized attention score matrix into a large PSD matrix that contains the un-normalized attention score matrix as the off-diagonal block. We further conduct theoretical analysis by showing that Skyformer has a small matrix approximation error on kernelized attention in the spectral norm. Our experiments on the LRA benchmark show that Skyformer consistently uses less space and time while achieving better accuracy than other baseline methods. In summary, our main contributions are: (1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers. (2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the NystrÃÂ¶m method to a non-PSD matrix. We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. (3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs. Related Work Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads [Voita et al., 2019, Michel et al., 2019] and model size reduction with knowledge distillation [Jiao et al., 2020, Tang et al., 2019, Liu et al., 2020b], we focus on attention approximation models, which are closely related to kernel methods. To reduce the time and space complexity by avoiding exhaustive computation over the attention metric, recent studies propose to apply sparse attention patterns to limit the numbers of elements participating in matrix multiplications [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020]. Beyond limiting the attention to fixed patterns, some approaches learn the patterns by determining token assignments to relevant groups [Kitaev et al., 2020, Roy et al., 2021]. Those models utilize local and global information in the attention score matrix to perform approximation, which coincides with the attempt to accelerate the computation in Gaussian processes [Snelson and Ghahramani, 2007]. The attention score matrix is known to exhibit a very fast rate of singular value decay [Bhojanapalli et al., 2020, Dong et al., 2021], similar to that of an empirical kernel matrix [Yang et al., 2017]. This near singular property motivates many low-rank attention approximation methods to skillfully leverage the computation techniques in kernel methods. Among them, Linformer [Wang et al., 2020] compresses the size of the key and value matrix with random projections based on the JohnsonÃ¢ÂÂLindenstrauss transform, a common randomized sketching method in Gaussian processes [Yang et al., 2017]; Reformer [Kitaev et al., 2020] applies locality-sensitive hashing (LSH) [Har-Peled et al., 2012] to simplify the computation of the attention score matrix, which is widely used in kernel density estimation [Charikar and Siminelakis, 2017, Backurs et al., 2019]; Performer [Choromanski et al., 2020] projects both query and key matrix through random Fourier features [Rahimi et al., 2007], heavily exploiting Bochner Theorem for stationary kernels. The most related papers to ours are linear attention [Katharopoulos et al., 2020], Synthesizer [Tay et al., 2020a], and NystrÃÂ¶mformer [Xiong et al., 2021]. Linear attention takes the softmax structure in self-attention as a measure of similarity and replaces it with the dot product of separately activated query and key matrices; Synthesizer aims to modify the original self-attention by replacing the dot product before softmax with Synthetic Attention, which generates the alignment matrix independent of token-token dependencies. Their attempts indicate that the softmax structure in self-attention is not the only feasible choice, and justify our usage of kernelized attention. Rather than remodeling self-attention, NystrÃÂ¶mformer applies the NystrÃÂ¶m method [Williams and Seeger, 2001, Drineas et al., 2005], a powerful and effective method for large-scale kernel machines acceleration, to approximate the attention score matrix. However, NystrÃÂ¶mformer applies the NystrÃÂ¶m method to a non-PSD matrix, and thus failsutilize the full potential of the NystrÃÂÃÂ¶m method. This issue is resolved in our proposed Skyformer by instead lifting the kernelized attention score matrix into a large PSD matrix which contains the target non-PSD matrix as its off-diagonal block. For more details on attention approximation methods, we refer readers to a survey paper on attention approximation methods which demonstrates the utility of the proposed method with fewer computational costs. Introduction The cost of language model training increases exponentially. Among different models, Transformerbased language models [Vaswani et al., 2017, Devlin et al., 2019, Liu et al., 2019, Lewis et al., 2020] are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity (O(n) where n is the input sequence length). Consequently, Transformers cannot support long sequence processing and large batch size with limited resources. The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020, Kitaev et al., 2020] or low-rank approximation [Choromanski et al., 2020, Wang et al., 2020]. However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible.",0
"Abstract Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.  Introduction Short texts such as tweets, news feeds and web search snippets appear daily in our life (Pang and Lee, 2005; Phan et al., 2008). To understand these short texts, short text classification (STC) is a fundamental task which can be found in many applications such as sentiment analysis (Chen et al., 2019), news classification (Yao et al., 2019) and query intent classification (Wang et al., 2017). STC is particularly hard in comparison to long text classification due to two key issues. The first key issue is that short texts only contain one or a few sentences whose overall length is small, which lack enough context information and strict syntactic structure to understand the meaning of texts (Tang et al., 2015; Wang et al., 2017). For example, it is hard to get the meaning of ""Birthday girl is an amusing ride"" without knowing ""Birthday girl"" is a 2001 movie. A harder case is to understand a web search snippet such as ""how much Tesla"", which usually does not contain word order nor function words (Phan et al., 2008). In addition, real STC tasks usually only have a limited number of labeled data compared to the abundant unlabeled short texts emerging everyday (Hu et al., 2019). Therefore, auxiliary knowledge is required to understand short texts, examples include concepts that can be found in common sense knowledge graphs (Wang et al., 2017; Chen et al., 2019), latent topics extracted from the short text dataset (Hu et al., 2019), and entities residing in knowledge graphs (Hu et al., 2019). However, simply enriching auxiliary knowledge cannot solve the shortage of labeled data, which is another key issue commonly faced by real STC tasks (Pang and Lee, 2005; Phan et al., 2008). Yet the popularly used deep models require large-scale labeled data to train well (Kim, 2014; Liu et al., 2016). Currently, graph neural networks (GNNs) designed for STC obtain the state-of-the-art performance (Hu et al., 2019; Ye et al., 2020). They both take the STC as the node classification problem on a graph with mixed nodes of different types: HGAT (Hu et al., 2019) builds a corpus-level graph modeling latent topics, entities and documents and STGCN (Ye et al., 2020) operates on a corpuslevel graph of latent topics, documents and words. In both works, each document is connected to its nodes of a different type such as entities and latent topics but not to other documents. However, they do not fully exploit interactions between nodes of the same type. They also fail to capture the similarities between short documents, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016). Besides, both works have large parameter sizes: HGAT (Hu et al., 2019) is a GNN with dual-level attention and STGCN (Ye et al., 2020) merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). To address the aforementioned problems, we propose a novel HIerarchical heterogeNEous graph representation learning method for STC called SHINE, which is able to fully exploit interactions between nodes of the same types and capture similarity between short texts. SHINE operates on a hierarchically organized heterogeneous corpuslevel graph, which consists of the following graphs at different levels: (i) word-level component graphs model interactions between words, partof-speech (POS) tags and entities which can be easily extracted and carry additional semantic and syntactic information to compensate for the lack of context information; and (ii) short document graph is dynamically learned and optimized to encode similarities between short documents which allows more effective label propagation among connected similar short documents. We conduct extensive experiments on a number of benchmark STC datasets including news, tweets, document titles and short reviews. Results show that the proposed SHINE consistently outperforms the state-of-theart with a much smaller parameter size. Related Works 2.1 Text Classification Text classification assigns predefined labels to documents of variable lengths which may consist of a single or multiple sentences (Li et al., 2020). Traditional methods adopt a two-step strategy: first extract human-designed features such as bagof-words (Blei et al., 2003) and term frequencyinverse document frequency (TF-IDF) (Aggarwal and Zhai, 2012) from documents, then learn classi- fiers such as support vector machine (SVM) (Cortes and Vapnik, 1995). Deep neural networks such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Liu et al., 2016) can directly obtain expressive representations from raw texts and conduct classification in an end-to-end manner. Recently, graph neural networks (GNNs) (Defferrard et al., 2016; Kipf and Welling, 2016) have obtained the state-of-the-art performance on text classification. They can be divided into two types. The first type of GNNs constructs document-level graphs where each document is modeled as a graph of word nodes, then formulates text classification as a whole graph classification problem (Defferrard et al., 2016). Examples are TLGNN (Huang et al., 2019), TextING (Zhang et al., 2020), HyperGAT (Ding et al., 2020), which establish word-word edges differently. In particular, some methods (Liu et al., 2019; Chen et al., 2020) propose to estimate the graph structure of the document-level graphs during learning. However, if only a few documents are labeled, these GNNs cannot work due to the lack of labeled graphs. As is known, GNNs such as graph convolutional network (GCN) (Kipf and Welling, 2016) can conduct semi-supervised learning to solve node classification task on a graph where only a small number of nodes are labeled (Kipf and Welling, 2016). Therefore, another type of GNNs instead operates on a heterogeneous corpus-level graph which takes both text and word as nodes, and classifies unlabeled texts by node classification. Examples include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021) and TG-Transformer (Zhang and Zhang, 2020) with different strategies to construct and handle heterogeneous nodes and edges. However, these methods cannot work well for short texts of limited length. 2.2 Short Text Classification (STC) Short text classification (STC) is particularly challenging (Aggarwal and Zhai, 2012; Li et al., 2020). Due to limited length, short texts lack context information and strict syntactic structure which are vital to text understanding (Wang et al., 2017). Therefore, methods tailored for STC strive to incorporate various auxiliary information to enrich short text representations. Popularly used examples are concepts existing in external knowledge bases such as Probase (Wang et al., 2017; Chen et al., 2019) and latent topics discovered in the corpus (Zeng et al., 2018). However, simply enriching semantic information cannot compensate for the shortage of labeled data, which is a common problem faced by real short texts such as queries and online reviews (Pang and Lee, 2005; Phan et al., 2008). Thus, GNN-based methods which perform node classification for semi-supervised STC are utilized. HGAT (Hu et al., 2019) applies a GNN with dual-level attention to forward messages on a corpus-level graph modeling topics, entities and documents jointly, where the entities are words linked to knowledge graphs. STGCN (Ye et al., 2020) operates on a corpus-level graph of topics, documents and words, and merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). Currently, the state-of-the-art method on STC is HGAT (Hu et al., 2019; Yang et al., 2021).  Conclusion In this paper, we propose SHINE, a novel hierarchical heterogeneous graph representation learning method for short text classification. It is particularly useful to compensate for the lack of context information and propagate the limited number of labels efficiently. Specially, SHINE can effectively learn from a hierarchical graph modeling different perspectives of the short text dataset: word-level component graphs are used to understand short texts from the semantic and syntactic perspectives, and the dynamically learned short document graph allows efficient and effective label propagation among similar short documents. Extensive experiments show that SHINE outperforms the others consistently. As for the future works, we plan to search graph structure (Zhao et al., 2021) and utilize automated machine learning (Yao et al., 2018) to improve learning performance.",1
"Abstract Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels. Introduction Short texts such as tweets, news feeds and web search snippets appear daily in our life (Pang and Lee, 2005; Phan et al., 2008). To understand these short texts, short text classification (STC) is a fundamental task which can be found in many applications such as sentiment analysis (Chen et al., 2019), news classification (Yao et al., 2019) and query intent classification (Wang et al., 2017). STC is particularly hard in comparison to long text classification due to two key issues. The first key issue is that short texts only contain one or a few sentences whose overall length is small, which lack enough context information and strict syntactic structure to understand the meaning of texts (Tang et al., 2015; Wang et al., 2017). For example, it is hard to get the meaning of ""Birthday girl is an amusing ride"" without knowing ""Birthday girl"" is a 2001 movie. A harder case is to understand a web search snippet such as ""how much Tesla"", which usually does not contain word order nor function words (Phan et al., 2008). In addition, real STC tasks usually only have a limited number of labeled data compared to the abundant unlabeled short texts emerging everyday (Hu et al., 2019). Therefore, auxiliary knowledge is required to understand short texts, examples include concepts that can be found in common sense knowledge graphs (Wang et al., 2017; Chen et al., 2019), latent topics extracted from the short text dataset (Hu et al., 2019), and entities residing in knowledge graphs (Hu et al., 2019). However, simply enriching auxiliary knowledge cannot solve the shortage of labeled data, which is another key issue commonly faced by real STC tasks (Pang and Lee, 2005; Phan et al., 2008). Yet the popularly used deep models require large-scale labeled data to train well (Kim, 2014; Liu et al., 2016). Currently, graph neural networks (GNNs) designed for STC obtain the state-of-the-art performance (Hu et al., 2019; Ye et al., 2020). They both take the STC as the node classification problem on a graph with mixed nodes of different types: HGAT (Hu et al., 2019) builds a corpus-level graph modeling latent topics, entities and documents and STGCN (Ye et al., 2020) operates on a corpuslevel graph of latent topics, documents and words. In both works, each document is connected to its nodes of a different type such as entities and latent topics but not to other documents. However, they do not fully exploit interactions between nodes of the same type. They also fail to capture the similarities between short documents, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016). Besides, both works have large parameter sizes: HGAT (Hu et al., 2019) is a GNN with dual-level attention and STGCN (Ye et al., 2020) merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). To address the aforementioned problems, we propose a novel HIerarchical heterogeNEous graph representation learning method for STC called SHINE, which is able to fully exploit interactions between nodes of the same types and capture similarity between short texts. SHINE operates on a hierarchically organized heterogeneous corpuslevel graph, which consists of the following graphs at different levels: (i) word-level component graphs model interactions between words, partof-speech (POS) tags and entities which can be easily extracted and carry additional semantic and syntactic information tofor the lack of context information; and (ii) short document graph is dynamically learned and optimized to encode similarities between short documents which allows more effective label propagation among connected similar short documents. We conduct extensive experiments on a number of benchmark STC datasets including news, tweets, document titles and short reviews. Results show that the proposed SHINE consistently outperforms the state-of-theart with a much smaller parameter size. Related Works 2.1 Text Classification Text classification assigns predefined labels to documents of variable lengths which may consist of a single or multiple sentences (Li et al., 2020). Traditional methods adopt a two-step strategy: first extract human-designed features such as bagof-words (Blei et al., 2003) and term frequencyinverse document frequency (TF-IDF) (Aggarwal and Zhai, 2012) from documents, then learn classi- fiers such as support vector machine (SVM) (Cortes and Vapnik, 1995). Deep neural networks such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Liu et al., 2016) can directly obtain expressive representations from raw texts and conduct classification in an end-to-end manner. Recently, graph neural networks (GNNs) (Defferrard et al., 2016; Kipf and Welling, 2016) have obtained the state-of-the-art performance on text classification. They can be divided into two types. The first type of GNNs constructs document-level graphs where each document is modeled as a graph of word nodes, then formulates text classification as a whole graph classification problem (Defferrard et al., 2016). Examples are TLGNN (Huang et al., 2019), TextING (Zhang et al., 2020), HyperGAT (Ding et al., 2020), which establish word-word edges differently. In particular, some methods (Liu et al., 2019; Chen et al., 2020) propose to estimate the graph structure of the document-level graphs during learning. However, if only a few documents are labeled, these GNNs cannot work due to the lack of labeled graphs. As is known, GNNs such as graph convolutional network (GCN) (Kipf and Welling, 2016) can conduct semi-supervised learning to solve node classification task on a graph where only a small number of nodes are labeled (Kipf and Welling, 2016). Therefore, another type of GNNs instead operates on a heterogeneous corpus-level graph which takes both text and word as nodes, and classifies unlabeled texts by node classification. Examples include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021) and TG-Transformer (Zhang and Zhang, 2020) with different strategies to construct and handle heterogeneous nodes and edges. However, these methods cannot work well for short texts of limited length. 2.2 Short Text Classification (STC) Short text classification (STC) is particularly challenging (Aggarwal and Zhai, 2012; Li et al., 2020). Due to limited length, short texts lack context information and strict syntactic structure which are vital to text understanding (Wang et al., 2017). Therefore, methods tailored for STC strive to incorporate various auxiliary information to enrich short text representations. Popularly used examples are concepts existing in external knowledge bases such as Probase (Wang et al., 2017; Chen et al., 2019) and latent topics discovered in the corpus (Zeng et al., 2018). However, simply enriching semantic information cannot compensate for the shortage of labeled data, which is a common problem faced by real short texts such as queries and online reviews (Pang and Lee, 2005; Phan et al., 2008). Thus, GNN-based methods which perform node classification for semi-supervised STC are utilized. HGAT (Hu et al., 2019) applies a GNN with dual-level attention to forward messages on a corpus-level graph modeling topics, entities and documents jointly, where the entities are words linked to knowledge graphs. STGCN (Ye et al., 2020) operates on a corpus-level graph of topics, documents and words, and merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). Currently, the state-of-the-art method on STC is HGATet al., 2019; Yang et al., 2021). Conclusion In this paper, we propose SHINE, a novel hierarchical heterogeneous graph representation learning method for short text classification. It is particularly useful to compensate for the lack of context information and propagate the limited number of labels efficiently. Specially, SHINE can effectively learn from a hierarchical graph modeling learned properties and methods, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016).",0
"Abstract In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art. Introduction Since dialogue is regarded as a fundamental and complex element of human cognition (Jurafsky and Martin, 2000), the development of systems capable of understanding human language and communicating with humans can have a significant impact. However, human communication requires the acknowledgment and the exchange of conversational partner’s emotions, as emotions play an important role in developing a confidential relationship between the speaker and the listener. Open domain conversational agents have been widely studied in the past years and both retrievalbased and generation-based approaches (Wu et al., 2019; Cai et al., 2019; Weston et al., 2018) have been developed. However, prior research has shown that most of those conversational agents are unable to imitate dialogues between humans, as the produced responses are generic and short (Vinyals and Le, 2015; Li et al., 2016b). Several efforts have been made to make the conversationa more engaging by keeping track of the conversational context (Sordoni et al., 2015b,a; Serban et al., 2016, 2017) or by producing more diverse responses (Li et al., 2016a,c). Subsequently, a recent trend that was followed by various researchers (Li et al., 2016b; Zhang et al., 2018; Kulikov et al., 2019; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Mazaré et al., 2018; Dinan et al., 2020; Madotto et al., 2019; Hancock et al., 2019; Yavuz et al., 2019; Wolf et al., 2019) in order to make the responses more coherent and consistent through the dialogue, was to produce personalized responses by conditioning the generation on a persona profile. Apart from understanding what is being discussed, a conversational agent should also acknowledge the emotional state of the conversational partner, as it is a significant part of human communication. A lot of researchers have focused on detecting emotion (Fan et al., 2018b; Xu et al., 2018; Winata et al., 2017, 2019) and empathy in dialogue systems (Bertero et al., 2016; Chatterjee et al., 2019). Zhou et al., 2018 introduced a seq2seq (Sutskever et al., 2014) Emotional Chatting Machine in order to generate responses with high emotional context, using emotional embeddings and an internal and external memory mechanism. A GAN-based (Goodfellow et al., 2014) framework was also proposed by Wang and Wan, 2018 that controlled the sentiment of the generated response. Wu and Wu, 2019 also used a dual-decoder to similarly generate emotional responses, given the sentiment. Zhou and Wang, 2018 introduced a Twitter dataset which used the emojis of the Twitter posts as emotionlabels and they also proposed a seq2seq model to generate emotional responses. Lubis et al., 2018 introduced a new dataset and proposed a hierarchical seq2seq response generator for affect-sensitive dialogue generation. Rashkin et al., 2019 introduced the EmpatheticDialogues dataset and trained the baselines to generate empathetic responses and simultaneously predict the corresponding emotion of the dialogue context. Later, Lin et al., 2019 introduced the ""Mixture of Empathetic Listeners"" framework improving the initial baselines. Santhanam and Shaikh, 2019 finetuned the GPT2 (Radford et al., 2019) model to improve the results further, while Shin et al., 2019 used reinforcement learning for predicting the user’s sentiment look-ahead along side with response generation. Lin et al., 2019 improved the performance on EmpatheticDialogues by finetuning the GPT2 model with the use of multitask learning, while Majumder et al., 2020 followed a different approach introducing stochasticity into the emotion mixture and arguing that empathetic responses do not always mirror the emotion of the user. Significant improvements were also made by Roller et al., 2021 and Shuster et al., 2020 who used multi-task training on multiple dialog tasks, achieving state-of-the-art results. In this work, in order to enforce empathetic response generation we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we use three objectives: response language modeling, sentiment understanding and empathy forcing. The sentiment understanding objective is crucial for tracking and acknowledging the emotional state of the conversational partner, while the empathy forcing objective favors empathetic response generation by penalizing responses that have an opposite sentiment of that of the conversational partner. Our key contribution is the inclusion of the sentiment understanding and empathy forcing auxiliary losses to promote empathetic behavior. The proposed approach, EmpBot, 1 , is on par with state-of-the-art in terms of BLEU score. However, our model produces significantly more fluent and empathetic responses, as indicated by human evaluation results. Conclusions In this work we propose EmpBot, a T5-based chatbot, augmented with a novel finetuning procedure for generating empathetic dialogue responses. The proposed loss consists of three parts: an LM loss that produces valid textual responses, a sentiment classification loss that introduces emotional awareness to the model and an empathy forcing loss that ensures that the responses are emotionally relevant. We evaluate EmpBot using standard evaluation metrics, i.e. perplexity and BLEU score, achieving state-of-the-art results. Our human evaluation results indicate that EmpBot produces more fluent and empathetic responses, when compared with both the baseline and the state-of-the-art models. In the future we want to extend the proposed method for other architectures, and explore more empathy forcing losses using raw emotion values instead of sentiment polarities.",1
"Abstract In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art. Introduction Since dialogue is regarded as a fundamental and complex element of human cognition (Jurafsky and Martin, 2000), the development of systems capable of understanding human language and communicating with humans can have a significant impact. However, human communication requires the acknowledgment and the exchange of conversational partnerÃ¢ÂÂs emotions, as emotions play an important role in developing a confidential relationship between the speaker and the listener. Open domain conversational agents have been widely studied in the past years and both retrievalbased and generation-based approaches (Wu et al., 2019; Cai et al., 2019; Weston et al., 2018) have been developed. However, prior research has shown that most of those conversational agents are unable to imitate dialogues between humans, as the produced responses are generic and short (Vinyals and Le, 2015; Li et al., 2016b). Several efforts have been made to make the conversationa more engaging by keeping track of the conversational context (Sordoni et al., 2015b,a; Serban et al., 2016, 2017) or by producing more diverse responses (Li et al., 2016a,c). Subsequently, a recent trend that was followed by various researchers (Li et al., 2016b; Zhang et al., 2018; Kulikov et al., 2019; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; MazarÃÂ© et al., 2018; Dinan et al., 2020; Madotto et al., 2019; Hancock et al., 2019; Yavuz et al., 2019; Wolf et al., 2019) in order to make the responses more coherent and consistent through the dialogue, was to produce personalized responses by conditioning the generation on a persona profile. Apart from understanding what is being discussed, a conversational agent should also acknowledge the emotional state of the conversational partner, as it is a significant part of human communication. A lot of researchers have focused on detecting emotion (Fan et al., 2018b; Xu et al., 2018; Winata et al., 2017, 2019) and empathy in dialogue systems (Bertero et al., 2016; Chatterjee et al., 2019). Zhou et al., 2018 introduced a seq2seq (Sutskever et al., 2014) Emotional Chatting Machine in order to generate responses with high emotional context, using emotional embeddings and an internal and external memory mechanism. A GAN-based (Goodfellow et al., 2014) framework was also proposed by Wang and Wan, 2018 that controlled the sentiment of the generated response. Wu and Wu, 2019 also used a dual-decoder to similarly generate emotional responses, given the sentiment. Zhou and Wang, 2018 introduced a Twitter dataset which used the emojis of the Twitter posts as emotionlabels and they also proposed a seq2seq model to generate emotional responses. Lubis et al., 2018 introduced a new dataset and proposed a hierarchical seq2seq response generator for affect-sensitive dialogue generation. Rashkin et al., 2019 introduced the EmpatheticDialogues dataset and trained the baselines to generate empathetic responses and simultaneously predict the corresponding emotion of the dialogue context. Later, Lin et al., 2019 introduced the ""Mixture of Empathetic Listeners"" framework improving the initial baselines. Santhanam and Shaikh, 2019 finetuned the GPT2 (Radford et al., 2019) model to improve the results further, while Shin et al., 2019 used reinforcement learning for predicting the userÃ¢ÂÂs sentiment look-ahead along side with response generation. Lin et al., 2019 improved the performance on EmpatheticDialogues by finetuning the GPT2 model with the use of multitask learning, while Majumder et al., 2020a different approach introducing stochasticity into the emotion mixture and arguing that empathetic responses do not always mirror the emotion of the user. Significant improvements were also made by Roller et al., 2021 and Shuster et al., 2020 who used multi-task training on multiple dialog tasks, achieving state-of-the-art results. In this work, in order to enforce empathetic response generation we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art. Introduction Since dialogue is regarded as a fundamental and complex element of human cognition (Jurafsky and Martin, 2000), the development of systems capable of understanding human language and communicating with humans can have a significant impact. However, human communication requires the acknowledgment and the exchange of conversational partnerÃ¢ÂÂs emotions, as emotions play an important role in developing a confidential relationship between the speaker and the listener. Open domain conversational agents have been widely studied in the past years and both retrievalbased and generation-based approaches (Wu et al., 2019; Cai et al., 2019; Weston et al., 2018) have been developed. However, prior research has shown that most of those conversational agents are unable to imitate dialogues between humans, as the produced responses are generic and short (Vinyals and Le, 2015; Li et al., 2016b).",0
"Abstract Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users’ access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems’ effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. Introduction Simultaneous speech translation (SimulST) is the task in which the translation of a source language speech has to be performed on partial, incremental input. This is a key feature to achieve low latency in scenarios like streaming conferences and lectures, where the text has to be displayed following as much as possible the pace of the speech SimulST is indeed a complex task in which the difficulties of performing speech recognition from partial inputs are exacerbated by the problem to project meaning across languages. Despite the increasing demand for such a system, the problem is still far from being solved. So far, research efforts mainly focused on the quality/latency trade-off, i.e. producing high quality outputs in the shortest possible time, balancing the need for a good translation with the necessity of a rapid text generation. Previous studies, however, disregard how the translation is displayed and, consequently, how it is actually perceived by the end users. After a concise survey of the state of the art in the field, in this paper we posit that, from the users’ experience standpoint, output visualization is at least as important as having a good translation in a short time. This raises the need for a broader, task-oriented and human-centered analysis of SimulST systems’ performance, also accounting for this third crucial factor. Background As in the case of offline speech translation, the adoption of cascade architectures (Stentiford and Steer, 1988; Waibel et al., 1991) was the first attempt made by the SimulST community to tackle the problem of generating text from partial, incremental input. Cascade systems (F ¨ugen, 2009; Fujita et al., 2013; Niehues et al., 2018; Xiong et al., 2019; Arivazhagan et al., 2020b) involve a pipeline of two components. First, a streaming automatic speech recognition (ASR) module transcribes the input speech into the corresponding text (Wang et al., 2020; Moritz et al., 2020). Then, a simultaneous text-to-text translation module translates the partial transcription into target language text (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). This approach suffers from error propagation, a well-known problem even in the offline scenario, where the transcription errors made by the ASR module are propagated to the MT module, which cannot recover from them as it does not have direct access to the audio. Another strong limitation of cascaded systems is the extra latency added by the two-step pipeline, since the MT module has to wait until the streaming ASR output is produced. To overcome these issues, the direct models initially proposed in B´erard et al. (2016; Weiss et al. (2017) represent a valid alternative that is gaining increasing traction (Bentivogli et al., 2021). Direct ST models are composed of an encoder, usually bidirectional, and a decoder. The encoder starts from the audio features extracted from the input signal and computes a hidden representation; the decoder transforms this representation into target language text. Direct modeling becomes crucial in the simultaneous scenario, as it reduces the overall system’s latency due to the absence of intermediate symbolic representation steps. Despite the data scarcity issue caused by the limited availability of speech-to-translation corpora, the adoption of direct architectures showed to be promising (Weiss et al., 2017; Ren et al., 2020; Zeng et al., 2021), driving recent efforts towards the development of increasingly powerful and efficient models.  Conclusions and Future directions SimulST systems have become increasingly popular in recent years and many efforts have been made to build robust and efficient models. Despite the difficulties introduced by the online framework, these models have rapidly improved, achieving comparable results to the offline systems. However, many research directions have not been explored enough (e.g., the adoption of dynamic or fixed segmentation, the offline or the online training). First among all, the visualization strategy that is adopted to display the output of the simultaneous systems is an important and largely under-analyzed aspect of the simultaneous experience. We posit that the presence of applicationoriented metrics (e.g., reading speed), which are strongly related to the visualization and, as an ultimate goal, to the user experience, is the factor that misses in the actual evaluation environment. Indeed, this paper points out that BLEU and Average Lagging are not the only metrics that matter to effectively evaluate a SimulST model, even if they are fundamental to judge a correct and real-timed translation. We hope that this will inspire the community to work on this critical aspect in the future.",1
"Abstract Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate usersÃ¢ÂÂ access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systemsÃ¢ÂÂ effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. Introduction Simultaneous speech translation (SimulST) is the task in which the translation of a source language speech has to be performed on partial, incremental input. This is a key feature to achieve low latency in scenarios like streaming conferences and lectures, where the text has to be displayed following as much as possible the pace of the speech SimulST is indeed a complex task in which the difficulties of performing speech recognition from partial inputs are exacerbated by the problem to project meaning across languages. Despite the increasing demand for such a system, the problem is still far from being solved. So far, research efforts mainly focused on the quality/latency trade-off, i.e. producing high quality outputs in the shortest possible time, balancing the need for a good translation with the necessity of a rapid text generation. Previous studies, however, disregard how the translation is displayed and, consequently, how it is actually perceived by the end users. After a concise survey of the state of the art in the field, in this paper we posit that, from the usersÃ¢ÂÂ experience standpoint, output visualization is at least as important as having a good translation in a short time. This raises the need for a broader, task-oriented and human-centered analysis of SimulST systemsÃ¢ÂÂ performance, also accounting for this third crucial factor. Background As in the case of offline speech translation, the adoption of cascade architectures (Stentiford and Steer, 1988; Waibel et al., 1991) was the first attempt made by the SimulST community to tackle the problem of generating text from partial, incremental input. Cascade systems (F ÃÂ¨ugen, 2009; Fujita et al., 2013; Niehues et al., 2018; Xiong et al., 2019; Arivazhagan et al., 2020b) involve a pipeline of two components. First, a streaming automatic speech recognition (ASR) module transcribes the input speech into the corresponding text (Wang et al., 2020; Moritz et al., 2020). Then, a simultaneous text-to-text translation module translates the partial transcription into target language text (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). This approach suffers from error propagation, a well-known problem even in the offline scenario, where the transcription errors made by the ASR module are propagated to the MT module, which cannot recover from them as it does not have direct access to the audio. Another strong limitation of cascaded systems is the extra latency added by the two-step pipeline, since the MT module has to wait until the streaming ASR output is produced. To overcome these issues, the direct models initially proposed in BÃÂ´erard et al. (2016; Weiss et al. (2017) represent a valid alternative that is gaining increasing traction (Bentivogli et al., 2021). Direct ST models are composed of an encoder, usually bidirectional, and a decoder. The encoder starts from the audio features extracted from the input signal and computes a hidden representation; the decoder transforms this representation into target language text. Direct modeling becomes crucial in the simultaneous scenario, as it reduces the overall systemÃ¢ÂÂs latency due to the absence of intermediate symbolic representation steps. Despite the data scarcity issue caused by the limited availability of speech-to-translation corpora, the adoption of direct architectures showed to be promising (Weiss et al., 2017; Ren et al., 2020; Zeng et al., 2021), driving recent efforts towards the development of increasingly powerful and efficient models. Conclusions and Future directions SimulST systems have become increasingly popular in recent years andefforts have been made to build robust and efficient models. Despite the difficulties introduced by the online framework, these models have rapidly improved, achieving comparable results to the offline systems. However, many research directions have not been explored enough (e.g., the adoption of dynamic or fixed segmentation, the offline or the online training). We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. Introduction Simultaneous speech translation (SimulST) is the task in which the translation of a source language speech has to be performed on partial, incremental input.",0
"ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformerbased language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong “out of the box” sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term Frequency–Inverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight Araci (2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: • Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. • Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci, 2019b) was fine-tuned on the Financial Phrasebank (Malo et al., 2014) and FiQA Task 1 sentiment scoring dataset,3 thereby achieving state-of-the-art results. Sentence-BERT In Reimers and Gurevych (2019), authors noted that the sentence embeddings obtained from vanilla BERT (the ones pre-trained with the NSP task) lack in quality. In fact, considerably simpler baselines are competitive with BERT in this regard (e.g., averaging word embeddings). Liu et al. (2019) emphasized that the NSP task was not as useful as thought, and authors suggested removing it from the BERT pre-training scheme. Consequently, Reimers and Gurevych (2019) propose the Sentence-BERT model. Starting from a pre-trained BERT checkpoint, they fine-tune it with supervision with a Siamese BERT network (meaning that they encode pairs of sentences with the same encoder), and predict the sentence entailment from the two sentence embeddings (Natural Language Inference (NLI) task). This approach results in more meaningful sentence representations. It is now well-known that pre-trained Transformers achieve state-of-the-art performance in NLP tasks (Araci, 2019a). In this article, unlike Araci (2019a), rather than starting from vanilla BERT, which is state-of-the-art for token-level embeddings but not for sentence-level tasks, we base our work on a model that has been fine-tuned for producing high-quality sentence embeddings. We believe this is a more sensible approach in the case of financial sentiment analysis. In contrast to Araci (2019a), we model financial sentiment as a continuous variable (from -1 to 1), instead of using discrete values.  Conclusion and Future Work We have demonstrated that FinEAS, a model based on BERT pre-trained on the general domain but fine-tuned for sentence-level tasks, is a sensible approach for financial sentiment classification. In conclusion, our model is simple to implement and outperforms several common baselines, including vanilla BERT and task-specific approaches. We make our code and model weights publicly available. In future work, we think it will be interesting to further explore Transformers in the financial domain, with an emphasis on models fine-tuned for sentence and/or document-level tasks.",1
"ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformerbased language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong Ã¢ÂÂout of the boxÃ¢ÂÂ sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term FrequencyÃ¢ÂÂInverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight(2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: Ã¢ÂÂ¢ Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. Ã¢ÂÂ¢ Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci et al., 2019) is fine-tuned on the Financial Phrasebank (Finow et al., 2019) and FinBERT (Araci et al., 2019) in the financial domain. In contrast, starting from the original Transformer pre-trained on large textual corpora (Malo et al., 2014). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term FrequencyÃ¢ÂÂInverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020).",0
"Abstract Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the system’s language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a system’s shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives. System misunderstandings and low user engagement are factors of low quality that are relatively easy to identify, but more subtle factors such as boring responses, awkward topic switches, and individual preferences can also have a substantial effect. Furthermore, the practical value of any approach to estimate the quality of individual dialogue turns is highly sensitive to the cost of collecting relevant data. Chatbots, and the settings they are placed in, can differ drastically in both their topics of conversation and interaction styles. And while conversation-level quality labels can be obtained relatively quickly by asking users to provide a rating at the end of a conversation, collecting data with turn-level labels that adequately characterizes a new chatbot or chat setting is an expensive process. In this paper we present our dialogue analysis approach, which addresses these challenges by producing quality scores for each utterance in a given conversation dataset using only conversation-level quality ratings. Unlike other work that focuses on utterance-level quality prediction using labeled data, our approach involves training a neural model to learn explicit relationships between utterancelevel features and conversation quality without the need for costly utterance-level annotations. We evaluate this approach on two conversation datasets and show high agreement between our model and experts for identifying problematic interactions. By developing an empirical technique that models the relationship between specific interactions and overall conversation quality, our work has the potential to remove much of the human effort and guesswork involved in dialogue system development. Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation breakdowns at the turn level. Ghazarian et al. (2020)’s work is similar, predicting utterance-level user engagement. Ghazarian et al. (2020) and Choi et al. (2019)’s work is similar to ours, as they build models targeted towards utterance-level quality outcomes. However, unlike our approach, these works are reliant on costly turn-level annotations: given conversations annotated for quality on the utterance level, their approach is to train a model that can predict utterance quality on unseen conversations within a similar conversation setting. This strategy incurs a substantial cost whenever the training data needs to be updated to fit a novel conversational setting or chatbot. To avoid the cost of collecting turn-level labels, our approach is more in line with techniques such as multiple regression analysis, where fitting a model to a dataset is used to explain the relationship between features and some outcome, rather than to predict an outcome for unseen examples. In our case, our model can be fit to any dataset of conversations with conversation-level quality labels in order to estimate the quality impact of each utterance on the overall conversation quality. This approach has a couple advantages over existing work. First, collecting utterance-level annotations in a supervised setting is not necessary for our approach as it was for Choi et al. (2019) and Ghazarian et al. (2020). Second, our model learns empirically-derived relationships between the utterance level and conversation quality, rather than learning to mimic human judgements of utterance level quality irrespective of conversation-level impact. Given the unreliability of human judgements on conversation quality noted in prior work (Liang et al., 2020), it is possible that removing human estimations of the relationship between turn-level and conversation-level quality eliminates a source of bias in the model’s training objective. To our knowledge, no previous work has presented an approach for utterance-level quality estimation that does not require data annotated on the turn level. Conclusion Our approach and evaluation results demonstrate the feasibility of an automatic approach for dialogue quality analysis. By training a model to learn the relationship between utterance-level features and conversation-level outcomes, it is possible to obtain an empirically-derived ranking of interactions based on whether they relate to positive or negative conversation quality. This work has obvious applications in dialogue system development, mitigating the high developer effort involved in manual analysis and human bias in determining the relative importance of system issues.",1
"Abstract Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the systemÃ¢ÂÂs language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives. System misunderstandings and low user engagement are factors of low quality that are relatively easy to identify, but more subtle factors such as boring responses, awkward topic switches, and individual preferences can also have a substantial effect. Furthermore, the practical value of any approach to estimate the quality of individual dialogue turns is highly sensitive to the cost of collecting relevant data. Chatbots, and the settings they are placed in, can differ drastically in both their topics of conversation and interaction styles. And while conversation-level quality labels can be obtained relatively quickly by asking users to provide a rating at the end of a conversation, collecting data with turn-level labels that adequately characterizes a new chatbot or chat setting is an expensive process. In this paper we present our dialogue analysis approach, which addresses these challenges by producing quality scores for each utterance in a given conversation dataset using only conversation-level quality ratings. Unlike other work that focuses on utterance-level quality prediction using labeled data, our approach involves training a neural model to learn explicit relationships between utterancelevel features and conversation quality without the need for costly utterance-level annotations. We evaluate this approach on two conversation datasets and show high agreement between our model and experts for identifying problematic interactions. By developing an empirical technique that models the relationship between specific interactions and overall conversation quality, our work has the potential to remove much of the human effort and guesswork involved in dialogue system development. Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation breakdowns at the turn level. Ghazarian et al. (2020)Ã¢ÂÂs work is similar, predicting utterance-level user engagement. Ghazarian et al. (2020) and Choi et al. (2019)Ã¢ÂÂs work is similar to ours, as they build models targeted towards utterance-level quality outcomes. However, unlike our approach, these works are reliant on costly turn-level annotations: given conversations annotated for quality on the utterance level, their approach is to train a model that can predict utterance quality on unseen conversations within a similar conversation setting. This strategy incurs a substantial cost whenever the training data needs to be updated to fit a novel conversational setting or chatbot. To avoid the cost of collecting turn-level labels, our approach is more in line with techniques such as multiple regression analysis, where fitting a model to a dataset is used to explain the relationship between features and some outcome, rather than to predict an outcome for unseen examples. In our case, our model can be fit to any dataset of conversations with conversation-level quality labels in order to estimate the quality impact of eachon the overall conversation quality. This approach has a couple advantages over existing work. First, collecting utterance-level annotations in a supervised setting is not necessary for our approach as it was for Choi et al. (2019) and Ghazarian et al. (2020). Second, our model learns empirically-derived relationships between the utterance-level quality prediction and the overall dialogue quality prediction, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the systemÃ¢ÂÂs language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives.",0
"Abstract Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding – a critical component of NLP applications – by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement. Introduction Large language models (with more than 1 billion parameters) perform well on a range of natural language processing (NLP) tasks in zero- and few-shot settings, without requiring task-specific supervision (e.g., Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). This observation suggests that given enough capacity, language models may extract the knowledge required to perform well on these NLP tasks from raw text, simply using the transformer architecture (Vaswani et al., 2017) and auto-regressive language modeling objective. Consequently, various recent efforts have focused on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark – questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. We also observe a correlation between zero-shot performance and the similarity of a benchmark to the pre-training data (measured as the perplexity of the benchmark under a pre-trained model). Our results show that the zero-shot performance of pretrained language models on commonsense benchmarks is mostly attributed to the dataset bias in the benchmarks; it also highlights the need of reporting strong baselines in our evaluations which is missing from some of the recent work (Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). Next we investigate to what extent hyperparameters (such as the prompt format) and increasing model size impact the zero-shot performance. In all commonsense benchmarks, we see a gap between the zero-shot performance of the worst and best hyper-parameter settings (ranging between 2 to 19 accuracy points). This result shows that for commonsense understanding tasks, it is best not to take pre-trained language models as off-the-shelf tools. When increasing model size, across benchmarks, we observe improvements on both zero-shot performance and the Answer-only baseline, suggesting that larger models are better in exploiting the surface cues. Finally, we examine if pre-trained language models benefit from adding more examples in a fewshot setting or leveraging knowledge extracted from existing commonsense knowledge bases. We do not observe a substantial gain from the few-shot evaluation compared to the zero-shot one. Adding commonsense knowledge also does not yield notable improvements, showing that our language models cannot leverage the relevant knowledge when it is simply added to the prompt. It is an exciting time for language research in both academia and industry, with various efforts working on pre-training stronger language models. However, to better understand the goodness of these models, we need to compare them with strong baselines. Moreover, we need to compare different models under similar hyper-parameter settings given that the choice of parameters can substantially impact model performance even in a zeroshot evaluation setting. We hope our work encourages the community to consider stronger evaluation protocols for pre-trained language models. Conclusion We examine the extent to which large language models exhibit commonsense understanding by evaluating them against a range of commonsense benchmarks in a zero-shot way. At first sight, these models show impressive zero-shot performance suggesting that they capture commonsense knowledge. However, a closer inspection reveals that the good performance of these models is due to existing dataset bias in our benchmarks: the zero-shot performance is closer to that of a strong baseline that does not use questions at all than to the state-ofthe-art performance. We also observe that as model size increases, the improvements in the zero-shot performance plateaus, indicating building larger models is not enough for achieving human-level commonsense understanding. In addition, the fewshot evaluation does not show notable improvements over the zero-shot setting except for Social IQa. Our results suggest that the language modeling objective and larger model capacity are not enough to extract commonsense knowledge from text. Future work needs to explore alternative modeling paradigms to better capture commonsense knowledge from text. Moreover, given the implicit nature of commonsense knowledge, the text modality alone might not be enough for improving the commonsense understanding capacity of NLP systems; an interesting future direction is leveraging other modalities such as images or videos to improve this capacity.",1
"Abstract Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding ÃÂ¢ÃÂÃÂ a critical component of NLP applications ÃÂ¢ÃÂÃÂ by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement. Introduction Large language models (with more than 1 billion parameters) perform well on a range of natural language processing (NLP) tasks in zero- and few-shot settings, without requiring task-specific supervision (e.g., Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). This observation suggests that given enough capacity, language models may extract the knowledge required to perform well on these NLP tasks from raw text, simply using the transformer architecture (Vaswani et al., 2017) and auto-regressive language modeling objective. Consequently, various recent efforts have focused on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark Ã¢ÂÂ questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. We also observe a correlation between zero-shot performance and the similarity of a benchmark to the pre-training data (measured as the perplexity of the benchmark under a pre-trained model). Our results show that the zero-shot performance of pretrained language models on commonsense benchmarks is mostly attributed to the dataset bias in the benchmarks; it also highlights the need of reporting strong baselines in our evaluations which is missing from some of the recent work (Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). Next we investigate to what extent hyperparameters (such as the prompt format) and increasing model size impact the zero-shot performance. In all commonsense benchmarks, we see a gap between the zero-shot performance of the worst and best hyper-parameter settings (ranging between 2 to 19 accuracy points). This result shows that for commonsense understanding tasks, it is best not to take pre-trained language models as off-the-shelf tools. When increasing model size, across benchmarks, we observe improvements on both zero-shot performance and the Answer-only baseline, suggesting that larger models are better in exploiting the surface cues. Finally, we examine if pre-trained language models benefit from adding more examples in a fewshot setting or leveraging knowledge extracted from existing commonsense knowledge bases. We do not observe a substantialfrom the few-shot evaluation compared to the zero-shot one. Adding commonsense knowledge also does not yield notable improvements, showing that our language models cannot leverage the relevant knowledge when it is simply added to the prompt. It is an exciting time for language research in both academia and industry, with various recent efforts working on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark Ã¢ÂÂ questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. ",0
"Abstract Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.  Introduction Unsupervised domain adaption (UDA) is an essential task in the realm of deep learning since it mitigates the expensive burden of manual annotation by focusing on cheap unlabeled data from target domains [Ramponi and Plank, 2020]. Among all existing approaches for UDA, pre-trained language model (PrLM) based approaches become the de-facto standard [Gururangan et al., 2020, Ben-David et al., 2020, Yu et al., 2021, Karouzos et al., 2021] since these PrLMs are equipped with generic knowledge learned from large corpora [Howard and Ruder, 2018] and lead to promising results. The primary focuses of UDA methods are to capture the transferable features for the target domain while reserving the knowledge learned from the source domain [Blitzer et al., 2006, Pan et al., 2010]. However, most existing pre-training-based UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . Our contributions can be summarized as: 1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters are introduced in a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. 2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. Related Work Unsupervised Domain Adaption: Existing UDA approaches can be generally classified into two categories: 1) The model-based methods target at augmenting the feature spaces [Glorot et al., 2011, Chen et al., 2012, Ziser and Reichart, 2019, Ben-David et al., 2020], designing new losses [Ganin and Lempitsky, 2015, Ganin et al., 2016] or refining model structures [Bousmalis et al., 2016]; 2) The data-based methods aim to utilize pseudo-labels [Ruder and Plank, 2018, Lim et al., 2020] and develop better data selection schemes [Han and Eisenstein, 2019, Ma et al., 2019]. Some works also try to tackle UDA tasks utilizing large PrLM [Li et al., 2019, Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are becoming the de-facto standard for various NLP tasks. Although promising results are reported, fine-tuning the whole model on a small amount of domain-specific data may distort underlying PrLM and lead to sub-optimal performances. Adapters: In NLP studies, adapter modules are primarily used for parameter-efficient fine-tuning of large PrLMs [Lauscher et al., 2020, Wang et al., 2020, Lin et al., 2021, Poth et al., 2021, Han et al., 2021, Mahabadi et al., 2021]. The most similar works comparing to our study are the models for zero-shot cross-lingual transfer tasks [Pfeiffer et al., 2020, Vidoni et al., 2020]. However, these models aim to separate language-specific knowledge using adapters, while our UDA task tries to capture common and transferable features across different domains. Conclusion This paper presents an adapter-based fine-tuning approach for unsupervised domain adaptation. Trainable adapter modules are inserted in a pre-trained LM, and a two-step training process is introduced to learn the parameters of these adapters. We demonstrate our method on two datasets with different sizes and domain similarities. The results show that the introduced adapter modules facilitate the adaption process, and our two-step training approach helps to further improve the UDA performance. As future works, we will explore more effective architectures of the adapter module.",1
"Abstract Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities. Introduction Unsupervised domain adaption (UDA) is an essential task in the realm of deep learning since it mitigates the expensive burden of manual annotation by focusing on cheap unlabeled data from target domains [Ramponi and Plank, 2020]. Among all existing approaches for UDA, pre-trained language model (PrLM) based approaches become the de-facto standard [Gururangan et al., 2020, Ben-David et al., 2020, Yu et al., 2021, Karouzos et al., 2021] since these PrLMs are equipped with generic knowledge learned from large corpora [Howard and Ruder, 2018] and lead to promising results. The primary focuses of UDA methods are to capture the transferable features for the target domain while reserving the knowledge learned from the source domain [Blitzer et al., 2006, Pan et al., 2010]. However, most existing pre-training-based UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . Our contributions can be summarized as: 1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters arein a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. 2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. Related Work Unsupervised Domain Adaption: Existing UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes.",0
"Abstract We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative-inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. These differences are due to the different semantic roles of the subject of unergative verbs, which shares its agentivity with subjects in transitive frames, and that of unaccusative verbs, more similar to the stereotypically patient- or theme-like objects in transitive frames. Moreover, these categories relate to the causative-inchoative alternation (Haspelmath, 1993) in which unaccusative verbs can express a same event with either agent and patient (1-a) or with a patient only (1-b), corresponding respectively to a causative and inchoative interpretation of the event: (1) a. Hannah popped the balloon. b. The balloon popped. The main property of the causative-inchoative alternation is the ability of the patient to be promoted from object to subject, and most theoretical accounts focus on this transitive to intransitive frame change. Once the event is in its intransitive form, however, it is syntactically indistinguishable from any other intransitive verb construction, and in order to categorize the verb one must know the specific semantic roles of its arguments. This phenomenon is particularly hard to mine because the different realisations are not disambiguated by the context but by the arguments of the verb, and the frequency of the constructions themselves is not an indication of their acceptability. Furthermore, the meaning of the sentence remains virtually the same. 1 However, picking up this categorisation is important not only for reasons involving the appropriateness of different types of subjects for each verb, but also because it has been shown to influence coreference patterns (Loáiciga et al., 2018). Nevertheless, efforts to discover verbs participating in the alternation using automatic methods are very limited. Here we focus on the unaccusative vs unergative distinction as it allows us to disambiguate the verbs based on their intransitive frames. While the subjects of unaccusatives are patients (1-b), subjects of unergatives are agents (2-a). The assumption is that alternating verbs belong to the unaccusative category and we can discover them by separating them from the unergative category. 2 (2) a. Hannah slept The key in using these categories is that, without knowing the semantic roles of the verb, we can measure how well a noun fits the subjective position using a large corpus. Our method relies on language modeling to do just this. We investigate the effect of different language models on the task of identifying unaccusatives vs unergatives by testing the intransitive frames of a large quantity of verbs. Related Work Levin’s (1993) seminal work on verb alternations remains the most comprehensive collection of alternating verbs for English. Other collections, for instance Framenet (Baker et al., 1998) also specify if a verb allows the alternation based on Levin. Typological work for other languages exists in the linguistics literature (Haspelmath, 1993; Schäfer, 2009), but large collections are practically nonexistent.3 Building on Haspelmath’s theory, Samardžic´ (2014) estimates a Spontaneity score based on the ratio of a verb’s transitive to intransitive occurrence. In this scale, verbs are ranked according to the degree to which they are non-agentive. In other words, verbs without an explicit agent causing the event are more likely to participate in the causative alternation. Samardžic and Merlo (2018) ´ report between 61% and 85% agreement between their model and theoretical classifications. Kann et al. (2019) rely on Levin’s work to create synthetic data and build classifiers able to discriminate between several alternations. Their data sets are built using proper names as subjects and common nouns as objects, and they focus on the transitive to intransitive construction case. In this paper we rely on intransitive constructions exclusively, as will be explained below in Section 3. In addition, we use their FAVA data set for evaluation in Section 4. Our method is most similar to the RNN-based method reported by Seyffarth (2019). Contrary to our work which queries language models with in- flected sentences, Seyffarth uses an RNN to score artificially created transitive and intransitive argument sequences of the type invite-Pat-Kim vs invite-Pat. Using Framenet as gold standard, they report an accuracy of 66% on all verbs. Conclusions We proposed a method to detect unaccusative vs unergative verbs based on the generation of intransitive sentence frames of candidate verbs. The results with a large language model show moderate success, highlighting that the causative-inchoative alternation is a challenging meaning distinction to detect automatically. Since the method relies primarily on parsed data and language models, it has the potential to be extended to languages where verbal annotated resources are scarce.",1
"Abstract We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative-inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. These differences are due to the different semantic roles of the subject of unergative verbs, which shares its agentivity with subjects in transitive frames, and that of unaccusative verbs, more similar to the stereotypically patient- or theme-like objects in transitive frames. Moreover, these categories relate to the causative-inchoative alternation (Haspelmath, 1993) in which unaccusative verbs can express a same event with either agent and patient (1-a) or with a patient only (1-b), corresponding respectively to a causative and inchoative interpretation of the event: (1) a. Hannah popped the balloon. b. The balloon popped. The main property of the causative-inchoative alternation is the ability of the patient to be promoted from object to subject, and most theoretical accounts focus on this transitive to intransitive frame change. Once the event is in its intransitive form, however, it is syntactically indistinguishable from any other intransitive verb construction, and in order to categorize the verb one must know the specific semantic roles of its arguments. This phenomenon is particularly hard to mine because the different realisations are not disambiguated by the context but by the arguments of the verb, and the frequency of the constructions themselves is not an indication of their acceptability. Furthermore, the meaning of the sentence remains virtually the same. 1 However, picking up this categorisation is important not only for reasons involving the appropriateness of different types of subjects for each verb, but also because it has been shown to influence coreference patterns (LoÃÂ¡iciga et al., 2018). Nevertheless, efforts to discover verbs participating in the alternation using automatic methods are very limited. Here we focus on the unaccusative vs unergative distinction as it allows us to disambiguate the verbs based on their intransitive frames. While the subjects of unaccusatives are patients (1-b), subjects of unergatives are agents (2-a). The assumption is that alternating verbs belong to the unaccusative category and we can discover them by separating them from the unergative category. 2 (2) a. Hannah slept The key in using these categories is that, without knowing the semantic roles of the verb, we can measure how well a noun fits the subjective position using a large corpus. Our method relies on language modeling to do just this. We investigate the effect of different language models on the task of identifying unaccusatives vs unergatives by testing the intransitive frames of a large quantity of verbs. Related Work LevinÃ¢ÂÂs (1993) seminal work on verb alternations remains the most comprehensive collection of alternating verbs for English. Other collections, for instance Framenet (Baker et al., 1998) also specify if a verb allows the alternation based on Levin. Typological work for other languages exists in the linguistics literature (Haspelmath, 1993; SchÃÂ¤fer, 2009), but large collections are practically nonexistent.3 Building on HaspelmathÃ¢ÂÂs theory, SamardÃ Â¾icÃÂ´ (2014) estimates a Spontaneity score based on the ratio of a verbÃ¢ÂÂs transitive to intransitive occurrence. In this scale, verbs are ranked according to the degree to which they are non-agentive. In other words, verbs without an explicit agent causing the event are more likely to participate in the causative alternation. SamardÃ Â¾ic and Merlo (2018) ÃÂ´ report between 61% and 85% agreement between their model and theoretical classifications. Kann et al. (2019) rely on LevinÃ¢ÂÂs work to create synthetic data and build classifiers able to discriminate between several alternations.data sets are built using proper names as subjects and common nouns as objects, and they focus on the transitive to intransitive construction case. In this paper we rely on intransitive constructions exclusively, as will be explained below in Section 3. In addition, we use their FAVA data set for evaluation in Section 4. Our method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. ",0
"Abstract This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover’s Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking - a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes. Introduction Modern computational methods for Natural Language Processing (NLP) rely on embeddings into metric spaces such as the Euclidean, and more recently non-linear spaces such as the Wasserstein space, to achieve state-of-art performance for various tasks. In these embeddings, the semantic differences and similarities between words and documents, correspond to the distances in the represented space. For embedding into Euclidean spaces, a large body of work is based on Word2vec (Mikolov et al., 2013), where each word is represented as a vector in the Euclidean space. From these word embeddings one can further compute document and sentence embeddings using various models Ramos et al. (2003), Arora et al. (2017), Wang and Kuo (2020), Le and Mikolov (2014), Kiros et al. (2015), Logeswaran and Lee (2018) for higher level NLP tasks. Instead of embedding and comparing documents in the Euclidean space, Word Mover’s Distance (WMD) Kusner et al. (2015) was proposed to measure the similarity between documents in the Wasserstein space Peyré and Cuturi (2018), representing the documents with (empirical) probability distributions. In Huang et al. (2016) WMD is used for supervised learning and more recently in (Yurochkin et al., 2019), for multi-scale representation. To understand how these models work, a lot of effort has been put into aiding interpretability of these embeddings. In Arora et al. (2018) the authors proposed a linear algebraic structure to explain the polysemy of words. Recent works attempted to explain the meaning of each dimension, such as the sparse word embedding Faruqui et al. (2015); Panigrahi et al. (2019) and the POLAR Framework Mathew et al. (2020). To make WMD embeddings interpretable, Xu et al. (2018) proposed an unsupervised topic model in the representation space. In this work our focus is on enabling interpretable supervised WMD embeddings of the documents. Below we summarize the main contributions in this direction. Summary of main contributions - A new approach for contrastive representation learning is proposed via enforcing a clustering promoting mechanism using a set of anchors that in turn are also learned from the data. This, in contrast to previous approaches Huang et al. (2016); Kusner et al. (2015), allows for interpretability, i.e. allows one to determine which words are important for a particular class. Furthermore, compared to the K Nearest Neighbour (KNN), our classification using the learned anchors is faster (O(n) for usual KNN vs O(1) for our NN using anchors), and our method can be generalized to any other supervised contrastive learning. Results on public data sets as well as a on a novel data set evaluating written scientific work by students show the superiority and utility of our method. Discussion of lab report results: The discriminatory words identified by our approach, suggest a good fit with the qualitative differences, namely, claim complexity, scope of evidence, and consistency and closure, used by human coders to make classifications. The words also suggest themes not directly coded for. For example, differences in adjectives reflect differences in claim structure. The importance of adjectives such as positive, negative and relative, reflect the more complex claim structure in high scoring reports. While low scoring reports stated simple claims, high scoring reports compared the relative influence of competing effects (i.e. positive and negative mutations). Another hallmark of high scoring report is qualified or conditional claims that indicate context specificity or uncertainty. The importance of adverbs such as predominantly, largely, and disproportionately, in high-scoring reports, reflects uncertainty, expressed as of probabilistic claims, that were common in these reports. While these properties were not observed from the top words generated by TF-IDF. The predominance of nouns and verbs that describe laboratory procedures (e.g. method, procedure, standardize) in low-scoring reports is an interesting difference not directly coded for by human coders. It is nevertheless consistent with the shift in the laboratory curriculum from an emphasis on reporting on procedures to interpreting and arguing about findings that underlies the shift from low to high scores. Overall these findings suggest that our method captures meaningful qualitative differences originally identified by qualitative researchers",1
"Abstract This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover’s Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking - a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes. Introduction Modern computational methods for Natural Language Processing (NLP) rely on embeddings into metric spaces such as the Euclidean, and more recently non-linear spaces such as the Wasserstein space, to achieve state-of-art performance for various tasks. In these embeddings, the semantic differences and similarities between words and documents, correspond to the distances in the represented space. For embedding into Euclidean spaces, a large body of work is based on Word2vec (Mikolov et al., 2013), where each word is represented as a vector in the Euclidean space. From these word embeddings one can further compute document and sentence embeddings using various models Ramos et al. (2003), Arora et al. (2017), Wang and Kuo (2020), Le and Mikolov (2014), Kiros et al. (2015), Logeswaran and Lee (2018) for higher level NLP tasks. Instead of embedding and comparing documents in the Euclidean space, Word Mover’s Distance (WMD) Kusner et al. (2015) was proposed to measure the similarity between documents in the Wasserstein space Peyré and Cuturi (2018), representing the documents with (empirical) probability distributions. In Huang et al. (2016) WMD is used for supervised learning and more recently in (Yurochkin et al., 2019), for multi-scale representation. To understand how these models work, a lot of effort has been put into aiding interpretability of these embeddings. In Arora et al. (2018) the authors proposed a linear algebraic structure to explain the polysemy of words. Recent works attempted to explain the meaning of each dimension, such as the sparse word embedding Faruqui et al. (2015); Panigrahi et al. (2019) and the POLAR Framework Mathew et al. (2020). To make WMD embeddings interpretable, Xu et al. (2018) proposed an unsupervised topic model in the representation space. In this work our focus is on enabling interpretable supervised WMD embeddings of the documents. Below we summarize the main contributions in this direction. Summary of main contributions - A new approach for contrastive representation learning is proposed via enforcing a clustering promoting mechanism using a set of anchors that in turn are also learned from the data. This, in contrast to previous approaches Huang et al. (2016); Kusner et al. (2015), allows for interpretability, i.e. allows one to determine which words are important for a particular class. Furthermore, compared to the K Nearest Neighbour (KNN), our classification using the learned anchors is faster (O(n) for usual KNN vs O(1) for our NN using anchors), and our method can be generalized to any other supervised contrastive learning. Results on public data sets as well as a on a novel data set evaluating written scientific work by students show the superiority and utility of our method. Discussion of lab report results: The discriminatory words identified by our approach, suggest a good fit with the qualitative differences, namely, claim complexity, scope of evidence, and consistency and closure, used by human coders to make classifications. The words also suggest themes not directly coded for. For example, differences in adjectives reflect differences in claim structure. The importance of adjectives such as positive, negative and relative, reflect the more complex claim structure in high scoring reports. While low scoring reports stated simple claims, high scoring reports compared the relative influence of competing effects (i.e. positive and negative mutations). Another hallmark of high scoring report is qualified or conditional claims that indicate context specificity or uncertainty. The importance of adverbs such as predominantly, largely, and disproportionately, in high-scoring reports, reflects uncertainty, expressed as of probabilistic claims, that were common in these reports. While these properties were not observed from the top words generated by TF-IDF. The predominance of nouns and verbs that describe laboratory procedures (e.g. method, procedure, standardize) in low-scoring reports is an interesting difference not directly coded for by human coders. It is nevertheless consistent with the shift in the laboratory curriculum from an emphasis on reporting on procedures to interpreting and arguing about findings that underlies the shift from low to high scores. Overall these findings suggest that our method captures meaningful qualitative differences originally identified by qualitative researchers",0
"Abstract Code-switching (CS), a ubiquitous phe- nomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks – POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Introduction Code-switching (CS) is a phenomenon of switching back and forth between multiple languages and is very common in multilingual communities such as India, Singapore, etc. Understanding mixed language texts has several applications in an in- creasingly online world like hateful content de- tection, maintaining engagement with virtual as- sistants. Despite this pervasive prevalence, CS is often overlooked in language processing research and current models still cannot effectively handle CS. We believe that the reasons behind this are (1) the lack of efforts in leveraging existing large scale multilingual resources or pretrained models and (2) dearth of annotated resources in switching scenar- ios. In this paper, we present solutions to address these two problems specifically. The advent of pretraining techniques marshalled the celebrated successes of several language un- derstanding and generation tasks in English (Dong et al., 2019) and multilingual tasks (Chaudhary et al., 2020). However, the same level of com- mendatory results are not translated to CS scenar- ios; as studied by Aguilar et al. (2020); Khanuja et al. (2020) presenting a preliminary evaluation of multi-lingual pretrained models for CS scenarios. It is still largely unclear if the inadequacies are re- sulting due to dearth of data or ineptitude of quick adoption of multilingual models. We study pre- cisely this problem of identifying the artifacts that hinder the competent performance of pretrained models on CS with a case study on sequence label- ing tasks including Part-Of-Speech (POS) tagging and Named Entity Recognition (NER). Our contributions from this work are as fol- lows: (1) We first conduct a comprehensive bench- marking of different pretrained models for two se- quence labeling tasks across 4 different language pairs. Specifically we evaluate datasets in Hinglish, Tenglish, Benglish and Spanglish CS for the tasks NER and POS. (2) To broaden understanding to- wards the usefulness of different fine-tuning strate- gies, we investigate multitasking, character model- ing uncovering the problematic switch point cases in §4. (3) We propose a novel switch-point bias based self training approach built upon on obser- vations from the benchmarks and demonstrate im- proved results on both tasks. Related Work CS benchmarks: From one of the recent surveys (Sitaram et al., 2019), linguistic CS has been stud- ied in the context of many NLP tasks including language identification (Solorio et al., 2014) (Bali et al., 2014), POS tagging (Soto and Hirschberg, 2018) (Molina et al., 2019) (Das, 2016), NER (Aguilar et al., 2019), parsing (Partanen et al., 2018), sentiment analysis(Vilares et al., 2015), and question answering (Chandu et al., 2019) (Raghavi et al., 2015). Many CS datasets have been made available through the shared-task series FIRE (Choudhury et al., 2014); (Roy et al., 2013) and CALCS (Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other re- searchers have provided datasets such as humor detection (Khandelwal et al., 2018), sub-word CS detection (Mager et al., 2019) among others. More recently new CS benchmarks (Aguilar et al., 2020) (Khanuja et al., 2020) have been developed to com- pare models across language pairs, domains and general language processing in CS. Pretrained Models for CS: Before the advent of pretrained multingual models, pretrained mono- lingual models were combined in different ways to derive word embeddings (AlGhamdi and Diab, 2019; Pratapa et al., 2018), POS tagging (Bhattu et al., 2020), sentiment analysis (Singh and Lefever, 2020) etc., Similarly, pretrained multilingual mod- els have been explored on various CS tasks like language identification, POS tagging, NER, ques- tion answering and Natural language inference (Khanuja et al., 2020). However, (Winata et al., 2021) show that these pretrained models do not assure high quality representations on CS. We ex- amine prospective reasons for this and present a data augmentation technique to mitigate this. Motivation for our work - Gaps in CS adapta- tion: Building off the prior work, we will briefly discuss primarily three techniques that demon- strated usefulness in adapting models to CS. First, non-standardization of cross-scripting (i.e, translit- eration of words to another language) is identified as one of the major reasons behind the noisiness of CS datasets (Chandu et al., 2019). Prior literature on noisy texts proved the superiority of charac- ter level modeling to combat this problem (Cherry et al., 2018); (Adouane et al., 2018). Secondly, the domains of most of these noisy datasets are still vastly scattered. In order to improve general- ization in CS patterns, prior studies have shown the potency of multitasking with an auxiliary task of language tag prediction (Winata et al., 2018). Thirdly, the dearth of annotated CS data has been a dramatic problem across tasks. (Bhattu et al.,2020) compare pretrained models with fined-tuned models augmented with unlabeled Twitter text to exemplify the improved performance with the lat- ter model. Despite these takeaways, the usefulness of the three points above is not thoroughly inves- tigated in the context of pretrained models for CS. To this end, we adapt these techniques in conjunc- tion with the pretraining strategies and propose a novel bias-based data iterative augmentation tech- nique to get more bang for the buck in terms of the performance to augmented dataset size ratio. Conclusions CS, despite being a natural and prevalent form of communication is still vastly understudied in em- pirical research. This mainly stems from the (1) lack of efforts in re-purposing the celebrated pre-trained models to CS scenarios and (2) lack of annotated resources. We tackle precisely these 2 problems with the main focus on evaluating and improving how these models fare at switch points between languages. First, we benchmark a suite of monolingual and multilingual pretrained models on CS and identify that particular switch points fare poorly. We propose a novel switch point bias based self training method to strategically use unla- beled data to enhance performance at switch points. While improving or retaining the overall perfor- mance compared to finetuning char-BERT and mul- titasking, we show that our approach improves the performance of underperforming switch points as well. We believe that this bias based augmentation technique particularly helps in scenarios with less annotated data. Broader Impact We believe that this work is a step towards effac- ing the hesitation of utilizing large scale pretrained mono and multilingual models for code-switched scenarios. We were able to successfully demon- strate the utility of a switch point based annota- tor model to perform biased data augmentation. We do not foresee any immediate ethical concerns branching directly from our work. However, we cautiously advise anyone using or extending our work for their application or research to bear in mind that we inherit any kinds of biases and tox- icity and privacy concerns that the pretrained lan- guage models bear. Although our end tasks are not directly affected forthwith due to these, we still rec- ommend caution when our self training approach is used for other tasks especially with user interaction such as dialog response generation etc., to ensure the model does not predict toxic content. Overall, we expect the users to benefit from our research to prospectively apply this to scenarios where there is a dearth of annotated resources, thereby economiz- ing on annotations cost and efforts and enabling scaling up to a wealth of crawled data, if available in those language-pairs.",1
"Abstract Code-switching (CS), a ubiquitous phe- nomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks Ã¢ÂÂ POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Introduction Code-switching (CS) is a phenomenon of switching back and forth between multiple languages and is very common in multilingual communities such as India, Singapore, etc. Understanding mixed language texts has several applications in an in- creasingly online world like hateful content de- tection, maintaining engagement with virtual as- sistants. Despite this pervasive prevalence, CS is often overlooked in language processing research and current models still cannot effectively handle CS. We believe that the reasons behind this are (1) the lack of efforts in leveraging existing large scale multilingual resources or pretrained models and (2) dearth of annotated resources in switching scenar- ios. In this paper, we present solutions to address these two problems specifically. The advent of pretraining techniques marshalled the celebrated successes of several language un- derstanding and generation tasks in English (Dong et al., 2019) and multilingual tasks (Chaudhary et al., 2020). However, the same level of com- mendatory results are not translated to CS scenar- ios; as studied by Aguilar et al. (2020); Khanuja et al. (2020) presenting a preliminary evaluation of multi-lingual pretrained models for CS scenarios. It is still largely unclear if the inadequacies are re- sulting due to dearth of data or ineptitude of quick adoption of multilingual models. We study pre- cisely this problem of identifying the artifacts that hinder the competent performance of pretrained models on CS with a case study on sequence label- ing tasks including Part-Of-Speech (POS) tagging and Named Entity Recognition (NER). Our contributions from this work are as fol- lows: (1) We first conduct a comprehensive bench- marking of different pretrained models for two se- quence labeling tasks across 4 different language pairs. Specifically we evaluate datasets in Hinglish, Tenglish, Benglish and Spanglish CS for the tasks NER and POS. (2) To broaden understanding to- wards the usefulness of different fine-tuning strate- gies, we investigate multitasking, character model- ing uncovering the problematic switch point cases in ÃÂ§4. (3) We propose a novel switch-point bias based self training approach built upon on obser- vations from the benchmarks and demonstrate im- proved results on both tasks. Related Work CS benchmarks: From one of the recent surveys (Sitaram et al., 2019), linguistic CS has been stud- ied in the context of many NLP tasks including language identification (Solorio et al., 2014) (Bali et al., 2014), POS tagging (Soto and Hirschberg, 2018) (Molina et al., 2019) (Das, 2016), NER (Aguilar et al., 2019), parsing (Partanen et al., 2018), sentiment analysis(Vilares et al., 2015), and question answering (Chandu et al., 2019) (Raghavi et al., 2015). Many CS datasets have been made available through the shared-task series FIRE (Choudhury et al., 2014); (Roy et al., 2013) and CALCS (Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other re- searchers have provided datasets such as humor detection (Khandelwal et al., 2018), sub-word CS detection (Mager et al., 2019) among others. More recently new CS benchmarks (Aguilar et2020) (Khanuja et al., 2020) have been developed to com- pare models across language pairs, domains and general language processing in CS. Pretrained Models for CS: Before the advent of pretrained multingual models, pretrained mono- lingual models were combined in different ways to derive word embeddings (AlGhamdi and Diab, 2019; Pratapa et al., 2018), POS tagging (Bhattu et al., 2020), sentiment analysis (Singh and Lefever, 2020) etc., Similarly, pretrained multilingual mod- els have been explored on various CS tasks like language identification, POS tagging, NER, ques- tion answering and Natural language inference (Khanuja et al., 2020). However, (Winata et al., 2021) show that these pretrained models do not assure high quality representations on CS. We ex- amine prospective reasons for this and present a data augmentation technique to mitigate this. Motivation for our work - Gaps in CS adapta- tion: Building off the prior work, we will briefly discuss primarily three techniques that demon- strated usefulness in adapting models to CS. First, non-standardization of cross-scripting (i.e, translit- eration of words to another language) is identified as one of the major reasons behind the noisiness of CS datasets (Chandu et al., 2019). Prior literature on noisy texts proved the superiority of charac- ter level modeling to combat this problem (Cherry et al., 2018); (Adouane et al., 2018). Secondly, the domains of most of these noisy datasets are still vastly scattered. In order to improve general- ization in CS patterns, prior studies have shown the potency of multitasking with an auxiliary task of language tag prediction (Winata et al., 2018). Thirdly, the dearth of annotated CS data has been a dramatic problem across tasks. (Bhattu et al.,2020) compare pretrained models with fined-tuned models augmented with unlabeled Twitter text to exemplify the improved performance with the lat- ter model. Despite these takeaways, the usefulness of the three points above is not thoroughly inves- tigated in the context of pretrained models for CS. To this end, we adapt these techniques in conjunc- tion with the pretraining strategies and propose a novel bias-based data iterative augmentation tech- nique to get more bang for the buck in terms of the performance to augmented dataset size ratio. Conclusions CS, despite being a natural and prevalent form of communication is still vastly understudied in em- pirical research. This mainly stems from the (1) lack of efforts in re-purposing the celebrated pre-trained models to CS scenarios and (2) lack of annotated resources. We tackle precisely these 2 problems with the main focus on evaluating and improving how these models fare at switch points between languages. First, we benchmark a suite of monolingual and multilingual pretrained models on CS and identify that particular switch points fare poorly. We propose a novel switch point bias based self training method to strategically use unla- beled data to enhance performance at switch points. While improving or retaining the overall perfor- mance compared to finetuning char-BERT and mul- titasking, we show that our approach improves the performance of underperforming switch points as well. We believe that this bias based augmentation technique particularly helps in scenarios with less annotated data. Broader Impact We believe that this work is a step towards effac- ing the hesitation of utilizing large scale pretrained mono and multilingual models for code-switched scenarios. We were able to successfully demon- strate the utility of a switch point based annota- tor model to perform biased data augmentation. We do not foresee any immediate ethical concerns branching directly from our work. However, we cautiously advise anyone using or extending our work for their application or research to bear in mind that we inherit any kinds of biases and tox- icity and privacy concerns that the pretrained lan- guage models bear. Although our end tasks are not directly affected forthwith due to these, we still rec- ommend caution when our self training approach is used for other tasks especially with user interaction such as dialog response generation etc., to ensure the model does not predict toxic content. Overall, we expect the users to benefit from our research to prospectively apply this to scenarios where there is a dearth of annotated resources, thereby economiz- ing on annotations cost and efforts and enabling scaling up to a wealth of crawled data, ifin those language-pairs.",0
"Abstract Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of re- cent work that uses these large language mod- els to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation ap- proaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function. Note that the latent representation needs to be learned afresh for each new NLP task, and that, in many cases, the size of the training data limits the quality of the latent feature representation. Given that the nu- ances of language are common to all NLP tasks, one could posit that we could learn a generic la- tent feature representations from some generic task once, and then share it across all NLP tasks. Lan- guage modeling, where the model needs to learn how to predict the next word given previous words, is such a generic task with abundant naturally oc- curring text to pre-train such a model (hence the name pre-trained language models). In fact, the lat- est, ongoing paradigm shift begins when PLMs are introduced: for numerous NLP tasks, researchers now leverage existing PLMs via fine-tuning for the task of interest, prompting the PLMs to perform the desired task, or reformulating the task as a text gen- eration problem with application of PLMs to solve it accordingly. Advances in these three PLM-based paradigms have continuously established new state- of-the-art performances. This paper surveys recent works that leverage PLMs for NLP. We organize these works into the following three paradigms: Pre-train then fine-tune (§ 2): perform general- purpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. Prompt-based learning (§ 3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLM’s pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompt- ing can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. NLP as text generation (§ 4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). Generative PLMs can be also used for text gen- eration tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic pars- ing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), senti- ment analysis, and so on. In addition to the three paradigms, there is an- other, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: Data generation (§ 5): run PLMs to automat- ically generate data for NLP tasks. The gen- erated data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as coun- terexamples, clarifications, contexts, or other. In the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the target task in some way. The paper is organized as follows: Section 2 provides background on the PLMs and describes the first paradigm, pre-train then fine-tune. Sec- tion 3 discusses the second paradigm, prompt- based learning. Section 4 summarizes works in the third paradigm, NLP as text generation. In Sec- tion 5, we describe approaches that generate data via PLMs for a broad range of NLP tasks. We dis- cuss limitations and provide directions for future research in Section 6 and conclude in Section 7. Conclusion In this paper, we present a survey of the three trend- ing paradigms that use pre-trained language models for NLP. We describe each of them in depth, and summarize prior works whose applications have shown promise. In addition, we describe the use of pre-trained language models to automatically generate data that is used to improve performance in NLP tasks. We hope this survey will provide readers with key fundamental concepts and a com- prehensive view of the paradigm shift.",1
"Abstract Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of re- cent work that uses these large language mod- els to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation ap- proaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function. Note that the latent representation needs to be learned afresh for each new NLP task, and that, in many cases, the size of the training data limits the quality of the latent feature representation. Given that the nu- ances of language are common to all NLP tasks, one could posit that we could learn a generic la- tent feature representations from some generic task once, and then share it across all NLP tasks. Lan- guage modeling, where the model needs to learn how to predict the next word given previous words, is such a generic task with abundant naturally oc- curring text to pre-train such a model (hence the name pre-trained language models). In fact, the lat- est, ongoing paradigm shift begins when PLMs are introduced: for numerous NLP tasks, researchers now leverage existing PLMs via fine-tuning for the task of interest, prompting the PLMs to perform the desired task, or reformulating the task as a text gen- eration problem with application of PLMs to solve it accordingly. Advances in these three PLM-based paradigms have continuously established new state- of-the-art performances. This paper surveys recent works that leverage PLMs for NLP. We organize these works into the following three paradigms: Pre-train then fine-tune (ÃÂ§ 2): perform general- purpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. Prompt-based learning (ÃÂ§ 3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLMÃ¢ÂÂs pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompt- ing can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. NLP as text generation (ÃÂ§ 4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). Generative PLMs can be also used for text gen- eration tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic pars- ing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), senti- ment analysis, and so on. In addition to the three paradigms, there is an- other, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: Data generation (ÃÂ§ 5): run PLMs to automat- ically generate data for NLP tasks. The gen-data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as coun- terexamples, clarifications, contexts, or other. In the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the abl- ity of the first com- prehensive language mod- el. Data can also be added to existing un- labeled data. In any of the cases, the size of the data augmentation and the quality of the data augmentation are not fully established. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function.",0
"Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently – utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracy over unsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised distri- butions and how they relate to each other. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of NLP tasks including sentiment classification, entity typing, text classi- fication, sentence-pair classification and relation classification. Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al., 2019). Unsupervised meta-learning has been explored in computer vi- sion (Hsu et al., 2019; Khodadadeh et al., 2019) and reinforcement learning (Gupta et al., 2018). Hsu et al. (2019) cluster images using pre-trained embeddings to create tasks. Metz et al. (2019) meta-learn an unsupervised update rule in a semi- supervised framework. Bansal et al. (2020b) de- veloped the SMLMT approach to unsupervised meta-learning in NLP. Contemporary work (Murty et al., 2021) explored the use of clustering, though focused only on natural language inference tasks. Curriculum learning (Bengio et al., 2009) in the context of meta-learning has been unexplored in NLP, prior to this work. Jabri et al. (2019) found unsupervised curriculum to be beneficial for meta- reinforcement learning. We refer to Hospedales et al. (2020) for a comprehensive review of meta- learning. Self-supervised learning has emerged as an effi- cient approach to representation learning in NLP (Howard and Ruder, 2018; Peters et al., 2018; De- vlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Multi-task learning of pre-trained models has shown improved results on many tasks (Phang et al., 2018; Liu et al., 2019a), including few-shot setting. Yin et al. (2020) leveraged entailment tasks for few-shot learning. Du et al. (2020) developed self-training methods for semi-supervised few-shot learning. Recently, extremely large language mod- els have been shown to have few-shot capacities (Brown et al., 2020), while Schick and Schütze (2020) demonstrated few-shot capacities for small models in the semi-supervised setting. Meanwhile, Bansal et al. (2020a,b) showed meta-learning to be effective at improving few-shot performance in multi-task and unsupervised settings, as well as improving performance for small models. Conclusion We explored several approaches to self-supervised task distribution for meta-learning. Our results demonstrate improvements in few-shot perfor- mance over a wide-range of classification tasks. This demonstrates the utility of meta-learning from unlabeled data, opening up the possibility of large- scale meta-learning for pertinent applications in NLP such as continual learning, architecture search, learning for low-resource languages, and more.",1
"Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning Ã¢ÂÂ adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently Ã¢ÂÂ utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracyunsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised methods on a wide range of tasks including sentiment classification, entity typing, text classi- fication, sentence-pair classification and relation classification. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of tasks including entity typing, sentence-pair classification and relation classification. Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Bansal et al., 2019; Linzen, 2020). Unsupervised tasks have been explored in computer vi- sion (Hsu et al., 2019; Bansal et al., 2019; Khodadadeh et al., 2020b), reinforcement learning (Gupta et al., 2019), natural language processing (Bengio et al., 2019), networked learning (Bengio et al., 2019), natural language processing (Bengio et al., 2019), natural language processing for NLP (Hospedales et al., 2020a), and data-driven learning (Gupta et al., 2019). Unsupervised meta-learning has been explored in NLP optimization (Hospedales et al., 2020b), learning systems (Hospedales et al., 2020c), as well as meta-learning methods in NLP that can be applied to many NLP tasks, however, meta-learning methods tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2019) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2019). Contemporary work (Murty et al., 2019) has been explored in NLP meta-learning for NLP, how- ever, is it possible to achieve significant improvements in downstream few-shot accuracy of the meta- learned models? In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2019), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2019).",0
"Abstract The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their neg-ative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detec-tion tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibil-ity of leveraging domain-specific word embed-ding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirec-tional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets. Introduction Social media has been used extensively for various purposes, such as advertising, business, news, etc. The idea of allowing users to post anything at any time on social media contributed to the existence of inappropriate content on social media. As a result, these platforms become a fertile environment for this type of content. Hate speech is the most com- mon form of destructive content on social media, and it can come in the form of text, photographs, or video. It is defined as an insult directed at a per-son or group based on characteristics such as color, gender, race, sexual orientation, origin, nationality, religion, or other characteristics (Weber, 2009). Hate speech poses a significant threat to commu-nities, either by instilling hatred in young people against others or by instigating criminal activity or violence against others. Hate speech on the internet is on the rise around the world, with approximately 60% of the global population (4:54 billion) using social media to com- municate (Ltd, 2020). According to studies, ap- proximately 53 percent of Americans have encoun- tered online harassment and hatred (League, 2019). This score is 12 points higher than the findings of a similar survey performed in 2017 (Duggan, 2017). According to Clement (2019), 21% of students frequently encounter hate speech on social media. Thus, the detection of hate content on social media is an essential and necessary requirement for so- cial media platforms. Social media providers work hard to get rid of this content for a safer social en- vironment. Detecting hateful content is considered one of the challenging NLP tasks as the content might target/attack individuals or groups based on various characteristics using different hate terms and phrases (Badjatiya et al., 2017). Social media users often employ abbreviations and ordinary words(not hateful) to express their hate intent implicitly that known as code words to evade from being detected (e.g., using Google to refer to dark skin people), which adds extra diffi- culties to detect hate speech. Many studies have proposed machine learning models to handle this problem by utilizing a wide range of features set and machine learning algorithms for classification (Magu et al., 2017; Agarwal and Sureka, 2015; Jaki and De Smedt, 2019; Hartung et al., 2017). These methods often utilize features that require considerable effort and time to be extracted, such as text-based, profile-based, and community-based features. Other studies have worked on linguistic-based features (e.g., word frequency) and deep learning for classification (de Gibert et al., 2018), or distributional based features (e.g., word embed-ding) and machine learning classifier (Gupta and Waseem, 2017; Badjatiya et al., 2017; Djuric et al., 2015; Nobata et al., 2016). Studies show that distributional features provide a promising result in NLP tasks such as sentiment analysis (Gupta and Waseem, 2017). Recently, deep learning methods also show that it performs well on various NLP problems (Socher et al., 2012). Thus, this study investigates the performance of em- ploying these two methods. Accordingly, this study uses the distributional-based learning method to extract meaningful domain-specific embedding as features and deep learning based on Bidirectional Long Short Term Memory (BiLSTM) classifier to detect hate speech. The word embedding/ distribu- tional representation in this research is built upon a hate speech corpus of 1; 048; 563 sentences to reach the closest meaningful representation vec-tor of hate words. Then, compare it with the domain- agnostic embedding model such as Google Word2Vec and GloVe under the same classifier. We also assess the performance of detecting hate speech using Google’s pre-trained BERT model, which has generally achieved a state-of-the-art for many NLP tasks. The contributions of this research are highlighted as follow: An unsupervised domain-specific word em-bedding model was developed to extract the meaning of commonly used terminology, acronyms, and purposefully misspelled hate words. A comparison between the domain- specific and domain agnostic embedding was provided. The findings show that domain agnostic em-bedding performs slightly better (about 1%), despite the huge difference in the trained cor-pus size. The evaluation of a BiLSTM-based deep model with domain-specific embeddings shows an improvement ranging from 5 to 6 points on available datasets over the state-of-the-art techniques. The evaluation of the BERT language model on the hate speech binary classification task shows an improvement of about 2 points com-pared to the domain-specific word embedding model. The remaining of this paper is constructed as follows: the background section provides back- ground information about hate detection and the used methodologies; the review of literature sec-tion includes most recent studies in the field; the methodology section describes the methods used in this study and its specification; the experiment and result section presents the used datasets, em- bedding models, and results of the experiments; the discussion section includes analysis and obser- vation from the results, and finally the conclusion section summarizes all the findings. Background This section gives an overview of hate speech detec-tion in the field and it provides information about the used methodologies for both of the features and classifiers. 2.1 Hate Speech Detection Several research have attempted to solve the prob- lem of detecting hate speech in general by differ- entiating hate and non-hate speech. (Ribeiro et al., 2017; Djuric et al., 2015). Others have tackled the issue of recognizing certain types of hate speech, such as anti-religious hate speech. (Albadi et al., 2018; Zhang et al., 2018), jihadist (De Smedt et al., 2018; Ferrara et al., 2016; Wei et al., 2016; Gialam- poukidis et al., 2017), sexist and racist (Badjatiya et al., 2017; Pitsilis et al., 2018; Gamback ̈ and Sik- dar, 2017). Several platforms have been used to collect datasets from various online resources such as websites or online forums (e.g., 4Chan, Dai- lyStorm), or recent social media platforms (e.g., Twitter, Facebook). Hate speech has been applied also on different languages (e.g., English, Arabic, German). 2.2 Word Embedding Word embedding (Bengio et al., 2003) is a promi-nent natural language processing (NLP) technique that seeks to convey the semantic meaning of a word. It provides a useful numerical description of the term based on its context. The words are repre-sented by a dense vector that can be used in estimat-ing the similarities between the words (Liu, 2018). The word is represented by an N- dimensional vec-tor appropriate to represent the word meaning in a specific language (Mikolov et al., 2013). The word embedding has been widely used in many recent NLP tasks due to its efficiency such as text classification (Gamback ̈ and Sikdar, 2017; Lilleberg et al., 2015), document clustering (Ailem et al., 2017), part of speech tagging (Wang et al., 2015), named entity recognition (Siencnikˇ, 2015), sentiment anal-ysis (Tang et al., 2014; Wang et al., 2016; Al-Azani and El-Alfy, 2017), and many other problems. The most common pretrained word embedding models are Google Word2Vec, Stanford GloVe and they are described as follow: 2.2.1 Word2Vec Word2Vec is one of the most recently used word embedding models. It is provided by the Google research team (Mikolov et al., 2013). Word2Vec associates each word with a vector-based on its surrounding context from a large corpus. The train- ing process for extracting the word vector has two types, the continuous bag of words model (CBOW), which predicts the target word from its context, and the Skip-Gram model (SG), which predicts the tar- get context from a given word. The feature vector of the word is manipulated and updated accord-ing to each context the word appears in the cor-pus. If the word embedding is trained well, similar words appear close to each other in the dimensional space. The word similarities between the words are measured by the cosine distance between their vec- tors. Google released a vector model called Google Word2Vec that has been trained on a massive cor- pus of over 100 billion words. 2.2.2 GloVe Pennington et al. (2014) provides another popular word embedding model named GloVe (Global Vec- tors for Word Representation). GloVe learns em- beddings using an unsupervised learning algorithm that is trained on a corpus to create the distribu- tional feature vectors. During the learning process, a statistics-based matrix is built to represent the words to words co-occurrence of the corpus. This matrix represents the word vectors. The learning process requires time and space for the matrix con- struction, which is a highly costly process. The difference between GloVe and Word2Vec is in the learning process, Word2Vec is a prediction based model, and GloVe is a count-based model. The GloVe is learned from Wikipedia, web data, Twit-ter, and each model is available with multiple vector dimensions. 2.3 Bidirectional Long Short-Term Memory (BiLSTM) LSTM (Hochreiter and Schmidhuber, 1997) is an enhanced version of the recurrent neural network, which is one of the deep learning models that is designed to capture information from a sequence of information. It differs from the feed-forward neural network in that it has a backward connection. RNN suffers from a vanishing gradient problem that happens when the weights are not updated anymore due to the small value of the received from error function in respect to the current weights in the iteration. The value is vanishing in very long sequences and becomes close to zero. This problem stops RNN from training. LSTM solves this problem by adding an extra interaction cell to preserve long sequence dependencies. Thus, LSTM saves data for long sequences, but it saves the data only from left to right. However, to save sequence data from both directions, a Bidirectional LSTM (BiLSTM) is used. BiLSTM consist of two LSTM, one process the data from left to right and the other in opposite direction then concatenates and flattens both forward and backward LSTM to improve the knowledge of the surrounding context. 2.4 BERT Pre-trained Language Model Bidirectional Encoder Representations from Trans- formers (BERT) (Devlin et al., 2018) is a language model trained on very huge data based on con- textual representations. BERT consists of feature extraction layers, which consist of word embed-ding and layer for the model (e.g., Classification, Question Answering, Named Entity Recognition). BERT is the most recent language model and pro- vides state of the art results in comparison to other language models for various NLP tasks. BERT training procedure of word embedding differs from other word embedding models. It creates a bidirec- tional representation of words that may be learned from both left and right directions. Word embed-ding approaches like Word2Vec and GloVe only examine one direction (either left to right or right to left), resulting in static word representations that do not alter with context. If the word’s meaning varies depending on the context, GloVe and Word2Vec map the word to only one embedding vector. As a result, Word2Vec and GloVe are referred to as context-free models. BERT is also different from previous language models (e.g., ELMo stands for Embeddings from Language Models (Peters et al., 2018)) in that it manipulates the context in all layers in both directions (left and right). Instead of shal-low combining processes such as concatenating, it use cooperatively conditioning to combine both left and right context. BERT is trained on Books Cor-pus (800M words) and English Wikipedia (2,500M words) (Devlin et al., 2018). Review of Literature It is worth noting that word embedding is an effec- tive approach to a variety of NLP issues. To extract bio-events from the scientific literature, Li et al. (2015) used word embedding. They used multiple sets of features such as, word embedding, BOW + n-gram joint model, and word embedding BOW joint model with SVM classifier and the overall per- formance of word embedding BOW is better than other models on different events, which reached to 77:37% f1-score. The pure word embedding model has lower performance because the dataset size is small. Wu et al. (2015) also used word embedding to distinguish clinical abbreviations as a special case of word sense disambiguation (WSD). The performance of SVM increased when employing word embedding features with an average accuracy of 93%. Hate speech identification is a prevalent issue that has gotten a lot of attention from researchers. Liu (2018) employed domain-specific word em- bedding model trained on the articles from hate speech websites and high centrality users’ tweets to reach to the semantics of code words used in hate speech. They experimented on CNN, and LSTM models and concluded that CNN performed better than LSTM on tweets due to the length of tweets. The achieved f1-score is 78% given that they ex- perimented on the previous tweet-length 180 char- acters. Gupta and Waseem (2017) evaluate the performance of using hate Word2Vec (i.e., domain- specific) model with Logistic Regression (LR) clas- sifier on three datasets. They achieved up to 91% f1-score. The results showed that domain-specific word embedding has a desirable performance and is suitable for unbalanced classes datasets. Nobata et al. (2016) aimed to detect abusive language using pretrained word embeddings on two domains (finance and news) and regression model for classification, they achieved 60:2% and 64:9% f1-score respectively. The results showed that Google Word2Vec provides better performance with 5% on both domains. Badjatiya et al. (2017) ​​employed deep learning techniques to extract em- bedding features from hate speech text and then used a decision tree model for classification. They reached 93% f1-score using random embeddings initialization that is fed to LSTM to construct fea- tures. The results proved that domain-specific em- bedding can provide better representation of hate words such as “racist” or “sexist” words, because it can extract the meaning of frequently used terms by the hate community, domain-specific-based de- tection is a promising method for the detection of hate speech, according to all of the research above. The authors of (Devlin et al., 2018) looked into the BERT model’s performance on a variety of NLP tasks. On eleven of these tasks, the model produced state-of-the-art results. It improved by 7:7 points in the General Language Understanding Evaluation (GLUE) benchmark, 4:6 points in Multi- Genre Nat-ural Language Inference (MultiNLI), and 1:5 to 5:1 points in the SQuAD various versions question an-swering tests. BERT language model was recently employed in a shared task to detect offensive language (Zhu et al., 2019; Pelicon et al., 2019; Wu et al., 2019). Zhu et al. (2019) fine tuned BERT model for this task and came in third place among competitors. They used 13; 240 in tweets to train the algorithm, with each message categorized as offensive or not offensive. They achieved 83:88% f1-score. Moza- fari et al. (2020) studied the performance of the BERT language model on hate speech detection as a multi-class problem in a recently released work. They employed a BERT basis and a variety of clas- sifiers, including CNN, which provided the highest score that reached to 92% f1-score. Conclusion and Future Work To conclude, BERT design provides an appropriate feature extraction and classification procedure for hate speech detection. BERT combines the benefits of domain agnostic and domain-specific word embedding by train the model on vast data then add an extra layer to trained on domain-specific data (fine-tuning). BERT also saves effort and time for building an embedding model from scratch. However, domain-specific word embedding overcomes BERT model in that it can detect hate terms and abbreviations and intentionally misspellings meaning. This study can be extended to detect multi-class hate speech for future work.",1
"Abstract The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their neg-ative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detec-tion tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibil-ity of leveraging domain-specific word embed-ding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirec-tional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets. Introduction Social media has been used extensively for various purposes, such as advertising, business, news, etc. The idea of allowing users to post anything at any time on social media contributed to the existence of inappropriate content on social media. As a result, these platforms become a fertile environment for this type of content. Hate speech is the most com- mon form of destructive content on social media, and it can come in the form of text, photographs, or video. It is defined as an insult directed at a per-son or group based on characteristics such as color, gender, race, sexual orientation, origin, nationality, religion, or other characteristics (Weber, 2009). Hate speech poses a significant threat to commu-nities, either by instilling hatred in young people against others or by instigating criminal activity or violence against others. Hate speech on the internet is on the rise around the world, with approximately 60% of the global population (4:54 billion) using social media to com- municate (Ltd, 2020). According to studies, ap- proximately 53 percent of Americans have encoun- tered online harassment and hatred (League, 2019). This score is 12 points higher than the findings of a similar survey performed in 2017 (Duggan, 2017). According to Clement (2019), 21% of students frequently encounter hate speech on social media. Thus, the detection of hate content on social media is an essential and necessary requirement for so- cial media platforms. Social media providers work hard to get rid of this content for a safer social en- vironment. Detecting hateful content is considered one of the challenging NLP tasks as the content might target/attack individuals or groups based on various characteristics using different hate terms and phrases (Badjatiya et al., 2017). Social media users often employ abbreviations and ordinary words(not hateful) to express their hate intent implicitly that known as code words to evade from being detected (e.g., using Google to refer to dark skin people), which adds extra diffi- culties to detect hate speech. Many studies have proposed machine learning models to handle this problem by utilizing a wide range of features set and machine learning algorithms for classification (Magu et al., 2017; Agarwal and Sureka, 2015; Jaki and De Smedt, 2019; Hartung et al., 2017). These methods often utilize features that require considerable effort and time to be extracted, such as text-based, profile-based, and community-based features. Other studies have worked on linguistic-based features (e.g., word frequency) and deep learning for classification (de Gibert et al., 2018), or distributional based features (e.g., word embed-ding) and machine learning classifier (Gupta and Waseem, 2017; Badjatiya et al., 2017; Djuric et al., 2015; Nobata et al., 2016). Studies show that distributional features provide a promising result in NLP tasks such as sentiment analysis (Gupta and Waseem, 2017). Recently, deep learning methods also show that it performs well on various NLP problems (Socher et al., 2012). Thus, this study investigates the performance of em- ploying these two methods. Accordingly, this study uses the distributional-based learning method to extract meaningful domain-specific embedding as features and deep learning based on BidirectionalShort Term Memory (BiLSTM) classifier to detect hate speech. The word embedding/ distribu- tional representation in this research is built upon a hate speech corpus of 1; 048; 563 sentences to reach the closest meaningful representation vec-tor of hate words. Then, compare it with the domain- agnostic embedding model such as Google Word2Vec and GloVe under the same classifier. We also assess the performance of detecting hate speech using GoogleÃ¢ÂÂs pre-trained BERT model, which has generally achieved a state-of-the-art for many NLP tasks. The contributions of this research are highlighted as follow: An unsupervised domain-specific word em-bedding model was developed to extract the meaning of commonly used terminology, acronyms, and purposefully misspelled hate words. A comparison between the domain- specific and domain agnostic embedding was provided. The findings show that domain agnostic em-bedding performs slightly better (about 1%), despite the huge difference in the trained cor-pus size. The evaluation of a BiLSTM-based deep model with domain-specific embeddings shows an improvement ranging from 5 to 6 points on available datasets over the state-of-the-art techniques. The evaluation of the BERT language model on the hate speech binary classification task shows an improvement of about 2 points com-pared to the domain-specific word embedding model. The remaining of this paper is constructed as follows: the background section provides back- ground information about hate detection and the used methodologies; the review of literature sec-tion includes most recent studies in the field; the methodology section describes the methods used in this study and its specification; the experiment and result section presents the used datasets, em- bedding models, and results of the experiments; the discussion section includes analysis and obser- vation from the results, and finally the conclusion section summarizes all the findings. Background This section gives an overview of hate speech detec-tion in the field and it provides information about the used methodologies for both of the features and classifiers. 2.1 Hate Speech Detection Several research have attempted to solve the prob- lem of detecting hate speech in general by differ- entiating hate and non-hate speech. (Ribeiro et al., 2017; Djuric et al., 2015). Others have tackled the issue of recognizing certain types of hate speech, such as anti-religious hate speech. (Albadi et al., 2018; Zhang et al., 2018), jihadist (De Smedt et al., 2018; Ferrara et al., 2016; Wei et al., 2016; Gialam- poukidis et al., 2017), sexist and racist (Badjatiya et al., 2017; Pitsilis et al., 2018; Gamback ÃÂ and Sik- dar, 2017). Several platforms have been used to collect datasets from various online resources such as websites or online forums (e.g., 4Chan, Dai- lyStorm), or recent social media platforms (e.g., Twitter, Facebook). Hate speech has been applied also on different languages (e.g., English, Arabic, German). 2.2 Word Embedding Word embedding (Bengio et al., 2003) is a promi-nent natural language processing (NLP) technique that seeks to convey the semantic meaning of a word. It provides a useful numerical description of the term based on its context. The words are repre-sented by a dense vector that can be used in estimat-ing the similarities between the words (Liu, 2018). The word is represented by an N- dimensional vec-tor appropriate to represent the word meaning in a specific language (Mikolov et al., 2013). The word embedding has been widely used in many recent NLP tasks due to its efficiency such as text classification (Gamback ÃÂ and Sikdar, 2017; Lilleberg et al., 2015), document clustering (Ailem et al., 2017), part of speech tagging (Wang et al., 2015), named entity recognition (SiencnikÃÂ, 2015), sentiment anal-ysis (Tang et al., 2014; Wang et al., 2016; Al-Azani and El-Alfy, 2017), and many other problems. The most common pretrained word embedding models are Google Word2Vec, Stanford GloVe and they are described as follow: 2.2.1 Word2Vec Word2Vec is one of the most recently used word embedding models. It is provided by theresearch team (Mikolov et al., 2013). Word2Vec associates each word with a vector-based on its surrounding context from a large corpus. The train- ing process for extracting the word vector has two types, the continuous bag of words model (CBOW), which predicts the target word from its context, and the Skip-Gram model (SG), which predicts the tar- get context from a given word. The feature vector of the word is manipulated and updated accord-ing to each context the word appears in the cor-pus. If the word embedding is trained well, similar words appear close to each other in the dimensional space. The word similarities between the words are measured by the cosine distance between their vec- tors. Google released a vector model called Google Word2Vec that has been trained on a massive cor- pus of over 100 billion words. 2.2.2 GloVe Pennington et al. (2014) provides another popular word embedding model named GloVe (Global Vec- tors for Word Representation). GloVe learns em- beddings using an unsupervised learning algorithm that is trained on a corpus to create the distribu- tional feature vectors. During the learning process, a statistics-based matrix is built to represent the words to words co-occurrence of the corpus. This matrix represents the word vectors. The learning process requires time and space for the matrix con- struction, which is a highly costly process. The difference between GloVe and Word2Vec is in the learning process, Word2Vec is a prediction based model, and GloVe is a count-based model. The GloVe is learned from Wikipedia, web data, Twit-ter, and each model is available with multiple vector dimensions. 2.3 Bidirectional Long Short-Term Memory (BiLSTM) LSTM (Hochreiter and Schmidhuber, 1997) is an enhanced version of the recurrent neural network, which is one of the deep learning models that is designed to capture information from a sequence of information. It differs from the feed-forward neural network in that it has a backward connection. RNN suffers from a vanishing gradient problem that happens when the weights are not updated anymore due to the small value of the received from error function in respect to the current weights in the iteration. The value is vanishing in very long sequences and becomes close to zero. This problem stops RNN from training. LSTM solves this problem by adding an extra interaction cell to preserve long sequence dependencies. Thus, LSTM saves data for long sequences, but it saves the data only from left to right. However, to save sequence data from both directions, a Bidirectional LSTM (BiLSTM) is used. BiLSTM consist of two LSTM, one process the data from left to right and the other in opposite direction then concatenates and flattens both forward and backward LSTM to improve the knowledge of the surrounding context. 2.4 BERT Pre-trained Language Model Bidirectional Encoder Representations from Trans- formers (BERT) (Devlin et al., 2018) is a language model trained on very huge data based on con- textual representations. BERT consists of feature extraction layers, which consist of word embed-ding and layer for the model (e.g., Classification, Question Answering, Named Entity Recognition). BERT is the most recent language model and pro- vides state of the art results in comparison to other language models for various NLP tasks. BERT training procedure of word embedding differs from other word embedding models. It creates a bidirec- tional representation of words that may be learned from both left and right directions. Word embed-ding approaches like Word2Vec and GloVe only examine one direction (either left to right or right to left), resulting in static word representations that do not alter with context. If the wordÃ¢ÂÂs meaning varies depending on the context, GloVe and Word2Vec map the word to only one embedding vector. As a result, Word2Vec and GloVe are referred to as context-free models. BERT is also different from previous language models (e.g., ELMo stands for Embeddings from Language Models (Peters et al., 2018)) in that it manipulates the context in all layers in both directions (left and right). Instead of shal-low combining processes such as concatenating, it use cooperatively conditioningcombine both left and right context. BERT is trained on Books Cor-pus (800M words) and English Wikipedia (2,500M words) (Devlin et al., 2018). Review of Literature It is worth noting that word embedding is an effec- tive approach to a variety of NLP issues. To extract bio-events from the scientific literature, Li et al. (2015) used word embedding. They used multiple sets of features such as, word embedding, BOW + n-gram joint model, and word embedding BOW joint model with SVM classifier and the overall per- formance of word embedding BOW is better than other models on different events, which reached to 77:37% f1-score. The pure word embedding model has lower performance because the dataset size is small. Wu et al. (2015) also used word embedding to distinguish clinical abbreviations as a special case of word sense disambiguation (WSD). The performance of SVM increased when employing word embedding features with an average accuracy of 93%. Hate speech identification is a prevalent issue that has gotten a lot of attention from researchers. Liu (2018) employed domain-specific word em- bedding model trained on the articles from hate speech websites and high centrality usersÃ¢ÂÂ tweets to reach to the semantics of code words used in hate speech. They experimented on CNN, and LSTM models and concluded that CNN performed better than LSTM on tweets due to the length of tweets. The achieved f1-score is 78% given that they ex- perimented on the previous tweet-length 180 char- acters. Gupta and Waseem (2017) evaluate the performance of using hate Word2Vec (i.e., domain- specific) model with Logistic Regression (LR) clas- sifier on three datasets. They achieved up to 91% f1-score. The results showed that domain-specific word embedding has a desirable performance and is suitable for unbalanced classes datasets. Nobata et al. (2016) aimed to detect abusive language using pretrained word embeddings on two domains (finance and news) and regression model for classification, they achieved 60:2% and 64:9% f1-score respectively. The results showed that Google Word2Vec provides better performance with 5% on both domains. Badjatiya et al. (2017) Ã¢ÂÂÃ¢ÂÂemployed deep learning techniques to extract em- bedding features from hate speech text and then used a decision tree model for classification. They reached 93% f1-score using random embeddings initialization that is fed to LSTM to construct fea- tures. The results proved that domain-specific em- bedding can provide better representation of hate words such as Ã¢ÂÂracistÃ¢ÂÂ or Ã¢ÂÂsexistÃ¢ÂÂ words, because it can extract the meaning of frequently used terms by the hate community, domain-specific-based de- tection is a promising method for the detection of hate speech, according to all of the research above. The authors of (Devlin et al., 2018) looked into the BERT modelÃ¢ÂÂs performance on a variety of NLP tasks. On eleven of these tasks, the model produced state-of-the-art results. It improved by 7:7 points in the General Language Understanding Evaluation (GLUE) benchmark, 4:6 points in Multi- Genre Nat-ural Language Inference (MultiNLI), and 1:5 to 5:1 points in the SQuAD various versions question an-swering tests. BERT language model was recently employed in a shared task to detect offensive language (Zhu et al., 2019; Pelicon et al., 2019; Wu et al., 2019). Zhu et al. (2019) fine tuned BERT model for this task and came in third place among competitors. They used 13; 240 in tweets to train the algorithm, with each message categorized as offensive or not offensive. They achieved 83:88% f1-score. Moza- fari et al. (2020) studied the performance of the BERT language model on hate speech detection as a multi-class problem in a recently released work. They employed a BERT basis and a variety of clas- sifiers, including CNN, which provided the highest score that reached to 92% f1-score. Conclusion and Future Work To conclude, BERT design provides an appropriate feature extraction and classification procedure for hate speech detection. BERT combines the benefits of domain agnostic and domain-specific word embedding by train the modelvast data then add an extra layer to trained on domain-specific data (fine-tuning). BERT also saves effort and time for building an embedding model from scratch. However, domain-specific word embedding overcomes BERT model in that it can detect hate terms and abbreviations and intentionally misspellings meaning.",0
"ABSTRACT This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a user’s account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012. The total loss from these attacks as of August of 2012 was around $690 million, and over $1.5 billion (Kessem 2013) by the end of the year. This is a 59% increase from 2011 in the number of phishing attacks and 22% higher in the losses. It clear that the phishing attacks are on the rise. The results of the phishing attack can be devastating, as a recent example of the attack on South Korea showed: an attack that shut down several prominent South Korean banks and broadcasters in March 2013 had an origin as a spear phishing campaign (Donohue 2013). The recommended defense against phishing attacks is to educate a user not to fall for them. Unfortunately, the awareness campaigns are not the most effective solution: it only takes one employee to fall for the bait for a company and for its clients to become vulnerable. Because phishing emails rely on human emotions, it is essential to provide an emotion-free solution to a phishing detection game. Such a solution should work before a user can click on the fraudulent link and, thus, before any damage can be made. At the same time, if a user is well educated and understands that an email is a phishing attempt, it should be possible for the user to indicate it as such, so that the solution software has a chance to validate its results or correct its evaluation based on real-time data. To make it more complicated, phishing emails can target a particular individual or a small group of individuals based on their known behaviors. Thus, it is important for the software to adapt to individual characteristics of an individual or a group of individuals and predict their vulnerabilities for phishing. There is a need for a phishing-detection application to understand “the buttons” that are being pushed by fraudulent emails. When such buttons are detected, it is possible to react to them in a desired way. To date, there have been a number of papers that report detection of phishing emails (Hong 2012, Basnet et al 2008, Cleber et al 2011, Mutton 2011, Aaron 2010, Xiang 2011, Shahriar & Zulkernine 2012) based on non-text features claiming an accuracy over 90%. Yet, the problem still exists and growing. One possible explanation of this is these methods don’t address the content of the messages that people fall for. BACKGROUND Phishing is a malicious use of internet resources done to trick internet users to reveal personal information to the attacker. An attack is typically performed by sending an email to an unsuspecting user that contains a link to a domain that is seemingly legitimate in the hopes that the user will input their private information for the attacker to steal (DigiCert 2009). Phishing is a criminal act which uses a combination of “social engineering and technical subterfuge” to steal user information (APWG 2012). The most common type of phishing attack leverages email. The emphasis of this paper is to detect phishing attempts within emails. The threat phishing poses to Internet users at large calls for action within the information security industry to create ways of detecting and preventing such attacks. Research into the area of phishing detection has yielded several types of email analysis to determine if an email should be classified as phishing such as link analysis, header analysis, and text analysis. Link analysis refers to the using information about the links included within an email to determine whether the email is legitimate or a phishing attempt. This usually involves checking to see if the displayed link in the email matches the actual website URL that the user is taken to if the link is clicked. Header analysis refers to examining the header contents of an email to decide if the email is a phishing email or not. This analysis typically includes checking that the 'From' field of the email matches the actual sender and checking the IP address from which the email was sent against phishing blacklists. Blacklist is a set of well-known phishing Web sites and addresses reported by trusted entities such as Google's and Microsoft's black list (Gaurav et al. 2012). For black listing, both a client and a server side are necessary. The client component's implementation can be done through an email or browser plug-in that communicate with a server component. The server component is a public Web site containing a list of phishing sites (Tout & Hafner 2009). Text analysis refers to the examination of text included in the body of the email to find out if an email has a suspicious content. CONCLUSION The paper expands a version of a text-based phishing detection algorithm. It is shown that though accounting for all POS rather than selective ones increases the phishing detection results. At the same time, false positive rate can slightly increase when more emphasis is put on the importance of detecting as many phishing emails as possible, as expected. It is expected that adding semantic component in addition to considering all POS will reduce the false positive rate while preserving the detection accuracy.",1
"ABSTRACT This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a userÃ¢ÂÂs account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012. The total loss from these attacks as of August of 2012 was around $690 million, and over $1.5 billion (Kessem 2013) by the end of the year. This is a 59% increase from 2011 in the number of phishing attacks and 22% higher in the losses. It clear that the phishing attacks are on the rise. The results of the phishing attack can be devastating, as a recent example of the attack on South Korea showed: an attack that shut down several prominent South Korean banks and broadcasters in March 2013 had an origin as a spear phishing campaign (Donohue 2013). The recommended defense against phishing attacks is to educate a user not to fall for them. Unfortunately, the awareness campaigns are not the most effective solution: it only takes one employee to fall for the bait for a company and for its clients to become vulnerable. Because phishing emails rely on human emotions, it is essential to provide an emotion-free solution to a phishing detection game. Such a solution should work before a user can click on the fraudulent link and, thus, before any damage can be made. At the same time, if a user is well educated and understands that an email is a phishing attempt, it should be possible for the user to indicate it as such, so that the solution software has a chance to validate its results or correct its evaluation based on real-time data. To make it more complicated, phishing emails can target a particular individual or a small group of individuals based on their known behaviors. Thus, it is important for the software to adapt to individual characteristics of an individual or a group of individuals and predict their vulnerabilities for phishing. There is a need for a phishing-detection application to understand Ã¢ÂÂthe buttonsÃ¢ÂÂ that are being pushed by fraudulent emails. When such buttons are detected, it is possible to react to them in a desired way. To date, there have been a number of papers that report detection of phishing emails (Hong 2012, Basnet et al 2008, Cleber et al 2011, Mutton 2011, Aaron 2010, Xiang 2011, Shahriar & Zulkernine 2012) based on non-text features claiming an accuracy over 90%. Yet, the problem still exists and growing. One possible explanation of this is these methods donÃ¢ÂÂt address the content of the messages that people fall for. BACKGROUND Phishing is a malicious use of internet resources done to trick internet users to reveal personal information to the attacker. An attack is typically performed by sending an email to an unsuspecting user that contains a link to a domain that is seemingly legitimate in the hopes that the user will input their private information for the attacker to steal (DigiCert 2009). Phishing is a criminal act which uses a combination of Ã¢ÂÂsocial engineering and technical subterfugeÃ¢ÂÂ to steal user information (APWG 2012). The most common type of phishing attack leverages email. The emphasis of this paper is to detect phishing attempts within emails. The threat phishing poses to Internet users at large calls for action within the information security industry to create ways of detecting and preventing such attacks. Research into the area of phishing detection has yielded several types of email analysis to determine if an email should be classified as phishing such as link analysis, header analysis, and text analysis. Link analysis refers to the using information about the links included within an email to determine whether the email is legitimate or a phishing attempt. This usually involves checking to see if the displayed link in the email matches the actual website URLthe user is taken to if the link is clicked. Header analysis refers to examining the header contents of an email to decide if the email is a phishing email or not. This analysis typically includes checking that the 'From' field of the email matches the actual sender and checking the IP address from which the email was sent against phishing blacklists. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a userÃ¢ÂÂs account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012. ",0
"Abstract While recent work on automated fact-checking has focused mainly on verifying and explain- ing claims, for which the list of claims is read- ily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive task and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the article’s veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021). End-to-end automated fact-checking systems in- volve three core objectives - (1) identifying check- worthy claims, (2) verifying claims against authori- tative sources, and (3) delivering corrections/ expla- nations on the claims (Graves, 2018). The majority of the recent work focuses on verification and ex- planation objectives for which a list of claims is readily available (Thorne et al., 2018; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020; Kazemi et al., 2021). Identifying check-worthy claims, which is a critical first step for fact-checking, remains a challenging task. ClaimBuster is the first work to target check- worthiness (Hassan et al., 2017). It is trained on transcripts of 30 US presidential elections de- bates. Each sentence of the transcripts is annotated for three categories - non-factual sentence, unimportant factual sentence, and check-worthy factual sentence. They then build classifiers to classify sentences into these three labels. Another classification-based approach is to predict whether the content of a given statement makes ”an asser- tion about the world that is checkable” (Konstanti- novskiy et al., 2021). This approach utilizes anno- tations for sentences extracted from subtitles of UK political shows. The models are trained to classify statements into binary labels - claim or non-claim. Finally, a system called ClaimRank (Jaradat et al., 2018) aims to prioritize the sentences that fact- checkers should consider first for fact-checking. ClaimRank is trained on pre-existing annotations on political debates from 9 fact-checking organiza- tions. This approach first classifies the statement as check-worthy or not. The statements are then ranked based on the probabilities that the model as- signs to a statement to belong to the positive class. While these works are fundamental towards ap- proaching the problem of check-worthy claim iden- tification, the focus is only on a single domain (pol- itics). Additionally, the models rely on sentence- level human annotations, which is an expensive task, challenging to conduct regularly for multiple domains, and subject to personal bias. In this work, we try to overcome these limitations by exploring the effectiveness of using internal signals from un- labeled data. We focus on fake news articles and experiment with two types of internal signals for overall supervision - headline and abstractive sum- mary. We make two hypotheses regarding these sig- nals - first, these two elements of an article contain the gist of the content. To support this hypothesis, we evaluate the headline and the abstractive sum- mary against the manually written Snopes claims for news articles. Claims that Snopes write con- tain the salient factual idea of the source article. Second, sentences that are semantically close to the headline or the summary are check-worthy. To assess this hypothesis, we experiment with end-to- end fact-checking pipelines. The pipelines lever- age the top-ranked sentences relevant to the head- line/summary for the downstream fact-checking tasks of evidence retrieval and veracity prediction. The dataset used for these experiments contains articles from multiple domains, such as medical, crime, politics, technology. Through comparative experiments, we find that the top-3 ranked sen- tences contain enough information for evidence- based fact-checking of a fake news article. We also observe that the summary-based pipeline is the most promising for an end-to-end automated fact-checking system. Conclusion We explore identification of check-worthy claim sentences from a news article without any sentence- level annotation. We show experiments leveraging two internal signals - headline and abstractive summary of the article. We test two hypotheses - (1) headline/abstractive summary contains the gist of the article, and (2) sentences of the content seman- tically relevant to the headline/summary are check- worthy claim sentences. We build fact-checking pipelines for this purpose and show that the check- worthy claim sentences identified based on the sum- mary of the article are adequate for downstream tasks of evidence gathering and veracity predic- tion of the article. Our experiments use articles ranging on a variety of topics and associated with four veracity labels. For future work, we aim to use the abstractive-summary-based methodology for fact-checking of other types of textual data - online discourse, debate transcripts, etc. We be- lieve that leveraging topic detection along with the summary-based check-worthiness identification can help overcome the issues and biases introduced due to sentence-level manual annotations.",1
"Abstract While recent work on automated fact-checking has focused mainly on verifying and explain- ing claims, for which the list of claims is read- ily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive task and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the articleÃ¢ÂÂs veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021). End-to-end automated fact-checking systems in- volve three core objectives - (1) identifying check- worthy claims, (2) verifying claims against authori- tative sources, and (3) delivering corrections/ expla- nations on the claims (Graves, 2018). The majority of the recent work focuses on verification and ex- planation objectives for which a list of claims is readily available (Thorne et al., 2018; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020; Kazemi et al., 2021). Identifying check-worthy claims, which is a critical first step for fact-checking, remains a challenging task. ClaimBuster is the first work to target check- worthiness (Hassan et al., 2017). It is trained on transcripts of 30 US presidential elections de- bates. Each sentence of the transcripts is annotated for three categories - non-factual sentence, unimportant factual sentence, and check-worthy factual sentence. They then build classifiers to classify sentences into these three labels. Another classification-based approach is to predict whether the content of a given statement makes Ã¢ÂÂan asser- tion about the world that is checkableÃ¢ÂÂ (Konstanti- novskiy et al., 2021). This approach utilizes anno- tations for sentences extracted from subtitles of UK political shows. The models are trained to classify statements into binary labels - claim or non-claim. Finally, a system called ClaimRank (Jaradat et al., 2018) aims to prioritize the sentences that fact- checkers should consider first for fact-checking. ClaimRank is trained on pre-existing annotations on political debates from 9 fact-checking organiza- tions. This approach first classifies the statement as check-worthy or not. The statements are then ranked based on the probabilities that the model as- signs to a statement toto the positive class. While these works are fundamental towards ap- proaching the problem of check-worthy claim iden- tification, the focus is only on a single domain (pol- itics). Additionally, the models rely on sentence- level human annotations, which is an expensive task, challenging to conduct regularly for multiple domains, and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the articleÃ¢ÂÂs veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021).",0
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively. CCS Concepts: • Computing methodologies → Information extraction. Additional Key Words and Phrases: speaker role identification, air traffic control, text classification, speech classification, spoken instruction understanding, multi-modal learning INTRODUCTION Speech communication between air traffic controllers (ATCOs) and pilots is one of the most important interaction ways in air traffic control (ATC) procedures. Recently, there is increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Lin 2021; Pardo et al. 2011]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [Smídl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. Subsequently, the TIU module converts the natural texts into predefined structured instructions that are further processed by the computer. Finally, the computer-readable instructions and the speaker role (ATCO or pilot which was output by the SRI module) jointly provide a conversation context for other downstream applications. As can be seen from mentioned illustrations, the SRI module is a critical component of the SIU system in the field of ATC. However, most of the existing research of the ATC SIU systems focuses on the ASR and TIU techniques [Lin et al. 2021a,c; Oualil et al. 2017; Zuluaga-Gomez et al. 2020], and no detailed description of the SRI task was presented. A instruction understanding model and ATC communication rule-based methods for the SRI tasks were studied in [Lin et al. 2019], without providing SRI performance. To the best of our knowledge, none of the published works have reported complete approaches and results for the SRI tasks in the ATC domain. Since the ATCO communicates with several pilots by radio in a single frequency, the role of the speaker cannot be distinguished from the communication data link. However, the speaker role is a kind of indispensable and important information in many ATC-related applications, such as safety detection systems, ATCO workload analysis systems. Therefore, the inability to identify the speaker role directly from communication brings a certain challenge to the ATC-related SIU tasks. Fortunately, there are two kinds of data that can be served as the potential entities for the SRI tasks. • Text: On the one hand, according to the communication rules recommended by the interna- tional civil aviation organization (ICAO), the ATCOs should declare the call sign of the target aircraft before issuing the instructions, while the pilots read back the instructions firstly and then reporting their call sign. In general, most of the controller-pilot speech communication follow these rules, allowing the text classification to be a promising technology for SRI tasks. • Speech: On the other hand, the speech can be considered as a representation of the speaker role from two aspects of signal and text. a) the controller-pilot speech communication presents distinctive features depending on the equipment and environment, such as a microphone, push-to-talk (PTT), background noise, radio. b) It implies the representation of its transcripts, which further provides more discriminative knowledge for the SRI task. In this paper, we define the SRI task as a binary classification problem, i.e., all the instructions are classified into two classes: ATCO or pilot. Meanwhile, the SRI task is addressed by the data- driven approaches from three different inputs, i.e., text, speech, speech-text. To this end, the text classification, audio classification, and multi-modal classification approaches are proposed to achieve the SRI task. In this procedure, several popular network architectures are introduced to serve as backbone networks for each approach to eliminate the impact of the difference between network architectures. The BiLSTM [Zhou et al. 2016], TextCNN [Kim 2014], and Transformer [Vaswani et al. 2017] architecture are developed as the backbone network in the text-based methods, while x-vector [Snyder et al. 2018], SincNet [Ravanelli and Bengio 2018a], and CRNN [Choi et al. 2017] architecture are built for the speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to learn the distinctive representations from both the speech and textual modalities for the speech-text based methods. Specifically, a modal attention mechanism is proposed to fuse the different representations to a joint feature vector. In addition, the self-attention pooling layer is applied to produce the joint vector by the weighted sum operations, which further be regarded as the multi-modal embedding. Finally, the multi-modal embedding is further fed into classification layers to generate the final probabilities of the speaker role. All the proposed methods were validated on the ATCSpeech corpus [Yang et al. 2020] that was collected from a real-world ATC environment. In addition, in order to analyze and compare the performance and robustness of the model, we evaluate the trained model in two ways: 1) The model is validated on the test set of the ATCSpeech to evaluate the performance on the seen samples. 2) A supplement test set called test-s is used to verify the robustness of the model on the unseen samples of in controller-pilot communication. In summary, our contributions are listed as follows: • A thorough comparison between the aforementioned deep learning based SRI techniques is investigated. To the best of our knowledge, this is the first work that investigates the SRI task in the ATC domain. • The robustness and performance of the comparative models are comprehensively analyzed and discussed on the seen and unseen samples. • A multi-modal model, called MMSRINet, is proposed to achieve the ATC-related SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5. Finally, this paper is concluded in Section 6.  RELATED WORK 2.1 Text Classification Text classification is a classical task in the field of natural language processing (NLP), which aims to classify a given text sequences into a certain class. In general, the approach can be grouped into two categories: rule-based methods and data-driven based methods. The rule-based approach usually requires a large number of predefined rules and is strongly dependent on domain knowledge, which can only be applied to limited scenarios due to poor flexibility. Thanks to the development of deep learning techniques, the performance of data-driven meth- ods has generally outperformed that of rule-based methods in recent years and has become the standard paradigm of text classification tasks [Minaee et al. 2021]. Zeng et al. [Kim 2014] utilized a convolutional neural network (CNN) [Lecun et al. 1998] to achieve the sentence classification tasks which makes representative progress in the NLP domain. To capture the long-term depen- dencies, the Att-BiLSTM model [Zhou et al. 2016] was built on a recurrent neural network (RNN) [Mikolov et al. 2010]. Currently, various improved methods based on the CNN or RNN block were proposed to achieve the text classification task, such as Character-level CNNs [Zhang et al. 2015], tree-based CNN [Mou et al. 2016], Tree-LSTM [Tai et al. 2015], Multi-Timescale LSTM [Liu et al. 2015]. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in text classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.2 Audio Classification Audio classification is widely applied in audio pattern recognition tasks, such as speaker identifica- tion [Ravanelli and Bengio 2018a; Snyder et al. 2018], acoustic event detection [Kumar and Raj 2016], accent classification [Hansen and Liu 2016; Lopez-Moreno et al. 2014], audio emotion recognition [Jermsittiparsert et al. 2020]. Recently, deep learning methods showed promising performance compared to traditional approaches for this task [Hershey et al. 2017]. Enormous works have been investigated to explore different model architectures and applications for audio classification. Shawn Hershey et al. [Hershey et al. 2017] demonstrated that the CNNs used in the image clas- sification task, such as AlexNet [Krizhevsky et al. 2012], VGG [Simonyan and Zisserman 2014], and ResNet [He et al. 2016], achieved desired performance for the large-scale audio classification task. Meanwhile, the convolutional recurrent neural network (CRNN) was proposed and to achieve music classification [Choi et al. 2017], audio event detection [Cakır et al. 2017], audio tagging [Xu et al. 2018], etc. In addition, in recent years, there are increasing interest in learning features from raw waveforms directly instead of handcraft features. Mirco Ravanelli et al. proposed the SincNet [Ravanelli and Bengio 2018a] to achieve speaker recognition which employs band-pass filters (based on the parametrized Sinc functions) in the first convolutional layer. Jee-weon Jung et al. proposed the RawNet [Jung et al. 2019] to improve the performance of the speaker verification from raw waveforms. In short, deep learning-based audio classification is still an interesting task in many applications. 2.3 Multi-modal Classification With the explosive growth of multi-modal data in the digit world, multi-modal learning is attracting increasing research interest and shows powerful performance than that of unimodal modal methods [Ngiam et al. 2011]. Various modalities can be used to achieve classification tasks, including audio- video [Nagrani et al. 2018], audio-text [Mittal et al. 2020], image-text [Gallo et al. 2018; Kiela et al. 2018], etc. In general, the fusion strategy of the classification task can be implemented in the following two ways: early fusion and later fusion [Baltrušaitis et al. 2019]. Early fusion methods fuse the multi-modal feature vectors to a joint representation that is further fed into the classifier, while the later fusion makes a second decision on the output of two classifiers by an extra strategy. Due to the advantages of early fusion in exploring the correlations and interactions between different modalities, in this paper, we introduce the early fusion methods to the SRI task. In an early work [Kiela and Bottou 2014], direct concatenation was employed to produce multi-modal joint representations. In order to identify the correlations of learned multi-modal features, a structural regularization was proposed in [Wu et al. 2014] to empower the deep neural network (DNN) based fusion layer, which also preserves the diversity of the different modality features. In addition to the classification task, more powerful fusion methods were successfully integrated into the ASR and NLP architectures. Modality attention was proposed to fuse the audio-visual features for ASR tasks in [Zhou et al. 2019]. The works of [Fukui et al. 2016] and [Ovalle et al. 2017] used compact bilinear pooling and complex gating mechanisms to obtain multi-modal representations. CONCLUSIONS In this paper, we presented a comprehensively comparative study for the SRI tasks using deep learning in the ATC domain. Three kinds of methods with different inputs were investigated to solve the problems of the SRI tasks, i.e., text-based methods, speech-based methods, and multi- modal methods. Firstly, we formulated the SRI task as the binary classification problem, and further refine the above methods as text classification, speech classification, and multi-modal classification task. Secondly, the efficacy of the above methods is confirmed by theoretical and experimental demonstrations. Finally, the experiments demonstrated that the proposed MMSRINet is a competitive approach that achieves the best performance and robustness in the seen and unseen ATC environments. In the future, we plan to explore more efficient and effective approaches for the SRI tasks. In addition, the fusion and application of the multi-modal data in the ATC environment would be also an interesting research topic.",1
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively. CCS Concepts: Ã¢ÂÂ¢ Computing methodologies Ã¢ÂÂ Information extraction. Additional Key Words and Phrases: speaker role identification, air traffic control, text classification, speech classification, spoken instruction understanding, multi-modal learning INTRODUCTION Speech communication between air traffic controllers (ATCOs) and pilots is one of the most important interaction ways in air traffic control (ATC) procedures. Recently, there is increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Lin 2021; Pardo et al. 2011]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [SmiÃÂdl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. Subsequently, the TIU module converts the natural texts into predefined structured instructions that are further processed by the computer. Finally, the computer-readable instructions and the speaker role (ATCO or pilot which was output by the SRI module) jointly provide a conversation context for other downstream applications. As can be seen from mentioned illustrations, the SRI module is a critical component of the SIU system in the field of ATC. However, most of the existing research of the ATC SIU systems focuses on the ASR and TIU techniques [Lin et al. 2021a,c; Oualil et al. 2017; Zuluaga-Gomez et al. 2020], and no detailed description of the SRI task was presented. A instruction understanding model and ATC communication rule-based methods for the SRI tasks were studied in [Lin et al. 2019], without providing SRI performance. To the best of our knowledge, none of the published works have reported complete approaches and results for the SRI tasks in the ATC domain. Since the ATCO communicates with several pilots by radio in a single frequency, the role of the speaker cannot be distinguished from the communication data link. However, the speaker role is a kind of indispensable and important information in many ATC-related applications, such as safety detection systems, ATCO workload analysis systems. Therefore, the inability to identify the speaker role directly from communication brings a certain challenge to the ATC-related SIU tasks. Fortunately, there are two kinds of data that can be served as the potential entities for the SRI tasks. Ã¢ÂÂ¢ Text: On the one hand, according to the communication rules recommended by the interna- tional civil aviation organization (ICAO),ATCOs should declare the call sign of the target aircraft before issuing the instructions, while the pilots read back the instructions firstly and then reporting their call sign. In general, most of the controller-pilot speech communication follow these rules, allowing the text classification to be a promising technology for SRI tasks. Ã¢ÂÂ¢ Speech: On the other hand, the speech can be considered as a representation of the speaker role from two aspects of signal and text. a) the controller-pilot speech communication presents distinctive features depending on the equipment and environment, such as a microphone, push-to-talk (PTT), background noise, radio. b) It implies the representation of its transcripts, which further provides more discriminative knowledge for the SRI task. In this paper, we define the SRI task as a binary classification problem, i.e., all the instructions are classified into two classes: ATCO or pilot. Meanwhile, the SRI task is addressed by the data- driven approaches from three different inputs, i.e., text, speech, speech-text. To this end, the text classification, audio classification, and multi-modal classification approaches are proposed to achieve the SRI task. In this procedure, several popular network architectures are introduced to serve as backbone networks for each approach to eliminate the impact of the difference between network architectures. The BiLSTM [Zhou et al. 2016], TextCNN [Kim 2014], and Transformer [Vaswani et al. 2017] architecture are developed as the backbone network in the text-based methods, while x-vector [Snyder et al. 2018], SincNet [Ravanelli and Bengio 2018a], and CRNN [Choi et al. 2017] architecture are built for the speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to learn the distinctive representations from both the speech and textual modalities for the speech-text based methods. Specifically, a modal attention mechanism is proposed to fuse the different representations to a joint feature vector. In addition, the self-attention pooling layer is applied to produce the joint vector by the weighted sum operations, which further be regarded as the multi-modal embedding. Finally, the multi-modal embedding is further fed into classification layers to generate the final probabilities of the speaker role. All the proposed methods were validated on the ATCSpeech corpus [Yang et al. 2020] that was collected from a real-world ATC environment. In addition, in order to analyze and compare the performance and robustness of the model, we evaluate the trained model in two ways: 1) The model is validated on the test set of the ATCSpeech to evaluate the performance on the seen samples. 2) A supplement test set called test-s is used to verify the robustness of the model on the unseen samples of in controller-pilot communication. In summary, our contributions are listed as follows: Ã¢ÂÂ¢ A thorough comparison between the aforementioned deep learning based SRI techniques is investigated. To the best of our knowledge, this is the first work that investigates the SRI task in the ATC domain. Ã¢ÂÂ¢ The robustness and performance of the comparative models are comprehensively analyzed and discussed on the seen and unseen samples. Ã¢ÂÂ¢ A multi-modal model, called MMSRINet, is proposed to achieve the ATC-related SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5. Finally, this paper is concluded in Section 6. RELATED WORK 2.1 Text Classification Text classification is a classical task in the field of natural language processing (NLP), which aims to classify a given text sequences into a certain class. In general, the approach can be grouped into two categories: rule-based methods and data-driven based methods. The rule-based approach usually requires a large number of predefined rules and is strongly dependent on domain knowledge, which can only be applied to limited scenarios due to poor flexibility. Thanks to the development of deep learning techniques, the performance of data-driven meth- ods has generally outperformed that of rule-based methods in recent years and has become the standard paradigm of text classification tasks [Minaee et2021]. Zeng et al. [Kim 2014] utilized a convolutional neural network (CNN) [Lecun et al. 1998] to achieve the sentence classification tasks which makes representative progress in the NLP domain. To capture the long-term depen- dencies, the Att-BiLSTM model [Zhou et al. 2016] was built on a recurrent neural network (RNN) [Mikolov et al. 2010]. Currently, various improved methods based on the CNN or RNN block were proposed to achieve the text classification task, such as Character-level CNNs [Zhang et al. 2015], tree-based CNN [Mou et al. 2016], Tree-LSTM [Tai et al. 2015], Multi-Timescale LSTM [Liu et al. 2015]. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in text classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.2 Audio Classification Audio classification is widely applied in audio pattern recognition tasks, such as speaker identifica- tion [Ravanelli and Bengio 2018a; Snyder et al. 2018], acoustic event detection [Kumar and Raj 2016], accent classification [Hansen and Liu 2016; Lopez-Moreno et al. 2014], audio emotion recognition [Jermsittiparsert et al. 2020]. Recently, deep learning methods showed promising performance compared to traditional approaches for this task [Hershey et al. 2017]. Enormous works have been investigated to explore different model architectures and applications for audio classification. Shawn Hershey et al. [Hershey et al. 2017] demonstrated that the CNNs used in the image clas- sification task, such as AlexNet [Krizhevsky et al. 2012], VGG [Simonyan and Zisserman 2014], and ResNet [He et al. 2016], achieved desired performance for the large-scale audio classification task. Meanwhile, the convolutional recurrent neural network (CRNN) was proposed and to achieve music classification [Choi et al. 2017], audio event detection [CakÃÂ±r et al. 2017], audio tagging [Xu et al. 2018], etc. In addition, in recent years, there are increasing interest in learning features from raw waveforms directly instead of handcraft features. Mirco Ravanelli et al. proposed the SincNet [Ravanelli and Bengio 2018a] to achieve speaker recognition which employs band-pass filters (based on the parametrized Sinc functions) in the first convolutional layer. Jee-weon Jung et al. proposed the RawNet [Jung et al. 2019] to improve the performance of the speaker verification from raw waveforms. In short, deep learning-based audio classification is still an interesting task in many applications. 2.3 Multi-modal Classification With the explosive growth of multi-modal data in the digit world, multi-modal learning is attracting increasing research interest and shows powerful performance than that of unimodal modal methods [Ngiam et al. 2011]. Various modalities can be used to achieve classification tasks, including audio- video [Nagrani et al. 2018], audio-text [Mittal et al. 2020], image-text [Gallo et al. 2018; Kiela et al. 2018], etc. In general, the fusion strategy of the classification task can be implemented in the following two ways: early fusion and later fusion [BaltrusÃÂaitis et al. 2019]. Early fusion methods fuse the multi-modal feature vectors to a joint representation that is further fed into the classifier, while the later fusion makes a second decision on the output of two classifiers by an extra strategy. Due to the advantages of early fusion in exploring the correlations and interactions between different modalities, in this paper, we introduce the early fusion methods to the SRI task. In an early work [Kiela and Bottou 2014], direct concatenation was employed to produce multi-modal joint representations. In order to identify the correlations of learned multi-modal features, a structural regularization was proposed in [Wu et al. 2014] to empower the deep neural network (DNN) based fusion layer, which also preserves the diversitythe different modality features. In addition to the classification task, more powerful fusion methods were successfully integrated into the ASR and NLP architectures. Modality attention was proposed to fuse the audio-visual features for ASR tasks in [Zhou et al. 2019]. The works of [Fukui et al. 2016] and [Ovalle et al. 2017] used deep learning [Xu et al. 2017] to achieve the SRI task. In addition, in recent years, there are increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Zhou et al. 2019]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [SmiÃÂdl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. ",0
"Abstract This paper presents a deep neural architec- ture, for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our anal- ysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious ques- tions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset. Introduction Natural Language Sentence Matching (NLSM) is a fundamental task in Natural Language Pro- cessing(NLP) designed to identify similarities in terms of semantic and conceptual content between two input sentence. A wide range NLP tasks, such as Natural Language Inference(NLI), Para- phrase Identification, Question Answering, and Machine Comprehension (MC), are implemented with NLSM. The NLI, also known as Recog- nizing Textual Entailment (RTE), seeks to deter- mine whether a hypothesis can be deduced from a premise. This requires examining the semantic similarity between the hypothesis and the premise. In Paraphrase Identification, the goal is to deter- mine if two texts are paraphrases or not (Lan and Xu). In Machine Comprehension, the model must match the sentence structure of the passage to the question, pointing out where the answer is located (Lan and Xu). Question answering systems rely on NLSM in two ways: (1) question retrieval, (2) the answer selection. In question retrieval, the matching between the query and existing ques- tions, and in answer selection, the matching be- tween the query and existing answers are deter- mined. In fact, answer selection is used to discover the relationship between the query and answer and ranks all possible answers. These NLSM- based question answering systems can be applied in question-answering forums. Many websites use these question-answering forums and they can use these systems to answer their user’s questions. There are so many repetitive questions in these fo- rums that NLSM-based question answering sys- tems can use FAQs to answer users questions. Users will see the answers similar questions, when a new question is asked in the forum. Using this NLSM-based question answering systems user questions will be answered faster (Singh Tomar et al.). Multiple datasets can be accessed in NLSM, including Stanford Natural Language In- ference (SNLI) (Bowman et al., 2015), Multi- Genre Natural Language Inference (MultiNLI) (Williams et al., 2018), FarsTail (Amirkhani et al., 2021), SciTail (Khot et al., 2018), and more. One of the challenges when using these datasets is that in many existing samples the two input texts have the same and many similar words. In fact, both texts are very similar in appearance but express different meanings, or conversely, two texts have many different words, but the nature and meaning of both questions are very similar. The objective of NLSM models is to predict a category or scale value for a pair of input texts, which indicates their similarity or relationship. To achieve this goal, NLSM models generally use two main steps: (1) designing a model to obtain a proper representation of whatever text will be ana- lyzed so that it can extract semantic features from it. (2) By using the representation obtained from texts, a matching mechanism can be developed to extract complex interactions. In the current NLSM field, deep neural net- works are the preferred approach. To determine semantic features and relationships among sen- tences, researchers use convolutional and recur- rent neural networks. The structure of convo- lutional neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNN’s that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract text features, deep learn- ing networks require a lot of labeled data, which is one of the challenges of using deep learning, es- pecially in natural language processing. To extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data. The advantage of these pre-trained language models is that they do not need labeled data to be trained, so they can be trained on a large amount of unlabeled data and then fine-tuned them in downstream tasks. A tech- nique known as transfer learning is one of the best ways to resolve the challenges of natural language processing in low-resource languages today. In this paper, we used the following steps to de- sign a model based on NLSM principles identify- ing the semantic similarity of the input text pair: 1. Collecting Persian Religious question match- ing dataset for training and evaluating the proposed NLSM model. 2. Investigating related models to the field of NLSM. 3. Implementing ERT with Deep Recursive En- coder (BERT-DRE) model by adding recur- sive encoder module to BERT (Devlin et al., 2019). 4. Evaluating BERT-DRE with related models using the introduced religious and benchmark datasets. Dataset annotated for this study includes 18,000 samples, contains two questions, appropriate an- swers, and a match or not-match label for each question pair, which is used for designing a chat- bot. This dataset is collected by crawling reli- gious questions from religious question-answering websites, and then by annotating two similar ques- tions are annotated for each question, and dissim- ilar questions are generated automatically. It was noted our BERT-DRE model, which used the an- notated religious dataset to train and evaluate, was able to achieve an F1-score of 90.27% on the test data, making it the strongest model among the ones studied. Furthermore, in order to better eval- uate the BERT-DRE model, the model was trained using SNLI, MultiNLI, FarsTail, and SciTail and achieved appropriate F1-scores. Related Works Natural language processing studies have focused on NLSM, an essential part of many natural lan- guage tasks. In this regard, various datasets and models have been developed. There have been dif- ferent models developed for NLI, semantic match- ing, and paraphrase identification. Since all of these tasks are similar, the same model can be used today. In most cases, deep neural networks are used for this purpose. To this end, in the following, we shall examine the relevant works. As stated in (Wang et al., 2017), in contrast to previous work performed in text-similarity that adapted the sentence from one direction or used a word-to-word or sentence-to-sentence corre- spondence only, this study used bilateral multi- perspective matching from several perspectives. As a result, one of the main architectural pil- lars is based on Bidirectional LSTM. Furthermore, they examined the mechanism of different meth- ods, such as attention. This interaction between aggregate words is then examined from multiple perspectives. In (Wang et al., 2017) (BiMPM), sentences are matched bidirectionally with a new function calculating similar vectors. This method can result in similarity in multiple ways, which is called multi-perspective cosine similarity. Language objects internal structures and inter- actions of language objects need to be properly modeled to achieve a successful matching algo- rithm. A component of this goal is achieved in (Hu et al., 2014) through combining ideas from neu- ral networks’ implementation in image and audio processing. Besides demonstrating the hierarchi- cal structures of sentences in layers, this model also maintains rich matching patterns on differ- ent levels. The (He et al., 2015) (MPCNN) pre- sented work based on CNN, analyzing multiple perspectives of sentences. CNN first extracts fea- tures for each sentence, then compares representa- tions of the sentences at various levels of granular- ity, and then uses various pooling techniques. Fol- lowing that, multiple similarity metrics are used to compare sentences at multiple levels of granular- ity. The MPCNN model processes each sentence independently, and there is no interactivity until the final Fully-Connected layer, which results in the loss of a great deal of useful information. For this reason, (Cao et al., 2018) and (He et al., 2016) make changes to the MPCNN architecture. In (Cao et al., 2018) has also improved MPCNN, one of which is the use of pre-trained embedding instead of random embedding. In the next step, characters are used to extract features. Finally, an input layer based on attention is added between the embedding and the multi-perspective sentence modeling layers. Attention-based neural network models have been successfully used in an- swer selection, which is an important sub-task of question answering. These models often represent the question using a vector and match it by refer- ring to the candidate answers. However, questions and answers may be interrelated in complex ways that cannot be represented by single-vector repre- sentations. In (Tran and Niedereée, 2018), the idea of using multipurpose attention networks (MANs) is proposed, which aims to discover this complex relationship for ranking question and answer pairs. Unlike previous models, this paper does not turn the question into a single vector, but focuses on different parts of the question from several vectors to show its general meaning, and applies different stages of attention to learning the representations of the candidate answers. The study (Zhao et al., 2019) proposes an inter- active attention model for semantic text matching that uses the global matching matrix to learn repre- sentations for source and target texts during inter- active attention. This model can take a rich repre- sentation of source and target texts and derive an entirely related encoding. In (Chen et al., 2017), a model with a good performance called ESIM has a non-complex structure of LSTM layers and is cited in many later works as a base model. It has been claimed that recursive architecture can achieve good performance in local inference mod- eling and inference composition. In (Tay et al., 2018), a deep learning model named CAFE is presented in which alignment pairs are compared, compressed, and then spread to the upper layers to increase representation learning. Then, the alignment vectors are ex- pressed in scalar features through factorization layers. One of the other models that was able to achieve good accuracy in SNLI and MultiNLI datasets (Yin et al., 2016) is another deep learning model that is intended for modeling sentence pairs with CNNs named ABCNN. In this study, three different models of the combination of convolu- tional layers along with the attention mechanism have been used and the results of these three mod- els have been examined. Based on BERT representation of sentences and using convolution layers and the semantic role la- beler as features added to the model, (Zhang et al., 2020) has achieved good accuracy on MultiNLI and QNLI (Wang et al., 2018) data. Using a com- bination of CNN, attention mechanism, and resid- ual connection structures for inference, a deep learning method was proposed in (Yang et al., 2019) named RE2. The authors regard attention to pointwise features, previous alignments, and con- textual features as three success principles. This model has the advantage of being fast in calculat- ing model inferences. In comparison with other existing models, this model increases speed by six times and uses fewer parameters. In the (Kim et al., 2018) a deep learning model is proposed using the LSTM structure named DRCN. In this paper, the alignment module is used to extract the relationship between the words of the first and second text sequences. Residual connections are also used to transfer the extracted features to each layer. The embedding layer in this model includes trainable word embedding, non-trainable word embedding, character embedding, and ex- act match feature. In this model, due to the use of residual connections, the dimensions of the ex- tracted feature along with the network increase. In order to reduce the feature dimensions in this model, an autoencoder is used. Conclusion In this paper, we proposed a novel architecture by adding recursive encoder module with BERT. The proposed model uses BERT as an embedding layer and On top of the embedding layer, three bidi- rectional LSTM were used densely to represent the input text semantically. In order to pay more or less emphasis on different words, an attention module is applied to the outputs of LSTMs. This makes the semantic representations more informa- tive. These semantic representations are passed to a pooling layer consisting average and max pooling To make the resulting feature maps more robust to features’ positional changes. To show the potential of this architecture, we evaluated it in four benchmark datasets and got competitive results. The evaluation results in our annotated data show that the accuracy of proposed model is very acceptable. Our empirical results suggest that our proposed model improves the performance on some NLP benchmarks (e.g., FarsTail) with the state-of-the-art pre-trained models (e.g., BERT). This has been developed for the Persian/English language but it could be easily extended to other languages.",1
"Abstract This paper presents a deep neural architec- ture, for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our anal- ysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious ques- tions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset. Introduction Natural Language Sentence Matching (NLSM) is a fundamental task in Natural Language Pro- cessing(NLP) designed to identify similarities in terms of semantic and conceptual content between two input sentence. A wide range NLP tasks, such as Natural Language Inference(NLI), Para- phrase Identification, Question Answering, and Machine Comprehension (MC), are implemented with NLSM. The NLI, also known as Recog- nizing Textual Entailment (RTE), seeks to deter- mine whether a hypothesis can be deduced from a premise. This requires examining the semantic similarity between the hypothesis and the premise. In Paraphrase Identification, the goal is to deter- mine if two texts are paraphrases or not (Lan and Xu). In Machine Comprehension, the model must match the sentence structure of the passage to the question, pointing out where the answer is located (Lan and Xu). Question answering systems rely on NLSM in two ways: (1) question retrieval, (2) the answer selection. In question retrieval, the matching between the query and existing ques- tions, and in answer selection, the matching be- tween the query and existing answers are deter- mined. In fact, answer selection is used to discover the relationship between the query and answer and ranks all possible answers. These NLSM- based question answering systems can be applied in question-answering forums. Many websites use these question-answering forums and they can use these systems to answer their userÃ¢ÂÂs questions. There are so many repetitive questions in these fo- rums that NLSM-based question answering sys- tems can use FAQs to answer users questions. Users will see the answers similar questions, when a new question is asked in the forum. Using this NLSM-based question answering systems user questions will be answered faster (Singh Tomar et al.). Multiple datasets can be accessed in NLSM, including Stanford Natural Language In- ference (SNLI) (Bowman et al., 2015), Multi- Genre Natural Language Inference (MultiNLI) (Williams et al., 2018), FarsTail (Amirkhani et al., 2021), SciTail (Khot et al., 2018), and more. One of the challenges when using these datasets is that in many existing samples the two input texts have the same and many similar words. In fact, both texts are very similar in appearance but express different meanings, or conversely, two texts have many different words, but the nature and meaning of both questions are very similar. The objective of NLSM models is to predict a category or scale value for a pair of input texts, which indicates their similarity or relationship. To achieve this goal, NLSM models generally use two main steps: (1) designing a model to obtain a proper representation of whatever text will be ana- lyzed so that it can extract semantic features from it. (2) By using the representation obtained from texts, a matching mechanism can be developed to extract complex interactions. In the current NLSM field, deep neural net- works are the preferred approach. To determine semanticand relationships among sen- tences, researchers use convolutional and recur- rent neural networks. The structure of convo- lutional neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNNÃ¢ÂÂs that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract text features, deep learn- ing networks require a lot of labeled data, which is one of the challenges of using deep learning, es- pecially in natural language processing. To extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data. The advantage of these pre-trained language models is that they do not need labeled data to be trained, so they can be trained on a large amount of unlabeled data and then fine-tuned them in downstream tasks. A tech- nique known as transfer learning is one of the best ways to resolve the challenges of natural language processing in low-resource languages today. In this paper, we used the following steps to de- sign a model based on NLSM principles identify- ing the semantic similarity of the input text pair: 1. Collecting Persian Religious question match- ing dataset for training and evaluating the proposed NLSM model. 2. Investigating related models to the field of NLSM. 3. Implementing ERT with Deep Recursive En- coder (BERT-DRE) model by adding recur- sive encoder module to BERT (Devlin et al., 2019). 4. Evaluating BERT-DRE with related models using the introduced religious and benchmark datasets. Dataset annotated for this study includes 18,000 samples, contains two questions, appropriate an- swers, and a match or not-match label for each question pair, which is used for designing a chat- bot. This dataset is collected by crawling reli- gious questions from religious question-answering websites, and then by annotating two similar ques- tions are annotated for each question, and dissim- ilar questions are generated automatically. It was noted our BERT-DRE model, which used the an- notated religious dataset to train and evaluate, was able to achieve an F1-score of 90.27% on the test data, making it the strongest model among the ones studied. Furthermore, in order to better eval- uate the BERT-DRE model, the model was trained using SNLI, MultiNLI, FarsTail, and SciTail and achieved appropriate F1-scores. Related Works Natural language processing studies have focused on NLSM, an essential part of many natural lan- guage tasks. In this regard, various datasets and models have been developed. There have been dif- ferent models developed for NLI, semantic match- ing, and paraphrase identification. Since all of these tasks are similar, the same model can be used today. In most cases, deep neural networks are used for this purpose. To this end, in the following, we shall examine the relevant works. As stated in (Wang et al., 2017), in contrast to previous work performed in text-similarity that adapted the sentence from one direction or used a word-to-word or sentence-to-sentence corre- spondence only, this study used bilateral multi- perspective matching from several perspectives. As a result, one of the main architectural pil- lars is based on Bidirectional LSTM. Furthermore, they examined the mechanism of different meth- ods, such as attention. This interaction between aggregate words is then examined from multiple perspectives. In (Wang et al., 2017) (BiMPM), sentences are matched bidirectionally with a new function calculating similar vectors. This method can result in similaritymultiple ways, which is called multi-perspective cosine similarity. Language objects internal structures and inter- actions of language objects need to be properly modeled to achieve a successful matching algo- rithm. A component of this goal is achieved in (Hu et al., 2014) through combining ideas from neu- ral networksÃÂ¢ÃÂÃÂ implementation in image and audio processing. Besides demonstrating the hierarchi- cal structures of sentences in layers, this model also maintains rich matching patterns on differ- ent levels. The (He et al., 2015) (MPCNN) pre- sented work based on CNN, analyzing multiple perspectives of sentences. CNN first extracts fea- tures for each sentence, then compares representa- tions of the sentences at various levels of granular- ity, and then uses various pooling techniques. Fol- lowing that, multiple similarity metrics are used to compare sentences at multiple levels of granular- ity. The MPCNN model processes each sentence independently, and there is no interactivity until the final Fully-Connected layer, which results in the loss of a great deal of useful information. For this reason, (Cao et al., 2018) and (He et al., 2016) make changes to the MPCNN architecture. In (Cao et al., 2018) has also improved MPCNN, one of which is the use of pre-trained embedding instead of random embedding. In the next step, characters are used to extract features. Finally, an input layer based on attention is added between the embedding and the multi-perspective sentence modeling layers. Attention-based neural network models have been successfully used in an- swer selection, which is an important sub-task of question answering. These models often represent the question using a vector and match it by refer- ring to the candidate answers. However, questions and answers may be interrelated in complex ways that cannot be represented by single-vector repre- sentations. In (Tran and NiedereeÃÂe, 2018), the idea of using multipurpose attention networks (MANs) is proposed, which aims to discover this complex relationship for ranking question and answer pairs. Unlike previous models, this paper does not turn the question into a single vector, but focuses on different parts of the question from several vectors to show its general meaning, and applies different stages of attention to learning the representations of the candidate answers. The study (Zhao et al., 2019) proposes an inter- active attention model for semantic text matching that uses the global matching matrix to learn repre- sentations for source and target texts during inter- active attention. This model can take a rich repre- sentation of source and target texts and derive an entirely related encoding. In (Chen et al., 2017), a model with a good performance called ESIM has a non-complex structure of LSTM layers and is cited in many later works as a base model. It has been claimed that recursive architecture can achieve good performance in local inference mod- eling and inference composition. In (Tay et al., 2018), a deep learning model named CAFE is presented in which alignment pairs are compared, compressed, and then spread to the upper layers to increase representation learning. Then, the alignment vectors are ex- pressed in scalar features through factorization layers. One of the other models that was able to achieve good accuracy in SNLI and MultiNLI datasets (Yin et al., 2016) is another deep learning model that is intended for modeling sentence pairs with CNNs named ABCNN. In this study, three different models of the combination of convolu- tional layers along with the attention mechanism have been used and the results of these three mod- els have been examined. Based on BERT representation of sentences and using convolution layers and the semantic role la- beler as features added to the model, (Zhang et al., 2020) has achieved good accuracy on MultiNLI and QNLI (Wang et al., 2018) data. Using a com- bination of CNN, attention mechanism, and resid- ual connection structures for inference, a deep learning method was proposed in (Yang et al., 2019) named RE2. The authors regard attention to pointwise features, previous alignments, and con- textual features as three success principles. This model has the advantage of being fast in calculat- ing model inferences. In comparison with other existing models, this model increases speed by six times and uses fewer parameters. In the (Kim et al., 2018) a deep learning model is proposed using the LSTM structure namedIn this paper, the alignment module is used to extract the relationship between the words of the first and second text sequences. Residual connections are also used to transfer the extracted features to each layer. The embedding layer in this model includes trainable word embedding, non-trainable word embedding, character embedding, and ex- act match feature. In this model, each layer includes a pooling layer, which is used to store and de- sign a large amount of data. The pooling layer in this model is used to store and de- sign a lot of data. To this end, in (Cao et al., 2018) has also improved embedding, which is used more data. In (Tran and NiedereeÃÂe, 2018), the pooling layer is also used more tightly. In (Cao et al., 2018), the ex- tractional layer is also used more tightly. Finally, in (Tay et al., 2018), the encoder layer, which is used for deter- mineing. This encoder layer is used for calculating similarity. In (Tay et al., 2018), the encoder is used more densely. In (Tay et al., 2018) has also improved encoder module, which is used more densely. In (Tay et al., 2018), the encoder module is used more densely. In (Amirkhani et al., 2019), the multi- perspective encode- ture layer 1 is added. In (Amirkhani et al., 2020), the encoder module is used more densely. In (Amirkhani et al., 2021), the encoder module is used more densely. In (Amirkhani et al., 2018), the encoder module is used more densely. In (Amirkhani et al., 2019), the multi- perspective encoder module is added.",0
"Keywords: Opinion Mining, Topic Modeling, Sentiment Analysis, Cross-Lingual, Multi-Lingual, Market Research. Abstract: User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics. on English and German newspaper and social media texts in Section 4. We aim at inspiring pragmatic ideas to explore the potential for comparative, inter- cultural market research and agenda setting studies. Unsolved problems and future potential are given in Section 5. RELATED WORK Topic modeling is meant to learn thematic structure from text corpora. With probabilistic topic model- ing methods, such as latent semantic indexing (LSI) (Deerwester et al., 1990) or latent Dirichlet allocation (LDA) (Blei et al., 2003), researchers try to extend the capabilities of topic modeling for application from a single language to multiple languages. Using multi- lingual dictionaries and translated corpora is an intu- itive way to tackle cross-lingual topic modeling prob- lems (Zhang et al., 2010; Vulic ́ et al., 2013). Further examples exist for topic modeling with either dictio- naries or translation text collections (Gutie ́rrez et al.,2016; Boyd-Graber and Blei, 2012; Jagarlamudi and Daume ́, 2010). However, this puts dependence on the availability of dictionary or good quality of transla- tions. Significant manual labor and verification are required to prevent deteriorating noise. Recently, methods converting words to vectors according to their semantics are widely adopted (Mikolov et al., 2013). Several studies showed text embeddings improve topic coherence (Bianchi et al., 2020; Srivastava and Sutton, 2017). Regarding multi- linguality, embeddings in word level and sentence level enable text in different languages to be projected to the same vector space (Cer et al., 2018) such that semantically similar texts are clustered together in- dependently of their languages. This favors studies on multi-lingual topic modeling without relying on dictionaries and translation (Xie et al., 2020; Chang et al., 2018). Although providing highly coherent top- ics, a recreation of word spaces is required when new text corpora are introduced. In our scenario, these limitations are not present. Regarding the application of topic modeling, vari- ous social media corpora are studied by domain ex- perts (Tsur et al., 2015; Ko et al., 2018). They covered different domains, such as politics, market- ing, and public health. Regarding media agenda set- ting, (Field et al., 2018) studied on how much degree a Russian newspaper related to economic downturn. They also ”introduced embedding-based methods for cross-lingually projecting English frame to Russian” based on Media Frames Corpus. In contrast, we pro- pose a straightforward topic modeling method with- out fine-tuning but only clustering necessary on a so- cial media corpus. This enables further investigation on media agenda setting cross-lingually and cross- culturally. CONCLUSION This case study shows that our technically simple ap- proach successfully generates an high proportion of relevant and coherent topics for our domain, i.e., or- ganic food products and related consumption behav- ior based on English and German social media texts. Moreover, the topics display the text contents cor- rectly and support a domain expert in the content analysis of social media texts written in multiple lan- guages. However, the presented paper did not provide quantitative measurements of topic coherences and comparisons with the state-of-the-art. For mono- language topic modeling, it would be LDA (Blei et al., 2003); for advanced cross-lingual topic modeling, it could be attention-based aspect extraction (He et al., 2017) utilizing aligned multi-lingual word vectors (Conneau et al., 2017). Several multi-lingual datasets would need to be included for a representative com- parison. Since pre-trained models trained on exter- nal data are used for the proposed method, it might be relevant for coherence score calculation to include intrinsic coherence scoring methods based on train test splits, such as, UMass coherence score (Mimno et al., 2011), and explore extrinsic methods calculated on external validation corpora, e.g., Wikipedia (Ro ̈der et al., 2015). Regarding multi-lingual sentiment analysis, the difference in the sentiment analysis frameworks for different languages must be considered. For example, since two independent but similar sentiment analysis models are applied for English and German,thesenti- ment distribution could be affected. Therefore, future studies on developing and evaluating comparable sentiment model should be conducted.",1
"Keywords: Opinion Mining, Topic Modeling, Sentiment Analysis, Cross-Lingual, Multi-Lingual, Market Research. Abstract: User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics. on English and German newspaper and social media texts in Section 4. We aim at inspiring pragmatic ideas to explore the potential for comparative, inter- cultural market research and agenda setting studies. Unsolved problems and future potential are given in Section 5. RELATED WORK Topic modeling is meant to learn thematic structure from text corpora. With probabilistic topic model- ing methods, such as latent semantic indexing (LSI) (Deerwester et al., 1990) or latent Dirichlet allocation (LDA) (Blei et al., 2003), researchers try to extend the capabilities of topic modeling for application from a single language to multiple languages. Using multi- lingual dictionaries and translated corpora is an intu- itive way to tackle cross-lingual topic modeling prob- lems (Zhang et al., 2010; Vulic ÃÂ et al., 2013). Further examples exist for topic modeling with either dictio- naries or translation text collections (Gutie ÃÂrrez et al.,2016; Boyd-Graber and Blei, 2012; Jagarlamudi and Daume ÃÂ, 2010). However, this puts dependence on the availability of dictionary or good quality of transla- tions. Significant manual labor and verification are required to prevent deteriorating noise. Recently, methods converting words to vectors according to their semantics are widely adopted (Mikolov et al., 2013). Several studies showed text embeddings improve topic coherence (Bianchi et al., 2020; Srivastava and Sutton, 2017). Regarding multi- linguality, embeddings in word level and sentence level enable text in different languages to be projected to the same vector space (Cer et al., 2018) such that semantically similar texts are clustered together in- dependently of their languages. This favors studies on multi-lingual topic modeling without relying on dictionaries and translation (Xie et al., 2020; Chang et al., 2018). Although providing highly coherent top- ics, a recreation of word spaces is required when new text corpora are introduced. In our scenario, these limitations are not present. Regarding the application of topic modeling, vari- ous social media corpora are studied by domain ex- perts (Tsur et al., 2015; Ko et al., 2018). They covered different domains, such as politics, market- ing, and public health. Regarding media agenda set- ting, (Fieldal., 2018) studied on how much degree a Russian newspaper related to economic downturn. They also ÃÂ¢ÃÂÃÂintroduced embedding-based methods for cross-lingually projecting English frame to RussianÃÂ¢ÃÂÃÂ based on Media Frames Corpus. In contrast, we pro- pose a straightforward topic modeling method with- out fine-tuning but only clustering necessary on a so- cial level. To demonstrate its feasibility, we apply the topic model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproduc- tions of our method, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproduc- tions of our method, we apply the method to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe.",0
"Abstract Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art meth- ods. However, resolving annotator bias pre- cisely and reliably is the key to understand annotators’ labeling behavior and to success- fully resolve corresponding individual miscon- ceptions and wrongdoings regarding the anno- tation task. Our contribution is an explana- tion and improvement for precise neural end- to-end bias modeling and ground truth esti- mation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has po- tential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators. Introduction Modeling annotator bias in conditions where each data point is annotated by multiple annotators, be- low referred to as multi-labeled crowdsourcing, has been investigated thoroughly. However, bias mod- eling when every data point is annotated by only one person, hereafter called singly labeled crowd- sourcing, poses a rather specific and difficult chal- lenge. It is in particular relevant for sentiment anal- ysis, where singly labeled crowdsourced datasets are prevalent. This is due to data from the social web which is annotated by the data creators them- selves, e.g., rating reviewers or categorizing image uploaders. This might further include multi-media contents such as audio, video, images, and other forms of texts. While the outlook for such forms of data is promising, end-to-end approaches have not yet been fully explored on these types of crowd- sourcing applications. With these benefits in mind, we propose a neural network model tailored for such data with singly labeled crowdsourced annotations. It computes a latent truth for each sample and the correct bias of every annotator while also considering input feature distribution during training. We modify the loss function such that the annotator bias con- verges towards the actual confusion matrix of the regarding annotator and thus models the annotator biases correctly. This is novel, as previous meth- ods either require a multi-labeled crowdsourcing setting (Dawid and Skene, 1979; Hovy et al., 2013) or do not produce a correct annotator bias during training which would equal the confusion matrix, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). A correct annotator- or annotator-group bias, however, is necessary to de- rive correct conclusions about the respective an- notator behavior. This is especially important for highly unreliable annotators who label a high num- ber of samples randomly – a setting, in which our proposed approach maintains its correctness, too. Our contributions are as follows. We describe the corresponding state-of-the-art for crowdsourc- ing algorithms and tasks in section 2. Our neural network model method for end-to-end crowdsourc- ing modeling is explained in section 3, which in- cludes a mathematical explanation that our linear bias modeling approach yields the actual confusion matrices. The experiments in section 4 underline our proof, show that the model handles annotator bias correctly as opposed to previous models, and demonstrate how the approach impacts classification. Related Work 2.1 Crowdsourcing Algorithms Problemdefinition. The need of data in the Growing research areas of machine learning has given rise to the generalized use of crowdsourcing. This method of data collection increases the amount of data, saves time and money but comes at the poten- tial cost of data quality. One of the key metrics of data quality is annotator reliability, which can be affected by various factors. For instance, the lack of rater accountability can entail spamming. Spam- mers are annotators that assign labels randomly and significantly reduce the quality of the data. Raykar and Yu (2012) and Hovy et al. (2013) addressed this issue by detecting spammers based on rater trustworthiness and the SpEM algorithm. How- ever, spammers are not the only source of label inconsistencies. The varied personal backgrounds of crowd workers often lead to annotator biases that affect the overall accuracy of the models. Sev- eral works have previously ranked crowd workers (Hovy et al., 2013; Whitehill et al., 2009; Yan et al., 2010), clustered annotators (Peldszus and Stede, 2013), captured sources of bias (Wauthier and Jor- dan, 2011) or modeled the varying difficulty of the annotation tasks (Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010) allowing for the elimi- nation of unreliable labels and the improvement of the model predictions. Ground truth estimation. One common chal- lenge in crowdsourced datasets is the ground truth estimation. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the raters’ abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al.,  2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data. Snow et al. (2008) proved that many non-expert annotators give a better performance than a few expert annotators and are cheaper in comparison. Singly Labeled Crowdsourced Datasets. Singly labeled datasets are an option given a fixed budget and unlimited data. Khetan et al. (2017) showed that it is possible to model worker quality with single labels even when the annotations are made by non-experts. Thus, multiple annotations can not only be redundant but come at the expense of fewer labeled samples. For singly labeled data, it can be distinguished between reviewer annotators and external annotators. Reviewer annotators rate samples they created themselves. It is common in forums for product and opinion reviews where a review is accompanied by a rating. As an example of this, we utilized the TripAdvisor dataset (Thel- wall, 2018). Further candidates are the Amazon review dataset (Ni et al., 2019), the Large Movie Review Dataset (Maas et al., 2011), and many more comprising sentiment. External annotators anno- tate samples they have not created. Experts are needed for complex annotation tasks requiring do- main knowledge. These are not crowdsourced, since the number of annotators is small and fixed. More common are external non-experts. Snow et al. (2008) showed that multi-labeled datasets anno- tated by non-expert improve performance. Khetan et al. (2017) showed that it also performs well in the singly labeled case. Thus, datasets made of singly labeled non-expert annotations can be cheaper, faster, and obtain performances compara- ble to those comprised of different types of annota- tions. Our organic dataset is annotated accordingly, see section 4.3. Conclusion We showed the efficacy of LTNet for modeling crowdsourced data and the inherent bias accurately and robustly. The bias matrices produced by our modified LTNet improve such that they are more similar to the actual bias between the latent truth and ground truth. Moreover, the produced bias shows high robustness under very noisy condi- tions making the approach potentially usable out- side of lab conditions. The latent truth, which is a hidden layer below all annotator biases, can be used for ground truth estimation in our sin- gle label crowdsourcing scenario, providing al- most identical ground truth estimates as pseudo labeling. Classification on three crowdsourced datasets show that LTNet approaches outperfom naive approaches not considering each annotator separately. The proposed log removal from the loss function showed better results on singly labeled crowdsourced datasets, but this observation needs further experiments to be substantiated. Further- more, there might be many use cases to explore the approach on other tasks than sentiment analysis.",1
"Abstract Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art meth- ods. However, resolving annotator bias pre- cisely and reliably is the key to understand annotatorsÃ¢ÂÂ labeling behavior and to success- fully resolve corresponding individual miscon- ceptions and wrongdoings regarding the anno- tation task. Our contribution is an explana- tion and improvement for precise neural end- to-end bias modeling and ground truth esti- mation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has po- tential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators. Introduction Modeling annotator bias in conditions where each data point is annotated by multiple annotators, be- low referred to as multi-labeled crowdsourcing, has been investigated thoroughly. However, bias mod- eling when every data point is annotated by only one person, hereafter called singly labeled crowd- sourcing, poses a rather specific and difficult chal- lenge. It is in particular relevant for sentiment anal- ysis, where singly labeled crowdsourced datasets are prevalent. This is due to data from the social web which is annotated by the data creators them- selves, e.g., rating reviewers or categorizing image uploaders. This might further include multi-media contents such as audio, video, images, and other forms of texts. While the outlook for such forms of data is promising, end-to-end approaches have not yet been fully explored on these types of crowd- sourcing applications. With these benefits in mind, we propose a neural network model tailored for such data with singly labeled crowdsourced annotations. It computes a latent truth for each sample and the correct bias of every annotator while also considering input feature distribution during training. We modify the loss function such that the annotator bias con- verges towards the actual confusion matrix of the regarding annotator and thus models the annotator biases correctly. This is novel, as previous meth- ods either require a multi-labeled crowdsourcing setting (Dawid and Skene, 1979; Hovy et al., 2013) or do not produce a correct annotator bias during training which would equal the confusion matrix, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). A correct annotator- or annotator-group bias, however, is necessary to de- rive correct conclusions about the respective an- notator behavior. This is especially important for highly unreliable annotators who label a high num- ber of samples randomly Ã¢ÂÂ a setting, in which our proposed approach maintains its correctness, too. Our contributions are as follows. We describe the corresponding state-of-the-art for crowdsourc- ing algorithms and tasks in section 2. Our neural network model method for end-to-end crowdsourc- ing modeling is explained in section 3, which in- cludes a mathematical explanation that our linear bias modeling approach yields the actual confusion matrices. The experiments in section 4 underline our proof, show that the model handles annotator bias correctly as opposed to previous models, and demonstrate how the approach impacts classification. Related Work 2.1 Crowdsourcing Algorithms Problemdefinition. The need of data in the Growing research areas of machine learning has given rise to the generalized use of crowdsourcing. This method of data collection increases the amount of data, saves time and money but comes at the poten- tial cost of data quality. One of the key metrics of data quality is annotator reliability, which can be affected by various factors. For instance, the lack of rater accountability can entail spamming. Spam- mers are annotators that assign labels randomly and significantly reduce the quality of the data. Raykar and Yu (2012) and Hovy et al. (2013) addressed this issue by detecting spammers based on rater trustworthiness and the SpEM algorithm. How- ever, spammers are not the only source of label inconsistencies. The varied personal backgrounds of crowd workers often lead to annotator biases thatthe overall accuracy of the models. Sev- eral works have previously ranked crowd workers (Hovy et al., 2013; Whitehill et al., 2009; Yan et al., 2010), clustered annotators (Peldszus and Stede, 2013), captured sources of bias (Wauthier and Jor- dan, 2011) or modeled the varying difficulty of the annotation tasks (Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010) allowing for the elimi- nation of unreliable labels and the improvement of the model predictions. Ground truth estimation. One common chal- lenge in crowdsourced datasets is the ground truth estimation. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the ratersÃ¢ÂÂ abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al., 2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data. Snow et al. (2008) proved that many non-expert annotators give a better performance than a few expert annotators and are cheaper in comparison. Singly Labeled Crowdsourced Datasets. Singly labeled datasets are an option given a fixed budget and unlimited data.et al. (2017) showed that it is possible to model worker quality with single labels even when the annotations are made by non-experts. Thus, multiple annotations can not only be redundant but come at the expense of fewer labeled samples. For singly labeled data, it can be distinguished between reviewer annotators and external annotators. Reviewer annotators rate samples they created themselves. It is common in forums for product and opinion reviews where a review is accompanied by a rating. As an example of this, we utilize the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates. However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2018). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data.",0
"Abstract This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be claimed that this corpus is currently the largest Persian corpus that has been prepared independently for the Persian language. This corpus is presented in both raw and preprocessed forms, and based on the preprocessed corpus some word embedding models are produced. By the provided models, the hmBlogs is compared with some of the most important corpora available in Persian, and the results show the superiority of the hmBlogs corpus over the others. These evaluations also present the importance and effects of corpora, evaluation datasets, model production methods, different hyperparameters and even the evaluation methods. In addition to evaluating the corpus and its produced language models, this research also presents a semantic analogy dataset. Keywords Persian Corpus, Blog, Word Embedding, Analogy Test Set Introduction Language corpora are basic resources in natural language processing (NLP). These corpora range from small to very big (including billions of words). They can include just raw text or have some meta-data such as tags and annotations. As a general rule, it can be said that a larger corpus leads to a more useful corpus. This is especially the case when the main reliance is on statistical methods. Persian (Farsi) has an especially wide range of speakers across the world, specifically in Iran, Tajikistan and Afghanistan and is used as either a native or second language. It is, therefore, especially important to develop the necessary NLP tools and resources for this language. A major downfall of Persian as a low resource language is its lack of sufficiently large, covered and up-to-date corpora. In recent years, the use of language models based on word embeddings has become a common phenomenon. These models are not only crucial and practical on their own, but also have deep neural networks applications. These models are produced by the corpora and their quality strongly depends to the used corpora. A basic requirement for a corpus is its ability to represent the language adequately and incorporate the necessary genres and domains. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora. Related work Among the various corpora (with different dimensions and areas) prepared for the Persian language, some are private and publicly unavailable and others are either publicly available or accessible to researchers. It can be claimed that the volume and variety of corpora available for the Persian language is much less than that available for other languages such as English, which is a laboratory language. Corpora can be obtained from different news websites, newspapers and magazines, books, blogs and tweets, and etc. Due to a lack of Persian content on the Internet in the past years, older sources, such as the Hamshahri corpus (AleAhmad et al., 2009), rely less on Internet-created data. Newer corpora, such as the MirasText (Sabeti et al., 2018), on the other hand, are made only from the Persian content found on the Internet; and, even the presence of snippets is due to the publication of those snippets on the Internet. The Hamshahri corpus (AleAhmad et al., 2009) is one of the oldest Persian corpora and was founded on the compilation of news and articles of the Hamshahri newspaper, one of the most famous newspapers in Iran. It contains more than 166 thousand news and more than 63 million words. Its content is dated between the years 1996 and 2002. Articles in this corpus are labeled in 12 main thematic categories. One problem with this corpus is its lack of up-to- date content, which means that it fails to include certain important language and content changes such as changes in language and linguistic framing, semantic drifts and appearing new names and events. Due to its news nature, it is also devoid of the different types of language uses common among Persian speakers. An advantage of this corpus, however, is its accuracy and lack of typographical errors of the compiled text. Peykareh (Bijankhan et al., 2011) is another early popular Persian corpus for NLP applications. This corpus contains 100 million words from a collection of news, web sites, and written sources. Peykareh texts range from the year 1978 to 2003. An important feature of this corpus from an NLP stance is its POS tagging, which is done semi-automatically. Of course, only about one tenth of this corpus has been labeled. Based on the word embedding training and the fact that its latest texts were compiled before 2004, Peykareh is a corpus of old texts. Furthermore, its size may not be large enough to make appropriate word embedding models. Another important resource used in NLP field is Wikipedia. The Persian Wikipedia (FaWiki) was launched in December 2003 and contained more than 775,000 articles by March 2021 (“Persian Wikipedia,” 2021). Persian is currently the 19th language on Wikipedia (List of Wikipedias, n.d.). Like the Hamshahri corpus, the FaWiki has an official language that cannot cover all language types. Furthermore, due to its encyclopedic nature, a large number of Wikipedia articles are too specialized for Persian speakers and everyday usage. For example, the vast majority of Persian speakers do not know the capital, or currency of many countries. Or many people are unfamiliar with the names of drugs, programming languages or specialized topics in astronomy or anatomy. The existence of such articles and the preparation of word embedding based on such can cause incorrect bias and a disconnect between the obtained model and the real language of the people. Of course, an important advantage of Wikipedia compared to others is its constant updates and continuous expansion. IrBlogs (AleAhmad et al., 2016) is a corpus that has been created by crawling Persian blogs. Blogs are one of the most important resources for making corpora and are a valuable and rich source of a variety of texts due to a diversity of authors and topics. IrBlogs is the result of the crawl of more than 564 thousand blogs and includes nearly 5 million posts. It contains posts collected from 2002 to 2013 to study the Persian blogging space. This corpus also includes posts that are not in Persian. The existence of posts with other languages impairs the preparation of a suitable language model. Also, the newest texts of irBlogs are about 7 years old and do not include the most recent changes such as new names and events. Another text corpus prepared for the Persian language is the MirasText (Sabeti et al., 2018). When published in 2018, the MirasText was considered to be the largest corpus available in Persian. It contains about 2.8 million documents and about 1.4 billion words collected from around 250 websites, of which at least 150 sites of them are news websites. In addition to plain text, this corpus provides information, such as titles, descriptions, and keywords for its documents. The Persian Raw Text (PRT) (Persiannlp/Persian-Raw-Text, 2020/2021) is a corpus provided by putting together a variety of Persian corpora (including MirasText and FaWiki) and is not produced independently. Also, no special processing has been performed on it to detect any duplication. The main part of PRT is obtained from the Persian section of the Common Crawl project (Common Crawl, n.d.). Common Crawl is a web crawl project that crawls and collects resources available on the web in any language, including some in Persian. PRT is larger than hmBlogs in size, but as shown in the evaluation section, it scores lower than hmBlogs. Conclusion This paper aimed to introduce a large, open and general corpus for the Persian language, as a low resource language. The introduced corpus is, by far, the largest of all the few independent and publicly available Persian corpora (with the exception of PRT, which is not an independent corpus but a collection of several corpora). The hmBlogs corpus is a rather large corpus even when compared to the English corpora (for example see (English Corpora, n.d.)). HmBlogs has had the latest style of Persian writing in recent years and has preserved texts from the last 15 years in the Persian blog space. HmBlogs showed overall better performance than other corpora such as irBlogs, PRT and FaWiki in word embedding models. A new analogy dataset, (FATS), was also presented as a side product of this research along with the obtained models. Also all factors including the corpora, evaluation data, evaluation methods, model construction methods and hyperparameters were found to have notable effects on the evaluation output. One of the challenges is keeping such corpora up to date. To solve this problem, it may be necessary to design processes that can keep corpora with the latest changes and adding new content. Designing such a process and its related systems could be a topic for future studies. The hmBlogs can also be considered for further development with new resources, such as crawling and adding post comments, in the future. Another possibility could be combining the corpus with other sources, such as news sources, the Persian wiki, and social media data. Tokenization poses another challenge to the Persian language because many words in Persian are multi-token. Even in some cases, particularly in some Persian verbs, tokens of a word are not consecutive and many words can fall between the gaps. Analyzing this phenomenon was beyond the scope of this research and should be addressed independently, but further research in this area can help to increase the quality of language models produced from corpora. A further task can also be the labeling of the present corpus. Topic modeling, official/informal/colloquial language labeling, tagging the polarity of posts, and annotating highly similar or nearly duplicated contents are all topics for further studies.",1
"Abstract This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be claimed that this corpus is currently the largest Persian corpus that has been prepared independently for the Persian language. This corpus is presented in both raw and preprocessed forms, and based on the preprocessed corpus some word embedding models are produced. By the provided models, the hmBlogs is compared with some of the most important corpora available in Persian, and the results show the superiority of the hmBlogs corpus over the others. These evaluations also present the importance and effects of corpora, evaluation datasets, model production methods, different hyperparameters and even the evaluation methods. In addition to evaluating the corpus and its produced language models, this research also presents a semantic analogy dataset. Keywords Persian Corpus, Blog, Word Embedding, Analogy Test Set Introduction Language corpora are basic resources in natural language processing (NLP). These corpora range from small to very big (including billions of words). They can include just raw text or have some meta-data such as tags and annotations. As a general rule, it can be said that a larger corpus leads to a more useful corpus. This is especially the case when the main reliance is on statistical methods. Persian (Farsi) has an especially wide range of speakers across the world, specifically in Iran, Tajikistan and Afghanistan and is used as either a native or second language. It is, therefore, especially important to develop the necessary NLP tools and resources for this language. A major downfall of Persian as a low resource language is its lack of sufficiently large, covered and up-to-date corpora. In recent years, the use of language models based on word embeddings has become a common phenomenon. These models are not only crucial and practical on their own, but also have deep neural networks applications. These models are produced by the corpora and their quality strongly depends to the used corpora. A basic requirement for a corpus is its ability to represent the language adequately and incorporate the necessary genres and domains. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora. Related work Among the various corpora (with different dimensions and areas) prepared for the Persian language, some are private and publicly unavailable and others are either publicly available or accessible to researchers. It can be claimed that the volume and variety of corpora available for the Persian language is much less than that available for other languages such as English, which is a laboratory language. Corpora can be obtained from different news websites, newspapers and magazines, books, blogs and tweets, and etc. Due to a lack of Persian content on the Internet in the past years, older sources, such as the Hamshahri corpus (AleAhmad et al., 2009), rely less on Internet-created data. Newer corpora, such as the MirasText (Sabeti et al., 2018), on the other hand, are made only from the Persian content found on the Internet; and, even the presence of snippets is due to the publication of those snippets on the Internet. The Hamshahri corpus (AleAhmad et al., 2009) is one of the oldest Persian corpora and was founded on the compilation of news and articles of the Hamshahri newspaper, one of the most famous newspapers in Iran. It contains more than 166 thousand news and more than 63 million words. Its content is dated between the years 1996 and 2002. Articles in this corpus are labeled in 12 main thematic categories. One problem with this corpus is its lack of up-to- date content, which means that it fails to include certain important language and content changes such as changes in language and linguistic framing, semantic drifts and appearing new names and events. Due to its news nature, it is also devoid of the different types of language uses common among Persian speakers. An advantage of this corpus, however, is its accuracy and lack of typographical errors of the compiled text. Peykareh (Bijankhanal., 2011) is another early popular Persian corpus for NLP applications. This corpus contains 100 million words from a collection of news, web sites, and written sources. Peykareh texts range from the year 1978 to 2003. An important feature of this corpus from an NLP stance is its POS tagging, which is done semi-automatically. Of course, only about one tenth of this corpus has been labeled. Based on the word embedding training and the fact that its latest texts were compiled before 2004, Peykareh is a corpus of old texts. Furthermore, its size may not be large enough to make appropriate word embedding models. Another important resource used in NLP field is Wikipedia. The Persian Wikipedia (FaWiki) was launched in December 2003 and contained more than 775,000 articles by March 2021 (Ã¢ÂÂPersian Wikipedia,Ã¢ÂÂ 2021). Persian is currently the 19th language on Wikipedia (List of Wikipedias, n.d.). Like the Hamshahri corpus, the FaWiki has an official language that cannot cover all language types. Furthermore, due to its encyclopedic nature, a large number of Wikipedia articles are too specialized for Persian speakers and everyday usage. For example, the vast majority of Persian speakers do not know the capital, or currency of many countries. Or many people are unfamiliar with the names of drugs, programming languages or specialized topics in astronomy or anatomy. The existence of such articles and the preparation of word embedding based on such can cause incorrect bias and a disconnect between the obtained model and the real language of the people. Of course, an important advantage of Wikipedia compared to others is its constant updates and continuous expansion. IrBlogs (AleAhmad et al., 2016) is a corpus that has been created by crawling Persian blogs. Blogs are one of the most important resources for making corpora and are a valuable and rich source of a variety of texts due to a diversity of authors and topics. IrBlogs is the result of the crawl of more than 564 thousand blogs and includes nearly 5 million posts. It contains posts collected from 2002 to 2013 to study the Persian blogging space. This corpus also includes posts that are not in Persian. The existence of posts with other languages impairs the preparation of a suitable language model. Also, the newest texts of irBlogs are about 7 years old and do not include the most recent changes such as new names and events. Another text corpus prepared for the Persian language is the MirasText (Sabeti et al., 2018). When published in 2018, the MirasText was considered to be the largest corpus available in Persian. It contains about 2.8 million documents and about 1.4 billion words collected from around 250 websites, of which at least 150 sites of them are news websites. In addition to plain text, this corpus provides information, such as titles, descriptions, and keywords for its documents. The Persian Raw Text (PRT) (Persiannlp/Persian-Raw-Text, 2020/2021) is a corpus provided by putting together a variety of Persian corpora (including MirasText and FaWiki) and is not produced independently. Also, no special processing has been performed on it to detect any duplication. The main part of PRT is obtained from the Persian section of the Common Crawl project (Common Crawl, n.d.). Common Crawl is a web crawl project that crawls and collects resources available on the web in any language, including some in Persian. PRT is larger than hmBlogs in size, but as shown in the evaluation section, it scores lower than hmBlogs. Conclusion This paper aimed to introduce a large, open and general corpus for the Persian language, as a low resource language. The introduced corpus is, by far, the largest of all the few independent and publicly available Persian corpora (with the exception of PRT, which is not an independent corpus but a collection of several corpora). The hmBlogs corpus is a rather large corpus even when compared to the English corpora (for example see (English Corpora, n.d.)). HmBlogs has had the latest style of Persian writing in recent years and has preserved texts from the last 15 years in the Persian blog space. HmBlogs showed overall better performance than other corpora such as irBlogs, PRT and FaWiki in word embedding models. A new analogy dataset, (FATS), was also presented as a side product of this research along with the obtained models. Also all factors including the corpora, evaluation data, evaluation methods, model construction methods and hyperparameters were found to have notable effects on the evaluationOne of the challenges is keeping such corpora up to date. To solve this problem, it may be necessary to design processes that can keep corpora with the latest changes and adding new content. Designing such a process and its related systems could be a topic for future studies. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora. Related work Among the various corpora (with different dimensions and areas) prepared for the Persian language, some are private and publicly unavailable and others are either publicly available or accessible to researchers. It can be claimed that the volume and variety of corpora available for the Persian language is much less than that available for other languages such as English, which is a laboratory language.",0
"Abstract Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on largescale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality. Introduction Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have achieved remarkable success in various natural language processing tasks (Sun et al., 2019). As such, there is a growing trend of using pre-trained language models for conversation modeling (Budzianowski and Vulic´, 2019; Zhang et al., 2019; Feng et al., 2021). For example, Zhang et al. (2019) proposed DialoGPT, a dialogue generation model that trains an extended GPT-2 (Radford et al., 2019) on large dialogue corpus. Feng et al. (2021) further explore the usage of DialoGPT for dialogue summarization. These pre-trained dialogue models are often pre-trained on large text corpora and fine-tuned on smaller dialogue datasets (Zhang et al., 2019). One limitation of PLM-based dialogue modeling, and even for other PLM tasks, is the trade-off between pre-training and fine-tuning (Ben-David et al., 2021). That is, the task-specific data used for fine-tuning is usually scarce and costly. As such, the reusability of prior knowledge learned in the pre-training phase can be limited during finetuning, hence some dialogue models are simply trained from scratch on the limited task-specific data. Consequently, recent works have resorted to prompt learning, a lightweight alternative to finetuning. Prompt learning keeps the PLM parameters frozen but optimizes only a small portion of task-specific prompts or related modules (Liu et al., 2021a; Shin et al., 2020; Liu et al., 2021b; Li and Liang, 2021). For example, Liu et al. (2021b) propose p-tuning, which preprends trainable prompt tokens to the input of a PLM. The trainable prompt embeddings are optimized while the PLM parameters are kept frozen. Prompt learning allows fewshot or nearly zero-shot learning for pre-trained models in new tasks with little or unlabeled data and it has been demonstrated to be substantially effective over fine-tuning in many tasks (Liu et al., 2021a; Qin and Eisner, 2021). However, applying prompt learning directly to conversation modeling is challenging. The general prompt-learning models assign universal prompt tokens to all inputs in the same task (Liu et al., 2021b). For example, prompts used for sentiment analysis share the same embeddings that are inferred from the training data (Liu et al., 2021b). In contrast, conversations are context-sensitive. Dialogue responses are affected by contextual information, such as the topic of discussion, pre-dialogue context, and participant personalities. “Blanket” prompts can restrict the expressiveness of prompt learning due to the lack of context-awareness, leading to sub-optimal performance in response generation. In this work, we present DialogPrompt, a novel prompt-based paradigm for response generation on top of large pre-trained language models. DialogPrompt prepends a sequence of prompt tokens to each dialogue context for eliciting response from large pre-trained language models. In order to construct context-aware prompts, we propose a conditional prompt encoder on top of the Transformer (Vaswani et al., 2017). The prompt tokens are initially encoded conditionally on the dialogue context. The resulting prompt encoding is then taken as the initial hidden state of the large PLM to generate responses. Compared to fine-tuning, DialogPrompt is encouraged to search proper prompts which controls the large PLMs into producing higher-quality responses directly. We evaluate DialogPrompt on popular multi-turn conversation datasets such as DailyDialog and MultiWOZ. Results show that DialogPrompt outperforms fine-tuning counterparts and other prompt tuning methods in terms of automated evaluation measures and the average length of generated responses. Human evaluation supports the superiority of our approach in generating informative and knowledgeable responses. Our contributions are summarized as follows: • To our knowledge, we are the first to propose prompt-based dialogue response generation. Our approach can better reuse knowledge from existing large-scale PLMs and produce more knowledgeable responses. • We design a novel conditional prompt encoder for encouraging context-aware prompt learning. • We extensively evaluated our approach on popular multi-turn conversation datasets and demonstrated the superiority of our approach in terms of quantitative automatic evaluations and qualitative human evaluations. Related Work This work is closely related to (1) pre-trained models for conversations, and (2) prompt learning for pre-trained language models. Pre-trained Models for Dialogue Generation. Recently, an emerging trend in dialogue generation explores the adaptation of large pre-trained language models on dialogue corpora (Golovanov et al., 2019; Zhang et al., 2019). For example, Golovanov et al. (2019) studied how pre-trained architectures can be adapted for natural language generation, comparing a number of architectural and training schemes. The state-of-the-art DialoGPT (Zhang et al., 2019) pre-trains a GPT-2 model on largescale conversation datasets and achieves a giant leap in performance against traditional conversation models. Another line of work related to exploiting the use of pre-trained models for task-oriented dialogues. For example, Budzianowski and Vulic´ (2019) proposed a task-oriented dialogue model that operates solely on text input. Their model is built on top of the TransferTransfo framework (Golovanov et al., 2019) that effectively bypasses explicit policy and language generation modules. TOD-BERT proposed by Wu et al. (2020) bridges the difference of general text and task-oriented dialogue by unifying nine human-human and multi-turn task-oriented dialogue datasets for language modeling. The model also incorporates user and system tokens into the masked language modeling and proposes a contrastive objective function to simulate the response selection task. Compared to these related works which directly fine-tune the dialogue model based on a pre-trained model, DialogPrompt is a novel paradigm for pretrained dialogue models which elicits knowledge from PLMs directly through minimal optimizing of prompt tokens. Prompt Learning for Pre-trained Language Models. There is a growing trend of automatically finding prompts to adapt pre-trained language models to downstream tasks (Shin et al., 2020; Li and Liang, 2021; Liu et al., 2021b). For example, Shin et al. (2020) proposed AutoPrompt which automatically optimizes prompts using a gradient signal. Unlike our method, AutoPrompt searches for hard prompts, thus it may be less versatile than the continuous methods. Instead, Liu et al. (2021b) proposed a continuous prompt tuning model named p-tuning. p-tuning optimizes fill-inthe-blank prompts in a continuous space, tested on GPT-2 and BERT models. A similar idea was proposed by Li and Liang (Li and Liang, 2021) who considered the tuning of prompts using a textual prefix. Specifically, they prepended a few taskspecific “soft tokens” (prefix) to the source text and tuned the hidden states of only these tokens (at all Transformer layers). Similarly, Lester et al. (2021) prepended a sequence of prompt tokens to the source text, but only the word embeddings of these tokens are optimized. Qin and Eisner (2021) proposed prompt-based learning on relation extraction tasks using data-dependent mixtures of prompt templates and parameters. Our method differs from existing prompt-based tuning methods in that we propose a novel contextaware prompt tuning mechanism that can optimize prompt encodings conditioned on dialogue contexts. Conclusion In this paper, we propose DialogPrompt, a novel prompt based response generation model. DialogPrompt prepends a prompt utterance to the dialogue context and only optimizes the prompt encoder. In order to adapt to different contexts, we propose a conditional prompt encoder that updates prompt activation based on the hidden states of context before response generation. Results on two popular conversation datasets, namely, DailyDialog and MultiWOZ show that DialogPrompt signifi- cantly outperforms fine-tuning counterparts and other prompt based models on both automatic and human evaluations. In the future, we will investigate prompt-based dialogue modeling based on more pre-trained language models.",1
"Abstract Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on largescale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality. Introduction Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have achieved remarkable success in various natural language processing tasks (Sun et al., 2019). As such, there is a growing trend of using pre-trained language models for conversation modeling (Budzianowski and VulicÃÂ´, 2019; Zhang et al., 2019; Feng et al., 2021). For example, Zhang et al. (2019) proposed DialoGPT, a dialogue generation model that trains an extended GPT-2 (Radford et al., 2019) on large dialogue corpus. Feng et al. (2021) further explore the usage of DialoGPT for dialogue summarization. These pre-trained dialogue models are often pre-trained on large text corpora and fine-tuned on smaller dialogue datasets (Zhang et al., 2019). One limitation of PLM-based dialogue modeling, and even for other PLM tasks, is the trade-off between pre-training and fine-tuning (Ben-David et al., 2021). That is, the task-specific data used for fine-tuning is usually scarce and costly. As such, the reusability of prior knowledge learned in the pre-training phase can be limited during finetuning, hence some dialogue models are simply trained from scratch on the limited task-specific data. Consequently, recent works have resorted to prompt learning, a lightweight alternative to finetuning. Prompt learning keeps the PLM parameters frozen but optimizes only a small portion of task-specific prompts or related modules (Liu et al., 2021a; Shin et al., 2020; Liu et al., 2021b; Li and Liang, 2021). For example, Liu et al. (2021b) propose p-tuning, which preprends trainable prompt tokens to the input of a PLM. The trainable prompt embeddings are optimized while the PLM parameters are kept frozen. Prompt learning allows fewshot or nearly zero-shot learning for pre-trained models in new tasks with little or unlabeled data and it has been demonstrated to be substantially effective over fine-tuning in many tasks (Liu et al., 2021a; Qin and Eisner, 2021). However, applying prompt learning directly to conversation modeling is challenging. The general prompt-learning models assign universal prompt tokens to all inputs in the same task (Liu et al., 2021b). For example, prompts used for sentiment analysis share the same embeddings that are inferred from the training data (Liu et al., 2021b). In contrast, conversations are context-sensitive. Dialogue responses are affected by contextual information, such as the topic of discussion, pre-dialogue context, and participant personalities. Ã¢ÂÂBlanketÃ¢ÂÂ prompts can restrict the expressiveness of prompt learning due to the lack of context-awareness, leading to sub-optimal performance in response generation. In this work, we present DialogPrompt, a novel prompt-based paradigm for response generation on top of large pre-trained language models. DialogPrompt prepends a sequence of prompt tokens to each dialogue context for eliciting response from large pre-trained language models. In order to construct context-aware prompts, we propose a conditional prompt encoder on top of the Transformer (Vaswani et al., 2017). The prompt tokens are initially encoded conditionally on the dialogue context. The resulting prompt encoding istaken as the initial hidden state of the large PLM to generate responses. Compared to fine-tuning, DialogPrompt is encouraged to search proper prompts which controls the large PLMs into producing higher-quality responses directly. We evaluate DialogPrompt on popular multi-turn conversation datasets such as DailyDialog and MultiWOZ. Results show that DialogPrompt outperforms fine-tuning counterparts and other prompt tuning methods in terms of automated evaluation measures and the average length of generated responses. Human evaluation supports the superiority of our approach in generating informative and knowledgeable responses. Our contributions are summarized as follows: Ã¢ÂÂ¢ To our knowledge, we are the first to propose prompt-based dialogue response generation. Our approach can better reuse knowledge from existing large-scale PLMs and produce more knowledgeable responses. Ã¢ÂÂ¢ We design a novel conditional prompt encoder for encouraging context-aware prompt learning. Ã¢ÂÂ¢ We extensively evaluated our approach on popular multi-turn conversation datasets and demonstrated the superiority of our approach in terms of quantitative automatic evaluations and qualitative human evaluations. Related Work This work is closely related to (1) pre-trained models for conversations, and (2) prompt learning for pre-trained language models. Pre-trained Models for Dialogue Generation. Recently, an emerging trend in dialogue generation explores the adaptation of large pre-trained language models on dialogue corpora (Golovanov et al., 2019; Zhang et al., 2019). For example, Golovanov et al. (2019) studied how pre-trained architectures can be adapted for natural language generation, comparing a number of architectural and training schemes. The state-of-the-art DialoGPT (Zhang et al., 2019) pre-trains a GPT-2 model on largescale conversation datasets and achieves a giant leap in performance against traditional conversation models. Another line of work related to exploiting the use of pre-trained models for task-oriented dialogues. For example, Budzianowski and VulicÃÂ´ (2019) proposed a task-oriented dialogue model that operates solely on text input. Their model is built on top of the TransferTransfo framework (Golovanov et al., 2019) that effectively bypasses explicit policy and language generation modules. TOD-BERT proposed by Wu et al. (2020) bridges the difference of general text and task-oriented dialogue by unifying nine human-human and multi-turn task-oriented dialogue datasets for language modeling. The model also incorporates user and system tokens into the masked language modeling and proposes a contrastive objective function to simulate the response selection task. Compared to these related works which directly fine-tune the dialogue model based on a pre-trained model, DialogPrompt is a novel paradigm for pretrained dialogue models which elicits knowledge from PLMs directly through minimal optimizing of prompt tokens. Prompt Learning for Pre-trained Language Models. There is a growing trend of automatically finding prompts to adapt pre-trained language models to downstream tasks (Shin et al., 2020; Li and Liang, 2021; Liu et al., 2021b). For example, Shin et al. (2020) proposed AutoPrompt which automatically optimizes prompts using a gradient signal. Unlike our method, AutoPrompt searches for hard prompts, thus it may be less versatile than the continuous methods. Instead, Liu et al. (2021b) proposed a continuous prompt tuning model named p-tuning. p-tuning optimizes fill-inthe-blank prompts in a continuous space, tested on GPT-2 and BERT models. A similar idea was proposed by Li and Liang (Li and Liang, 2021) who considered the tuning of prompts using a textual prefix. Specifically, they prepended a few taskspecific Ã¢ÂÂsoft tokensÃ¢ÂÂ (prefix) to the source text and tuned the hidden states of only these tokens (at all Transformer layers). Similarly, Lester et al. (2021) prepended a sequence of prompt tokens to the source text, but only the word embeddings of these tokens are optimized. Qin and Eisner (2021) proposed prompt-based learning on relation extraction tasks using data-dependent mixtures of prompt templates and parameters. Our method differs from existing prompt-based tuning methods in that we propose a novel contextaware prompt tuning mechanism that can optimize prompt encodings conditioned on dialogue contexts. Conclusion In this paper, we propose DialogPrompt, a novel prompt based response generation model. DialogPrompt prepends a prompt utterance to the dialogue context and only optimizes the prompt encoder. In order to adapt to different contexts, we propose a conditional prompt encoder that updates prompt activation based onhidden states of context before response generation. Results on two popular conversation datasets, namely, DailyDialog and MultiWOZ show that DialogPrompt signifi- cantly outperforms fine-tuning counterparts and other prompt based models on both automatic and human evaluations. In the future, we will investigate prompt-based dialogue modeling based on more pre-trained language models.",0
"Abstract Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models’ performance in terms of Accuracy in LAMBADA and Children’s Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. To make this architecture more efficient, a plethora of approaches have been introduced (Kitaev et al., 2020; Beltagy et al., 2020; Child et al., 2019, inter alia), aiming to lower the computational complexity without sacrificing performance. Tay et al. (2020) summarizes these approaches and categorizes them based on type of attention mechanism in their survey. Yet, even with linear complexity, physical resources will always limit the amount of contextual information that can be handled simultaneously. In terms of language, entities represent a natural way to tie words together through large pieces of discource. In NLP, Coreference Resolution is the task that aims to identify and group these entity mentions together, when they refer to the same real world entity (Stylianou and Vlahavas, 2021). Therefore, Coreference Resolution presents a natural way to link the context that we are currently handling with distant information, far outside the capabilities model architectures and current hardware resources. However, while this information is present in the text, it is usually not annotated and when annotated are very sparse and in small quantities making it extremely difficult to train large LMs (Kunz and Hardmeier, 2019). In this paper we present a framework to effectively use coreference annotations to further finetune large PLMs, increasing their performance far more than just by fine-tuning on the same data. By using large PLMs we take advantage of existing resources that are expensive to reproduce and come with a big environmental cost (Strubell et al., 2019). What is more, fine-tuning takes advantage of the massive amount of data that essentially initialize the model, making it possible to introduce new capabilities to models with small amounts of annotated data. In our approach, we use GPT2 as our base model and extend its architecture with the addition of a new Entity-Gating layer that handles entity annotations along with a gating mechanism that handles information flow between the base model and the Entity-Gating layer. As such, our approach uses entity representations when they are available through both training and inference, without imposing any constraints to the model’s functionality. For our experiments, we compare the performance of GPT2, post and pre fine-tuning, with and without our changes in a series of relative tasks. Furthermore, since most coreference annotated datasets are either very small or hard to acquire, we use GUMBY (Gessler et al., 2020) as our fine-tuning dataset which is a model annotated corpus. In addition, by using noisy annotations we aim to show the resilience of our approach to noise and the universality of our framework. Our results highlight the effects of our framework in language modeling, modeling long-range dependencies, and in specific word types where our fine-tuned model achieves better performance than GPT2. Conclusions and Future Work In this paper we presented CoreLM, a modular Fine-Tuning framework for PLMs to exploit modelcreated coreference annotations in order to create better mention representations and an overall better LM. In our experiments we showcased a performance increase when evaluating in a zero-shot setting, compared to the similarly fine-tuned model, even when the fine-tuning corpus did not generalize well to the end tasks. Our analysis shows that coreference annotations play a significant role in both Fine-Tuning and in downstream task performance, with correct annotations leading to better performance when used. In addition, our work helps in adding a new frontier to Coreference Resolution through the effective use of coreference annotations in Language Modeling. In this paper we showcased the effects of coreference annotation even when the information is within the context window of the model. Using coreference annotations can further lead to the decrease of the required context window and boost approaches like Shortformer (Press et al., 2020), leading to better and more efficient LMs. In the future we aim to create a more efficient approach to LM through the use of both intralinguistic (Coreference) and extra-linguistic (KG) features. Undeniably, KGs provide a means for structured, high quality information that cannot be found in a single text. We believe that an information fusion from coreference annotation and graph nodes, along with short context window will not be computationally prohibitive and lead in better, information rich, LMs.",1
"Abstract Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the modelsÃ¢ÂÂ performance in terms of Accuracy in LAMBADA and ChildrenÃ¢ÂÂs Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. To make this architecture more efficient, a plethora of approaches have been introduced (Kitaev et al., 2020; Beltagy et al., 2020; Child et al., 2019, inter alia), aiming to lower the computational complexity without sacrificing performance. Tay et al. (2020) summarizes these approaches and categorizes them based on type of attention mechanism in their survey. Yet, even with linear complexity, physical resources will always limit the amount of contextual information that can be handled simultaneously. In terms of language, entities represent a natural way to tie words together through large pieces of discource. In NLP, Coreference Resolution is the task that aims to identify and group these entity mentions together, when they refer to the same real world entity (Stylianou and Vlahavas, 2021). Therefore, Coreference Resolution presents a natural way to link the context that we are currently handling with distant information, far outside the capabilities model architectures and current hardware resources. However, while this information is present in the text, it is usually not annotated and when annotated are very sparse and in small quantities making it extremely difficult to train large LMs (Kunz and Hardmeier, 2019). In this paper we present a framework to effectively use coreference annotations to further finetune large PLMs, increasing their performance far more than just by fine-tuning on the same data. By using large PLMs we take advantage of existing resources that are expensive to reproduce and come with a big environmental cost (Strubell et al., 2019). What is more, fine-tuning takes advantage of the massive amount of data that essentially initialize the model, making it possible to introduce new capabilities to models with small amounts of annotated data. In our approach, we use GPT2 as our base model and extend its architecture with the addition of a new Entity-Gating layer that handles entity annotations along with a gating mechanism that handles information flow between the base model and the Entity-Gating layer. As such, our approach uses entity representations when they are available through both training and inference, without imposing any constraints to the modelÃ¢ÂÂs functionality. For our experiments, we compare the performance of GPT2, post and pre fine-tuning, with and without our changes in a series of relative tasks. Furthermore, since most coreference annotated datasets are either very small or hard to acquire, we use GUMBY (Gessler et al., 2020) as our fine-tuning dataset which is a model annotated corpus. In addition, by using noisy annotations we aim to show the resilience of our approach to noise anduniversality of our framework. Our results highlight the effects of our framework in language modeling, modeling long-range dependencies, and in specific word types where our fine-tuned model achieves better performance than GPT2. Conclusions and Future Work In this paper we presented CoreLM, a modular Fine-Tuning framework for PLMs to exploit the Random Forest architecture and apply the approach with significant performance improvements. We also presented Pretrained Language Models, modeling text representations well, with a big performance penalty. We also presented Short Form Language Models, modeling text representations well, with a big performance penalty. We also presented Language Models, modeling text representations well, with a big performance penalty. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. ",0
"Abstract: In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients. Keywords: MeQA, AEMPS, Question Answering.  Introduction The leaflets for medicinal products for human use include their complete composition and instructions for their administration, use, and storage. Adverse effects, their interactions and contraindications are also specified. In addition, the text is written clearly, and they have to pass a readability test 4 so that the number of possible lexical, syntactic, or semantic errors is very low. These characteristics make leaflets a relatively easy resource to process using Natural Language Processing (NLP) techniques. This article presents the description of a project of the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ¿El ibuprofeno está contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ¿el Ibuprofeno acelera la pérdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called “Doctor Answers”, in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals. In section 2 the state of the art in QA is reviewed, in section 3 the main features of MeQA are described, section 4 describes the evaluation process, and finally section 5 contains the conclusions and future work. State of the Art QA systems can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, they suffer from misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (Fan et al., 2019), where questions require a long answer; community QA, ComQA (Abujabal et al., 2019), which makes use of data sets of pairs of questions and answers created by a certain community such as Quora or Stack Overflow. In addition, this task is also present in other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (Gordon et al., 2017), or Visual Question Answering, VQA (Antol et al. ., 2015), in which the objective is to answer questions about certain elements present in photographs, such as the color of certain elements, objects, etc. It is such an active field of AI that a new task called Embodied Question Answering (Das et al., 2018) has recently been created, which consists of generating an agent at a random location in a 3D environment and asking it a question (such as ¿What color is the car?). To answer, the agent must first intelligently navigate to explore the environment, collect the necessary visual information through first-person vision, and then answer the question (e.g. orange). This task combines different fields of AI such as language comprehension (LU), visual recognition, active perception, goal-based navigation, common sense reasoning, long-term memory, and conversion of language into actions. Within the biomedical domain, BioASQ (Tsatsaronis et al., 2015) has organized several QA tasks from structured data and free text, as well as the Medical Question Answering tasks organized in the TREC (Ben Abacha et al., 2017). The proposed system, MeQA, has been developed following the paradigm of QA systems based on information retrieval and, as we will see, it combines machine learning and deep learning techniques. It has the advantage of being easily expandable to other languages, as it does not need to annotate large amounts of question-answer pairs, something that QA systems based on complete neural architectures such as encoder-decoders (also known as seq2seq) do need (Sutskever, Vinyals and Quoc, 2014), or those who perform fine-tuning on pre-trained models such as BERT (Devlin et al., 2019). Conclusions and Future Work This paper describes a system developed at the AEMPS, called MeQA, which allows answering questions about medicines through the leaflet. Its architecture has been shown and explained in a general way, and also the modules that compose it. MeQA can be very useful for users, since there are a large number of web pages offering a service called ""Doctor Answers"", in which most of the questions deal with medicines for human use that can be answered through the leaflet. MeQA has been evaluated both automatically and manually. The automatic evaluation has been carried out in a general way as well as of each of the described modules, obtaining, in general, an F1 performance of 87%. MeQA combines machine learning and deep learning methods, and although it is not a semi-supervised system, it is a low-supervision system. We believe that this is precisely the great advantage of MeQA over other approaches based entirely on deep learning. MeQA hardly needs annotated data to work, only the module that predicts the sections in which the answer is likely to be found uses annotated data, the rest of the modules are unsupervised. As mentioned, the annotation of this information is very simple, and very fast, but not the complete annotation of the answer, which would be needed to develop systems based entirely on deep learning. In the future, improvements are expected to increase the performance of the system. In particular, because the leaflets are divided into sections, MeQA is able to go directly to the predicted sections. However, it would be useful to have a module that allows to go through the entire leaflet (without splitting) and obtain those sections. As explained above, MeQA performs worst on very complex questions. A possible solution to assess would consist of dividing these questions into fragments, analyzing and answering each one of them, and combining the answers into one.",1
"Abstract: In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients. Keywords: MeQA, AEMPS, Question Answering. Introduction The leaflets for medicinal products for human use include their complete composition and instructions for their administration, use, and storage. Adverse effects, their interactions and contraindications are also specified. In addition, the text is written clearly, and they have to pass a readability test 4 so that the number of possible lexical, syntactic, or semantic errors is very low. These characteristics make leaflets a relatively easy resource to process using Natural Language Processing (NLP) techniques. This article presents the description of a project of the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ÃÂ¿El ibuprofeno estÃÂ¡ contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ÃÂ¿el Ibuprofeno acelera la pÃÂ©rdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called Ã¢ÂÂDoctor AnswersÃ¢ÂÂ, in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals. In section 2 the state of the art in QA is reviewed, in section 3 the main features of MeQA are described, section 4 describes the evaluation process, and finally section 5 contains the conclusions and future work. State of the Art QA systems can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, theyfrom misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (which makes use of data sets of questions and answers created by a certain community such as Quora or Stack Overflow), where questions require a long answer; community QA, ComQA (which makes use of data sets of questions and answers created by a certain community such as Quora or Stack Overflow), which handle a collection of questions created by a certain community such as Quora or Stack Overflow. In addition, this task is also present in other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), or Visual Question Answering, VQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), which handle a collection of questions created by a certain community such as Quora or Stack Overflow, and then query those answers using those answers. This task is also present in other fields of AI such as image processing, called Image Question Answering, IQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), or Visual Question Answering, VQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), which handle a collection of questions created by a certain community such as Quora or Stack Overflow, and then query those answers using those answers. In addition, this task is also present in other fields of AI such as image processing, called Image Question Answering, IQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), or Visual Question Answering, VQA (which makes use of data sets of images created by a certain community such as Quora or Stack Overflow), which handle a collection of questions created by a certain community such as Quora or Stack Overflow, and then query those answers using those answers. Conclusions and Future Work This paper describes a project that belongs to the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ÃÂ¿El ibuprofeno estÃÂ¡ contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ÃÂ¿el Ibuprofeno acelera la pÃÂ©rdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called Ã¢ÂÂDoctor AnswersÃ¢ÂÂ, in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals.",0
"ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformerbased models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model · Transformer · Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a novel bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed method’s loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. This methodology has been well adapted for use in natural language processing domain for developing language models for word embedding. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, which usually cause curse of dimensionality during testing because of unseen words in training set. Distributed representation is proposed to overcome this limitation like this, for word sequence containing word not seen in training set but similar to words in a seen sentence, the target word sequence can still obtain high probability because of similar word vectors with that of sentence in the training set[Bengio et al., 2003]. Moreover, Mikolov et al[Mikolov et al., 2013] produced many works to distributed representation such as Skip-gram, negative sampling, all of which make word embedding in NLP an essential methodology and corner stone in many tasks including machine translation speech recognition. However, the above-mentioned word level representations cannot capture information arising from various polysemy of word use across linguistic context. To address this issue, Peters et al[Peters et al., 2018] trained bidirectional LSTM with a coupled language model objective for producing word vectors, which led to that the derived representations improved the state of the art in diverse language understanding problems. Radford et al[Radford et al., 2018] proposed a framework with transformer as base architecture for achieving long-range dependency, the ablation study shows that apparent score drop without using transformers. All of the results show that contextualized representation are beneficial in language modelling. Furthermore, Devlin et al[Devlin et al., 2018] devised bidirectional encoder representation transformers which show significant progress in eleven natural language processing tasks. Although deep contextualized language model achieved excellent performance, huge amount of parameters of these models incur extremely high cost of computing hardware and computation time. Another line of research tries to address this problem. Lan et al[Lan et al., 2019] proposed A Lite BERT architecture which has 18 times fewer parameters and 1.7 times faster tranning time than a traditional BERT architecture. While Sanh et al[Sanh et al., 2019] presented a method to pretrain a smaller model that can be finetuned for the downstream task, and achieved a 1.4 times fewer parameter with 1.6 times faster inference. However, none of these studies tried to investigate the effect of attention layer’s information leakage, for the purpose of information recombination, on the language model’s training speed and language representation efficacy for the purpose of decreasing computational resources utilization and training time. Conclusion In this work, we propose a attention based bidirectional language model named text denoised autoencoder, in order to save the training time for bidirectional language models as well as reduce the computation time of context language representations for unsupervised applications. We conduct both reranking test and the semantic textual similarity tasks to validate the proposed method in downstream applications, the result of which demonstrate that the proposed text denoised autoencoder is apparently faster than the conventional BERT based method in terms of producing contextualized representation. Moreover, the proposed method yields context representations which have more beneficial effect for downstream applications, demonstrating its improved encoding ability when compared with that of BERT and GPT.",1
"ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformer based models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model· Transformer ÃÂ· Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a deep bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed methodÃ¢ÂÂs loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, based on which statistical language model is trained most efficiently. Moreover, Mikolov et al[Mikolov et al., 2013] trained bidirectional language model with a large corpus and large parameters, which led to that the derived language model improved the state of the art in diverse language understanding problems. Recently, Mikolov et al[Mikolov et al., 2018] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. To summarize, the above-mentioned advantages and advantages of deep bidirectional language model are presented. The proposed objective is to predict the original ids of explicitly masked words from the input sentence, which lead to usage of-hardware equipment and long duration of training time. Moreover, the improved performance of the proposed objective is attributed to the fact that the explicitly masked words in the sentence are randomly masked. To further demonstrate its superiority, the derived objective achieves competitive performances to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks.",0
"ABSTRACT State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This rises the question whether different, possibly time-specific, representations of literal meanings may impact on the metaphor detection task. To the best of our knowledge, this is the first study which examines the metaphor detection task with a detailed explorative analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned to different state-of-the-art approaches. The results suggest that different word embeddings do impact on the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. However, results also suggest that temporal word embeddings may provide representations of words’ core meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented. Keywords Metaphor Detection · Temporal Word Embeddings · Word Embeddings Introduction Accounting for figurative language is one of the key challenges in Natural Language Processing (NLP). Many recent methods have proposed solutions to this problem based on machine learning methods [Recupero et al., 2019, Shutova, 2015, Leong et al., 2020]. Figurative language often contains metaphorical expressions which map one concept from a source domain to another concept in a target domain. For instance, in the sentence “The wheels of Stalin’s regime were well-oiled and already turning"", a political system (target concept) is viewed in terms of a mechanism (source concept) which can function, break, have wheels, etc. This association allows us to transfer knowledge from the domain of mechanical engineering to that of politics. Therefore, political systems are thought about in terms of mechanisms, leading to multiple metaphorical expressions. The phenomenon of source-target domain mapping was first introduced by George Lakoff known as Conceptual Metaphor Theory [Lakoff and Johnson, 1980]. Due to previously defined characteristics, the presence of metaphorical expression in text causes misinterpretation in the algorithms such as machine translation or sentiment analysis [Saif Mohammad, 2016]. For example, in machine translation, the text may be translated literally choosing inappropriate words, instead of those ones that capture the metaphorical meaning. Similarly, in the case of sentiment analysis, the polarity of the sentences can be misinterpreted and even inverted due to the presence of metaphors. Therefore, several studies have addressed the problem of detecting metaphors in text. The studies addressing the metaphor detection problem in the last decade [Shutova, 2015] usually exploit word embeddings to encode word meaning. Word embeddings are distributed representations, i.e., vectors, derived from the usage of words in large text corpora: words occurring in similar contexts have similar meanings and are eventually close to each other in a vector space [Almeida and Xexéo, 2019]. In metaphor detection, these vector-based representations can be used as signals to classify sentences or individual words as being used with a metaphorical meaning or not. The key intuition is to recognize that words are used in a context that is different from their usual context, i.e., that their usage in a specific context significantly differs from their usage in their most frequent contexts, which qualify their literal meanings. In the previous example, “wheels"" is collocated close to “Stalin"" and “regime"", which define a context different from the contexts in which it usually appears, i.e., in the domain of mechanical engineering. In other words, the classifier, usually implemented as a neural network, is expected to compare literal word meanings with word meanings in specific contexts to detect if in that context a metaphor is used. Most of the recent approaches have therefore combined non-contextual and contextual word embeddings to provide signals for this comparison (e.g., exploiting the concatenation of non-contextualized and contextualized word vectors) [Mao et al., 2019, Swarnkar and Singh, 2018]. For example, Mao et al. [2019], Gulordava and Baroni [2011], Tomas Mikolov and Dean [2013a] combine non-contextual GloVe embeddings [Jeffrey Pennington, 2014] with contextual ELMo embeddings [Peters M, 2018] within a BiLSTM neural network for sequence labelling. Glove embeddings account for literal word meanings, while ELMo embeddings account for contextual word meanings. The network tries to learn when the comparison between the two vectors indicates a metaphorical word usage. Word embedding techniques such as GloVe or Word2Vec [Tomas Mikolov and Dean, 2013a] are static embeddings that associate each word with one, context independent, representation. Contextual word embeddings, such as ELMo and BERT [Devlin et al., 2018], associate each word with one representation per sentence. For example, in the following two sentences: “Apple sells phones” and “I eat an apple”, contextual embeddings will represent “apple” differently in each sentence, while static embedding can not distinguish the semantic difference between the two references of “apple”. Usually, the latter represents a word with its core meaning, i.e., with the meaning in those contexts that are more frequent in the corpus used to train the embeddings (when more meanings compete as core meanings, words may be associated with vectors that mediate across contexts). An important linguistic phenomenon that is not accounted in static and contextual word embeddings is language evolution. While contextual word embeddings are expected to be sentence-specific and thus de-contextualized with respect to time, static word embeddings are expected to account for core meanings - and literal meanings in metaphor detection. Core meanings of course change over time and several approaches have been proposed to capture this changes [William L. Hamilton, 2018]. The trait of evolution of the meaning over time is also shared by metaphorical expressions which can be due to various reasons such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings derive this evolution to some extent [Smith and Höfler, 2015a, Aitchison, 2003]. This leads to the question whether different, possibly time-specific, representations of literal meanings impact the task of metaphor detection. In conclusion, if metaphor detection approaches tend to compare a sentence-specific and a literal meaning, we must be aware that literal meaning as accounted in static word embeddings 1) depends on the corpus and method used to train the embeddings and 2) evolve over time. To this end, the current empirical study focuses on analyzing the impact of different embeddings accounting for literal word meaning on the task of metaphor detection. In particular, we want to study the interactions between metaphor detection approaches and different word embeddings used to account for the literal meaning of words in these approaches. We want to dedicate a special attention to possible interactions between metaphor detection and time-specific (non–contextual) word representations used to account for literal meanings at different times. To the best of our knowledge, this is the first study seeking for interactions between time-dependent word representations and metaphor detection approaches. The empirical study discussed in this paper aims to make a first step into addressing the co-evolution of metaphors and language evolution that is known to be an important factor for language evolution itself [Smith and Höfler, 2015a, Aitchison, 2003]. The methodology adopted in our study consists in following protocol. First, we select a state-of-the-art Recurrent Neural Networks (RNN)-based model [Ge Gao and Zettlemoyer, 2018] for metaphor detection which uses static word embeddings to account for literal word meaning; the model performs metaphor detection as a sequence classification task where each word occurrence is labeled as either a metaphor usage or a literal usage. Second, we select three benchmark data sets widely used to evaluate the performance of metaphor detection approaches. Third, we feed the RNN-based model with literal meaning vectors obtained from different (non contextual) word embeddings spaces; these spaces differs for the corpora used to train them and, especially include temporal word embeddings computed for different decades and aligned with state-of-the-art alignment methods, such as Procrustes [Edouard Grave, 2018] and the Compass method (first referred to as Temporal Word Embeddings with a Compass [Di Carlo et al., 2019] - TWEC and later as Compass-aligned Distributional Embeddings [Bianchi et al., 2020]. The experimental results indicate that different word embeddings impact the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. These quantitative results are then explained with the help of a qualitative analysis of the predictions made by the models. During this analysis, some interesting recurring patterns were observed. Some patterns concern the interactions between literal meanings and domains of discourse. For example, in sentences containing correctly identified metaphors, the topics related to economics, politics, and emotions are the most recurring ones. Besides, verbs having a literal meaning characterized by physical connotations, often assume figurative meanings when used in the sentences related to the previously listed contexts. Some patterns concern indeed the interaction between time and language. Studying the predictions obtained with the embeddings of one specific time period, we noticed that none of the sentences belonging to the “news"" domain of the VUA dataset were correctly predicted. This could indicate that for that specific time period (1990 decade), words’ representations of that domain are biased towards their metaphorical meaning, and this would prevent the neural networks from correctly identifying the metaphors. Furthermore, if temporal word embeddings provided words’ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting them would correctly identify metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding space, e.g., obtained with GloVe [Jeffrey Pennington, 2014] and in a decade-specific temporal space, e.g., obtained from the CoHa1 corpus with Procrustes [Edouard Grave, 2018].This pattern is highlighted by the following sentence example: “The virus attacked Argonne National Laboratory outside Chicago starting at 11.54 pm EST Wednesday and throughout the night"". If we investigate the ten nearest neighbors of “virus"", in the temporal embedding we find words such as “infection, respiratory and organism"", while in the atemporal one there are for example “malware and spyware"", that diverge from the core literal meaning and are related to a modern connotation of the word. When exploiting the temporal word embedding, the model is able to understand that the “virus"" in this sentence is a computer one, and therefore that it is used in a metaphorical way along with the verb “attacked"". The paper is organized as follows: Section 2 discusses the related work about metaphor detection as well as temporal word embeddings. Section 3 discusses the methodology followed while Section 4 shows the experimental results of the paper. Finally, Section 5 concludes the paper. Discussion and Conclusion In the previous sections, we analyzed the results of our work. As an additional step to evaluate the impact of temporal embeddings on the evolution of language and the meaning of the words, we also performed our experiments using the words with the biggest semantic shift across time. The words’ list was retrieved from the SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection Competition2.We tried building a custom dataset by searching for metaphorical and literal sentences that would contain all these words so that it would be possible to perform metaphor detection tasks exploiting different temporal embeddings. Unfortunately, this final experiment was not feasible, due to the fact that there were not enough metaphorical statements in the competition dataset or in the state-of-the-art ones with the aforementioned words being used in a proper way. We performed metaphor detection as a sequence classification task in order to identify words with figurative meanings inside sentences. This approach allowed us to take advantage of different types of word representations, especially temporal ones, and evaluate their impact on metaphor detection. Looking at the numerous and diversified results, we can affirm that temporal word embeddings do generally improve the performance of the task of metaphor detection, however, their overall impact is rather limited. Besides, independently from the absolute performance, the interaction effect between the specificity of the embeddings (especially their temporal specificity) and metaphor detection is found in the experiments conducted in this study. In fact, these experiments verify that if the core meaning of the words of interest in a sentence is too similar to their figurative one in the word embedding, a metaphorical sentence could get misclassified as literal. Moreover, when temporal word embeddings provide words’ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting end up correctly identifying metaphors more easily. Furthermore, embeddings of words belonging to some language domains in specific time periods can be biased towards their metaphorical meaning, leading to words being used in metaphorical contexts way more than in literal ones. This would prevent neural models from correctly identifying the words as metaphors. We could verify these hypotheses by examining the examples analyzed in section 1. We explored the nearest neighbors of the target metaphorical words of sentences extracted from state of the art datasets, both in a static and atemporal word embedding such as GloVe [Jeffrey Pennington, 2014] and CoHa SGNS respectively. When exploiting the temporal word embedding, the model could correctly understand that the words (in our examples: “apple, virus, attack, hearts, and glow"") were used in a figurative way, thus correctly classifying them as metaphors. Furthermore, the entire sentences were correctly classified as metaphorical too, since the words’ core meanings were closer to their literal core ones. This also explains the fluctuations across slices, because corpora are never fully representative, and some contexts may be represented more than others in one specific decade. Finally, we performed qualitative analyses on the predictions made by sequence classification neural networks-based approaches and fine-tuned models for metaphor detection. The results suggest that topics related to economics, politics, and emotions are the most recurring ones in sentences containing correctly identified metaphors and that verbs having a literal meaning characterized by physical connotations, often assume a metaphorical meaning when used in sentences related to the aforementioned contexts.",1
"ABSTRACT State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This rises the question whether different, possibly time-specific, representations of literal meanings may impact on the metaphor detection task. To the best of our knowledge, this is the first study which examines the metaphor detection task with a detailed explorative analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned to different state-of-the-art approaches. The results suggest that different word embeddings do impact on the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. However, results also suggest that temporal word embeddings may provide representations of wordsÃ¢ÂÂ core meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented. Keywords Metaphor Detection ÃÂ· Temporal Word Embeddings ÃÂ· Word Embeddings Introduction Accounting for figurative language is one of the key challenges in Natural Language Processing (NLP). Many recent methods have proposed solutions to this problem based on machine learning methods [Recupero et al., 2019, Shutova, 2015, Leong et al., 2020]. Figurative language often contains metaphorical expressions which map one concept from a source domain to another concept in a target domain. For instance, in the sentence Ã¢ÂÂThe wheels of StalinÃ¢ÂÂs regime were well-oiled and already turning"", a political system (target concept) is viewed in terms of a mechanism (source concept) which can function, break, have wheels, etc. This association allows us to transfer knowledge from the domain of mechanical engineering to that of politics. Therefore, political systems are thought about in terms of mechanisms, leading to multiple metaphorical expressions. The phenomenon of source-target domain mapping was first introduced by George Lakoff known as Conceptual Metaphor Theory [Lakoff and Johnson, 1980]. Due to previously defined characteristics, the presence of metaphorical expression in text causes misinterpretation in the algorithms such as machine translation or sentiment analysis [Saif Mohammad, 2016]. For example, in machine translation, the text may be translated literally choosing inappropriate words, instead of those ones that capture the metaphorical meaning. Similarly, in the case of sentiment analysis, the polarity of the sentences can be misinterpreted and even inverted due to the presence of metaphors. Therefore, several studies have addressed the problem of detecting metaphors in text. The studies addressing the metaphor detection problem in the last decade [Shutova, 2015] usually exploit word embeddings to encode word meaning. Word embeddings are distributed representations, i.e., vectors, derived from the usage of words in large text corpora: words occurring in similar contexts have similar meanings and are eventually close to each other in a vector space [Almeida and XexeÃÂo, 2019]. In metaphor detection, these vector-based representations can be used as signals to classify sentences or individual words as being used with a metaphorical meaning or not. The key intuition is to recognize that words are used in a context that is different from their usual context, i.e., that their usage in a specific context significantly differs from their usage in their most frequent contexts, which qualify their literal meanings. In the previous example, Ã¢ÂÂwheels"" is collocated close to Ã¢ÂÂStalin"" and Ã¢ÂÂregime"", which define a context different from the contexts in which it usually appears, i.e., in the domain of mechanical engineering. In other words, the classifier, usually implemented as a neural network, is expected to compare literal word meanings with word meanings in specific contexts to detect if in that context a metaphor is used. Most of the recent approachestherefore combined non-contextual and contextual word embeddings to provide signals for this comparison (e.g., exploiting the concatenation of non-contextualized and contextualized word vectors) [Mao et al., 2019, Swarnkar and Singh, 2018]. For example, Mao et al. [2019], Gulordava and Baroni [2011], Tomas Mikolov and Dean [2013a] combine non-contextual GloVe embeddings [Jeffrey Pennington, 2014] with contextual ELMo embeddings [Peters M, 2018] within a BiLSTM neural network for sequence labelling. Glove embeddings account for literal word meanings, while ELMo embeddings account for contextual word meanings. The network tries to learn when the comparison between the two vectors indicates a metaphorical word usage. Word embedding techniques such as GloVe or Word2Vec [Tomas Mikolov and Dean, 2013a] are static embeddings that associate each word with one, context independent, representation. Contextual word embeddings, such as ELMo and BERT [Devlin et al., 2018], associate each word with one representation per sentence. For example, in the following two sentences: Ã¢ÂÂApple sells phonesÃ¢ÂÂ and Ã¢ÂÂI eat an appleÃ¢ÂÂ, contextual embeddings will represent Ã¢ÂÂappleÃ¢ÂÂ differently in each sentence, while static embedding can not distinguish the semantic difference between the two references of Ã¢ÂÂappleÃ¢ÂÂ. Usually, the latter represents a word with its core meaning, i.e., with the meaning in those contexts that are more frequent in the corpus used to train the embeddings (when more meanings compete as core meanings, words may be associated with vectors that mediate across contexts). An important linguistic phenomenon that is not accounted in static and contextual word embeddings is language evolution. While contextual word embeddings are expected to be sentence-specific and thus de-contextualized with respect to time, static word embeddings are expected to account for core meanings - and literal meanings in metaphor detection. Core meanings of course change over time and several approaches have been proposed to capture this changes [William L. Hamilton, 2018]. The trait of evolution of the meaning over time is also shared by metaphorical expressions which can be due to various reasons such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings derive this evolution to some extent [Smith and HoÃÂfler, 2015a, Aitchison, 2003]. This leads to the question whether different, possibly time-specific, representations of literal meanings impact the task of metaphor detection. In conclusion, if metaphor detection approaches tend to compare a sentence-specific and a literal meaning, we must be aware that literal meaning as accounted in static word embeddings 1) depends on the corpus and method used to train the embeddings and 2) evolve over time. To this end, the current empirical study focuses on analyzing the impact of different embeddings accounting for literal word meaning on the task of metaphor detection. In particular, we want to study the interactions between metaphor detection approaches and different word embeddings used to account for the literal meaning of words in these approaches. We want to dedicate a special attention to possible interactions between metaphor detection and time-specific (nonÃ¢ÂÂcontextual) word representations used to account for literal meanings at different times. To the best of our knowledge, this is the first study seeking for interactions between time-dependent word representations and metaphor detection approaches. The empirical study discussed in this paper aims to make a first step into addressing the co-evolution of metaphors and language evolution that is known to be an important factor for language evolution itself [Smith and HoÃÂfler, 2015a, Aitchison, 2003]. The methodology adopted in our study consists in following protocol. First, we select a state-of-the-art Recurrent Neural Networks (RNN)-based model [Ge Gao and Zettlemoyer, 2018] for metaphor detection which uses static word embeddings to account for literal word meaning; the model performs metaphor detection as a sequence classification task where each word occurrence is labeled as either a metaphor usage or a literal usage. Second, we select three benchmark data sets widely used to evaluate the performance of metaphor detection approaches. Third, we feed the RNN-based model with literal meaning vectors obtained from different (non contextual) word embeddings spaces; these spaces differs for the corpora used to train them and, especially includeword embeddings computed for different decades and aligned with state-of-the-art alignment methods, such as Procrustes [Edouard Grave, 2018] and the Compass method (first referred to as Temporal Word Embeddings with a Compass [Di Carlo et al., 2019] - TWEC and later as Compass-aligned Distributional Embeddings [Bianchi et al., 2020]. The experimental results indicate that different word embeddings impact the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. These quantitative results are then explained with the help of a qualitative analysis of the predictions made by the models. During this analysis, some interesting recurring patterns were observed. Some patterns concern the interactions between literal meanings and domains of discourse. For example, in sentences containing correctly identified metaphors, the topics related to economics, politics, and emotions are the most recurring ones. Besides, verbs having a literal meaning characterized by physical connotations, often assume figurative meanings when used in the sentences related to the previously listed contexts. Some patterns concern indeed the interaction between time and language. Studying the predictions obtained with the embeddings of one specific time period, we noticed that none of the sentences belonging to the Ã¢ÂÂnews"" domain of the VUA dataset were correctly predicted. This could indicate that for that specific time period (1990 decade), wordsÃ¢ÂÂ representations of that domain are biased towards their metaphorical meaning, and this would prevent the neural networks from correctly identifying the metaphors. Furthermore, if temporal word embeddings provided wordsÃ¢ÂÂ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting them would correctly identify metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding space, e.g., obtained with GloVe [Jeffrey Pennington, 2014] and in a decade-specific temporal space, e.g., obtained from the CoHa1 corpus with Procrustes [Edouard Grave, 2018].This pattern is highlighted by the following sentence example: Ã¢ÂÂThe virus attacked Argonne National Laboratory outside Chicago starting at 11.54 pm EST Wednesday and throughout the night"". If we investigate the ten nearest neighbors of Ã¢ÂÂvirus"", in the temporal embedding we find words such as Ã¢ÂÂinfection, respiratory and organism"", while in the atemporal one there are for example Ã¢ÂÂmalware and spyware"", that diverge from the core literal meaning and are related to a modern connotation of the word. When exploiting the temporal word embedding, the model is able to understand that the Ã¢ÂÂvirus"" in this sentence is a computer one, and therefore that it is used in a metaphorical way along with the verb Ã¢ÂÂattacked"". The paper is organized as follows: Section 2 discusses the related work about metaphor detection as well as temporal word embeddings. Section 3 discusses the methodology followed while Section 4 shows the experimental results of the paper. Finally, Section 5 concludes the paper. Discussion and Conclusion In the previous sections, we analyzed the results of our work. As an additional step to evaluate the impact of temporal embeddings on the evolution of language and the meaning of the words, we also performed our experiments using the words with the biggest semantic shift across time. The wordsÃ¢ÂÂ list was retrieved from the SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection Competition2.We tried building a custom dataset by searching for metaphorical and literal sentences that would contain all these words so that it would be possible to perform metaphor detection tasks exploiting different temporal embeddings. Unfortunately, this final experiment was not feasible, due to the fact that there were not enough metaphorical statements in the competition dataset or in the state-of-the-art ones with the aforementioned words being used in a proper way. We performed metaphor detection as a sequence classification task in order to identify words with figurative meanings inside sentences. This approach allowed us to take advantage of different types of word representations, especially temporal ones, and evaluate their impact on metaphor detection. Looking at the numerous and diversified results, we can affirm that temporal word embeddings do generally improve the performance of the task of metaphor detection, however, their overall impact is rather limited. Besides, independently from the absolute performance, the interaction effect between the specificity of the embeddings (especially their temporal specificity) and metaphor detection is found in the experiments conducted in this study. In fact, these experiments verify that if the core meaning of the words ofin a sentence is too similar to their figurative one in the word embedding, a metaphorical sentence could get misclassified as literal. Moreover, when temporal word embeddings provide wordsÃÂ¢ÃÂÃÂ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting end up correctly identifying metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding space, e.g., obtained with GloVe [Jeffrey Pennington, 2014] and in a decade-specific temporal space, e.g., obtained from the CoHa1 corpus with Procrustes [Edouard Grave, 2018].This pattern is highlighted by the following sentence example: Ã¢ÂÂThe virus attacked Argonne National Laboratory outside Chicago starting at 11.54 pm EST Wednesday and throughout the night"". If we investigate the ten nearest neighbors of Ã¢ÂÂvirus"", in the temporal embedding we find words such as Ã¢ÂÂinfection, respiratory and organism"", while in the atemporal one there are for example Ã¢ÂÂmalware and spyware"", that diverge from the core literal meaning and are related to a modern connotation of the word. When exploiting the temporal word embedding, the model is able to understand that the Ã¢ÂÂvirus"" in this sentence is a computer one, and therefore that it is used in a metaphorical way along with the verb Ã¢ÂÂattacked"". The paper is organized as follows: Section 2 discusses the related work about metaphor detection as well as temporal word embeddings. Section 3 discusses the methodology followed while Section 4 shows the experimental results of the paper. Finally, Section 5 concludes the paper. Discussion and Conclusion In the previous sections, we analyzed the results of our work.",0
"Abstract— Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media. Keywords— Sexism classification, social media, natural language processing, neural networks, machine learning, BERT, transfer learning. INTRODUCTION Social media has completely altered the way communities are formed and utilised, which provides incredible advantages while also having severe repercussions. Due to the ‘online disinhibition effect’ (Suler, 2004, p. 1), when users are provided with an anonymised and accessible platform, they engage in behaviours they would not partake in when interacting face-to-face (Wright et al., 2019). A significant example of this is the hate speech produced and propagated through social media platforms. Hate speech is defined as language which is ‘insulting, degrading, defaming, negatively stereotyping, or inciting hatred, discrimination or violence against people in virtue of their race, ethnicity, nationality, religion, sexual orientation, disability, gender identity’ (Brown, 2017, p. 1). The prevalence of hate speech in everyday life has increased in correlation with social media usage, particularly during the COVID-19 pandemic, with internet usage levels having increased between 50% to 70% as of early April 2020 (UN Women, 2020). Online hate speech, especially targeted discrimination, has been associated with an increase in hate crimes offline (Hatzipanagos, 2018; Laub, 2019; Relia et al., 2019); therefore, the ability to successfully tackle this issue within the virtual space itself is vital. Sexism refers to a sub-classification of hate speech where the targeted people are typically female. Women are more likely to report having experienced sexual harassment online (16% vs. 5%) or being cyber-stalked (13% vs. 9%) compared to men (Vogels, 2021); with 1 in 10 women reporting having experienced cyber harassment since the age of 15 in the European Union (UN Women, 2020). Women and girls are specifically seen to face a digital gender divide1, especially with the COVID-19 pandemic being the first major one in the age of social media2. While social media platforms like Twitter do ban hate speech3, these policies are enforced primarily through manual methods which cannot scale up to counteract the data being produced (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (Rodríguez-Sánchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the model’s performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest of this paper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; Rodríguez-Sánchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018). An alternative architecture was proposed by Zhang and Luo (2019) with two deep neural network models to tackle hate speech classification on Twitter datasets. These models consist of CNN and Gated Recurrent Unit (GRU) architectures with the results outperforming the best methods at the time. More recently, the introduction of BERT has led to new state-of-the-art performances across a range of natural language processing tasks, including text classification (Devlin et al., 2018). BERT is a multi-layer bidirectional transformer encoder which notably uses bidirectional self- attention to learn contextual information between words and sub-words within a text (Alammar, 2018). BERT has been pre-trained using BooksCorpus (800M words) and English Wikipedia (2500M words) on masked language modelling and next sentence prediction (Devlin et al., 2018). This causes the embeddings taken from the model to contain useful contextual information that can be fine-tuned for specific tasks. Rodríguez-Sánchez et al. (2020) show BERT being used to give the best performance on the task of identifying sexist content through fine tuning pre-trained mBERT-Base parameters with a fully connected layer. Multi-label sexism classification was first seen in a paper by Parikh et al. (2020) where a BERT based neural architecture was used along with distributional and word level embeddings. Samghabadi et al. (2020) also show BERT being used without fine-tuning to identify aggression and misogyny in English, Hindi, and Bengali tweets with positive results from the model. Limited research has been conducted on the automatic classification of subtle expressions of sexism encompassing a broad range of categories, compared to the sole use of profanities or explicit hatred against women. Rodríguez- Sánchez et al. (2020) collected instances of various types of sexism, ranging from subtle inequality to explicit violence to create a dataset to then be used in an automatic classification task. The range of expressions collated is similar to the dataset used in this paper. An important point to note is that hate speech and sexism is defined differently across these papers, with offensive language often considered to be equivalent (Davidson et al., 2017). Another challenge for automatic sexism classification is the lack of an established benchmark dataset. Detecting sexist expressions is a challenge for human coders as well, with racist or homophobic tweets often considered to be hate speech while sexist or derogatory terms are found to be offensive as opposed to hateful (Waseem and Hovy, 2016; Davidson et al., 2017). To tackle these issues of subtlety and context, the DistilBERT, which is a lightweight version of BERT with 40% fewer parameters (Sanh et al., 2019), and BERT models used in this paper are fine-tuned with additional layers to allow them to learn contextual embeddings and perform effectively on the sexism classification tasks. CONCLUSION AND FUTURE WORK The increased use of social media has enabled hate speech, including sexist speech, to easily propagate and affect people globally. With online hate speech linked to offline violence, it is essential to successfully classify speech as hateful through automatic methods. This paper presented a variety of deep neural networks using BERT and DistilBERT to differentiate sexist tweets and gabs from non-sexist ones, as well as further classify sexist text into types of sexism using the EXIST dataset. The best model for the binary classification task used BERT along with a CNN architecture using filter sizes of 4, 6, and 8 to achieve an accuracy of 76.2% while the best accuracy from the competition was 77%. The same model along with data augmentation achieved the best performance on the multi-class classification task with an F1 score of 51.9% which was lower than the best F1 score (56%) from the competition. Due to the subjectivity involved in annotating sexist text as well as the complexity of natural language in tweets and gabs, this task proved to be challenging for the models to achieve ideal results on. Categories which contain explicit hatred like ‘sexual-violence’ were seen to be labelled more accurately than more subtle instances of sexism such as those seen under ‘ideological-inequality’. Profanities are often used on social media platforms without necessarily insinuating sexist speech, such as in casual conversation or song lyrics and hence were not very helpful features for classification. Similarly, speech can be sexist without using any explicit and specific words to indicate this. Due to the varying perceptions of sexism by humans, some labels within the dataset were found to have been potentially mislabelled, which creates further challenges for using the dataset to train models. The type of texts (tweets versus gabs) was not seen to affect the BERT + MultiCNN model’s performance significantly for either task. This could be attributed to the use of the convolutional layer with different filter sizes with max pooling. The model was seen to perform better for text of length 500+ characters and <100 characters, although the impact of length would need to be further examined to confirm this by creating more balanced datasets for this purpose. Another avenue for further exploration could be using additional features such as the gender or ethnicity of authors as stated by Waseem and Hovy (2016), although this information may be challenging to obtain. The models trained on the EXIST dataset could also be tested on a different sexism dataset to observe the generalisability of the models across different annotated data. Finally, a benchmark annotated dataset for sexism would also allow for better development and comparison of models.",1
"Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media. KeywordsÃ¢ÂÂ Sexism classification, social media, natural language processing, neural networks, machine learning, BERT, transfer learning. INTRODUCTION Social media has completely altered the way communities are formed and utilised, which provides incredible advantages while also having severe repercussions. Due to the Ã¢ÂÂonline disinhibition effectÃ¢ÂÂ (Suler, 2004, p. 1), when users are provided with an anonymised and accessible platform, they engage in behaviours they would not partake in when interacting face-to-face (Wright et al., 2019). A significant example of this is the hate speech produced and propagated through social media platforms. Hate speech is defined as language which is Ã¢ÂÂinsulting, degrading, defaming, negatively stereotyping, or inciting hatred, discrimination or violence against people in virtue of their race, ethnicity, nationality, religion, sexual orientation, disability, gender identityÃ¢ÂÂ (Brown, 2017, p. 1). The prevalence of hate speech in everyday life has increased in correlation with social media usage, particularly during the COVID-19 pandemic, with internet usage levels having increased between 50% to 70% as of early April 2020 (UN Women, 2020). Online hate speech, especially targeted discrimination, has been associated with an increase in hate crimes offline (Hatzipanagos, 2018; Laub, 2019; Relia et al., 2019); therefore, the ability to successfully tackle this issue within the virtual space itself is vital. Sexism refers to a sub-classification of hate speech where the targeted people are typically female. Women are more likely to report having experienced sexual harassment online (16% vs. 5%) or being cyber-stalked (13% vs. 9%) compared to men (Vogels, 2021); with 1 in 10 women reporting having experienced cyber harassment since the age of 15 in the European Union (UN Women, 2020). Women and girls are specifically seen to face a digital gender divide1, especially with the COVID-19 pandemic being the first major one in the age of social media2. While social media platforms like Twitter do ban hate speech3, these policies are enforced primarily through manual methods which cannot scale up to counteract the data being produced (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (RodriÃÂguez-SaÃÂnchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the modelÃ¢ÂÂs performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest ofpaper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; RodriÃÂguez-SaÃÂnchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018). An alternative architecture was proposed by Zhang and Luo (2019) with two deep neural network models to tackle hate speech classification on Twitter datasets. These models consist of CNN and Gated Recurrent Unit (GRU) architectures with the results outperforming the best methods at the time. More recently, the introduction of BERT has led to new state-of-the-art performances across a range of natural language processing tasks, including text classification (Devlin et al., 2018). BERT is a multi-layer bidirectional transformer encoder which notably uses bidirectional self- attention to learn contextual information between words and sub-words within a text (Alammar, 2018). BERT has been pre-trained using BooksCorpus (800M words) and English Wikipedia (2500M words) on masked language modelling and next sentence prediction (Devlin et al., 2018). This causes the embeddings taken from the model to contain useful contextual information that can be fine-tuned for specific tasks. RodriÃÂguez-SaÃÂnchez et al. (2020) show BERT being used to give the best performance on the task of identifying sexist content through fine tuning pre-trained mBERT-Base parameters with a fully connected layer. Multi-label sexism classification was first seen in a paper by Parikh et al. (2020) where a BERT based neural architecture was used along with distributional and word level embeddings. Samghabadi et al. (2020) also show BERT being used without fine-tuning to identify aggression and misogyny in English, Hindi, and Bengali tweets with positive results from the model. Limited research has been conducted on the automatic classification of subtle expressions of sexism encompassing a broad range of categories, compared to the sole use of profanities or explicit hatred against women. RodriÃÂguez- SaÃÂnchez et al. (2020) collected instances of various types of sexism, ranging from subtle inequality to explicit violence to create a dataset to then be used in an automatic classification task. The range of expressions collated is similar to the dataset used in this paper. An important point to note is that hate speech and sexism is defined differently across these papers, with offensive language often considered to be equivalent (Davidson et al., 2017). Another challenge forsexism classification is the lack of an established benchmark dataset. Detecting sexist expressions is a challenge for human coders as well, with racist or homophobic tweets often considered to be hate speech while sexist or derogatory terms are found to be offensive as opposed to hateful (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (RodriÃÂguez-SaÃÂnchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the modelÃ¢ÂÂs performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest of this paper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; RodriÃÂguez-SaÃÂnchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018).",0
"Abstract Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based model with a Fusion of Adapter layers which leverages knowledge from more simple sentiment analysis tasks to improve the emotion detection task on large scale dataset, such as CMU-MOSEI, using the textual modality only. Results show that our proposed method is competitive with other approaches. We obtained state-of-the-art results for emotion recognition on CMU-MOSEI even while using only the textual modality. Introduction Sentiment analysis is a subject that has long interested multiple researchers in the domain of natural language understanding. It is a task which aims to identify sentiment polarity for a given signal, which can be of the audio, visual or textual modality. Emotion recognition is a related task which consists of assigning more fine-grained labels, such as anger, joy, sadness, etc. This work focuses on analyzing textual inputs. The ability to recognize the sentiment or emotion behind a given sentence or paragraph can lead to multiple applications, such as empathetic dialogue agents and tools to assess the mental state of a patient. While sentiment analysis in the form of assigning polarities (positive, negative, and sometimes neutral) to text data is a task that is often studied and for which adequate results have already been obtained for multiple datasets, identifying finer-grained labels such as specific emotions is still a challenge. In addition to the task complexity, in most datasets available for this task, some emotions are much less represented than others, making the training data unbalanced. To address this issue, the model proposed in this work combines knowledge from less complex tasks and is trained using methods to counteract class imbalance. It is based on a Transformer-based model with a Fusion of Adapter layers to leverage knowledge from the more simple sentiment analysis task. The results obtained are competitive with state-of-the-art multi-modal models on the CMU-MOSEI dataset (Bagher Zadeh et al., 2018), while only utilizing the textual modality. Our main contribution can be formulated as: • We designed a method that capitalizes on both pretrained Transformer language models and knowledge from complementary tasks to improve on the emotion recognition task, whilst using Adapter layers that require less training parameters than the conventional fine-tuning approach and taking into account class imbalance. Prior Works and Background There are multiple approaches that have been used to solve text-based sentiment analysis and emotion detection tasks, namely rule-based and machine learning approaches. Rule-based approaches consist of creating grammatical and logical rules to assign emotions and use lexicons to assign emotions or polarities to words. Previous works using this approach include the ones of Udochukwu and He (2015), Tan et al. (2015) and Seal et al. (2019). These methods are limited by the size and contents of the lexicon used and by the ambiguity of some keywords. Most recent methods are based on the machine learning approach were the network is trained to learn the relationships between words and emotions. Methods such as those proposed by Abdul- Mageed and Ungar (2017), Tang et al. (2015) and Ma et al. (2019) use recurrent neural networks to solve sentiment analysis tasks to break down sentences and understand the relationship between the succession of words and sentiments or emotions. Since the release of pretrained models, recent works (Park et al., 2019; Acheampong et al., 2021) have been focused on fine-tuning transformer models, which have consistently outperformed previous methods thanks to the multi-head attention applied on words. To improve previous textual emotion recognition methods, we believe that in addition to transfer learning, multi-task learning and class imbalance should be considered. 2.1 Transfer Learning Transfer learning is a method where the weights of a model trained on a task are used as starting point to train a model for another task. The use of transfer learning with pretrained models has been, for the past few years, the way to obtain state-of-the-art results for multiple natural language understanding (NLU) tasks. Transformer-based pretrained models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), etc. have been dominating the field over previously used methods. 2.2 Multi-Task Learning Multi-task learning is used to train one model to solve multiple tasks instead of fine-tuning separate models. Multiple approaches have been used to solve multi-task learning problems. Liu et al. (2019a) proposed a Multi-Task Deep Neural Network (MT-DNN) with a shared transformer encoder and task-specific heads. Clark et al. (2019) and Liu et al. (2019b) presented a new training procedure based on knowledge distillation to improve the performances of the MT-DNN. These approaches allow the model to learn a shared representation between all tasks. Houlsby et al. (2019) introduced a new model architecture using task-specific adapter layers and keeping the weights of the pretrained encoder frozen. This method, while preventing task interference and catastrophic forgetting, does not allow to transfer knowledge between tasks. To counter this weakness, Pfeiffer et al. (2020a) proposed AdapterFusion, a way to combine knowledge from multiple adapters. 2.3 Class Imbalance Class imbalance is a challenge in resolving many artificial intelligence tasks. It occurs when one or multiple classes make up significantly less samples of the data than the majority class or classes, often leading to a poor predictive performance for those minority classes. Classic approaches to this problem include re-sampling minority class samples or weighting the loss function according to class frequency. In the field of computer vision, Lin et al. (2018) proposed a modified version of the cross-entropy loss called the focal loss to handle imbalance. Conclusion The model presented in this work surpasses state-of-the-art results for emotion recognition on CMU- MOSEI even while using only the textual modality. There is still improvement needed for the rarer emotions in the dataset, but at of the time of producing this article, the results presented are substantially stronger than other contributions in terms of F1-scores. Due to the lack of large-scale datasets for emotion detection in text, testing the model on purely textual data will have to be done in further studies as the data will become available.",1
"Abstract Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based model with a Fusion of Adapter layers which leverages knowledge from more simple sentiment analysis tasks to improve the emotion detection task on large scale dataset, such as CMU-MOSEI, using the textual modality only. Results show that our proposed method is competitive with other approaches. We obtained state-of-the-art results for emotion recognition on CMU-MOSEI even while using only the textual modality. Introduction Sentiment analysis is a subject that has long interested multiple researchers in the domain of natural language understanding. It is a task which aims to identify sentiment polarity for a given signal, which can be of the audio, visual or textual modality. Emotion recognition is a related task which consists of assigning more fine-grained labels, such as anger, joy, sadness, etc. This work focuses on analyzing textual inputs. The ability to recognize the sentiment or emotion behind a given sentence or paragraph can lead to multiple applications, such as empathetic dialogue agents and tools to assess the mental state of a patient. While sentiment analysis in the form of assigning polarities (positive, negative, and sometimes neutral) to text data is a task that is often studied and for which adequate results have already been obtained for multiple datasets, identifying finer-grained labels such as specific emotions is still a challenge. In addition to the task complexity, in most datasets available for this task, some emotions are much less represented than others, making the training data unbalanced. To address this issue, the model proposed in this work combines knowledge from less complex tasks and is trained using methods to counteract class imbalance. It is based on a Transformer-based model with a Fusion of Adapter layers to leverage knowledge from the more simple sentiment analysis task. The results obtained are competitive with state-of-the-art multi-modal models on the CMU-MOSEI dataset (Bagher Zadeh et al., 2018), while only utilizing the textual modality. Our main contribution can be formulated as: Ã¢ÂÂ¢ We designed a method that capitalizes on both pretrained Transformer language models and knowledge from complementary tasks to improve on the emotion recognition task, whilst using Adapter layers that require less training parameters than the conventional fine-tuning approach and taking into account class imbalance. Prior Works and Background There are multiple approaches that have been used to solve text-based sentiment analysis and emotion detection tasks, namely rule-based and machine learning approaches. Rule-based approaches consist of creating grammatical and logical rules to assign emotions and use lexicons to assign emotions or polarities to words. Previous works using this approach include the ones of Udochukwu and He (2015), Tan et al. (2015) and Seal et al. (2019). These methods are limited by the size and contents of the lexicon used and by the ambiguity of some keywords. Most recent methods are based on the machine learning approach were the network is trained to learn the relationships between words and emotions. Methods such as those proposed by Abdul- Mageed and Ungar (2017), Tang et al. (2015) and Ma et al. (2019) use recurrent neural networks to solve sentiment analysis tasks to break down sentences and understand the relationship between the succession of words and sentiments or emotions. Since the release of pretrained models, recent works (Park et al., 2019; Acheampong et al., 2021) have been focused on fine-tuning transformer models, which have consistently outperformed previous methods thanks to the multi-head attention applied on words. To improve previous textual emotion recognition methods, we believe that in addition to transfer learning, multi-task learning and class imbalance should be considered. 2.1 Transfer Learning Transfer learning is a method where the weights of a model trained on a task are used as starting point to train a model for another task. The use of transfer learning with pretrained models has been, for the past few years, the way to obtain state-of-the-art results for multiple natural language understanding (NLU) tasks. Transformer-based pretrained models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), etc. have been dominating the field over previously used methods. 2.2 Multi-Task Learning Multi-taskis used to train one model to solve multiple tasks instead of fine-tuning separate models. Multiple approaches have been used to solve multi-task learning problems. Liu et al. (2019a) proposed a Multi-Task Deep Neural Network (MT-DNN) with a shared transformer encoder and task-specific heads. Clark et al. (2019) and Liu et al. (2019b) presented a new training procedure based on knowledge distillation to improve the performances of the MT-DNN. These approaches are competitive with state-of-the-art multi-modal networks on the CMU-MOSEI dataset (Bagher Zadeh et al., 2019; Acheampong et al., 2019c), while only utilizing the textual modality. Our main contribution can be formulated as: Ã¢ÂÂ¢ We designed a method that capitalizes on both knowledge distillation and multi-task learning to improve the performances of the MT-DNN. Prior Works and Background There are multiple approaches that have been used to solve text-based sentiment analysis and emotion detection tasks, namely rule-based and machine learning approaches. Rule-based approaches consist of creating grammatical and logical rules to assign emotions and use lexicons to assign emotions or polarities to words. Previous works using this approach include the ones of Udochukwu and He (2015), Tan et al. (2015) and Seal et al. (2019). These methods are limited by the size and contents of the lexicon used and by the ambiguity of some keywords. Most recent methods are based on the machine learning approach were the network is trained to learn the relationships between words and emotions. Methods such as those proposed by Liu et al. (2019) and Liu et al. (2019b) use recurrent neural networks to solve sentiment analysis tasks to break down sentences and understand the relationship between the succession of words and sentiments or emotions. Since the release of pretrained models, recent works (Park et al., 2019; Acheampong et al., 2019c) have been focused on fine-tuning previously used methods to break down sentences and understand the relationship between the succession of words and sentiments or emotions.",0
"Abstract Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically de- tect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is re- ceived by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo. Introduction We present an approach for identifying the dialect of a speaker automatically solely based on text and on audio and text together. We compare the uni- modal approach to the bimodal one. There are no previous dialect identification approaches for Finnish. There are several situations were a dialect identification method can be of use. For example, if we have ASR models fine tuned for specific di- alects, the dialect identification from audio could be used as a preprocessing step. The model could also be used to label recorded materials automatically in order to create archival metadata. In order to make our contribution useful for others, we have released our code, models and processed data openly on GitHub1 and Zenodo2. Finnish is a large Uralic language that is one of the official languages of Finland, and is used essen- tially at all levels of the modern society. There are approximately five million Finnish speakers. The language belongs to the Finnic branch of the Uralic language family, and is very closely related to Kare- lian, Meänkieli and Kveeni, and is also closely re- lated to the Estonian language. It is more distantly related to numerous Uralic languages spoken in Russia. The history of written Finnish starts in the 16th century. Current orthography is connected to this written tradition, which developed into the current form in the late 19th century with a conscious plan- ning and systematic development of the lexicon. After this, the changes have been minor (Häkkinen, 1994, 16), and also impacted lexicon, especially what it comes to the development of the vocabu- lary of the modern society and traditional agrarian terminology becoming less known. The Finnish spoken language, however, is still largely based on Finnish dialects. In the 20th cen- tury some of the strongest dialectal features have been disappearing, but there are still clearly dis- tinguishable spoken vernacular varieties that are regionally marked. It has been shown that instead of clear disappearance of dialects there are vari- ous features that are spreading, but not at uniform rate, and reportedly younger speakers use the are- ally marked features less than the older speakers (Lappalainen, 2001, 92). Finnish vernaculars also represent historically rather different Finnic vari- eties, with major split between Eastern and Western dialects. There are, however, also dialect continu- ums and traditionally found gradual differentiation from region to region. Many of the changes have been lexical due to technical innovations and modernization of the society: orthographic spelling conventions have largely remained the same. Spoken Finnish, on the other hand, traditionally represents an areally di- vided dialect continuum, with several sharp bound- aries, and many regions of gradual differentiation from one municipality to another municipality. As mentioned, in the later parts of the 20th cen- tury relatively strong dialect leveling has been tak- ing place. Some of the Finnish dialects may already be concerned endangered, although the complex re- lationship between contemporary vernaculars and the most traditional dialectal forms makes this hard to ascertain. Dialect leveling in itself is a process known from many parts of Europe (Auer, 2018). However, in the case of Finnish the written stan- dard has remained relatively far from the spoken Finnish, besides individual narrow domains such as news broadcasts were the written form is used also in speech. Additionally there have been distinct text col- lections that include materials from this dialect archive. These include dialect books specific regions and municipalities, such as Oulun mur- rekirja [Dialect Book of Oulu] (Pääkkönen, 1994) or Savonlinnan seudun murrekirja [Dialect book of Savonlinna region] (Palander, 1986). There have also been more recent larger collections that contains excerpts from essentially all dialects (Lyytikäinen et al., 2013). Especially in the later parts of 21th century the spoken varieties have been leveling away from very specific local dialects, and although regional vari- eties still exist, most of the local varieties have certainly became endangered. Similar processes of dialect convergence have been reported from dif- ferent regions in Europe, although with substantial variation (Auer, 2018). In the case of Finnish this has not, however, resulted in merging of the written and spoken standards, but the spoken Finnish has remained, to our day, very distinct from the written standard. In a late 1950s, a program was set up to document extant spoken dialects, with the goal of recording 30 hours of speech from each municipal- ity. This work resulted in very large collections of dialectal recordings (Lyytikäinen, 1984, 448-449). Many of these have been published, and some por- tion has also been manually normalized. Dataset used is described in more detail in Section 3 Data. In Finnish linguistics the dialect identification has primarily been studied in the context of folk linguistics. In this line of research the perceptions of native speakers are investigated (Niedzielski and Preston, 2000). This type of studies have been done for Finnish, for example, by Mielikäinen and Palander (2014), Räsänen and Palander (2015) and Palander (2011). It has been possible to suggest for individual dialects which features are the most stable and will remain as local regional markers, and which seem to be in retention (Räsänen and Palander, 2015, 25). In this study we conduct just individual experiments and report their results, but in the further research we hope the results could be analyzed in more detail in connection with the earlier dialect perception studies, as we believe the differences in perceived dialect differences could be compared to the difficulties and successes the model has to differentiate individual varieties. Related work The current approaches to Finnish dialect have fo- cused on the textual modality only. Previously, bi- directional LSTM (long short-term memory) based models have been used to normalize Finnish di- alects to standard Finnish (Partanen et al., 2019) and to adapt standard Finnish text into different dialectal forms (Hämäläinen et al., 2020). Similar approach has also been used to normalize historical Finnish (Hämäläinen et al., 2021; Partanen et al., 2021). The closest research to our paper conducted for Finnish has been detection of foreign accents from audio. Behravan et al. (2013) have detected for- eign accents from audio only by using i-vectors. However, foreign accent detection is a very differ- ent task to native speaker dialect detection. Many foreign accents have clear cues through phonemes that are not part of the Finnish phonotactic system, where as with dialects, all phonemes are part of Finnish. There have been several recent approaches for Arabic to detect dialect from text (Balaji et al., 2020; Talafha et al., 2020; Alrifai et al., 2021). Textual dialect detection has been done also for German (Jauhiainen et al., 2018), Romanian (Za- haria et al., 2021) and Low Saxon (Siewert et al., 2020). The methods used range from traditional machine learning with features such as n-grams to neural models with pretrained embeddings, as it is typically the case in NLP research. None of these approaches use audio, as they rely on text only. At the same time, North Sami dialects have been identified from audio by training several models, kNNs, SVMs, RFs, CRFs, and LSTM, based on ex- tracted features (Kakouros et al., 2020). Kethireddy et al. (2020) use Mel-weighted SFF spectrogram to detect spoken Arabic dialects. Mel spectograms are also used by Draghici et al. (2020). All these approaches are mono-modal and use only audio. Based on our literature review, the existing ap- proaches use either text or audio for dialect detec- tion. We, however, use both modalities and apply them on a language with no existing dialect detec- tion models. Conclusions We have presented the first model for Finnish di- alect classification for a relatively large number of different dialects, 23 in total. Based on our ex- periments, a text only model is not as effective in dialect classification as a model with text and audio. It is clear that the amount of data alone is not the only variable that constitutes a high performance of the model for a given dialect, but also how dis- tinctive a given dialect is from other dialects. Since the speakers in the test set were not present in the training, we are confident that the dialect is the feature that the model has learned to predict. Using the audio materials offers in itself new interesting possibilities for dialect clustering and comparison. Traditional dialect atlases have also been used in automatic comparison and grouping of different Finnish dialects (Syrjänen et al., 2016). In further research we believe also this kind of infor- mation could be connected to the analysis to show how the dialect identification exactly interacts with the dialectal variation and differences at close mu- nicipality level. At the same time the identifiability of a dialect must be connected to the degree of di- alect leveling, linguistic distances and differences between them, so applying the model into newer recordings could also yield information about these processes. We have made all the data, code and models openly available on Github11 and Zenodo12. We believe that this is the only way to ensure this line of research continues for the Finnish language in the future as well.",1
"Abstract Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically de- tect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is re- ceived by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo. Introduction We present an approach for identifying the dialect of a speaker automatically solely based on text and on audio and text together. We compare the uni- modal approach to the bimodal one. There are no previous dialect identification approaches for Finnish. There are several situations were a dialect identification method can be of use. For example, if we have ASR models fine tuned for specific di- alects, the dialect identification from audio could be used as a preprocessing step. The model could also be used to label recorded materials automatically in order to create archival metadata. In order to make our contribution useful for others, we have released our code, models and processed data openly on GitHub1 and Zenodo2. Finnish is a large Uralic language that is one of the official languages of Finland, and is used essen- tially at all levels of the modern society. There are approximately five million Finnish speakers. The language belongs to the Finnic branch of the Uralic language family, and is very closely related to Kare- lian, MeaÃÂnkieli and Kveeni, and is also closely re- lated to the Estonian language. It is more distantly related to numerous Uralic languages spoken in Russia. The history of written Finnish starts in the 16th century. Current orthography is connected to this written tradition, which developed into the current form in the late 19th century with a conscious plan- ning and systematic development of the lexicon. After this, the changes have been minor (HaÃÂkkinen, 1994, 16), and also impacted lexicon, especially what it comes to the development of the vocabu- lary of the modern society and traditional agrarian terminology becoming less known. The Finnish spoken language, however, is still largely based on Finnish dialects. In the 20th cen- tury some of the strongest dialectal features have been disappearing, but there are still clearly dis- tinguishable spoken vernacular varieties that are regionally marked. It has been shown that instead of clear disappearance of dialects there are vari- ous features that are spreading, but not at uniform rate, and reportedly younger speakers use the are- ally marked features less than the older speakers (Lappalainen, 2001, 92). Finnish vernaculars also represent historically rather different Finnic vari- eties, with major split between Eastern and Western dialects. There are, however, also dialect continu- ums and traditionally found gradual differentiation from region to region. Many of the changes have been lexical due to technical innovations and modernization of the society: orthographic spelling conventions have largely remained the same. Spoken Finnish, on the other hand, traditionally represents an areally di- vided dialect continuum, with several sharp bound- aries, and many regions of gradual differentiation from one municipality to another municipality. As mentioned, in the later parts of the 20th cen- tury relatively strong dialect leveling has been tak- ing place. Some of the Finnish dialects may already be concerned endangered, although the complex re- lationship between contemporary vernaculars and the most traditional dialectal forms makes this hard to ascertain. Dialect leveling in itself is a process known from many parts of Europe (Auer, 2018). However, in the case of Finnish the written stan- dard has remained relatively far from the spoken Finnish, besides individual narrow domains such as news broadcasts were the written form is used also in speech. Additionally there have been distinct text col- lections that include materials from this dialect archive. These include dialect books specific regions and municipalities, such as Oulun mur- rekirja [Dialect Book of Oulu] (PaÃÂaÃÂkkoÃÂnen, 1994) or Savonlinnan seudun murrekirja [Dialect book of Savonlinna region] (Palander, 1986). There have also been more recent larger collections that contains excerpts from essentiallydialects (LyytikaÃÂÃÂinen et al., 2013). Especially in the later parts of 21th century the spoken varieties have been leveling away from very specific local dialects, and although regional vari- eties still exist, most of the local varieties have certainly became endangered. Similar processes of dialect convergence have been reported from dif- ferent regions in Europe, although with substantial variation (Auer, 2018). In the case of Finnish this has not, however, resulted in merging of the written and spoken standards, but the spoken Finnish has remained, to our day, very distinct from the written standard. In a late 1950s, a program was set up to document extant spoken dialects, with the goal of recording 30 hours of speech from each municipal- ity. This work resulted in very large collections of dialectal recordings (LyytikaÃÂinen, 1984, 448-449). Many of these have been published, and some por- tion has also been manually normalized. Dataset used is described in more detail in Section 3 Data. In Finnish linguistics the dialect identification has primarily been studied in the context of folk linguistics. In this line of research the perceptions of native speakers are investigated (Niedzielski and Preston, 2000). This type of studies have been done for Finnish, for example, by MielikaÃÂinen and Palander (2014), RaÃÂsaÃÂnen and Palander (2015) and Palander (2011). It has been possible to suggest for individual dialects which features are the most stable and will remain as local regional markers, and which seem to be in retention (RaÃÂsaÃÂnen and Palander, 2015, 25). In this study we conduct just individual experiments and report their results, but in the further research we hope the results could be analyzed in more detail in connection with the earlier dialect perception studies, as we believe the differences in perceived dialect differences could be compared to the difficulties and successes the model has to differentiate individual varieties. Related work The current approaches to Finnish dialect have fo- cused on the textual modality only. Previously, bi- directional LSTM (long short-term memory) based models have been used to normalize Finnish di- alects to standard Finnish (Partanen et al., 2019) and to adapt standard Finnish text into different dialectal forms (HaÃÂmaÃÂlaÃÂinen et al., 2020). Similar approach has also been used to normalize historical Finnish (HaÃÂmaÃÂlaÃÂinen et al., 2021; Partanen et al., 2021). The closest research to our paper conducted for Finnish has been detection of foreign accents from audio. Behravan et al. (2013) have detected for- eign accents from audio only by using i-vectors. However, foreign accent detection is a very differ- ent task to native speaker dialect detection. Many foreign accents have clear cues through phonemes that are not part of the Finnish phonotactic system, where as with dialects, all phonemes are part of Finnish. There have been several recent approaches for Arabic to detect dialect from text (Balaji et al., 2020; Talafha et al., 2020; Alrifai et al., 2021). Textual dialect detection has been done also for German (Jauhiainen et al., 2018), Romanian (Za- haria et al., 2021) and Low Saxon (Siewert et al., 2020). The methods used range from traditional machine learning with features such as n-grams to neural models with pretrained embeddings, as it is typically the case in NLP research. None of these approaches use audio, as they rely on text only. At the same time, North Sami dialects have been identified from audio by training several models, kNNs, SVMs, RFs, CRFs, and LSTM, based on ex- tracted features (Kakouros et al., 2020). Kethireddy et al. (2020) use Mel-weighted SFF spectrogram to detect spoken Arabic dialects. Mel spectograms are also used by Draghici et al. (2020). All these approaches are mono-modal and use only audio. Based on our literature review, the existing ap- proaches use either text or audio for dialect detec- tion. We, however, use both modalities and apply them on a language with no existing dialect detec- tion models. Conclusions We have presented the first model for Finnish di- alect classification for a relatively large number of different dialects, 23 in total. Based on our ex- periments,text only model is not as effective in dialect classification as a model with text and audio. It is clear that the amount of data alone is not the only variable that constitutes a high performance of the model for a given dialect, but also how dis- tinctive a given dialect is from other dialects. Text and audio create interesting dialect vari- eties because they are the modalities for identifying dialects automatically solely based on text and on audio and text together. We present the first approach to automatically de- tect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is re- ceived by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo. Introduction We present an approach for identifying the dialect of a speaker automatically solely based on text and on audio and text together. We compare the uni- modal approach to the bimodal one. There are no previous dialect identification approaches for Finnish.",0
"Abstract In this work we consider the col- lection of deceptive April Fools’ Day (AFD) news articles as a useful addition in existing datasets for deception detec- tion tasks. Such collections have an es- tablished ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that in- cludes diachronic AFD and normal ar- ticles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current re- search thread, we also discuss the individ- ualism/collectivism dimension in decep- tion with respect to these two datasets. Lastly, we build classifiers by testing vari- ous monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observa- tions of other deception detection works. Introduction April Fools’ Day (for short AFD) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide. Specifically, the contributions of this work are: • A new corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites2, adding one more AFD collection to the currently unique one in En- glish (Dearden and Baron, 2019). • A study and discussion of the linguistic cues of deception that prevail in the Greek and En- glish collection, along with their similarities. • A discussion on whether the consideration of the individualism/collectivism cultural di- mension in the context of AFD aligns with the results of our previous work. • An examination of the performance of vari- ous classifiers in identifying AFD articles, in- cluding multilanguage setups. Related Work The creation of reliable and realistic ground truth datasets for the deception detection task is a chal- lenging task (Fitzpatrick and Bachenko, 2012). Crowdsourcing, in the form of online campaigns in which people express themselves in truth- ful and/or deceitful manner for a small pay- ment are a well established way to collect de- ceptive data (Ott et al., 2011). Real-life situations such as trials (Soldner et al., 2019) or the use of data from board games have also been employed (Peskov et al., 2020). Also a popular approach is the reuse of content from sites that debunk ar- ticles like fake news and hoaxes (Wang, 2017; Kochkina et al., 2018). Lastly, satire news are another way to collect deceptive texts, but with some particularities due to humorous deception (Skalicky et al., 2020). The only work that explores AFD articles is that of Dearden et al. (2019). They collected 519 AFD and 519 truthful stories and articles in English for a period of 14 years. A large set of features was exploited to identify deception cues in AFD sto- ries. Structural complexity and level of detail were among the most valuable features while the ex- ploitation of the same feature set to a fake news dataset resulted in similar observations. To the best of our knowledge, the only decep- tion related dataset for the Greek language is that of Karidi et al. (2019). This work proposed an automatic process for the creation of a fake news and hoaxes articles corpus, but unfortunately the created corpus over Greek websites is not avail- able. If we also consider that the creation of a Greek dataset for deception through crowdsourc- ing is a cumbersome and expensive task, that is further hindered by the exceptionally limited num- ber of native Greek crowd workers, it is easy to understand why there is a lack of datasets. Regarding the individualism/collectivism cul- tural dimension, it constitutes a well-known divi- sion of cultures that concerns the degree in which members of a culture value more individual over group goals and vice versa. In individualism, ties between individuals are loose and individuals are expected to take care of only themselves and their immediate families, whereas in collectivism ties in society are stronger. In Papantoniou et al. (2021) there is an preliminary effort driven by prior work in psychology discipline (Taylor et al., 2017) to examine if deception cues are altered across cul- tures and if this can be attributed to this divide. Among the conclusions were that people from in- dividualistic cultures employ more third and less first person pronouns to distance themselves from the deceit when they are deceptive, whereas in the collectivism group this trend is milder, signalling the effort of the deceiver to distance the group from the deceit. In addition, in individualistic cul- tures positive sentiment is employed in deceptive language, whereas in collectivists there is a re- straint of expression of sentiment both in truthful and deceptive texts. To this end, this work explores the deception- related characteristics of a new Greek cor- pus based on AFD articles from a variety of sources, and compares them with the En- glish ones3 . Further, since related stud- ies (Triandis and Vassiliou, 1972; Hofstede, 1980; Koutsantoni, 2005) describe Greece as a culture with more collectivistic characteristics (by using country as proxy from culture), we also discuss differences in deception cues along this cultural dimension. Conclusion and Future work We introduced a new dataset with AFD news articles in Greek and analyzed and compared its de- ception cues with another English one. The results showcased the use of emotional language, es- pecially of positive sentiment, for deceptive arti- cles which is even more prevalent in the individ- ualistic English dataset. Further, deceptive arti- cles use less concrete language, as manifested by the increased use of adverbs, hedges, and boosters and less usage of named entities, spatial related words and conjunctions compared to the truth- ful ones. The future and past tenses were corre- lated with deceptive and truthful articles respec- tively. All the above, mainly align with previ- ous work (Papantoniou et al., 2021), except from some differences in the usage of pronouns for the Greek dataset, which is attributed to the idiosyn- crasies of the news domain. The accuracy of the deployed classifiers offered adequate perfor- mance, with no statistically significant differences between the best logistic regression and the BERT models. In the future we aim at creating even more crosslingual datasets for deception detection tasks through crowdsourcing and by employing the Chattack platform (Smyrnakis et al., 2021).",1
"Abstract In this work we consider the col- lection of deceptive April FoolsÃÂ¢ÃÂÃÂ Day (AFD) news articles as a useful addition in existing datasets for deception detec- tion tasks. Such collections have an es- tablished ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that in- cludes diachronic AFD and normal ar- ticles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current re- search thread, we also discuss the individ- ualism/collectivism dimension in decep- tion with respect to these two datasets. Lastly, we build classifiers by testing vari- ous monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observa- tions of other deception detection works. Introduction April FoolsÃ¢ÂÂ Day (for short AFD) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide. Specifically, the contributions of this work are: Ã¢ÂÂ¢ A new corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites2, adding one more AFD collection to the currently unique one in En- glish (Dearden and Baron, 2019). Ã¢ÂÂ¢ A study and discussion of the linguistic cues of deception that prevail in the Greek and En- glish collection, along with their similarities. Ã¢ÂÂ¢ A discussion on whether the consideration of the individualism/collectivism cultural di- mension in the context of AFD aligns with the results of our previous work. Ã¢ÂÂ¢ An examination of the performance of vari- ous classifiers in identifying AFD articles, in- cluding multilanguage setups. Related Work The creation of reliable and realistic ground truth datasets for the deception detection task is a chal- lenging task (Fitzpatrick and Bachenko, 2012). Crowdsourcing, in the form of online campaigns in which people express themselves in truth- ful and/or deceitful manner for a small pay- ment are a well established way to collect de- ceptive data (Ott et al., 2011). Real-life situations such as trials (Soldner et al., 2019) or the use of data from board games have also been employed (Peskov et al., 2020). Also a popular approach is the reuse of content from sites that debunk ar- ticles like fake news and hoaxes (Wang, 2017; Kochkina et al., 2018). Lastly, satire news are another way to collect deceptive texts, but with some particularities due to humorous deception (Skalicky et al., 2020). The only work that explores AFD articles is that of Dearden et al. (2019). They collected 519 AFD and 519 truthful stories and articles in English for a period ofyears. A large set of features was exploited to identify deception cues in AFD sto- ries. Structural complexity and level of detail were among the most valuable features while the ex- ploitation of the same feature set to a fake news dataset resulted in similar observations. To the best of our knowledge, the only decep- tion related dataset for the Greek language is that of Karidi et al. (2005). This work proposed an automatic process for the detection of deception cues, but unfortunately the created corpus over Greek websites is not avail- able. If we also consider that the creation of a fake news dataset for Greek websites through a previous work (2019) is a cumbersome and expensive task, and thats (2019) and (2019)_ in Dearden and Baron (2019) work. (2019) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide. Specifically, the contributions of this work are: Ã¢ÂÂ¢ A new corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites2, adding one more AFD collection to the currently unique one in En- glish (Dearden and Baron, 2019). Ã¢ÂÂ¢ A study and discussion of the linguistic cues of deception that prevail in the Greek and En- glish collection, along with their similarities. Ã¢ÂÂ¢ A discussion on whether the consideration of the individualism/collectivism cultural di- mension in the context of AFD aligns with the results of our previous work. Ã¢ÂÂ¢ An examination of the performance of vari- ous classifiers in identifying AFD articles, in- cluding multilanguage setups. Related Work The creation of reliable and realistic ground truth datasets for the deception detection task is a chal- lenging task (Fitzpatrick and Bachenko, 2012). Crowdsourcing, in the form of online campaigns in which people express themselves in truth- ful and/or deceitful manner for a small pay- ment are a well established way to collect de- ceptive data (Ott et al., 2011). Real-life situations such as trials (Soldner et al., 2019) or the use of data from board games have also been employed (Peskov et al., 2020).",0
"Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a ‘subcontinent’. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ̄ra ̄o, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally, would pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: • We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of Indic language data. This data has been curated in a short time frame from public sources which have a permissive license. It indicates that the feasibility of collecting large amount of pretraining data and further efforts can be made to significantly expand this collection. • Starting from the wav2vec 2.0 model, we perform extensive ablation studies on architecture choices, pretraining and fine-tuning strategies, language models and choice of lexicon to arrive at a training and decoding regimen that works well for Indian languages. • Our ASR models achieve SOTA performance on 9 Indian languages on 3 publicly available bench- marks with small fine-tuning datasets. These re- sults indicate that end-to-end ASR systems based on multilingual pretraining with the wav2vec model hold promise for Indic languages. • Our ablation studies reveal that the accuracy of the ASR system on Indic languages sensitively de- pends on the size of the pretraining corpus, amount of labelled data for fine-tuning, and access to task- specific lexicon. In summary, we establish that the recent ad- vances of pretraining wav2vec models transfer to Indic ASR and achieve SOTA results against mod- els proposed over multiple years. However, unlike in the reported results of English ASR, we observe that the WER reported for Indic ASR is signifi- cantly higher and sensitively depends on availability of resources: pretraining corpus, fine-tuning data, and task-specific language information. This suggests that the ASR task on Indic languages re- mains far from being solved and requires model innovation and continued efforts on curating re- sources. We publicly release all the artifacts of our work to spur further work in the area of Indic ASR. This includes: (a) sources of pretraining data along with scripts for their collection and pre-processing, (b) pretraining, fine-tuning and decoding scripts, (c) language models, and (d) our best ASR models. Conclusion We report results of applying two recent and suc- cessful ideas from English ASR to Indic ASR: use of wav2vec like model architecture and use of un- labelled data to pretrain the model. We implement this with a curated dataset on Indic languages and a range of ablation studies on architecture, pretrain- ing, fine-tuning, and decoding choices. Through this, we obtain state-of-the-art results on 9 Indic languages across 3 datasets. While advancing ASR systems for the next billion users from the sub- continent, our results highlight the need for larger resources and benchmarks across more languages. All the models developed as a part of this work, viz., the pretrained model, the language-specific fine-tuned models and the language models along with the Fairseq and KenLM scripts and configu- ration files used for building them will be publicly released. We hope that these models will help in advancing the state of the art for Indian Speech Technology.",1
"Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a Ã¢ÂÂsubcontinentÃ¢ÂÂ. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ÃÂra ÃÂo, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally,pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: ÃÂ¢ÃÂÃÂ¢ We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of ASR data for Indian languages. This data has been curated in a short time frame from public sources which have a permissive license. This has resulted in more than a million hours of raw audio data for pretraining across 40 languages from the public domain. This data has been curated in a short time frame from public sources which have a permissive license. This file pretrainers for 40 Indian languages pretrained from a large corpus of data, making it one of the largest and most diverse collections of ASR data for pretraining. It is a significant achievement since it indicates that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a Ã¢ÂÂsubcontinentÃ¢ÂÂ. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ÃÂra ÃÂo, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective.",0
"Abstract Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expen- sive to train. We propose a simple and effi- cient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classifi- cation datasets in four domains, TLM achieves results better than or similar to pretrained lan- guage models (e.g., RoBERTa-Large) while re- ducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development. Introduction Pretrained language models (PLMs) have drawn much attention from the natural language process- ing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are finetuned on a small amount of labeled data for specific downstream tasks. This pretraining-finetuning framework has significantly improved the performance of many NLP tasks. However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36×1021 FLOPs3. Larger PLMs such as GPT- 3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretrain- ing loss functions. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Even though there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019). In this work, we explore alternatives to the stan- dard pretraining-finetuning paradigm, aiming at more drastic efficiency improvement without per- formance drop. We propose a simple, efficient, pretraining-free framework, Task-driven LanguageModeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, hu- mans master a task by using only a small portion of world knowledge (e.g., students only need to re- view a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a spe- cific task. Second, training on supervised labeled data is much more data efficient for downstream performance than optimizing the language model- ing objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modeling objective using both the retrieved data and the task data. We evaluate TLM on eight different tasks cov- ering the domains of news, review, computer sci- ence, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude. Related work Pretrained Language Models Pretrained lan- guage models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those mod- els are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB En- glish corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger cor- pus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% ∼ 70% reduc- tion in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled represen- tation of content and position, which brings about 50% ∼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efficiency by reducing training data redundancy. Our approach also re- sults in more drastic improvements. Efficient Inference of Pretrained Models An- other line of work aims at improving inference efficiency of PLMs. Some works improve in- ference efficiency by distilling large PLMs into small-sized models and using the distilled mod- els for inference, such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), Mobile- BERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with low- precision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). An- other type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distil- lation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our ap- proach doesn’t rely on large-scale pre-training and achieves better or at least comparable performance. Domain and Task Adaptation for Pretrained Models Domain-adaptive finetuning is a method that finetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adapta- tion (Gururangan et al., 2020; Zhang et al., 2019; Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive fine- tuning and TLM. First, TLM is a general method to improve training efficiency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In com- parison, domain-adaptive finetuning uses domain data to improve domain adaptation. Second, while previous works on domain-adaptive finetuning are built upon a model pretrained on the general corpus, TLM learns from scratch without large-scale pretraining to substantially save computation costs. Conclusions In this paper, we have proposed a simple, efficient, pretraining-free framework, TLM. The core idea is to only use a tiny, task-relevant subset of the general corpus for language model training. Our experiments show that TLM achieves results simi- lar to or even better than PLMs, with a reduction of training FLOPs by two orders of magnitude. TLM opens the possibility of reducing the heavy reliance on large-scale PLMs and training a model from scratch in an efficient manner, while not hurt- ing the overall performance. We hope TLM will contribute to democratizing NLP and expediting its development by allowing most researchers to freely explore the architectures, loss functions, al- gorithms, and other design choices in the neighbor- hood of a state-of-the-art solution. As discussed in Section 3.2, there are several potential directions for future work. It will be in- teresting to study how to use TLM to match the performance even larger-scale PLMs. Moreover, further extending and improving TLM for few-shot and zero-shot learning is a crucial problem.",1
"Abstract Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expen- sive to train. We propose a simple and effi- cient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classifi- cation datasets in four domains, TLM achieves results better than or similar to pretrained lan- guage models (e.g., RoBERTa-Large) while re- ducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development. Introduction Pretrained language models (PLMs) have drawn much attention from the natural language process- ing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are finetuned on a small amount of labeled data for specific downstream tasks. This pretraining-finetuning framework has significantly improved the performance of many NLP tasks. However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36ÃÂ1021 FLOPs3. Larger PLMs such as GPT- 3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretrain- ing loss functions. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Even though there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019). In this work, we explore alternatives to the stan- dard pretraining-finetuning paradigm, aiming at more drastic efficiency improvement without per- formance drop. We propose a simple, efficient, pretraining-free framework, Task-driven LanguageModeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, hu- mans master a task by using only a small portion of world knowledge (e.g., students only need to re- view a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a spe- cific task. Second, training on supervised labeled data is much more data efficient for downstream performance than optimizing the language model- ing objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modelingusing both the retrieved data and the task data. We evaluate TLM on eight different tasks cov- ering the domains of news, review, computer sci- ence, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude. Related work Pretrained Language Models Pretrained lan- guage models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those mod- els are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB En- glish corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger cor- pus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% Ã¢ÂÂ¼ 70% reduc- tion in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled represen- tation of content and position, which brings about 50% Ã¢ÂÂ¼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efficiency by reducing training data redundancy. Our approach also re- sults in more drastic improvements. Efficient Inference of Pretrained Models An- other line of work aims at improving inference efficiency of PLMs. Some works improve in- ference efficiency by distilling large PLMs into small-sized models and using the distilled mod- els for inference, such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), Mobile- BERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with low- precision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). An- other type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distil- lation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our ap- proach doesnÃ¢ÂÂt rely on large-scale pre-training and achieves better or at least comparable performance. Domain and Task Adaptation for Pretrained Models Domain-adaptive finetuning is a method that finetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adapta- tion (Gururangan et al., 2020; Zhang et al.,Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive fine- tuning and TLM. First, TLM is a general method to improve training efficiency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In BERT, all books in the world, to use the example of Boston Globe, "", to improve the efficiency of in- ference, but these methods rely on pretraining a large general corpus. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Second, while there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019).",0
"Abstract Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation of the administering agency and the COVID-19 vaccines themselves. This paper proposes a semi-supervised ma- chine learning pipeline to perform topic modeling, sentiment analysis, and an analysis of vaccine brand reputation to obtain an in-depth understanding of national public opinion of Filipinos on Facebook. The methodology makes use of a multilingual version of Bidirectional Encoder Representations from Transformers or BERT for topic modeling, hierarchical clustering, five dif- ferent classifiers for sentiment analysis, and cosine similarity of BERT topic embeddings for vaccine brand reputation analysis. Results suggest that any type of COVID-19 misinformation is an emergent property of COVID-19 public opinion, and that the detection of COVID-19 misinformation can be an unsupervised task. Sentiment analysis aided by hierarchical clustering reveal that 21 of the 25 topics extrapolated by topic modeling are negative topics. Such negative comments spike in count whenever the Department of Health in the Philippines posts about the COVID-19 situation in other countries. Additionally, the high numbers of laugh reactions on the Face- book posts by the same agency—without any humorous content—suggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase neg- ative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4. Sputnik V, AstraZeneca, and Sinovac suffer from a negative public reputation 5. Ministries of health and stakeholders in vaccination campaigns are rec- ommended to employ interventions that correct misinformation, engage people and use local narratives of success Introduction The COVID-19 pandemic has drastically affected the overall wellness and health of the entire world. On January 30, 2020, the first case of COVID- 19, in the Philippines, has been reported by the country’s Department of Health (DOH). COVID-19 is a respiratory disease caused by the SARS-CoV- 2 virus—first identified in the province of Wuhan, located in China (Paules et al., 2020). Over a year later, on March 1, 2021, the Philippines started with its COVID-19 vaccination program. Vaccine hesitancy among Filipinos is an on- going phenomenon that the national vaccine campaign efforts are struggling with (Alfonso et al., 2021). According to a recent study in the Philippines (Caple et al., 2021), only 62.5% of their 7,193 respondents are willing to be vaccinated against COVID-19. A majority of the same respondents are only willing to be inoculated after many others have received the vaccine or after political figures have done so (Caple et al., 2021). Additionally, the partici- pants’ preferences of vaccine brand are also studied; 59.7% of the participants are confident in a USA-made or European-made COVID-19 vaccine (Caple et al., 2021). Sentiment analysis is a common natural language processing (NLP) task that has been done on a number of studies regarding COVID-19 public opin- ion (Melton et al., 2021; Garcia and Berton, 2021). It computationally classi- fies the polarity of text data—neutral, positive, or negative sentiment. This is primarily done since gauging the sentiment of the public, especially on critical topics such as a pandemic like COVID-19, help determine possible policies and interventions that could shape the actions that society takes. Furthermore, topic modeling is proposed as part of the NLP pipeline of the same studies mentioned earlier. Topic modeling is an unsupervised technique for obtaining the relevant ideas that public opinion holds, more of which is discussed in a later section of this article. This paper proposes a pipeline for understanding the public opinion in the Philippines regarding COVID-19 and the country’s vaccination efforts. The semi-supervised pipeline is named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions (VER- TEBRATE). It is comprised of three main modules: a topic extraction or topic modeling of opinions, an analysis of vaccine brand reputation, and a sentiment analysis of the same set of opinions. Topic modeling is per- formed through the use of contextual embeddings provided by a multilingual transformer architecture—discussed further in Section 2.3. The topics ob- tained are compared and associated with the vaccine brands available in the Philippines through cosine similarity. Next, a hierarchical clustering of the topics extrapolated through topic modeling is used to assign the labels used for sentiment analysis. Sentiment analysis on the data is performed by five different classification algorithms—assessing which architecture models the sentiments of the text data most effectively. The classification algorithms proposed are XGBoost, LightGBM, K-nearest neighbors, Naive Bayes’ al- gorithm, and support vector machine. The public opinions considered for this study are comments made on 50 different Facebook posts by the official page of the Department of Health Philippines. The posts are part of the RESBAKUNA campaign of the same government department. Conclusion The COVID-19 pandemic has radically changed the world. Over a year after the start of the outbreak, worldwide vaccine efforts against COVID-19 have begun. On the first day of March 2021, the Philippines started its vac- cination program under the RESBAKUNA campaign. A recent study shows that vaccine hesitancy is a prevalent issue in the Philippines. The same concerns and complaints over the COVID-19 vaccines can also be found on social media, like Facebook. To understand and gauge national public opinion via the Facebook platform, the top comments from 50 Facebook posts of the official Facebook page of the Department of Health Philippines were scraped via Selenium and Python. The 50 Facebook posts are part of the “#RESBAKUNA” Facebook campaign by the agency. Next, a semi- supervised pipeline named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions or VERTE- BRATE is proposed. It is a pipeline that leverages topic modeling via multi- lingual BERT, analysis of vaccine brand reputation through cosine similarity of the BERT topic embeddings, hierarchical clustering of the BERT topic embeddings, and sentiment classification of the Facebook comments that are labeled automatically by the clades produced by the hierarchical clustering model. The BERT topic model extracted 25 topics—2 of which are indeter- minable, 2 of which are positive, and 21 topics are negative comments. These topics are precisely clustered through Ward’s linkage hierarchical clustering. The model suggests that COVID-19 misinformation and COVID-19 vaccine misinformation are emergent properties of COVID-19 public opinion. Using supervised techniques to model such misinformation is unnecessary. In terms of the temporal distribution of these topics, the analysis suggests that neg- ative comments drastically increase when the Department of Health Philip- pines posts about other countries. It can also be noted that the huge numbers of laugh reactions throughout the Facebook posts—without the presence of any humorous content—entail that the Department of Health Philippines has a public image issue. In this instance, people tend to react the way they do, not because of the posts but because of who posted them. As for the brand reputation of the different COVID-19 vaccines, Pfizer and Moderna are doing relatively well—especially Pfizer’s vaccine. Mean- while, Sputnik V, AstraZeneca, and Sinovac suffer from negative associations. Additionally, clade-assisted sentiment classification effectively models public sentiment. The best-performing classifier, LightGBM, that is proposed in the study has managed to perform with 92.4% accuracy. It also has a strong level of agreement in terms of Cohen’s kappa statistic, with a value of 0.847. Our present study highlights the persisting prevalence of COVID-19 vac- cine misinformation in social media. Conspiracy beliefs and other forms of misinformation had been noted as a significant predictor of complete vac- cine hesitancy (Al-Sanafi and Sallam, 2021). We noticed that DOH and its entities offered no responses to the comments posted by netizens under these infographics. Meta-analytic evidence suggests the importance of iden- tifying misinformation most susceptible to correction, and engage experts in responding to misinformation (Walter et al., 2020). We recommend for DOH to form a social media team composed of health care professionals and in- terdisciplinary communication practitioners whose mandate is to respond to misinformation found in the comments of their posts. This engaging way of correcting false vaccine information will not only help quell vaccine doubts, but also hopefully improve the image of DOH among citizens. Also, our present study suggests that instead of drawing aspirational sen- timents from people, posting about the success of wealthier countries in terms of vaccination only intensifies Filipinos’ xenocentric tendencies to rationalize the poor COVID outcomes of the country. We recommend for DOH and other stakeholders involved in vaccine promotion to use narratives that are closer to home, like the diseases curbed by the expanded program on immu- nization in the Philippines. Using relatable narratives and storytelling had been indicated as effective means to combat anti-vaccine conspiracies (Lazi ́c and Zˇeˇzelj, 2021). The VERTEBRATE pipeline effectively highlights the contents to avoid when posting on social media about COVID-19. Future work could include the implementation of an automatic labeling procedure for the topic model to further reduce manual effort.",1
"Abstract Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation of the administering agency and the COVID-19 vaccines themselves. This paper proposes a semi-supervised ma- chine learning pipeline to perform topic modeling, sentiment analysis, and an analysis of vaccine brand reputation to obtain an in-depth understanding of national public opinion of Filipinos on Facebook. The methodology makes use of a multilingual version of Bidirectional Encoder Representations from Transformers or BERT for topic modeling, hierarchical clustering, five dif- ferent classifiers for sentiment analysis, and cosine similarity of BERT topic embeddings for vaccine brand reputation analysis. Results suggest that any type of COVID-19 misinformation is an emergent property of COVID-19 public opinion, and that the detection of COVID-19 misinformation can be an unsupervised task. Sentiment analysis aided by hierarchical clustering reveal that 21 of the 25 topics extrapolated by topic modeling are negative topics. Such negative comments spike in count whenever the Department of Health in the Philippines posts about the COVID-19 situation in other countries. Additionally, the high numbers of laugh reactions on the Face- book posts by the same agencyÃ¢ÂÂwithout any humorous contentÃ¢ÂÂsuggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase neg- ative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4. Sputnik V, AstraZeneca, and Sinovac suffer from a negative public reputation 5. Ministries of health and stakeholders in vaccination campaigns are rec- ommended to employ interventions that correct misinformation, engage people and use local narratives of success Introduction The COVID-19 pandemic has drastically affected the overall wellness and health of the entire world. On January 30, 2020, the first case of COVID- 19, in the Philippines, has been reported by the countryÃ¢ÂÂs Department of Health (DOH). COVID-19 is a respiratory disease caused by the SARS-CoV- 2 virusÃ¢ÂÂfirst identified in the province of Wuhan, located in China (Paules et al., 2020). Over a year later, on March 1, 2021, the Philippines started with its COVID-19 vaccination program. Vaccine hesitancy among Filipinos is an on- going phenomenon that the national vaccine campaign efforts are struggling with (Alfonso et al., 2021). According to a recent study in the Philippines (Caple et al., 2021), only 62.5% of their 7,193 respondents are willing to be vaccinated against COVID-19. A majority of the same respondents are only willing to be inoculated after many others have received the vaccine or after political figures have done so (Caple et al., 2021). Additionally, the partici- pantsÃ¢ÂÂ preferences of vaccine brand are also studied; 59.7% of the participants are confident in a USA-made or European-made COVID-19 vaccine (Caple et al., 2021). Sentiment analysis is a common natural language processing (NLP) task that has been done on a number of studies regarding COVID-19 public opin- ion (Melton et al., 2021; Garcia and Berton, 2021). It computationally classi- fies the polarity of text dataÃ¢ÂÂneutral, positive, or negative sentiment. This is primarily done since gauging the sentiment of the public, especially on critical topics such as a pandemic like COVID-19, help determine possible policies and interventions that could shape the actions that society takes. Furthermore, topic modeling is proposed as part of the NLP pipeline of the same studies mentioned earlier. Topic modeling is an unsupervised technique for obtaining the relevant ideas that public opinion holds, more of which is discussed in a later section of this article. This paper proposes a pipeline for understanding the public opinion in the Philippines regarding COVID-19 and the countryÃ¢ÂÂs vaccination efforts. The semi-supervised pipeline is named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, andEmotions (VER- TEBRATE). It is comprised of three main modules: a topic extraction or topic modeling of opinions, an analysis of vaccine brand reputation, and a sentiment analysis of the same set of opinions. Topic modeling is per- formed through the use of contextual embeddings provided by a multilingual transformer architectureÃ¢ÂÂdiscussed further in Section 2.3. The topics ob- tained are compared and associated with the vaccine brands available in the Philippines through cosine similarity. Next, a hierarchical clustering of the topics extrapolated through topic modeling is used to assign the labels used for sentiment analysis. Sentiment analysis on the data is performed by five different classification algorithmsÃ¢ÂÂassessing which architecture models the sentiments of the text data most effectively. The classification algorithms proposed are XGBoost, LightGBM, K-nearest neighbors, Naive BayesÃ¢ÂÂ al- gorithm, and support vector machine. The public opinions considered for this study are comments made on 50 different Facebook posts by the official page of the Department of Health Philippines. The posts are part of the RESBAKUNA campaign of the same government department. Conclusion The COVID-19 pandemic has radically changed the world. Over a year after the start of the outbreak, worldwide vaccine efforts against COVID-19 have begun. On the first day of March 2021, the Philippines started its vac- cination program under the RESBAKUNA campaign. A recent study shows that vaccine hesitancy is a prevalent issue in the Philippines. The same concerns and complaints over the COVID-19 vaccines can also be found on social media, like Facebook. To understand and gauge national public opinion via the Facebook platform, the top comments from 50 Facebook posts of the official Facebook page of the Department of Health Philippines were scraped via Selenium and Python. The 50 Facebook posts are part of the Ã¢ÂÂ#RESBAKUNAÃ¢ÂÂ Facebook campaign by the agency. Next, a semi- supervised pipeline named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions or VERTE- BRATE is proposed. It is a pipeline that leverages topic modeling via multi- lingual BERT, analysis of vaccine brand reputation through cosine similarity of the BERT topic embeddings, hierarchical clustering of the BERT topic embeddings, and sentiment classification of the Facebook comments that are labeled automatically by the clades produced by the hierarchical clustering model. The BERT topic model extracted 25 topicsÃ¢ÂÂ2 of which are indeter- minable, 2 of which are positive, and 21 topics are negative comments. These topics are precisely clustered through WardÃ¢ÂÂs linkage hierarchical clustering. The model suggests that COVID-19 misinformation and COVID-19 vaccine misinformation are emergent properties of COVID-19 public opinion. Using supervised techniques to model such misinformation is unnecessary. In terms of the temporal distribution of these topics, the analysis suggests that neg- ative comments drastically increase when the Department of Health Philip- pines posts about other countries. It can also be noted that the huge numbers of laugh reactions throughout the Facebook postsÃ¢ÂÂwithout the presence of any humorous contentÃ¢ÂÂentail that the Department of Health Philippines has a public image issue. In this instance, people tend to react the way they do, not because of the posts but because of who posted them. As for the brand reputation of the different COVID-19 vaccines, Pfizer and Moderna are doing relatively wellÃ¢ÂÂespecially PfizerÃ¢ÂÂs vaccine. Mean- while, Sputnik V, AstraZeneca, and Sinovac suffer from negative associations. Additionally, clade-assisted sentiment classification effectively models public sentiment. The best-performing classifier, LightGBM, that is proposed in the study has managed to perform with 92.4% accuracy. It also has a strong level of agreement in terms of CohenÃ¢ÂÂs kappa statistic, with a value of 0.847. Our present study highlights the persisting prevalence of COVID-19 vac- cine misinformation in social media. Conspiracy beliefs and other forms of misinformation had been noted as a significant predictor of complete vac- cine hesitancy (Al-Sanafi and Sallam, 2021). We noticed that DOH and its entities offered no responses to the comments posted by netizens under these infographics. Meta-analytic evidence suggests the importance of iden- tifying misinformation most susceptible to correction, and engage experts in responding to misinformation (Walter et al., 2020). We recommend for DOH to form a social media team composed of health care professionals and in- terdisciplinary communication practitioners whose mandate is to respond to misinformation found in theof their posts. This engaging way of correcting false vaccine information will not only help quell vaccine doubts, but also hopefully improve the image of DOH among citizens. Also, our present study suggests that instead of drawing aspirational sen- timents from people, posting about the success of wealthier countries in terms of vaccination and COVID-19 vaccine brand are also showing in the Philippines and the Philippines is due to the partici- pantsÃ¢ÂÂ preferences of vaccine brand are also studied. Results suggest that instead of drawing aspirational sen- timents from the people who are most likely to be affected by COVID-19, instead of drawing them about the COVID-19 situation in other countries increase neg- ative comments and increase swear words. ",0
"Abstract We present the first openly available cor- pus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based mod- els. We achieve a 77.53% accuracy with a Thai BERT model in detecting depres- sion. This establishes a good baseline for future researcher on the same corpus. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuﬀicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see Hämäläinen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. Related work In this section we present some of the recent related work in more detail. While there have been several digital humanities driven research efforts in better understanding depression in text (Girard and Cohn, 2015; Abd Yusof et al., 2017; Loveys et al., 2018), we only focus on work that has been conducted on depression detection. There are also several approaches to depression detection in other languages (Pi- rina and Çöltekin, 2018; Husseini Orabi et al., 2018; Song et al., 2018). An onset of major depression can be char- acterized and detected by investigating social media data such as Twitter (De Choudhury et al., 2013b,a). By exploring a large cor- pus of Twitter posting by using crowd-sourcing methodology. An SVM classifier was built by divesting a variety of social media measures such as social activity, egonetwork, style, user engagement, emotion, and language. The clas- sifier model predicted with high accuracy (70% and 73 % respectively) predicting ahead of the reported onset of depression and whether or not a post on Twitter could be depressive- indicative postings. Depression levels is de- tected by a proposed social media depression index. Computerized analysis of various kind of texts related to depression reveals signals of psychiatric disorders. Depression is measured by self-reported symptoms (Rude et al., 2002), by clinical interview (Rude et al., 2003). The Scrambled Sentences Test (SST) (Wenzlaff, 1993) was used to measure of cognitive process- ing bias of a large sample of college students. Negative cognitive processing biases in resolv- ing ambiguous verbal information can predict depression and subsequent depression symp- toms. Depression has been detected automatically before in Thai Facebook users (Katchapakirin et al., 2018). The authors train several models on RapidMiner. The models rely on metadata for activity on Facebook such as the number of posts posted on a given week day, the number of day-time and night-time posts and the num- ber of shared posts. The authors did not train the models on text, but rather used numerical features extracted from Facebook posts such as the number of first person pronouns and number of positive words. A screening text-based classification model was also applied to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). Similarly to the pre- vious approach, the authors apply several tra- ditional machine learning techniques to Thai social media text with pre-extracted features. Kumnunt and Sornil, 2020 present a CNN (convolutional neural network) based approach to depression detection in Thai social media posts. The authors crawl posts tagged with a depression hashtag to build their dataset. Contrary to any of the current work on Thai depression detection, our approach does not deal with social media posts but lengthier blog posts. We also make our dataset available un- like the existing work. Discussion and conclusions In this paper, we have presented a new ex- pert curated dataset for depression detection in Thai blog posts. Our sentence level results are promising and we are sure we can use our models to snowball more depressed blog posts from platforms such as Storylog for further linguistic analysis as a part of our interdisci- plinary research project. Based on our results, we can identify one easy future direction for enhancing the results obtained by our models. Currently, all freely available pretrained Thai embbeddings have been trained on Wikipedia. This is not opti- mal for several reasons, one being the ency- clopedic genre of Wikipedia, the other being the fact that while Wikipedia is written in for- mal ”correct” Thai, blog posts are written in a more colloquial language variety. This means that the vocabulary coverage of Wikipedia data is poor when compared to blog posts. Our blog corpus consists of 21,002 unique tokens while the word2vec model trained on Wikipedia has embeddings for 51,358 words. The blog corpus (training, testing and vali- dation combined) contains 6,488 words that are not present in the word2vec model, this means that around 31% of the words present in our blog depression corpus are simply not in a Wikipedia based model. In the future, it is clear that Thai language calls for openly available models that are trained on a larger and more varied internet corpus than solely on Wikipedia.",1
"Abstract We present the first openly available cor- pus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based mod- els. We achieve a 77.53% accuracy with a Thai BERT model in detecting depres- sion. This establishes a good baseline for future researcher on the same corpus. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuÃ¯Â¬Âicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see HaÃÂmaÃÂlaÃÂinen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. Related work In this section we present some of the recent related work in more detail. While there have been several digital humanities driven research efforts in better understanding depression in text (Girard and Cohn, 2015; Abd Yusof et al., 2017; Loveys et al., 2018), we only focus on work that has been conducted on depression detection. There are also several approaches to depression detection in other languages (Pi- rina and CÃÂ§oÃÂltekin, 2018; Husseini Orabi et al., 2018; Song et al., 2018). An onset of major depression can be char- acterized and detected by investigating social media data such as Twitter (De Choudhury et al., 2013b,a). By exploring a large cor- pus of Twitter posting by using crowd-sourcing methodology. An SVM classifier was built by divesting a variety of social media measures such as social activity, egonetwork, style, user engagement, emotion, and language. The clas- sifier model predicted with high accuracy (70% and 73 % respectively) predicting ahead of the reported onset of depression and whether or not a post on Twitter could be depressive- indicative postings. Depression levels is de- tected by a proposed social media depression index. Computerized analysis of various kind of texts related to depression reveals signals of psychiatric disorders. Depression is measured by self-reported symptoms (Rude et al., 2002), by clinical interview (Rude et al., 2003). The Scrambled Sentences Test (SST) (Wenzlaff, 1993) was used to measure of cognitive process- ing bias of a large sample of college students. Negative cognitive processing biases in resolv- ing ambiguous verbal information can predict depression and subsequent depression symp- toms. Depression has been detected automatically before in Thai Facebook users (Katchapakirin et al., 2018). The authors train several models on RapidMiner. The models rely on metadata for activity on Facebook such as the number of posts posted on a given week day, the number of day-time and night-time posts and the num- ber of shared posts. The authorsnot train the models on text, but rather used numerical features extracted from Facebook posts such as the number of first person pronouns and number of positive words. A screening text-based classification model was also applied to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). Similarly to the current study, we apply a screening text-based classification model to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). Thai users were also trained to submit a proposed cut-away of their blog posts. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuÃ¯Â¬Âicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see HaÃÂmaÃÂlaÃÂinen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. ",0
"Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination – wherein a model generates factually inaccurate statements – has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our model’s output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response model’s task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the model’s output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models – without any fine-tuning – with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval models (Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as “hallucination”, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate hallu- cination by using recent advancements in retrieval- augmented generative models developed for open- domain QA tasks (Lewis et al., 2020b; Izacard and Grave, 2021). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions (Khot et al., 2020; Jiang and Bansal, 2019; Gupta et al., 2020). Khot et al. (2020) learn a generative model that decomposes the task in the language of existing QA models for HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019). Herzig et al. (2021) solve text-to-SQL tasks with intermediate text representations. For storytelling, hierarchical generation procedures have been pro- posed (Fan et al., 2018). In reinforcement learning settings, generating natural language has been used as an intermediate planning step (Sharma et al., 2021; Hu et al., 2019), and in particular in goal- oriented dialogue (Yarats and Lewis, 2018) and open-domain QA (Adolphs et al., 2021) as well. For summarization tasks, the work of Baziotis et al. (2019) proposes an intermediate autoencoder latent representation. Similarly, West et al. (2019) ap- ply the information bottleneck principle to find an intermediate compressed sentence that can best pre- dict the next sentence. For knowledge-grounded dialogue, an approach using internet search can also be seen as a modular intermediate step, where the search query is first generated (Komeili et al., 2021). In that sense retrieval-based QA has also been seen as a modular technique in many studies (Chen et al., 2017; Yan et al., 2019). Previous work has also explored the intersection of QA and dialogue models from multiple different angles. The DREAM dataset (Sun et al., 2019) con- sists of multiple-choice questions about a conver- sation. Yang and Choi (2019) propose a question- answering task based on dialogue histories of the TV show Friends. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) datasets are de- signed to have the questions asked in the conver- sational flow, with possibly, multiple follow-ups. However, while these datasets require a model to understand a dialogue’s history, the target re- sponses are short-form answers. Therefore, these tasks do not train a dialogue model that gener- ates an engaging, conversationally appropriate re- sponse; instead, they result in a QA model that understands dialogue-structured context. Conclusion In this work, we presented K2R: a modular ap- proach for knowledge-based dialogue models. We showed that by decomposing the knowledge step and response generation into explicit sequence-to- sequence subtasks, we could improve dialogue sys- tems by incorporating knowledge or turning short QA model answers into an appropriate conversa- tional form. In detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answer- ing questions, and improves generation metrics on open-domain dialogue. Furthermore, it allows for more interpretable results and supports knowledge injection. Future work should continue to investi- gate methods with modular reasoning steps to help in difficult language tasks.",1
"Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this Ã¢ÂÂreasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination wherein a model generates factually inaccurate statements Ã¢ÂÂ has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our modelÃ¢ÂÂs output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the modelÃ¢ÂÂs output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models Ã¢ÂÂ without any fine-tuning Ã¢ÂÂ with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval(Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as hallucination, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate this problem by using recent advancements in retrieval- augmented generative models developed for open- domain dialogue (Dinan et al., 2019). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions and situations. The modular modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the modelÃ¢ÂÂs output and the possibility for knowledge injections. The modular modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the models output and the possibility for knowledge injections. The modular models task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the models output and the possibility for knowledge injections. The modular intermediate step Ã¢ÂÂ wherein and due to the dialogue response result in the modified response. While this approach is not required a priori, it is recommended that learners develop and evaluate the modified response in a zero-shot setting. After this reasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the modular intermediate step wherein and due to the dialogue response result in the modified response. While this approach is not required a priori, it is recommended that learners develop and evaluate the modified response in a zero-shot setting. After this reasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue (Dinan et al., 2019). ",0
"Abstract Crowdsourcing requesters on Amazon Me- chanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they ex- pect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workers’ forums. Some of the issues causing the mass rejections stem from the requesters not tak- ing the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourc- ing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records in- formation from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation is- sues such as missing instructions or HITs that are not doable. 1 Introduction The rise of artificial intelligence has led to a surge in the need for data. Models are becoming increas- ingly more complex, needing more fine-grained data. In order to quickly collect such data, many researchers have turned to crowdsourcing. The crowdsourcing platform that is most familiar to re- questers is Amazon Mechanical Turk (AMT). Any- one in many countries can create an Amazon ac- count and offer work, in the form of HITs, to AMT workers. Desiring rapid results, requesters often post HITs quickly, not taking into account that the way in which a HIT is presented has a direct effect on the quality of the data they obtain. They may also not be aware that their actions as requesters affect the quality of the data they gather. For ex- ample, a HIT could have unclear instructions, low payment, or in some cases might not even function. The two former cases result can lead to lower data quality. Lately, this has lead requesters, seeing how to revise their HIT for better results, to refuse to pay for their earlier errorful HIT. Thus they send out mass rejections (rejecting all of the work on a given HIT regardless of its quality). While for the requester this is just restarting a HIT, the effect of this mass rejection is felt on both sides. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workers’ forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.1 Worker Payment A major issue affecting both the interaction be- tween the requester and the worker and the data quality is worker payment. Hara et al (Hara et al., 2018) have shown that the mean wage for a worker on AMT is very low, $3.13 per hour at the time, while requesters generally pay on average $11.58 per hour. This apparent disparity is due to the fact that the most abundant HITs tend to be the lower paid ones. Researchers may believe that AMT workers are willing to work for very little, and that the amount of compensation does not af- fect data quality (Buhrmester et al., 2016) (Mason and Watts, 2009). Indeed, a good worker will try to work on the better paying HITs when possible. But if it is the end of the month and rent is due, workers are obliged to take whatever HITs are available at that time even if they don’t pay well. This may result in the worker spending less time reading in- structions or in actually working on a HIT, in order to make the meager payment cover less work time. This usually results in lower quality HIT data. 2.2 Communication In addition to worker payment, the quality of com- munication between requesters and workers is fre- quently mentioned on forums such as Turkopticon (Hanrahan et al., 2021). Good communication is important when a worker wants to tell a requester about issues they have when attempting to do a HIT or when they want to find out why their work has been rejected. Good communication can lead to better data quality since it not only provides the worker with a better idea of what the requester re- ally wants, but also helps the requester to be aware of any issues with a given HIT. 2.3 Rejecting Work Rejections are a problem for both the requester and the worker. If a requester rejects work and does not use some subset of the data they collected, they may need to re-publish their HIT so that they can fulfill the originally desired quantity of data. This delays their work on their results. For the worker, if their work is rejected, their reputation suffers and they may not meet certain qualifications like HIT approval percentage for well-paying HITs (Hara et al., 2018). Even though rejections constitute a small percentage of work (Hara et al., 2018), real workers who have honestly put effort into their work should be paid for that time. On the other hand, requesters feel justified in rejecting work that has obviously come from bots. While this seems reasonable, lately the creators of those bots have started to write requester reviews, thus tarnishing the reputation of the requester. AMT suggests that when a requester is sure that a specific worker ac- count is a bot, they should pay them and at the same time report them to AMT. There are several other factors that affect the quality of the work, such as HITs that do not work (buggy), HITs that have no standard for evaluation (McInnis et al., 2016) (Schmidt and Jettinghoff, 2016) and poor instructions. The aforementioned websites and worker forums are used, by about 60% of all workers (Yang et al., 2018). Thus it is important to be attentive to worker feedback. Conclusion This paper reveals that, for a significant amount of HITs, there is a disconnect between the workers and the requesters. Payment is often not fair in both the workers’ eyes and according to federal wage standards. HIT instructions, communication, and understanding of rejections need improvement for requesters to express what they are actually looking to obtain and thus for them to find that their data is of the quality they were expecting.",1
"Abstract Crowdsourcing requesters on Amazon Me- chanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they ex- pect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workersÃ¢ÂÂ forums. Some of the issues causing the mass rejections stem from the requesters not tak- ing the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourc- ing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records in- formation from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation is- sues such as missing instructions or HITs that are not doable. 1 Introduction The rise of artificial intelligence has led to a surge in the need for data. Models are becoming increas- ingly more complex, needing more fine-grained data. In order to quickly collect such data, many researchers have turned to crowdsourcing. The crowdsourcing platform that is most familiar to re- questers is Amazon Mechanical Turk (AMT). Any- one in many countries can create an Amazon ac- count and offer work, in the form of HITs, to AMT workers. Desiring rapid results, requesters often post HITs quickly, not taking into account that the way in which a HIT is presented has a direct effect on the quality of the data they obtain. They may also not be aware that their actions as requesters affect the quality of the data they gather. For ex- ample, a HIT could have unclear instructions, low payment, or in some cases might not even function. The two former cases result can lead to lower data quality. Lately, this has lead requesters, seeing how to revise their HIT for better results, to refuse to pay for their earlier errorful HIT. Thus they send out mass rejections (rejecting all of the work on a given HIT regardless of its quality). While for the requester this is just restarting a HIT, the effect of this mass rejection is felt on both sides. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workersÃ¢ÂÂ forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.1 Worker Payment A major issue affecting both the interaction be- tween the requester and the worker and the data quality is worker payment. Hara et al (Hara et al., 2018) have shown that the mean wage for a worker on AMT is very low, $3.13 per hour at the time, while requesters generally pay on average $11.58 per hour. This apparent disparity is due to the fact that the most abundant HITs tend to be the lower paid ones. Researchers may believe that AMT workers are willing to work for very little, and that the amount of compensation does not af- fect data quality (Buhrmester et al., 2016) (Mason and Watts, 2009). Indeed, a good worker will try to work on the better paying HITs when possible. But if it is the end of the month and rent is due, workers are obliged to take whatever HITs are available attime even if they donÃÂ¢ÃÂÃÂt pay well. This may result in the worker spending less time reading in- structions or in actually working on a HIT, in order to make the meager payment cover less work time. This usually results in lower quality HIT data. 2.2 Communication In addition to its regular nature, the HITs themselves are communicative. While for the requester this is just restarting a HIT, the effect of this mass rejection is felt on both sides. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workersÃ¢ÂÂ forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.1 Worker Payment A major issue affecting both the interaction be- tween the requester and the worker and the data quality is worker payment. Hara et al (Hara et al., 2018) have shown that the mean wage for a worker on AMT is very low, $3.13 per hour at the time, while requesters generally pay on average $11.58 per hour. This apparent disparity is due to the fact that the most abundant HITs tend to be the lower paid ones. Researchers may believe that AMT workers are willing to work for very little, and that the amount of compensation does not af- fect data quality (Buhrmester et al., 2016) (Mason and Watts, 2009). Indeed, a good worker will try to work on the better paying HITs when possible. But if it is the end of the month and rent is due, workers are obliged to take whatever HITs are available at that time even if they donÃ¢ÂÂt pay well. This may result in the worker spending less time reading in- structions or in actually working on a HIT, in order to make the meager payment cover less work time. This usually results in lower quality HIT data. 2.3 Communication In addition to its regular nature, the interaction between requesters and workers is text-based.",0
"Abstract Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models’ weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model’s size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scripts to reproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning.1 Both methods suggest pruning BERT-Base while fine-tuning to downstream tasks paired with model distillation, and present results showing 90% sparsity for several tasks. However, both methods require a long fine-tuning time as well as tuning pruning related hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we present better or comparable results at a much lower computation budget at the transfer learning phase. Gordon et al. [2020] explored the effect of weight pruning during transfer learning and concluded that pruning BERT-Base at the pre-training phase does not degrade the performance of the model compared to pruning at fine-tuning. We improve upon the suggested method and present better results at a much higher sparsity ratio. Chen et al. [2020] explored the Lottery Ticket Hypothesis [Frankle and Carbin, 2018] for BERT pre-trained models. More specifically, they analyzed the possibility of finding winning tickets in a BERT-Base pre-trained model that transfer to other downstream tasks. The authors concluded that winning tickets found while pre-training on a Masked-LM task, transfer well to other downstream tasks. Lagunas et al. [2021] presented a structured pruning method, removing rows, columns and attention heads, while achieving less than 1% loss in F1 for a BERT architecture on SQuADv1.1. Mishra et al. [2021] performed structured 2:4 pruning on BERT while further pre-training BERT; The method produced a 50% sparse model which can be fine-tuned without accuracy loss. Michel et al. [2019] explored the significance of each head in the multi-head attention mechanism of BERT and presented a method for pruning attention heads with their associated weights. Other works propose knowledge distillation to compress Transformer models to a smaller dense counter part that can be tuned to downstream tasks [Sanh et al., 2019, Jiao et al., 2020, Sun et al., 2020]. Quantization of Transformer-based language models is also a well known method for compression. Shen et al. [2020] proposes a method to quantize BERT at a different bit-width per layer. Other works implement quantization-aware training to quantize BERT to 8bits [Kim et al., 2021, Zafrir et al., 2019]. Zhang et al. [2020] created a method of producing a ternary weight BERT. Kim and Hassan [2020] presented a compression pipeline for Transformer models that includes model distillation, quantization and head pruning. Conclusion and future work We introduced Prune OFA, an architecture-agnostic method for producing sparse pre-trained language models. We also showed how these sparse models can be used to obtain fine-tuned sparse models without the burden of task-specific pruning. Our results suggest that using these sparse pre-trained models for transfer learning produces results with minimal performance degradation loss w.r.t their dense counterpart for a variety of NLP tasks. We further demonstrated that integrating quantization can lead to more efficient sparse and quantized models at a small cost to the model’s accuracy. A possible direction for future research is to explore whether a large and sparse pre-trained model is better at capturing and transferring natural language knowledge than a smaller dense model of the same architecture with similar non-zero parameters count. We hope that the release of our code and sparse pre-trained models to the community will help develop more efficient models.",1
"Abstract Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse modelsÃ¢ÂÂ weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the modelÃ¢ÂÂs size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scriptsreproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning.1 Both methods suggest pruning BERT-Base, BERT-Large and DistilBERT while maintaining their sparsity pattern. However, both methods require a long fine-tuning time as well as tuning pruning related hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we prune the model once for all tasks once pre-training. This method produces a 50% sparsity ratio with minimal accuracy loss for BERT-Base, BERT-Large and DistilBERT. The method also produces a 50% sparse pre-trained model which can be fine-tuned without accuracy loss. The method produced a 50% sparse model with minimal accuracy loss for BERT-Base, BERT-Large and DistilBERT. We improve upon the method and present a 50% sparse model with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scripts to reproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning.1 Both methods suggest pruning BERT-Base while maintaining their sparsity pattern, and present results showing 90% sparsity for several tasks. However, both methods require a long fine-tuning time as well as tuning pruning hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we present better or comparable results at a much lower computation budget at the transfer learning phase. Gordon et al. [2020] explored the effect of weight pruning during transfer learning and concluded that pruning BERT-Base at the pre-training phase does not degrade the performance of the model compared to pruning at fine-tuning. We improve upon the suggested method and present better results at a much higher sparsity ratio. Chen et al. [2020] explored the Lottery Ticket Hypothesis [Frankle and Carbin, 2018] for BERT pre-trained models. More specifically, they analyzed the possibility of finding winning tickets in a BERT-Base pre-trained model that transfer to other downstream tasks. The authors concluded that winning tickets found while pre-training on a Masked-LM task, transfer well to other downstream tasks. Lagunas et al. [2021] presented a structured pruning method, removing rows, columns and attention heads, while achieving less than 1% loss in F1 for a BERT architecture on SQuADv1.1. Mishra et al. [2021] performed structured 2:4 pruning on BERT while further pre-training BERT; The method produced a 50% sparse model which can be fine-tuned without accuracy loss.",0
"Abstract Virtual Adversarial Training (VAT) has been effective in learning robust models under su- pervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se- mEval2018 multilabel and multilingual emo- tion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models. Introduction Emotion recognition is an active and crucial area of research, especially for social media platforms. Un- derstanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, ha- tred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text under- standing has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, an- notating emotion categories is expensive and time consuming as emotion categories are highly cor- related and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like ""anger"" and ""sadness"" are co- related and co-occur more frequently than ""anger"" and ""happiness"" (Plutchik, 1980). In a multilingual setup, the annotation becomes even more challeng- ing as annotator team are expected to be familiar with different languages and culture for understand- ing the emotions accurately. Imbalance in availabil- ity of the data across languages further creates prob- lems, especially in case of resource impoverished languages. In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework. We formulate semi-supervised Virtual Adversarial Training (VAT) (Miyato et al.,2018) for multilabel emotion classification using contextual models and perform extensive exper- iments to demonstrate that unlabelled data from other languages Lul = {L1, L2, . . . , Ln} improves the classification on the target language Ltgt. We obtain competitive performance by reducing the amount of annotated data demonstrating cross- language learning. To effectively leverage the multilingual content, we use multilingual contex- tual models for representing the text across lan- guages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following: We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classifi- cation on multilingual corpus. Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Ara- bic, Spanish and English by leveraging un- labelled data of other languages while using 10% of labelled samples. Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respec- tively. Experiments showcasing the advantages of domain-adaptive and task-adaptive pre- training. Related Work Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and un- supervised learning by leveraging the informa- tion hidden in large amount of unlabelled data along with small amount of labelled data (Yang et al., 2021), (Van Engelen and Hoos, 2020). Early approaches used self-training for leveraging the model’s own predictions on unlabelled data to obtain additional information during training (Yarowsky,1995)(McCloskyetal.,2006). Clark et al. (2018) proposed cross-view training (CVT) for various tasks like chunking, dependency pars- ing, machine translation and reported state-of-the- art results. CVT forces the model to make consis- tent predictions when using the full input or partial input. Ladder networks (Laine and Aila, 2016), Mean Teacher networks (Tarvainen and Valpola, 2017) are another way for semi-supervised learn- ing where temporal and model-weights are ensem- bled. Another popular direction towards semi- supervised learning is adversarial training where the data point is perturbed with random or care- fully tuned perturbations to create an adversarial sample. The model is then encouraged to main- tain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models (Goodfellow et al., 2014), (Xiao et al., 2018), (Saadatpanah et al., 2020) to pre- vent attacks. Miyato et al. (2016), Cheng et al. (2019), Zhu et al. (2019) showed that adversarial training can improve both robustness and gener- alization for classification tasks, machine transla- tion and GLUE benchmark respectively. Miyato et al. (2016), Sachan et al. (2019), Miyato et al. (2018) then applied the adversarial training for semi-supervised image and text classification show- ing substantial improvements. Emotion recognition is an important problem and has received lot of attention from the com- munity (Yadollahi et al., 2017), (Sailunaz et al., 2018). The taxonomies of emotions suggested by Plutchik wheel of emotions (Plutchik, 1980) and (Ekman, 1984) have been used by the majority of the previous work in emotion recognition. Emo- tion recognition can be formulated as a multiclass problem (Scherer and Wallbott, 1994), (Moham- mad, 2012) or a multilabel problem (Mohammad et al., 2018), (Demszky et al., 2020). In the multi- class formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emo- tion can be present in the text instance. Binary relevance approach (Godbole and Sarawagi, 2004) is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation be- tween emotions. Seq2Seq approaches (Yang et al., 2018), (Huang et al., 2021) solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis (Felbo et al., 2017), hash- tags (Mohammad, 2012) or pretraining emotion specific embeddings and language models (Barbi- eri et al., 2021), (Ghosh et al., 2017). With the emergence of contextual models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) etc., the field of NLP and text classifi- cation has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different lan- guages and domains (Hassan et al., 2021), (Bar- bieri et al., 2021). Social media content contains linguistic errors, idiosyncratic styles, spelling mis- takes, grammatical inconsistency, slangs, hashtags, emoticons etc. (Barbieri et al., 2018), (Derczynski et al., 2013) due to which off-the-shelf contextual models may not be optimum. We use language- adaptive, domain-adaptive and task-adaptive pre- training which has shown performance gains (Pe- ters et al., 2019), (Gururangan et al., 2020), (Barbi- eri et al., 2021), (Howard and Ruder, 2018), (Lee et al., 2020) for different tasks and domains. Conclusion In this work, we explored semi-supervised learning using Virtual Adversarial Training (VAT) for multi- label emotion classification in a multilingual setup and showed performance improvement by leverag- ing unlabelled data from different languages. We used Mean Square Error (MSE) as the divergence measure for leveraging VAT for multilabel emotion classification. We also evaluated the performance of monolingual, multilingual and domain-adaptive and task-adaptive multilingual contextual models across three languages - English, Spanish and Ara- bic for multilabel and multilingual emotion recog- nition and obtained state-of-the-art results. We also performed probing experiments for understanding the impact of different layers of contextual models.",1
"Abstract Virtual Adversarial Training (VAT) has been effective in learning robust models under su- pervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se- mEval2018 multilabel and multilingual emo- tion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models. Introduction Emotion recognition is an active and crucial area of research, especially for social media platforms. Un- derstanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, ha- tred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text under- standing has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, an- notating emotion categories is expensive and time consuming as emotion categories are highly cor- related and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like ""anger"" and ""sadness"" are co- related and co-occur more frequently than ""anger"" and ""happiness"" (Plutchik, 1980). In a multilingual setup, the annotation becomes even more challeng- ing as annotator team are expected to be familiar with different languages and culture for understand- ing the emotions accurately. Imbalance in availabil- ity of the data across languages further creates prob- lems, especially in case of resource impoverished languages. In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework. We formulate semi-supervised Virtual Adversarial Training (VAT) (Miyato et al.,2018) for multilabel emotion classification using contextual models and perform extensive exper- iments to demonstrate that unlabelled data from other languages Lul = {L1, L2, . . . , Ln} improves the classification on the target language Ltgt. We obtain competitive performance by reducing the amount of annotated data demonstrating cross- language learning. To effectively leverage the multilingual content, we use multilingual contex- tual models for representing the text across lan- guages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following: We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classifi- cation on multilingual corpus. Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Ara- bic, Spanish and English by leveraging un- labelled data of other languages while using 10% of labelled samples. Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish,and English respec- tively. Experiments showcasing the advantages of domain-adaptive and task-adaptive pre- training. Related Work Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and un- supervised learning by leveraging the informa- tion hidden in large amount of unlabelled data along with small amount of labelled data (Yang et al., 2021), (Van Engelen and Hoos, 2020). Early approaches used self-training for leveraging the modelÃ¢ÂÂs own predictions on unlabelled data to obtain additional information during training (Yarowsky,1995)(McCloskyetal.,2006). Clark et al. (2018) proposed cross-view training (CVT) for various tasks like chunking, dependency pars- ing, machine translation and reported state-of-the- art results. CVT forces the model to make consis- tent predictions when using the full input or partial input. Ladder networks (Laine and Aila, 2016), Mean Teacher networks (Tarvainen and Valpola, 2017) are another way for semi-supervised learn- ing where temporal and model-weights are ensem- bled. Another popular direction towards semi- supervised learning is adversarial training where the data point is perturbed with random or care- fully tuned perturbations to create an adversarial sample. The model is then encouraged to main- tain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models (Goodfellow et al., 2014), (Xiao et al., 2018), (Saadatpanah et al., 2020) to pre- vent attacks. Miyato et al. (2016), Cheng et al. (2019), Zhu et al. (2019) showed that adversarial training can improve both robustness and gener- alization for classification tasks, machine transla- tion and GLUE benchmark respectively. Miyato et al. (2016), Sachan et al. (2019), Miyato et al. (2018) then applied the adversarial training for semi-supervised image and text classification show- ing substantial improvements. Emotion recognition is an important problem and has received lot of attention from the com- munity (Yadollahi et al., 2017), (Sailunaz et al., 2018). The taxonomies of emotions suggested by Plutchik wheel of emotions (Plutchik, 1980) and (Ekman, 1984) have been used by the majority of the previous work in emotion recognition. Emo- tion recognition can be formulated as a multiclass problem (Scherer and Wallbott, 1994), (Moham- mad, 2012) or a multilabel problem (Mohammad et al., 2018), (Demszky et al., 2020). In the multi- class formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emo- tion can be present in the text instance. Binary relevance approach (Godbole and Sarawagi, 2004) is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation be- tween emotions. Seq2Seq approaches (Yang et al., 2018), (Huang et al., 2021) solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis (Felbo et al., 2017), hash- tags (Mohammad, 2012) or pretraining emotion specific embeddings and language models (Barbi- eri et al., 2021), (Ghosh et al., 2017). With the emergence of contextual models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) etc., the field of NLP and text classifi- cation has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different lan- guages and domains (Hassan et al., 2021), (Bar- bieri et al., 2021). Social media content contains linguistic errors, idiosyncratic styles, spelling mis- takes, grammatical inconsistency, slangs, hashtags, emoticons etc. (Barbieri et al., 2018), (Derczynski et al., 2013) due to which off-the-shelfmodels may not be optimum. We use language- adaptive, domain-adaptive and task-adaptive pre- training which has shown performance gains (Pe- ters et al., 2019), (Gururangan et al., 2020), (Barbi- eri et al., 2021), (Howard and Ruder, 2018), (Lee et al., 2020) for different tasks and domains. Conclusion In this work, we explore semi-supervised learning across different languages for multilabel classification. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. ",0
"Abstract African languages still lag in the advances of Natu- ral Language Processing techniques, one reason be- ing the lack of representative data, having a tech- nique that can transfer information between lan- guages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to cre- ate cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where se- mantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evalu- ate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic qual- ity of the embeddings to determine if there is im- provement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation Introduction Many African languages have insufficient language resources (data, tools, people) (Abbott & Martinus 2019, Martinus & Abbott 2019, Nekoto et al. 2020, Sefaraetal. 2021) and fall into the classification of low resource languages (Ranathunga et al. 2021) in the Natural Language Processing (NLP) field. This lack of resources makes it harder to capi- talise on the recent advances in many NLP down- stream tasks such as Neural Machine Transla- tion (Choetal. 2014), Large Language Models (Devlin et al. 2018, Howard & Ruder 2018), Q&A systems (Kwiatkowskietal. 2019), etc. There may be more downstream approaches to deal with some of these challenges such as Transfer Learning (Ruder et al. 2019), Data Augmentation (Marivate & Sefara 2020a), Multilingual Models (Hedderich et al. 2020), etc. Additionally, the lack of research attention to existing NLP tech- niques results in difficulties finding a benchmark (Abbott & Martinus 2019). In this work, we focus on word representations through word embeddings and how we can leverage one language to assist in the representation of another related language. These embeddings can then be used to develop tools for other downstream tasks. Word Embeddings are a mathematical technique to learn general language vector representations from a large amount of unlabelled text using co-occurring statistics. In recent years, monolingual word em- beddings techniques are increasingly becoming an important resource in NLP. Word embeddings are widely used in NLP problems such as sen- timent analysis (Socher et al. 2013), named-entity- recognition (Guo et al. 2014), parts-of-speech tag- ging, and document retrieval. Word2Vec is a vector training model proposed by Mikolov et al. (2013). Word2Vec produces a low-dimensional real-value vector representing the meaning of a word. The word vector represents grammatical and semantic properties, which results in words with similar se- mantic relations being close to each other. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). Cross-lingual word embeddings have been receiv- ing more and more attention from the NLP com- munity, mainly because it has provided a path to effectively align two disjoint monolingual embed- dings with no bilingual dictionary for unsuper- vised techniques or no more than a small bilingual dictionary for supervised techniques (Lample et al. 2018, Artetxeetal. 2020). Cross-lingual tech- niques also enable knowledge transfer between lan- guages with rich resources and low resources. For languages lacking bilingual parallel corpus with other languages, cross-lingual embeddings can be utilised to train high-quality cross-lingual embed- dings (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Artetxe et al. (2018) created the cross- lingual unsupervised or supervised word embed- ding (VecMap library) approach for training cross- lingual word embedding models. The approaches can be used to construct cross-language word vec- tors with or without a bilingual dictionary. The majority of South African languages lag bilin- gual parallel corpus with other languages. In this work, we aim to investigate how cross-lingual em- beddings could be used to improve the state of one or both languages. We used data (corpus) from different domains to train Word2Vec and fastText (Bojanowski et al. 2016) monolingual embeddings. When using VecMap, the two embeddings are aligned. VecMap requires two monolingual word vectors from source and target (Artetxe et al. 2018). To evaluate the effectiveness of the cross-lingual em- bedding for Setswana and Sepedi, we use intrinsic evaluation (Bakarov 2018) through Setswana and Sepedi versions of WordSim (Finkelstein et al. 2001) and Simlex (Hill et al. 2015). This is following on an approach that has been used for Yoruba and Twi (Alabi et al. 2019). We also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment. Conneau et al. (2018) show that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: ”neighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list.” In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsic embedding was assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLWEs are resilient even when target language training data is scaled-down and that initialisation of NMLM parameters leads to good performance. Artetxe & Schwenk (2019) introduced an architec- ture that can be used to learn multilingual sentence representations for more than 90 languages. The languages belonged to 30 different families. The re- search used a single BiLSTM encoder with a shared Byte Pair Encoding (BPE) vocabulary coupled with an auxiliary decoder and trained on parallel corpora. They learn a classifier using English annotated data only and transfer it to any language without modification. The research mainly focused on vector rep- resentations of sentences that are general for the in- put language and the NLP task. Alabi et al. (2019) worked on massive vs. curated embeddings for low-resourced languages: the case of Yoru` ba ́ and Twi. Authors compare two types of word embeddings obtained from curated cor- pora and a language-dependent processing. They move further to collect high quality and noisy data for the two languages. They quantify that im- provements that is based on the quality of data and not only on the amount of data. In their ex- periments, they use different architectures to learn word representations both from characters and sur- face forms. They evaluate multilingual BERT on a down stream task, specifically named entity recog- nition and WordSim-353 word pairs dataset. Feng et al. (2018) investigates a cross-lingual knowl- edge transfer technique to improve the seman- tic representation of low-resourced languages and improving low resource named-entity recognition. In their research, neural networks are used to do knowledge transfer from high resource language us- ing bilingual lexicons to improve low resource word representation. They automatically learn semantic projections using a lexicon extension strategy that is designed to address out-of lexicon problem. Fi- nally, they regard word-level entity type distribu- tion features as an external language independent knowledge and incorporate them into their neural architecture. The experiment is done on two low resource languages (Dutch and Spanish) to demon- strate the effectiveness of these additional semantic representations. Banerjee et al. (2021) show that initialising the em- bedding layer of Unsupervised Neural Machine Translation (UNMT) models with cross-lingual embeddings shows significant improvements in BLEU score. Authors show that freezing the em- bedding layer weights lead to better gains com- pared to updating the embedding layer weights during training. They experimented using De- noising Autoencoder (DAE) and Masked Sequence to Sequence (MASS) for three different unrelated language pairs (for English-Hindi, English-Bengali, and English-Gujarati). The analysis shows the im- portance of using cross-lingual embedding as com- pared to other techniques. The literature shows that there is a substantial amount of work done on cross-lingual transfer and empirical proof that the method improves the per- formance of models. The literature does not relay solely on intrinsic evaluation but the solutions are applied to some downstream tasks. In the next sec- tion, we detail the data used for conducting experi- ments. Conclusion In this paper, VecMap was used to align Setswana- Sepedi to the same vector space. Through this work, we wanted to use cross-lingual (VecMap) technique to enable knowledge transfer between languages with rich resources and low resources. The results show that it is possible to align two monolingual embeddings to get cross-lingual embeddings. We mapped Setswana to Sepedi and used Spearman’s to check correlation. Interestingly we get different re- sults on fastText and word2Vec-based embeddings though we used the same data to train the embed- dings. In future work, it would be interesting to use the cross-lingual embedding on a downstream task like translation or sentiment analysis specifically for low-resourced languages.",1
"Abstract African languages still lag in the advances of Natu- ral Language Processing techniques, one reason be- ing the lack of representative data, having a tech- nique that can transfer information between lan- guages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to cre- ate cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where se- mantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evalu- ate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic qual- ity of the embeddings to determine if there is im- provement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation Introduction Many African languages have insufficient language resources (data, tools, people) (Abbott & Martinus 2019, Martinus & Abbott 2019, Nekoto et al. 2020, Sefaraetal. 2021) and fall into the classification of low resource languages (Ranathunga et al. 2021) in the Natural Language Processing (NLP) field. This lack of resources makes it harder to capi- talise on the recent advances in many NLP down- stream tasks such as Neural Machine Transla- tion (Choetal. 2014), Large Language Models (Devlin et al. 2018, Howard & Ruder 2018), Q&A systems (Kwiatkowskietal. 2019), etc. There may be more downstream approaches to deal with some of these challenges such as Transfer Learning (Ruder et al. 2019), Data Augmentation (Marivate & Sefara 2020a), Multilingual Models (Hedderich et al. 2020), etc. Additionally, the lack of research attention to existing NLP tech- niques results in difficulties finding a benchmark (Abbott & Martinus 2019). In this work, we focus on word representations through word embeddings and how we can leverage one language to assist in the representation of another related language. These embeddings can then be used to develop tools for other downstream tasks. Word Embeddings are a mathematical technique to learn general language vector representations from a large amount of unlabelled text using co-occurring statistics. In recent years, monolingual word em- beddings techniques are increasingly becoming an important resource in NLP. Word embeddings are widely used in NLP problems such as sen- timent analysis (Socher et al. 2013), named-entity- recognition (Guo et al. 2014), parts-of-speech tag- ging, and document retrieval. Word2Vec is a vector training model proposed by Mikolov et al. (2013). Word2Vec produces a low-dimensional real-value vector representing the meaning of a word. The word vector represents grammatical and semantic properties, which results in words with similar se- mantic relations being close to each other. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). Cross-lingual word embeddings havereceiv- ing more and more attention from the NLP com- munity, mainly because it has provided a path to effectively align two disjoint monolingual embed- dings with no bilingual dictionary for unsuper- vised techniques or no more than a small bilingual dictionary for supervised techniques (Lample et al. 2018, Artetxeetal. 2020). Cross-lingual tech- niques also enable knowledge transfer between lan- guages with rich resources and low resources. For languages lacking bilingual parallel corpus with other languages, cross-lingual embeddings can be utilised to train high-quality cross-lingual embed- dings (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Artetxe et al. (2018) created the cross- lingual unsupervised or supervised word embed- ding (VecMap library) approach for training cross- lingual word embedding models. The approaches can be used to construct cross-language word vec- tors with or without a bilingual dictionary. The majority of South African languages lag bilin- gual parallel corpus with other languages. In this work, we aim to investigate how cross-lingual em- beddings could be used to improve the state of one or both languages. We used data (corpus) from different domains to train Word2Vec and fastText (Bojanowski et al. 2016) monolingual embeddings. When using VecMap, the two embeddings are aligned. VecMap requires two monolingual word vectors from source and target (Artetxe et al. 2018). To evaluate the effectiveness of the cross-lingual em- bedding for Setswana and Sepedi, we use intrinsic evaluation (Bakarov 2018) through Setswana and Sepedi versions of WordSim (Finkelstein et al. 2001) and Simlex (Hill et al. 2015). This is following on an approach that has been used for Yoruba and Twi (Alabi et al. 2019). We also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment. Conneau et al. (2018) show that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: Ã¢ÂÂneighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list.Ã¢ÂÂ In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsicwas assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLWEs can do a good deal of the work done by Hoffer & Ruder (Ruder et al. 2013) and that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: Ã¢ÂÂneighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list.Ã¢ÂÂ In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsic embedding was assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLWEs are resilient even when target language training data is scaled-down and that initialisation of NMLM parameters leads to good performance. Artetxe & Schwenk (2019) introduced an architec- ture that can be used to learn multilingual sentence representations for more than 90 languages. The languages belonged to 30 different families. The re- search used a single BiLSTM encoder with a shared Byte Pair Encoding (BPE) vocabulary coupled with an auxiliary decoder and trained on parallel corpora. They learn a classifier using English annotated data only and transfer it to any language without modification. The research mainly focused on vector rep- resentations of sentences that are general for the in- put language and the NLP task. Alabi et al. (2019) worked on massive vs. curated embeddings for low-resourced languages: the case of Yoru` ba ÃÂ and Twi. Authors compare two types of word embeddings obtained from curated cor- pora and a language-dependent processing. They move further to collect high quality and noisy data for the two languages. They quantify that im- provements that is based on the quality of data and not only on the amount of data. In their ex- periments, they use different architectures to learn word representations both from characters and sur- face forms. They evaluate multilingual BERT on a down stream task, specifically named entity recog- nition and WordSim-353 word pairs dataset. Feng et al. (2018) investigates a cross-lingual knowl- edge transfer technique to improve the seman- tic representation of low-resourced languages and improving low resource named-entity recognition. In their research, neural networks are used to do knowledge transfer from high resource language us- ing bilingual lexicons to improve low resource word representation. They automatically learn semantic projections using a lexicon extension strategy that is designed to address out-of lexicon problem. ",0
"Abstract. Communication is compositional if complex signals can be represented as a combi- nation of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experi- mentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence. Introduction In emergent communication studies, one often considers agents who can share information about a set of objects described by the common features. Such a situation is common in multi-agent systems with partial observation (Foerster et al. (2016), Lazaridou et al. (2017), Jaques et al. (2019), Raczaszek-Leonardi et al. (2018)) and it is the major theme in signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). In a signaling game, one agent (a sender) conveys information about an object to another agent (a receiver), which then has to infer the object’s features. Typically, agents are rewarded if some of the features are correctly identified. During this process, the agents develop a communication protocol. A recent line of work has studied conditions under which compositionality emerges (Batali (1998); Kottur et al. (2017); Choi et al. (2018); Korbak et al. (2019); Li and Bowling (2019); Słowik et al. (2020b,a); Guo et al. (2020)). Compositionality is a crucial feature of natural languages and it has been investigated extensively in cognitive science (see e.g. Chomsky (1957) Fodor and Pylyshyn (1988)). It is often measured using dedicated metrics such as topographic similarity (Brighton and Kirby (2006); Lazaridou et al. (2018); Kriegeskorte (2008); Bouchacourt and Baroni (2018)), context independence Bogin et al. (2018), conflict count Kucin ́ski et al. (2020), or positional disentanglement (Chaabouni et al. (2020)). In signaling games it bears a strong resemblance to the concept of disentangled representations, see (Higgins et al. (2017), Kim and Mnih (2018), Locatello et al. (2019)). In machine learning context, compositionality is perceived as a generalization mechanism (Lake et al. (2017)) and has been used e.g. for goal composition (Jiang et al. (2019)) or knowledge transfer (Li and Bowling (2019)). In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for compositionality to emerge. A similar observation has been made by Kottur et al. (2017); however, our result is more fundamental and points out a common misconception that compositionality can be learned in a purely unsupervised way. Such a result can be perceived as a discrete analog of Locatello et al. (2019), applicable in the communication context. We then prove that adding an inductive bias in the loss function coupled with communication over a noisy channel leads to the spontaneous emergence of compositionality. This shows the catalytic role of noise in this process. Intuitively, this can be attributed to the (partial) robustness of compositional language with respect to message corruption caused by a noisy channel. We experimentally verify that a certain range of noise levels, dependent on the model and the data, promotes compositionality. We provide a wide range of experiments that illustrate the influence of different priors. For the inductive biases in the training framework, we look into the impact of the network architecture as well as implementation and temporal variation in noise. On the data side, we study the effect of scrambling visual input or its description. We also study the generalization properties of the proposed training framework. Related work The topic of communication is actively studied in multi-agent RL, see Hernandez-Leal et al. (2020, Table 2) for a recent survey. Compositionality is often investigated in the context of signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). Recent research has shown that strong inductive biases or grounding of communication protocols are necessary for the protocol to be compositional (see e.g. Kottur et al. (2017), Słowik et al. (2020b)). The inductive bias can be imposed into the architecture of the agents or the training procedure. For instance, Das et al. (2017) place pressure on agents, to use symbols consistently across varying contexts, by a frequent reset of the agent’s memory. A model-based approach was proposed by Choi et al. (2018) and Bogin et al. (2018), who build upon the obverter algorithm (Oliphant and Batali (1997), Batali (1998)). Słowik et al. (2020a) explore games with hierarchical inputs and shows how agents implemented as graph convolutional networks obtain good generalization. Korbak et al. (2019) implemented the idea of template transfer (Barrett and Skyrms, 2017) by pre-training the agents on simpler subtasks before the target task. Kirby (2001) studied the iterative learning paradigm, where each generation of agents learns the language spoken by the previous generation before starting to communicate. In the machine learning literature, this idea was explored by Li and Bowling (2019), Cogswell et al. (2019) and Ren et al. (2020) with the generation transfer typically implemented as reinitializing the weights of agents’ neural networks. Such an approach inevitably introduces noise into the learning process. This naturally leads to a question of whether the noise itself may be a sufficient mechanism of compositionality, which we will try to address in this paper. Guo et al. (2020) have shown that the choice of a game has a large impact on the properties of a communication protocol emerging in that game, foreshadowing what we call grounding. The noisy channel model of communication was famously introduced by Shannon (1948). The idea of noise as a driving force in the emergence of communication was first proposed by Nowak and Krakauer (1999), who showed that word-level compositionality is the optimal solution to the problem of communication in a noisy environment under a particular fitness function. Noise is also used in deep learning, e.g. as a regularizer (see e.g. dropout (Srivastava et al., 2014)) or a mechanism allowing backpropagation through a discrete latent (see e.g. Salakhutdinov and Hinton (2009), Kaiser and Bengio (2018)). Noise in the latter context was used in Foerster et al. (2016) in order to learn to communicate. The authors observed that it is essential for successful training. Conclusions In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for the compositionality to emerge spontaneously in signaling games. We then formulate inductive biases in the loss function and prove that they are sufficient to achieve compositionality when coupled with communication over a noisy channel. Consequently, we highlight the catalytic role of noise in the emergence of compositionality. We perform a series of experiments in order to understand different aspects of the proposed framework better. We empirically validate that, indeed, a certain range of noise levels, dependent on the model and the data, promotes compositionality. Our work is foundational research and does not lead to any direct negative applications.",1
"Abstract. Communication is compositional if complex signals can be represented as a combi- nation of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experi- mentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence. Introduction In emergent communication studies, one often considers agents who can share information about a set of objects described by the common features. Such a situation is common in multi-agent systems with partial observation (Foerster et al. (2016), Lazaridou et al. (2017), Jaques et al. (2019), Raczaszek-Leonardi et al. (2018)) and it is the major theme in signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). In a signaling game, one agent (a sender) conveys information about an object to another agent (a receiver), which then has to infer the objectÃ¢ÂÂs features. Typically, agents are rewarded if some of the features are correctly identified. During this process, the agents develop a communication protocol. A recent line of work has studied conditions under which compositionality emerges (Batali (1998); Kottur et al. (2017); Choi et al. (2018); Korbak et al. (2019); Li and Bowling (2019); SÃ Âowik et al. (2020b,a); Guo et al. (2020)). Compositionality is a crucial feature of natural languages and it has been investigated extensively in cognitive science (see e.g. Chomsky (1957) Fodor and Pylyshyn (1988)). It is often measured using dedicated metrics such as topographic similarity (Brighton and Kirby (2006); Lazaridou et al. (2018); Kriegeskorte (2008); Bouchacourt and Baroni (2018)), context independence Bogin et al. (2018), conflict count Kucin ÃÂski et al. (2020), or positional disentanglement (Chaabouni et al. (2020)). In signaling games it bears a strong resemblance to the concept of disentangled representations, see (Higgins et al. (2017), Kim and Mnih (2018), Locatello et al. (2019)). In machine learning context, compositionality is perceived as a generalization mechanism (Lake et al. (2017)) and has been used e.g. for goal composition (Jiang et al. (2019)) or knowledge transfer (Li and Bowling (2019)). In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for compositionality to emerge. A similar observation has been made by Kottur et al. (2017); however, our result is more fundamental and points out a common misconception that compositionality can be learned in a purely unsupervised way. Such a result can be perceived as a discrete analog of Locatello et al. (2019), applicable in the communication context. We then prove that adding an inductive bias in the loss function coupled with communication over a noisy channel leads to the spontaneous emergence of compositionality. This shows the catalytic role of noise in this process. Intuitively, this can be attributed to the (partial) robustness of compositional language with respect to message corruption caused by a noisy channel. We experimentally verify that a certain range of noise levels, dependent on the model and the data, promotes compositionality. We provide a wide range of experiments that illustrate the influence of different priors. For the inductive biases in the training framework, we look into the impact of the network architecture as well as implementation and temporal variation in noise. On the data side, we study the effect of scrambling visual input or its description. We also study the generalization properties of the proposed training framework. Related work The topic of communication is actively studied in multi-agent RL, see Hernandez-Leal et al. (2020, Table 2) for a recent survey. Compositionality is often investigated in the context of signaling games (Fudenberg and Tirole (1991),(1969), Skyrms (2010), Lazaridou et al. (2018)). Recent research has shown that strong inductive biases or grounding of communication protocols are necessary for the protocol to be compositional (see e.g. Kottur et al. (2017), SÃ ÃÂowik et al. (2020b)). The inductive bias can be imposed into the architecture of the agents or the training procedure. For instance, Das et al. (2017) place pressure on agents, to use symbols consistently across varying contexts, by a frequent reset of the agentÃ¢ÂÂs memory. A model-based approach was proposed by Choi et al. (2018) and Bogin et al. (2018), who build upon the obverter algorithm (Oliphant and Batali (1997), Batali (1998)). SÃ Âowik et al. (2020a) explore games with hierarchical inputs and shows how agents implemented as graph convolutional networks obtain good generalization. Korbak et al. (2019) implemented the idea of template transfer (Barrett and Skyrms (2019)) by pre-training the agents on simpler subtasks before the target task. Kirby (2001) studied the iterative learning paradigm, where each generation of agents learns the language spoken by the previous generation before starting to communicate. In the machine learning literature, this idea was explored by Li and Bowling (2019), Cogswell et al. (2019) and Ren et al. (2020) with the generation transfer typically implemented as reinitializing the weights of agentsÃ¢ÂÂ neural networks. Such an approach inevitably introduces noise into the learning process. This naturally leads to a question of whether the noise itself may be a sufficient mechanism of learning, which we will try to address in this paper. Guo et al. (2020) have shown that the choice of a game has a large impact on the properties of a communication protocol emerging in that game, foreshadowing what we call grounding. The noisy channel model of communication was famously introduced by Shannon (1948). The idea of noise as a driving force in the emergence of communication was first proposed by Nowak and Krakauer (1999), who showed that word-level compositionality is the optimal solution to the problem of communication in a noisy environment under a particular fitness function. Noise is also used in deep learning, e.g. as a regularizer (see e.g. dropout (Srivastava et al., 2014)) or a mechanism allowing backpropagation through a discrete latent (see e.g. Salakhutdinov and Hinton (2009), Kaiser and Bengio (2018)). Noise in the latter context was used in Foerster et al. (2016) in order to learn to communicate. The authors observed that it is essential for successful training. Conclusions In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for the compositionality to emerge spontaneously in signaling games. We then formulate inductive biases on both the training framework and the data, and prove that inductive biases on both the training framework and the data are needed for the compositionality to emerge spontaneously in signaling games. Consequently, we highlight the need for a robust inductive bias in signaling games. A strong inductive bias is necessary for the compositionality to emerge spontaneously in signaling games, for both the agents and the training procedure.",0
"Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Lee et al., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, Epsilon Non-Negative rectification (ENN) and Prox- imal Alternating Linearized Minimization rectification (PALM). We also propose the Low-rank Anchor Word algorithm (LAW) that learns the latent topics and their cor- relations only from the compressed statistics, guaranteeing the same performance as the original Anchor Word algo- rithm under a certain condition. Our experiments show that applying LAW after ENN learns topics of quality compara- ble to using AW after AP based on the full co-occurrence. We then introduce the Low-Rank Joint Stochastic Matrix Factorization pipeline (LR-JSMF) that first adopts a ran- domized algorithm to construct a low-rank approximation of the full co-occurrence C directly from the raw data; then performs ENN and LAW. While PALM needs access to the full co-occurrence, ENN can work solely with a low-rank initialization, eliminating the burden to ever construct a full co-occurrence matrix. This new pipeline scales to large vocabularies that were previously intractable for spectral inference, and offers a 10x∼100x speedup over previous methods on various textual and non-textual datasets. Note that second-order spectral topic models often rely on the separability assumption that forces at least one anchor word for each topic. This has led to criticism in theory despite their superior performance in practice compared to probabilistic counterparts (Lee et al., 2017) and third-order tensor models (Lee et al., 2019). As most topic models with large vocabularies are proven separable (Ding et al., 2015), we show that our capability to process large vocabularies not only fits for modern datasets, but also alleviates the theoreti- cal limitation. In addition, we also develop a new approach that helps better interpretation of topics by jointly reading characteristic words as well as traditional prominent words. By defining the characteristic words as the terms that are highly associated with each anchor word, we design a graph- based metric that can measure the degree of incoherence in individual topics. To the best of our knowledge, this work makes the first principled attempt to utilize anchor words for quantitative and qualitative interpretations of topics with the prominent words. Given our on-the-fly methods, users are now capable of efficiently understanding latent topics and their correlations from noisy co-occurrence statistics within time and space complexity linear in the size of vocabulary. Conclusion Spectral algorithms provide appealing alternatives for iden- tifying interpretable low-rank subspaces by simple factoriza- tions of higher-order co-occurrence data. But this simplicity is also a weakness: the size of the co-occurrence limit us to small vocabularies, and these methods perform poorly without rectifications that previously suffered quadratic scal- ing. Anchor words are guaranteed to be exclusive to the corresponding topics, but they are rarely used for topic inter- pretations because they are often chosen as too rare terms. We develop a robust and scalable pipeline: Low-Rank Joint Stochastic Matrix Factorization based on our two comple- mentary on-the-fly rectification methods (ENN/PALM) and a sufficiently general low-rank inference algorithm (LAW). These methods simultaneously compress and rectify the co- occurrence from raw data; learn high-quality topics from the compressed matrix factorization; and achieve low-rank non- negative approximations without quadratic blowup. They also provide orders of magnitude speedups for rectification even on small vocabularies. In addition, we verify that us- ing large vocabularies benefits inference quality by better satisfying the separability assumption. It also improves model interpretability by jointly understanding the promi- nent words with the characteristic words, and by measuring our MST-Incoherence metric for individual topics. Given all these new development, we can now learn and evalu- ate useful low-dimensional structures in high-dimensional datasets on laptop-grade hardware, massively increasing the applicability and potential use of the spectral algorithms.",1
"Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Leeal., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also propose a new approach that learns posterior con- figurations from the compressed statistics, enabling users to better interpret noisy and com- plex data even if the data is synthesized from the compressed statistics. Our experiments show that applying the p- ulary algorithm on a corpus of jade-white words grows the size of a football field even when the words are synthesized from the compressed statistics, rendering the data useless for neural networks. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015).",0
"Abstract Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive– definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extrac- tion and generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT- based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl- ative definitional information (i.e., definitions of related terms). For generation, we design a BART- based definition generator to produce the final defi- nition by incorporating the extracted knowledge. From another perspective, we propose to reform the problem of definition modeling, which is pre- viously mainly defined as generating definitions of terms conditioned with a target term and a given context. Instead, we restudy this problem as defin- ing terms with extracted knowledge. This setting is in line with human behavior: to understand a term, compared to reading the given sentence it is used in, it is more straightforward and helpful to search and read its relevant content on the Internet. Our framework for definition modeling is simple and flexible that can easily be further expanded by leveraging more advanced language models. Ex- perimental results demonstrate our simple model outperforms state-of-the-art models significantly (e.g., BLEU score from 8.76 to 22.66, human an- notated score from 2.34 to 4.04), with several inter- esting findings: 1) for computer science terms, our extractive model can achieve performance compa- rable to (even better than) state-of-the-art abstrac- tive models; 2) both self- and correlative defini- tional information are significant to define a term; 3) the quality of definitions generated by our best model is high, while the state-of-the-art models suffer severely from hallucinations, i.e., generating irrelevant or contradicted facts. Our contributions are summarized as follows:  As far as we know, we are the first to connect and combine definition extraction and definition generation– a simple idea that can significantly improve the performance of definition modeling.  We propose to restudy definition modeling as generating definitions of terms with extracted knowledge. We design a novel framework for definition modeling by incorporating both self- and correlative definitional information of terms. We publish two datasets for technical terms, along with definitions of ~75,600 computer science terms generated by our model. Related Work Definition Extraction. Existing works for defini- tion extraction can be roughly divided into three cat- egories: 1) rule-based, which extracts definitions with defined linguistic rules and templates (Klavans and Muresan, 2001; Cui et al., 2004; Fahmi and Bouma, 2006); 2) machine learning-based, which extracts definitions by statistical machine learning with carefully designed features (Westerhout, 2009; Jin et al., 2013); 3) deep learning-based, the state- of-the-art approach for definition extraction, which is based on deep learning models such as CNN, LSTM, and BERT (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020). Definition Generation. Definition generation, or definition modeling, was first introduced in (No- raset et al., 2017), which aims to generate defi- nitions of words with word embeddings. Later works on definition generation put more empha- sis on generating definitions of words/phrases with given contexts (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019; Li et al., 2020;Reid et al., 2020; Bevilacqua et al., 2020). There are also recent works on definition modeling for other languages, e.g., Chinese, by incorporating the special properties of the specific language (Yang et al., 2020; Zheng et al., 2021). Conclusion In this paper, we combine extraction and gener- ation for definition modeling. We show that, by incorporating extracted self- and correlative def- initional information, the generator can produce high-quality definitions for technical terms. Ex- perimental results demonstrate the effectiveness of our framework. As future work, we plan to apply our methods to more domains and construct several online domain dictionaries.",1
"Abstract Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractiveÃÂ¢ÃÂÃÂ definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extrac- tion and generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT- based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl- ative definitional information (i.e., definitions of related terms). For generation, we design a BART- based definition generator to produce the final defi- nition by incorporating the extracted knowledge. From another perspective, we propose to reform the problem of definition modeling, which is pre- viously mainly defined as generating definitions of terms conditioned with a target term and a given context. Instead, we restudy this problem as defin- ing terms with extracted knowledge. This setting is in line with human behavior: to understand a term, compared to reading the given sentence it is used in, it is more straightforward and helpful to search and read its relevant content on the Internet. Our framework for definition modeling is simple and flexible that can easily be further expanded by leveraging more advanced language models. Ex- perimental results demonstrate our simple model outperforms state-of-the-art models significantly (e.g., BLEU score from 8.76 to 22.66, human an- notated score from 2.34 to 4.04), with several inter- esting findings: 1) for computer science terms, our extractive model can achieve performance compa- rable to (even better than) state-of-the-art abstrac- tive models; 2) both self- and correlative defini- tional informationsignificant to define a term; 3) the quality of definitions generated by our best model is high, while the state-of-the-art models suffer severely from hallucinations, i.e., generating irrelevant or contradicted facts. Our contributions are summarized as follows: As far as we know, we are the first to connect and combine definition extraction and definition generationÃ¢ÂÂ a simple idea that can significantly improve the performance of definition modeling. We propose to combine definition extraction and definition generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. ",0
"Abstract When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting tempo- ral misalignment can degrade end-task perfor- mance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and re- views) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today’s pretraining-finetuning paradigm, this misalignment can affect a pretrained language model—a situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021; Peters et al., 2018; Raffel et al., 2020; Röttger and Pierrehumbert, 2021)—or the finetuned task model, or both. We suspect that the effects of temporal misalignment will vary depending on the genre or domain of the task’s text, the nature of that task or application, and the specific time periods. We focus primarily on measuring the extent of temporal misalignment on task performance. We consider eight tasks, each with datasets that span at least five years (§2.4), ranging from summarization to entity typing, a subproblem of entity recognition (Grishman and Borthwick, 1999). Notably, these task datasets span four different domains: social media, scientific articles, news, and reviews. We introduce an easily interpretable metric that summa- rizes the rate at which task performance degrades as function of time. Our research questions are:(Q1) how does temporal misalignment affect downstream tasks over time? (Q2) how does sensitivity to temporal misalign- ment vary with text domain and task? (Q3 )how does temporal misalignment affect lan- guage models across domains and spans of time? (Q4) how effective is temporal adaptation, or ad- ditional pretraining on a target year, in miti- gating temporal misalignment? We find that temporal misalignment affects both language model generalization and task perfor- mance. We find considerable variation in degra- dation across text domains (§3.2) and tasks (§3.1). Over 5 years, classifiers’ F1 score can deteriorate as much as 40 points (political affiliation in Twitter) or as little as 1 point (Yelp review ratings). Two distinct tasks defined on the same domain can show different levels of degradation over time. We explore domain adaptation of a language model, using temporally selected (unannotated) data, as a way to curtail temporal misalignment (Röttger and Pierrehumbert, 2021). We find that this does not offer much benefit, especially relative to performance that can be achieved by finetuning on temporally suitable data (i.e., from the same time period as the test data). We conclude that tem- poral adaptation should not be seen as a substitute for finding temporally aligned labeled data. The evidence and benchmarks we offer motivate careful attention to temporal misalignment in many applications of NLP models, and further research on solutions to this problem. Contributions. To facilitate the study of tempo- ral misalignment phenomenon on downstream ap- plications, we compile a suite of eight diverse tasks across four important language domains. We de- fine an interpretable metric that summarizes tempo- ral misalignment of a model on a task with times- tamped data. Our experiments reveal key factors in how temporal misalignment affects NLP model performance. Conclusion Changes in language use over time, and how lan- guage relates to other quantities of interest in NLP applications, has clear effects on the performance of those applications. We have explored how tem- poral misalignment between training data—both data used to train LMs and annotated data used to finetune them—affects performance across a range of NLP tasks and domains, taking advantage of datasets where timestamps are available. We com- pile these datasets as a benchmark for future re- search as well. We also introduced a summary metric, TD score, that makes it easier to compare models in terms of their temporal misalignment. Our experiments revealed considerable variation in temporal degradation accross tasks, more so than found in previous studies (Röttger and Pierrehum- bert, 2021). These findings motivate continued study of temporal misalignment across applica- tions of NLP, its consideration in benchmark evalu- ations,13 and vigilance on the part of practitioners able to monitor live system performance over time. Notably, we observed that continued training of LMs on temporally aligned data does not have much effect, motivating further research to find effective temporal adaptation methods that are less costly than ongoing collection of annotated/labeled datasets over time.",1
"Abstract When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting tempo- ral misalignment can degrade end-task perfor- mance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and re- views) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today pretraining-finetuning paradigm, this misalignment can affect a pretrained language model situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021; Peters et al., 2018; Raffel et al., 2020; RoÃÂttger and Pierrehumbert, 2021)Ã¢ÂÂor the finetuned task model, or both. We suspect that the effects of temporal misalignment will vary depending on the genre or domain of the taskÃ¢ÂÂs text, the nature of that task or application, and the specific time periods. We focus primarily on measuring the extent of temporal misalignment on task performance. We consider eight tasks, each with datasets that span at least five years (ÃÂ§2.4), ranging from summarization to entity typing, a subproblem of entity recognition (Grishman and Borthwick, 1999). Notably, these task datasets span four different domains: social media, scientific articles, news, and reviews. We introduce an easily interpretable metric that summa- rizes the rate at which task performance degrades as function of time. Our research questions are:(Q1) how does temporal misalignment affect downstream tasks over time? (Q2) how does sensitivity to temporal misalign- ment vary with text domain and task? (Q3 )how does temporal misalignment affect lan- guage models across domains and spans of time? (Q4) how effective is temporal adaptation, or ad- ditional pretraining on a target year, in miti- gating temporal misalignment? We find that temporal misalignment affects both language model generalization and task perfor- mance. We find considerable variation in degra- dation across text domains (ÃÂ§3.2) and tasks (ÃÂ§3.1). Over 5 years, classifiersÃ¢ÂÂ F1 score can deteriorate as much as 40 points (political affiliation in Twitter) or as little as 1 point (Yelp review ratings). Two distinct tasks defined on the same domain can show different levels of degradation over time. We explore domain adaptation of a language model, using temporally selected (unannotated) data, as a way to curtail temporal misalignment (RoÃÂttger and Pierrehumbert, 2021). We find that this does not offer much benefit, especially relative to performance that can be achieved by finetuning on temporally suitable data (i.e., from the same time period as the test data). We conclude that tem- poral adaptation should not be seen as a substitute for finding temporally aligned labeled data. The evidence and benchmarks we offer motivate careful attention to temporal misalignment in many applications of NLP models, and further research on solutions to this problem. Contributions. To facilitate the study of tempo- ral misalignment phenomenon on downstream ap- plications, we compile a suite of eight diverse tasks across four important language domains. We de- fine an interpretable metric that summarizes tempo- ral misalignment of a model on a task withtamped data. Our experiments reveal key factors in how temporal misalignment affects NLP model performance. Conclusion Changes in language use over time, and how lan- guage relates to other quantities of interest in NLP applications, has clear effects on the performance of those applications. We have explored how tem- poral misalignment can affect NLP model performance. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time.",0
"Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learners’ writing ability, many corpus studies compare L2 learners’ writing with native experts’ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native experts’ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing students’ academic writing. However, native experts’ publication has been proofread concerning linguistic use, while students’ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of students’ writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papers’ abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liu (2017) also proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic analysis systems by Anthony and Lashkia (2003) and Dayrell et al. (2012). Hence, the micro-level linguistic features of high-impact English academic papers should be considered and analyzed to provide a greater number of dimensions for future automatic analysis and feedback systems of research articles. Thanks to the rapid development of NLP technology, many emerging methods can help automatically extract features with bigger weights beyond corpus-based studies, for example, feature engineering. It has been widely applied in fault detection (e.g., Li et al., 2021), image detection (e.g., Cai, Nee, & Loh, 1996; Cai & Chen, 2011), etc., but has not been tried in the micro-level linguistic feature extraction of research articles. Our study will employ this method to extract and validate the selected features to provide consistent and accurate predictions for journal articles with different academic impacts. Conclusion In summary, we used four feature selection methods to reduce the feature dimensions. We verified the selected 24 features, including word imagery, third-person plural pronouns, all sentence actual word overlap, adjacent sentence actual word overlap, the semantic overlap of all sentences, the semantic overlap of adjacent sentences, the ratio of old information to new information in all sentences, the overlap of all sentence nouns, text ease of vocabulary actuality, content word meaning, content word concretization, all sentence argument overlap, adjacent sentence noun overlap, modal verb, focus past, focus present, focus future, articles, numbers, positive emotions, causal vocabulary can better distinguish journal articles with different impacts. Besides, we found that based on these 24 language features, the random forest model can be more consistent and accurate to classify high- and moderate-impact journal articles. The selected linguistic features at the micro-level and machine learning model with better performance can provide reference for future automatic analysis and feedback systems of English research articles.",1
"Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learnersÃ¢ÂÂ writing ability, many corpus studies compare L2 learnersÃ¢ÂÂ writing with native expertsÃ¢ÂÂ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native expertsÃ¢ÂÂ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing studentsÃ¢ÂÂ academic writing. However, native expertsÃ¢ÂÂ publication has been proofread concerning linguistic use, while studentsÃ¢ÂÂ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of studentsÃ¢ÂÂ writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papersÃ¢ÂÂ abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liualso proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic evaluation systems by comparison (such as the Random Access Memories feature in the above studies) and has been shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learnersÃ¢ÂÂ writing ability, many corpus studies compare L2 learnersÃ¢ÂÂ writing with native expertsÃ¢ÂÂ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native expertsÃ¢ÂÂ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing studentsÃ¢ÂÂ academic writing. However, native expertsÃ¢ÂÂ publication has been proofread concerning linguistic use, while studentsÃ¢ÂÂ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent.",0
"Abstract The development of neural networks for clini- cal artificial intelligence (AI) is reliant on in- terpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clin- ical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes. The transparency and explainability of neural networks is critical to trust and acceptance in clinical environments as useful tools. Research has sought to provide improved inter- pretability of NLP models using methods to derive extractive rationales for neural networks predictions by the model itself (Bastings et al., 2019; Lei et al., 2016; Sha et al., 2020). The aim of rationale produc- tion is to increase explainability of models by extract- ing the minimal crucial input required to make a class prediction. In NLP, the rationales are subsets of the input text which maintain predictive power. One of the ways rationales have been achieved is by creating a two-module network, i.e., a selector followed by a predictor, which are trained jointly (Lei et al., 2016). Given an input x, the selector picks a subset of the input features r(x) (the rationale) by specifying a dis- tribution over the possible rationales. The predictor acts as a standard classifier, taking as input r(x) and predicting a class yˆ to compare with the ground truth class y. Sha et al. (2020) proposed InfoCal, an improved type of selector-predictor model that uses an information calibration technique and an additional guider module trained jointly with the selector and predictor in an adversarial manner. InfoCal achieves the current state-of-the-art in rationale extraction on tasks such as sentiment analysis and legal judgment prediction, hence we are interested to see how it per- forms in the medical domain. In this work, we apply InfoCal to the task of predicting hospital readmission from EHRs (John- son et al., 2016). We compare InfoCal with clinical domain Bidirectional Encoder Transformer (BERT) models, ClinicalBERT (Huang et al., 2019) and Bio- ClinicalBERT (Alsentzer et al., 2019). Additionally we compare InfoCal exrractive rationales with impor- tance features in the BERT models via self-attention and layerwise relevance propagation (LRP). We find that the BERT models outperform InfoCal on the classification task, but has a relatively limited mech- anism for interpretability in the form of self-attention. InfoCal was able to produce extractive rationales which reach baseline performance on the classifica- tion task, and we argue the difficulty lies in the do- main expertise created by pretraining present in the BERT based models. Related Work With the advent of big data and machine learning, research is beginning to glean insights from the many types of EHRs data (Li et al., 2020; Huang et al., 2019; Kormilitzin et al., 2020; Weng et al., 2017; Wang et al., 2018; Kuruvilla and Gunavathi, 2014; Barak-Corren et al., 2017; Johnson et al., 2016). Data within EHRs can be either structured (following a pre-defined data structure and type, such as ECG recordings, x-ray images, laboratory results, and de- mographics) or unstructured data (lacking formal rules, type, and bounds, such as the free-text clin- ical notes which remain in a natural language for- mat). The frequency and volume in which clinical notes are recorded for individual patients surpass that of any other data type within EHRs, with some pa- tients having hundreds of individual notes over their entire history of care (Huang et al., 2019; Naumann and Boag, 2020; Barak-Corren et al., 2017; Wang et al., 2018; Weng et al., 2017). Consequently, the information within these clinical notes arguably pro- vides a nuanced, rich picture of a patient’s symptoms and trajectory to augment other measures. Other research has applied similar selector-predictor style models to medical text for certain tasks, such as diagnosis code prediction,and treatment success inference (Mullenbach et al., 2018; Lehman et al., 2019). To our knowledge this work is the first to apply InfoCal to a clinical domain task, and to compare rationales with self-attention and layerwise relevance propaga- tion (LRP) in BERT models.",1
"Abstract The development of neural networks for clini- cal artificial intelligence (AI) is reliant on in- terpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clin- ical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes. The transparency and explainability of neural networks is critical to trust and acceptance in clinical environments as useful tools. Research has sought to provide improved inter- pretability of NLP models using methods to derive extractive rationales for neural networks predictions by the model itself (Bastings et al., 2019; Lei et al., 2016; Sha et al., 2020). The aim of rationale produc- tion is to increase explainability of models by extract- ing the minimal crucial input required to make a class prediction. In NLP, the rationales are subsets of the input text which maintain predictive power. One of the ways rationales have been achieved is by creating a two-module network, i.e., a selector followed by a predictor, which are trained jointly (Lei et al., 2016). Given an input x, the selector picks a subset of the input features r(x) (the rationale) by specifying a dis- tribution over the possible rationales. The predictor acts as a standard classifier, taking as input r(x) and predicting a class yÃÂ to compare with the ground truth class y. Sha et al. (2020) proposed InfoCal, an improved type of selector-predictor model that uses an information calibration technique and an additional guider module trained jointly with the selector and predictor in an adversarial manner. InfoCal achieves the current state-of-the-art in rationale extraction on tasks such as sentiment analysis and legal judgment prediction, hence we are interested to see how it per- forms in the medical domain. In this work, we apply InfoCal to the task of predicting hospital readmission from EHRs (John- son et al., 2016). We compare InfoCal with clinical domain Bidirectional Encoder Transformer (BERT) models, ClinicalBERT (Huang et al., 2019) and Bio- ClinicalBERT (Alsentzer et al., 2019). Additionally we compare InfoCal exrractive rationales with impor- tance features in the BERT models via self-attention and layerwise relevance propagation (LRP). We find that the BERT models outperform InfoCal on the classification task, but has a relatively limited mech- anism for interpretability in the form of self-attention. InfoCal was able to produce extractive rationales which reach baseline performance on the classifica- tion task, and we argue the difficulty lies in the do- main expertise created by pretraining present in the BERT based models. Related Work With the advent of big data and machine learning, research is beginning to glean insights from the many types of EHRs data (Li et al., 2020; Huang et al., 2019;et al., 2020; Weng et al., 2017; Wang et al., 2018; Kuruvilla and Gunavathi, 2014; Barak-Corren et al., 2017; Johnson et al., 2016). Data within EHRs can be either structured (following a pre-defined data structure and type, such as ECG recordings, x-ray images, laboratory results, and de- mographics) or unstructured data (lacking formal rules, type, and bounds, such as the free-text clin- ical free-text contained within EHRs). The frequency and volume in which clinical text is recorded for the first time is critical to trust and acceptance in clinical environments. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability.",0
"Abstract Developing speech technologies is a challenge for low-resource lan- guages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such lan- guages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to de- termine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model. Keywords Data Augmentation; Speech Recognition; Maltese Language; Unsupervised Transcriptions; Multilingual Training; Synthesized Speech Introduction In recent years, the field of Natural Language Processing has seen renewed in- terest in multilingual models. New techniques of transfer and multi-task learn- ing, as well as the availability of multilingual datasets, have motivated the de- velopment of large-scale reusable models (Devlin et al., 2019; Conneau et al., 2020) and NLP benchmarks (e.g. Hu et al., 2020; Liang et al., 2020). Many of these models rely on large quantities of data for pretraining. As a result, ‘under-resourced’ languages, for which such data is less easy to come by, remain under-represented in these developments. To take an example, the large-scale language models currently in use in many language understanding tasks, such as Multilingual BERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are pretrained on multilingual data available in online resources such as Wikipedia or CommonCrawl (Wenzek et al., 2020). This, however, also means that lan- guages with small speaker populations, which also tend to be under-represented on the web, are not represented in these models. Similar developments are being seen in Automatic Speech Recognition (ASR) (Baevski et al., 2020a). There has been a lot of work focused on transfer learning-based improvements to speech recognition systems in order to make many modern architectures accessible to a wider set of languages. This has taken a number of different forms. For example Wang et al. (2020) use high-to- low resource language machine translation as an intermediate task to create a language model for a low-resource target language. More in line with parallel developments in NLP, recent research has also turned towards large, multilin- gual, pretrained ASR architectures. For example, Pratap et al. (2020) show that great benefits for low-resource speech recognition can accrue from a multilingual acoustic model. However, the majority of languages used in this work have over 100 hours of training data available. A different line of research has been pur- sued in Wav2Vec2 (Baevski et al., 2020b), which builds on the earlier Wav2Vec model Schneider et al. (2019). This architecture can learn robust speech repre- sentations of sufficient quality to perform recognition even for target languages with a very low volume of labelled data (in the order of one hour), reaching the same performance as previous state-of-the-art systems requiring 100 times more data (100 hours). Wav2Vec2 can also provide good results with just ten minutes of labelled speech data, reaching word error rates as low as 4.8, given substantial pretraining on 53,000 hours of unlabelled data. This work therefore demonstrates that semi- or self-supervised pretraining, similar in spirit to what we have seen in recent years with language models in NLP, can result in ro- bust representations that lower the requirements on labelled data. Once again, however, the key is in ensuring that sufficient data is available for pretraining, in this case in the form of audio. Indeed, Baevski et al. (2020b) describe two settings, one with 960 hours of unlabelled data, and another with 60,000 hours. In summary, research in NLP in both the textual and speech modalities has convincingly shown the benefits of pretraining with minimal supervision. Yet, from the perspective of under-resourced languages with low presence on the web, in the form of either text or speech, their feasibility is far from guar- anteed. Maltese provides a good example of this kind of scenario. Web-scale textual or speech data is harder to obtain for Maltese compared to languages such as English or Mandarin and we expect that similar challenges arise for many other under-resourced languages (Besacier et al., 2014). Furthermore, as recent critiques of large-scale pretraining approaches have emphasised, even where web-scale data is available, there are significant risks arising from its ‘unfathomable’ nature, not least that it is likely to be extremely noisy, while not guaranteeing representativeness across demographic or ethnic groups, and/or across language varieties Bender et al. (2021); Rogers (2021). Lastly, the com- putational resources needed for such experiments are not available to all research teams. Given these challenges, this paper presents an exploratory analysis of a set of related techniques for improving ASR for Maltese using different data augmen- tation strategies, relying on the use of smaller, curated datasets in the target language, the use of larger datasets in related languages, and the contribution of artificial data augmentation. Briefly, the scenario in which these experiments were designed consists of the following: Approximately7hoursoflabelledspeechdatainthetargetlanguage(here: Maltese); A small amount of unlabelled speech data in the target language; A medium sized corpus of Maltese text (250m tokens); A concatenative speech synthesis system whose output, while not of high quality, can be exploited for data augmentation purposes; Substantial data in a number of other languages – notably Arabic, Italian and English – which are typologically and historically related to Maltese. In the remainder of this section, we first introduce some salient features of the Maltese language itself, followed by an explanation of the motivation for these experiments. The section concludes with an outline of the rest of the paper. Maltese and other languages Maltese is spoken as a first language by just under half a million citizens of the Maltese islands (Mena et al., 2020); it is also taught as a second language in a few countries. It is the national language of Malta, but co-exists with two other official languages (English and Maltese Sign Language). In recent years, there has been significant gain in digital support for the Maltese language. A large (250m tokens) annotated text corpus is available, as are tools for segmentation and morphosyntactic labelling (Gatt and Cˇ ́eplo ̈, 2013) and electronic lexical repositories (Camilleri, 2013).1 In addition, some studies have focused on natural language analysis tasks such as morphologi- cal labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) and dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Despite these advances, Maltese remains under-resourced on a number of fronts, especially where speech is concerned. Concatenative speech synthesis systems have been developed using classical (e.g. diphone synthesis) methods (Micallef, 1997; Borg et al., 2011), but there are currently no tools for ASR, except for some preliminary experiments done by Mena et al. (2020). The most important reason for this is a lack of resources. In the absence of large reposito- ries of paired speech and text samples, it is not realistic to use current end-to-end methods for speech recognition. As argued above, it is also hard to obtain large samples of unlabelled speech for pretraining purposes. The present paper focuses on a variety of approaches designed to overcome this bottleneck, conducted in the context of the project MASRI - Maltese Auto- matic Speech Recognition2. Starting from a relatively small dataset of Maltese text and speech, we describe experiments deploying a variety of techniques for data augmentation. The broad question we address is therefore the following: Given an under-resourced language for which a relatively small speech-to-text dataset is available, what data augmentation methods work best to bring ASR performance to a level which provides a suitable, competitive baseline for future development? In our experiments, we maintain a focus on data augmentation for end-to- end ASR. While our focus is on the Maltese language, the findings presented here are of relevance to speech technology researchers working on other under- resourced languages. While a variety of data augmentation methods can be envisaged, our focus on Maltese makes two lines of inquiry particularly relevant. Here, we give an outline of the motivation for each one. Typological relatedness Maltese bears strong historical relationships to three major languages. From a historical perspective, it has been characterised as having a Semitic/Arabic stratum, a Romance (Italian/Sicilian) superstratum, and an English adstratum (Brincat, 2011). This characterisation reflects the his- torical development of Maltese, originally a variety of Arabic, which came into intensive contact with Italian due to its geographical proximity and historical relations with that country. This was followed by a period of intensive contact with English (Malta was a British colony from 1800 to 1964), as a result of which, the Maltese population is largely bilingual. The impact of this linguistic history is clearly evident in the language at many levels of analysis, including the lexical and morphological (Borg and Gatt, 2017). From the perspective of the present experiments, this suggests that a promising way to approach the data augmentation problem is to exploit the (much larger) resources available for these languages. We do this by transcribing speech data from one of these languages with a baseline Maltese acoustic model trained on a very small corpus (Section 5.2); and using a ‘language mixture’ approach (Section 5.3), in which an end-to-end system is pretrained on data using mixtures of such languages. In both cases, pretraining is followed by a fine-tuning step. Synthesis Given the existence of a concatenative speech synthesis system for Maltese, we also discuss data augmentation experiments involving artificially synthesised training data (Section 5.4). Outline of the paper The rest of this paper is structured as follows. Section 2 reviews data augmen- tation methods, especially for speech. In Section 3 we describe the datasets we use whilst Section 4 describes the experimental methodology. Section 5 presents a variety of experiments and results, in which we aim to identify the best data augmentation methods for ASR in Maltese. Section 6 concludes with a general discussion and some pointers to future work. Conclusions and future work The work presented in this paper had one over-arching theme - the analysis of whether ASR performance for under-resourced languages can be improved by various data augmentation methods. We elicited a detailed taxonomy of what types of augmentation exist in the literature, and we also devised a consistent set of experiments that sift through the many possibilities, whilst providing clear results for each. The results obtained are specific to ASR for the Maltese language, but we postulate that similar approaches would yield similar improve- ments for other under-resourced languages. In order to assess that we were indeed testing for improvements in acoustic modelling based on data augmentation, this work specifically makes no use of language modelling as part of the ASR pipeline. We do however show that language model rescoring does indeed have a big effect on WER (section 4.6), and in future experimentation, we intend to combine the best methods from this work with further language modelling experiments to obtain state-of-the- art performance for Maltese ASR. That is however, a different research question altogether. As part of our efforts in maintaining consistency in our experiments, ques- tions were also raised as to whether augmentation is helpful for pretraining methodologies, whilst keeping the network architectures unchanged. To this end we employ training cycles, and show that some measure of performance improvement is obtained, as shown in Section 5.2. We also postulate that this approach is generic enough to be proposed for any ASR setup for an under- resourced target language. The one aspect of the work we present here that is probably dependent on the target language is in section 5.3. The ancillary language data chosen is not random. This augmentation is based on languages which are somehow related to Maltese - either historically, culturally or linguistically. The results show that supervised transcriptions of some closely related languages provide a substantial improvement in WER (Section 5.3.1). The choice of these languages, however, has to be assessed individually based on the target language. Furthermore, this paper then analyzed the use of synthesized speech for ASR training (Section 5.4). Whilst data from a speech synthesis system might not always be readily available for under-resourced languages, we highlight the fact that the synthetic speech quality need not be of very good quality for immediate benefits in WER. In fact, the synthesis system used is produces highly unnatural speech with many pronunciation errors, yet we gained the largest improvements from using this method of data augmentation. In summary, the conclusions we can draw from the work presented is as follows: Both gold and noisy transcriptions can be used as data augmentation techniques, up until a limit is reached after various training cycles. Noisy transcriptions have poorer performance with respect to gold tran- scriptions. In the absence of sufficient gold transcription quantities, how- ever, the utilization of noisy transcriptions of the target language shows substantial improvements in WER. Mixing pretraining data from non-target languages is useful, especially with a small data batch from the target language. There are however limits to this and adding more data beyond a certain point yields diminishing returns. The use of synthesized speech in the target language, as training data, outperforms the use of gold or noisy transcriptions in languages different to the target, even when the synthesized speech has a low quality. The results we obtain from this work are very encouraging, as we have ob- served an absolute reduction of 15% in WER from our baseline systems (63.71% WER) to the system resulting from the best setup based on pretraining and fine tuning from an augmented dataset (48.97% WER). This is a remarkable improvement in light of the fact that they revolve around a curated Maltese ASR corpus of less than 7 hours of speech. We believe this work sets a starting point for all future work in ASR for the Maltese language, and propose that all languages which are similarly under- resourced could follow the methodology we outline in this paper to assess and capitalize on data augmentation methods. A promising direction for future work is to explore the augmentation tech- niques presented here with a different neural architecture. Recent research in multilingual NKP has showing the remarkable performance of models in the Transformer paradigm (Vaswani et al., 2017), both for textual, language- understanding tasks with models such as multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) and for speech recognition with Wav2Vec- 2 Baevski et al. (2020b). These approaches provide impetus for further experi- mentation, with a view to identifying the best strategies for data augmentation to make ASR more feasible for low-resource languages.",1
"Abstract Developing speech technologies is a challenge for low-resource lan- guages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such lan- guages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to de- termine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model. Keywords Data Augmentation; Speech Recognition; Maltese Language; Unsupervised Transcriptions; Multilingual Training; Synthesized Speech Introduction In recent years, the field of Natural Language Processing has seen renewed in- terest in multilingual models. New techniques of transfer and multi-task learn- ing, as well as the availability of multilingual datasets, have motivated the de- velopment of large-scale reusable models (Devlin et al., 2019; Conneau et al., 2020) and NLP benchmarks (e.g. Hu et al., 2020; Liang et al., 2020). Many of these models rely on large quantities of data for pretraining. As a result, Ã¢ÂÂunder-resourcedÃ¢ÂÂ languages, for which such data is less easy to come by, remain under-represented in these developments. To take an example, the large-scale language models currently in use in many language understanding tasks, such as Multilingual BERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are pretrained on multilingual data available in online resources such as Wikipedia or CommonCrawl (Wenzek et al., 2020). This, however, also means that lan- guages with small speaker populations, which also tend to be under-represented on the web, are not represented in these models. Similar developments are being seen in Automatic Speech Recognition (ASR) (Baevski et al., 2020a). There has been a lot of work focused on transfer learning-based improvements to speech recognition systems in order to make many modern architectures accessible to a wider set of languages. This has taken a number of different forms. For example Wang et al. (2020) use high-to- low resource language machine translation as an intermediate task to create a language model for a low-resource target language. More in line with parallel developments in NLP, recent research has also turned towards large, multilin- gual, pretrained ASR architectures. For example, Pratap et al. (2020) show that great benefits for low-resource speech recognition can accrue from a multilingual acoustic model. However, the majority of languages used in this work have over 100 hours of training data available. A different line of research has been pur- sued in Wav2Vec2 (Baevski et al., 2020b), which builds on the earlier Wav2Vec model Schneider et al. (2019). This architecture can learn robust speech repre- sentations of sufficient quality to perform recognition even for target languages with a very low volume of labelled data (in the order of one hour), reaching the same performance as previous state-of-the-art systems requiring 100 times more data (100 hours). Wav2Vec2 can also provide good results with just ten minutes of labelled speech data, reaching word error rates as low as 4.8, given substantial pretraining on 53,000 hours of unlabelled data. This work therefore demonstrates that semi- or self-supervised pretraining, similar in spirit to what we have seen in recent years with language models in NLP, can result in ro- bust representations that lower the requirements on labelled data. Once again, however, the key is in ensuring that sufficient data is available for pretraining, in this case in the form of audio. Indeed, Baevski et al. (2020b) describe two settings, one with 960 hours of unlabelled data, and another with 60,000 hours. In summary, research in NLP in both the textual and speech modalities has convincingly shown the benefits of pretraining with minimal supervision. Yet, fromperspective of under-resourced languages with low presence on the web, in the form of either text or speech, their feasibility is far from guar- anteed. Maltese provides a good example of this kind of scenario. Web-scale textual or speech data is harder to obtain for Maltese compared to languages such as English or Mandarin and we expect that similar challenges arise for many other under-resourced languages (Besacier et al., 2014). Furthermore, as recent critiques of large-scale pretraining approaches have emphasised, even where web-scale data is available, there are significant risks arising from its Ã¢ÂÂunfathomableÃ¢ÂÂ nature, not least that it is likely to be extremely noisy, while not guaranteeing representativeness across demographic or ethnic groups, and/or across language varieties Bender et al. (2021); Rogers (2021). Lastly, the com- putational resources needed for such experiments are not available to all research teams. Given these challenges, this paper presents an exploratory analysis of a set of related techniques for improving ASR for Maltese using different data augmen- tation strategies, relying on the use of smaller, curated datasets in the target language, the use of larger datasets in related languages, and the contribution of artificial data augmentation. Briefly, the scenario in which these experiments were designed consists of the following: Approximately7hoursoflabelledspeechdatainthetargetlanguage(here: Maltese); A small amount of unlabelled speech data in the target language; A medium sized corpus of Maltese text (250m tokens); A concatenative speech synthesis system whose output, while not of high quality, can be exploited for data augmentation purposes; Substantial data in a number of other languages Ã¢ÂÂ notably Arabic, Italian and English Ã¢ÂÂ which are typologically and historically related to Maltese. In the remainder of this section, we first introduce some salient features of the Maltese language itself, followed by an explanation of the motivation for these experiments. The section concludes with an outline of the rest of the paper. Maltese and other languages Maltese is spoken as a first language by just under half a million citizens of the Maltese islands (Mena et al., 2020); it is also taught as a second language in a few countries. It is the national language of Malta, but co-exists with two other official languages (English and Maltese Sign Language). In recent years, there has been significant gain in digital support for the Maltese language. A large (250m tokens) annotated text corpus is available, as are tools for segmentation and morphosyntactic labelling (Gatt and CÃÂ ÃÂeplo ÃÂ, 2013) and electronic lexical repositories (Camilleri, 2013).1 In addition, some studies have focused on natural language analysis tasks such as morphologi- cal labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) and dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Despite these advances, Maltese remains under-resourced on a number of fronts, especially where speech is concerned. Concatenative speech synthesis systems have been developed using classical (e.g. diphone synthesis) methods (Micallef, 1997; Borg et al., 2011), but there are currently no tools for ASR, except for some preliminary experiments done by Mena et al. (2020). The most important reason for this is a lack of resources. In the absence of large reposito- ries of paired speech and text samples, it is not realistic to use current end-to-end methods for speech recognition. As argued above, it is also hard to obtain large samples of unlabelled speech for pretraining purposes. The present paper focuses on a variety of approaches designed to overcome this bottleneck, conducted in the context of the project MASRI - Maltese Auto- matic Speech Recognition2. Starting from a relatively small dataset of Maltese text and speech, we describe experiments deploying a variety of techniques for data augmentation. The broad question we address is therefore the following: Given an under-resourced language for which a relatively small speech-to-text dataset is available, what data augmentation methods work best to bring ASR performance to a level which provides a suitable, competitive baseline for future development? In our experiments, we maintain a focus on data augmentation for end-to- end ASR. While our focus is on the Maltese language, the findings presented here are of relevance to speech technology researchers working on other under- resourced languages. Whilevariety of data augmentation methods can be envisaged, our focus on Maltese makes two lines of inquiry particularly relevant. Here, we give an outline of the motivation for each one. Typological relatedness Maltese bears strong historical relationships to three major languages. From a historical perspective, it has been characterised as having a Semitic/Arabic stratum, a Romance (Italian/Sicilian) superstratum, and an English adstratum (Brincat, 2011). This characterisation reflects the his- torical development of Maltese, originally a variety of Arabic, which came into intensive contact with Italian due to its geographical proximity and historical relations with that country. This was followed by a period of intensive contact with English (Malta was a British colony from 1800 to 1964), as a result of which, the Maltese population is largely bilingual. The impact of this linguistic history is clearly evident in the language at many levels of analysis, including the lexical and morphological (Borg and Gatt, 2017). From the perspective of the present experiments, this suggests that a promising way to approach the data augmentation problem is to exploit the (much larger) resources available for these languages. We do this by transcribing speech data from one of these languages with a baseline Maltese acoustic model trained on a very small corpus (Section 5.2); and using a Ã¢ÂÂlanguage mixtureÃ¢ÂÂ approach (Section 5.3), in which an end-to-end system is pretrained on data using mixtures of such languages. In both cases, pretraining is followed by a fine-tuning step. Synthesis Given the existence of a concatenative speech synthesis system for Maltese, we also discuss data augmentation experiments involving artificially synthesised training data (Section 5.4). Outline of the paper The rest of this paper is structured as follows. Section 2 reviews data augmen- tation methods, especially for speech. In Section 3 we describe the datasets we use whilst Section 4 describes the experimental methodology. Section 5 presents a variety of experiments and results, in which we aim to identify the best data augmentation methods for ASR in Maltese. Section 6 concludes with a general discussion and some pointers to future work. Conclusions and future work The work presented in this paper had one over-arching theme - the analysis of whether ASR performance for under-resourced languages can be improved by various data augmentation methods. We elicited a detailed taxonomy of what types of augmentation exist in the literature, and we also devised a consistent set of experiments that sift through the many possibilities, whilst providing clear results for each. The results obtained are specific to ASR for the Maltese language, but we postulate that similar approaches would yield similar improve- ments for other under-resourced languages. In order to assess that we were indeed testing for improvements in acoustic modelling based on data augmentation, this work specifically makes no use of language modelling as part of the ASR pipeline. We do however show that language model rescoring does indeed have a big effect on WER (section 4.6), and in future experimentation, we intend to combine the best methods from this work with further language modelling experiments to obtain state-of-the- art performance for Maltese ASR. That is however, a different research question altogether. As part of our efforts in maintaining consistency in our experiments, ques- tions were also raised as to whether augmentation is helpful for pretraining methodologies, whilst keeping the network architectures unchanged. To this end we employ training cycles, and show that some measure of performance improvement is obtained, as shown in Section 5.2. We also postulate that this approach is generic enough to be proposed for any ASR setup for an under- resourced target language. The one aspect of the work we present here that is probably dependent on the target language is in section 5.3. The ancillary language data chosen is not random. This augmentation is based on languages which are somehow related to Maltese - either historically, culturally or linguistically. The results show that supervised transcriptions of some closely related languages provide a substantial improvement in WER (Section 5.3.1). The choice of these languages, however, has to be assessed individually based on the target language. Furthermore, this paper then analyzed the use of synthesized speech for ASR training (Section 5.4). Whilst data from a speech synthesis system might not always be readily available for under-resourced languages, we highlight the fact that the synthetic speech quality need not be of very good quality for immediate benefits in WER. In fact, the synthesis system used is produces highly unnatural speech with many pronunciationyet we gained the largest improvements from using this method of data augmentation. In summary, the conclusions we can draw from the work presented is as follows: Both gold and noisy transcriptions can be used as data augmentation techniques, up until a limit is reached after various training cycles. Noisy transcriptions have poorer performance with respect to gold transtrung transtrung methods. Mixing pretraining data from non-target languages is useful, especially with a small dataset of Maltese text samples. However, the majority of languages used in this work have over 100 hours of training data available. A promising way to approach the data augmentation problem is to exploit the (much larger) resources available for these languages. We do this by transcribing speech data from one of these languages with a baseline Maltese acoustic model trained on a very small corpus (Section 5.2); and using a Ã¢ÂÂlanguage mixtureÃ¢ÂÂ approach (Section 5.3), in which an end-to-end system is pretrained on data using mixtures of such languages. In both cases, pretraining is followed by a fine-tuning step. Synthesis Given the existence of a concatenative speech synthesis system for Maltese, we also discuss data augmentation experiments involving artificially synthesised training data (Section 5.4). Outline of the paper The rest of this paper is structured as follows. Section 2 reviews data augmen- tation methods, especially for speech. In Section 3 we describe the datasets we use whilst Section 4 describes the experimental methodology. Section 5 presents a variety of experiments and results, in which we aim to identify the best data augmentation methods for ASR in Maltese. Section 6 concludes with a general discussion and some pointers to future work. Conclusions and future work The work presented in this paper had one over-arching theme - the analysis of whether ASR performance for under-resourced languages can be improved by various data augmentation methods. We elicited a detailed taxonomy of what types of augmentation exist in the literature, and we also devised a consistent set of experiments that sift through the many possibilities, whilst providing clear results for each. The results obtained are specific to ASR for the Maltese language, but we postulate that similar approaches would yield similar improve- ments for other under-resourced languages.",0
"Abstract Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. How- ever, most current dialogue personalization ap- proaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona infor- mation based on the dialogue history to per- sonalize the dialogue agent without relying on any explicit persona descriptions during infer- ence. Experimental results on the PersonaChat dataset show that the proposed method can im- prove the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated re- sponses when conditioning on the predicted persona of the dialogue partner (i.e. “their per- sona”). We also find that a trained persona pre- diction model can be successfully transferred to other datasets and help generate more rele- vant responses. Introduction Recently, end-to-end dialogue response genera- tion models (Sordoni et al., 2015; Serban et al., 2016; Bordes et al., 2017) based on recent ad- vances of neural sequence-to-sequence learning models (Sutskever et al., 2014; Vaswani et al., 2017) have gained increasing popularity as they can generate fluent responses. However, as the dialogue agent is trained with datasets contain- ing dialogues from many different speakers, it can not generate personalized responses for the current speaker, making the generated responses less rele- vant and engaging (Li et al., 2016b). To address this problem, recent studies attempt to personalize dialogue systems by generating di- alogue responses conditioning on given persona descriptions have been shown to help dialogue agents perform better (Zhang et al., 2018; Mazare ́ et al., 2018). However, a major drawback of the current dialogue agent personalization approaches is that they require explicit persona descriptions in both training and inference stages, which severely limits their application in real-world scenarios be- cause detailed persona descriptions for current speakers are not available in most scenarios. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speaker’s persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a “persona approximator” which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a “persona generator” which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both “self-persona” which is the persona information of the dialogue agent, and “their- persona” which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred “self- persona” can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches. Our experiments on dialogue datasets with and without the persona description demonstrate the effectiveness of the proposed approach and show that a trained persona detection model can be suc- cessfully transferred to datasets without persona description. Related Work Preliminary study on dialogue personalization (Li et al., 2016b) attempts to use a persona-based neu- ral conversation model to capture individual char- acteristics such as background information and speaking style. However, it requires the current speaker during inference to have sufficient dialogue utterances included in the training set, which is quite restricted by the cold-start problem. More recently, Zhang et al. (2018) released the PersonaChat dataset which incorporates per- sona of two speakers represented as multiple sen- tences of profile description to personalize dia- logue agents. They propose a profile memory net- work by considering the dialogue history as in- put and then performing attention over the per- sona to be combined with the dialogue history. Mazare ́ et al. (2018) proposed to train a persona encoder and combine the encoded persona em- bedding with context representation by concate- nation. The combined representation is then fed into the dialogue decoder to generate personal- ized responses. (Yavuz et al., 2019) designed the DeepCopy model, which leverages copy mech- anism to incorporate persona texts and Madotto et al. (2019) propose to use meta-learning to adapt to the current speaker quickly, their approach also requires several dialogues of the speaker to per- form dialogue personalization, which is different from our approach. Wellecketal.(2019)propose a dialogue natural language inference dataset and use it to measure and improve the consistency of the dialogue system. More recently, Zheng et al. (2019) propose personalized dialogue generation with diversified traits. Song et al. (2020) introduce a multi-stage response generation stage to improve the personalization of generated responses. Wu et al. (2020) propose a variational response gener- ator to better exploit persona information. Differ- ent from the aforementioned works, our approach does not require persona information during test time, which makes it more generally applicable. Conclusion In this paper, we propose a novel dialogue per- sonalization approach that automatically infers the current speakers’ persona based on the dialogue history, which enables neural dialogue systems to generate personalized dialogue responses without using persona description at test time. Our exper- iments on the PersonaChat dataset show that the proposed models can improve the model’s con- sistency and engagingness when conditioning on the inferred persona information of the dialogue agent itself or the dialogue partner. We also con- duct experiments on the Dailydialog dataset where persona description is not available and find that pre-trained persona detection models can be suc- cessfully transferred to other datasets without an- notated persona descriptions. This further demon- strates the potential of our approach to enable per- sonalized dialogue response generation for various domains where persona descriptions are not avail- able or expensive to collect.",1
"Abstract Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. How- ever, most current dialogue personalization ap- proaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona infor- mation based on the dialogue history to per- sonalize the dialogue agent without relying on any explicit persona descriptions during infer- ence. Experimental results on the PersonaChat dataset show that the proposed method can im- prove the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. Ã¢ÂÂself personaÃ¢ÂÂ), and improve the engagingness of the generated re- sponses when conditioning on the predicted persona of the dialogue partner (i.e. Ã¢ÂÂtheir per- sonaÃ¢ÂÂ). We also find that a trained persona pre- diction model can be successfully transferred to other datasets and help generate more rele- vant responses. Introduction Recently, end-to-end dialogue response genera- tion models (Sordoni et al., 2015; Serban et al., 2016; Bordes et al., 2017) based on recent ad- vances of neural sequence-to-sequence learning models (Sutskever et al., 2014; Vaswani et al., 2017) have gained increasing popularity as they can generate fluent responses. However, as the dialogue agent is trained with datasets contain- ing dialogues from many different speakers, it can not generate personalized responses for the current speaker, making the generated responses less rele- vant and engaging (Li et al., 2016b). To address this problem, recent studies attempt to personalize dialogue systems by generating di- alogue responses conditioning on given persona descriptions have been shown to help dialogue agents perform better (Zhang et al., 2018; Mazare ÃÂ et al., 2018). However, a major drawback of the current dialogue agent personalization approaches is that they require explicit persona descriptions in both training and inference stages, which severely limits their application in real-world scenarios be- cause detailed persona descriptions for current speakers are not available in most scenarios. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speakerÃ¢ÂÂs persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a Ã¢ÂÂpersona approximatorÃ¢ÂÂ which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a Ã¢ÂÂpersona generatorÃ¢ÂÂ which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both Ã¢ÂÂself-personaÃ¢ÂÂ which is the persona information of the dialogue agent, and Ã¢ÂÂtheir- personaÃ¢ÂÂ which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred Ã¢ÂÂself- personaÃ¢ÂÂ can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches. Our experiments on dialogue datasets with and without the persona description demonstrate the effectiveness of the proposed approach and show that a trained persona detection model can be suc- cessfully transferred to datasets withoutdescription. Related Work Preliminary study on dialogue personalization (Li et al., 2016b) attempts to use a persona-based neu- ral conversation model to capture individual char- acteristics such as background information and speaking style. However, it requires the current speaker during inference to have sufficient dialogue utterances included in the training set, which is quite restricted by the cold-start problem. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speakerÃ¢ÂÂs persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a Ã¢ÂÂpersona approximatorÃ¢ÂÂ which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a Ã¢ÂÂpersona generatorÃ¢ÂÂ which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both self-persona which is the persona information of the dialogue agent, and Ã¢ÂÂtheir- personaÃ¢ÂÂ which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred Ã¢ÂÂself- personaÃ¢ÂÂ can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches.",0
"Abstract The inception of modeling contextual information using mod- els such as BERT, ELMo, and Flair has significantly im- proved representation learning for words. It has also given SOTA results in almost every NLP task — Machine Trans- lation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these domi- nant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities. Introduction Named Entity Recognition (NER) is the task of locating and classifying named entities in a given piece of text into pre- defined entity categories such as Person (PER), Location (LOC), Organisation (ORG), etc. NER is considered an es- sential preprocessing step that can benefit many downstream applications in Natural Language Processing (NLP), such as Machine Translation (Babych and Hartley 2003), Informa- tion Retrieval (Antony and G S 2015) and Text Classification (Armour, Japkowicz, and Matwin 2005). Over the past few years, Deep Learning has been the key to solving not only NER but many other NLP applications (Le et al. 2018; Kouris, Alexandridis, and Stafylopatis 2019). On the downside, these models also demand a lot of well- structured and annotated data for their training. This restricts the applicability of trained models to a real-world scenario as the model’s behavior and predictions become very specific to the type of data they are trained on. To conquer this, many studies have recently evolved that focus on building models that can incorporate world knowledge for enhanced modeling and inference on the task at hand, such as He et al. (2020) for NER, Denk and Peleteiro Ramallo (2020) for Representation Learning and Kim et al. (2015) for Dependency Parsing, etc. Although recent works in literature have successfully in- corporated world knowledge for Sequence Labeling (He et al. 2020), they come with certain limitations, which we dis- cuss ahead. First, as words in a language can be polysemous (Lin et al. 2002), entities and relations in a knowledge graph can be polysemous too (Xiao, Huang, and Zhu 2016). To introduce Knowledge Graph Embeddings (KGEs), we no- ticed that previously proposed approaches have primarily used pre-trained static embeddings obtained from extensive sources such as Wikidata. KGEs in these models fundamen- tally relies on the assumption that the tail entity is a linear transformation of the head entity and the relation, making them non-contextualized in nature. Second, we noticed that prior work only considered head-entity and relation embed- ding to get the knowledge graph embedding and ignored the tail-entity of the triplet completely. Dropping the tail entity entirely could lead to a potential loss of information. We observed that in addition to carrying information about the triplet itself, the head-relation-tail also helps in understanding and extracting implicit relationships existing between entities across triplets. Therefore, the model must know where the head and the relation are leaning towards to achieve accurate embedding estimation. The final limitation lies in applying a Recurrent architecture to obtain KGEs, introducing time inef- ficiency and a high computation cost (Annervaz, Chowdhury, and Dukkipati 2018). To further understand the importance and our motivation behind using world knowledge for NER, consider a couple of examples mentioned below. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ”Google” is an organization and ”Pichai” indicates a Person. A: Berlin died in Season 2 of Money Heist. B: Messi saved Barcelona with an equalizer. In the first sentence above, ”Berlin” refers to a person, whereas in the second sentence, ”Barcelona” refers to an organization. There are reasonable chances of misclassifi- cation in these two sentences because of a high probability of training data missing such nuance differences in all the possible entity tags for a named entity. From the examples mentioned above, we can infer that for the model to be aware of such subtle differences, we should provide it with the ability to look up relevant details from a re- liable source. Therefore, world knowledge can open the gates for the model to access such information and learn details about entities that it might never come across in the train- ing data. In addition to this, with access to structured world knowledge, far better applicability to a real-world setting can be expected. Setting these points as our objective, in this work, we pro- pose Knowledge Aware Representational Learning Network for Named Entity Recognition using Transformer (KARL- Trans-NER), which 1. Encodes the entities and relations existing in a knowledge base using a self-attention network to obtain Knowledge Graph Embeddings (KGEs). The embeddings thus ob- tained are dynamic and fully contextualized in nature. 2. Takes the encoded contextualized representations for enti- ties and relations and generates a knowledge-aware repre- sentation for words. The representation obtained, which we also call ”Global Representation” for words, can be augmented with the other underlying features to boost the NER model’s performance. 3. Generates sentence embeddings using BERT by fusing task-specific information through NER tag embeddings. 4. And lastly, relies on a Transformer as its context encoder incorporating direction-aware, distance-aware, and un- scaled attention for enhanced encoder representation learn- ing. To verify the effectiveness of our proposed model, we conduct our experiments on three publicly available datasets for NER. These are CoNLL 2003 (Sang and Meulder 2003), CoNLL++ (Wang et al. 2019) and OntoNotes v5 (Pradhan et al. 2013). Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the three NER datasets. Also, to validate the model’s generalizability and applicability in a real-world setting, we generate the model’s prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence. Related work The research community in NER moved from approaches us- ing character and word representations (Yao et al. 2015; Zhou et al. 2017; Kuru, Can, and Yuret 2016) to sentence-level con- textual representations (Yang, Zhang, and Dong 2017; Zhang, Liu, and Song 2018), and recently to document-level rep- resentations as proposed by Qian et al. (2018) and Akbik, Blythe, and Vollgraf (2018). Expanding the scope of embed- dings from character and word level to document level has shown significant improvements in the results for many NLP tasks, including NER (Luo, Xiao, and Zhao 2019). To expand the scope further, researchers have explored external knowl- edge bases to learn facts existing in the universe that may not be present in the training data (Annervaz, Chowdhury, and Dukkipati 2018; He et al. 2020). Incorporating information present in Knowledge Graph is an emerging research topic in NLP. While some methods focus on graph structure encoding (Lin et al. 2015; Das et al. 2017), others focus on learning entity-relation embeddings (Wang et al. 2020a; Jiang, Wang, and Wang 2019). Zhong et al. (2015) proposed an alignment model for jointly embedding a knowledge base and a text corpus that achieved better or comparable performance on four NLP tasks: link prediction, triplet classification, relational fact ex- traction, and analogical reasoning. Xiao, Huang, and Zhu (2016) proposed a generative embedding model, TransG, which can discover the latent semantics of a relation and leverage a mixture of related components for generating em- bedding. They also reported substantial improvements over the state-of-the-art baselines on the task of link prediction. Lukovnikov et al. (2017) presented a neural network to an- swer simple questions over large-scale knowledge graphs using a hierarchical word and character-level question en- coder. Annervaz, Chowdhury, and Dukkipati (2018) lever- aged world knowledge in training task-specific models and proposes a novel convolution-based architecture to reduce the attention space over entities and relations. It outperformed other models on text classification and natural language in- ference tasks. Despite producing state-of-the-art results in many NLP tasks, Knowledge Graphs are relatively unexplored for NER. He et al. (2020) introduced a Knowledge-Graph Augmented Word Representation (KAWR). The proposed model encoded the prior knowledge of entities from an external knowledge base into the representation. Though KAWR performed better than its benchmark BERT (Devlin et al. 2018), the model underperformed compared to the SOTA models for NER. Conclusion and Future Work This work proposed a novel world knowledge augmentation technique that leveraged large knowledge bases represented as fact triplets and successfully extracted relevant informa- tion for word-level augmentation. The model was trained and tested in an NER setting. Experimental results showed that knowledge level representation learning outperformed most NER systems in literature and made the model highly applicable to a real-world scenario by accurately predicting entities in random pieces of text. Since we augmented features at the word level, we be- lieve our method could facilitate many other NLP tasks, such as Chunking, Word Sense Disambiguation, Question An- swering, etc. Therefore, as future work, we plan to test the applicability of the proposed methods on other NLP tasks as well. Our intuition says that any system can leverage the pro- posed system as a general knowledge representation learning tool. Moreover, being among the very few works in this di- rection, we see an ample scope of improvement. For instance, the Knowledge Graph Embedding model was trained sepa- rately on a Masked Language Modelling task, and then the trained model was used on the task at hand. This restricted the model from interacting and learning from the task at hand, NER in our case. We believe that a technique to incorporate and train the NER model with the knowledge representation module can be more beneficial. Another improvement that could be made lies in the entity shortlisting step. Although the technique is quite reliable, it does not consider any se- mantic information about the entities. Different entities in a knowledge base can be highly correlated to each other and yet have different names. Therefore, we plan to improve the entity shortlisting technique further for more accurate and robust shortlisting.",1
"Abstract The inception of modeling contextual information using mod- els such as BERT, ELMo, and Flair has significantly im- proved representation learning for words. It has also given SOTA results in almost every NLP task ÃÂ¢ÃÂÃÂ Machine Trans- lation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these domi- nant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities. Introduction Named Entity Recognition (NER) is the task of locating and classifying named entities in a given piece of text into pre- defined entity categories such as Person (PER), Location (LOC), Organisation (ORG), etc. NER is considered an es- sential preprocessing step that can benefit many downstream applications in Natural Language Processing (NLP), such as Machine Translation (Babych and Hartley 2003), Informa- tion Retrieval (Antony and G S 2015) and Text Classification (Armour, Japkowicz, and Matwin 2005). Over the past few years, Deep Learning has been the key to solving not only NER but many other NLP applications (Le et al. 2018; Kouris, Alexandridis, and Stafylopatis 2019). On the downside, these models also demand a lot of well- structured and annotated data for their training. This restricts the applicability of trained models to a real-world scenario as the modelÃ¢ÂÂs behavior and predictions become very specific to the type of data they are trained on. To conquer this, many studies have recently evolved that focus on building models that can incorporate world knowledge for enhanced modeling and inference on the task at hand, such as He et al. (2020) for NER, Denk and Peleteiro Ramallo (2020) for Representation Learning and Kim et al. (2015) for Dependency Parsing, etc. Although recent works in literature have successfully in- corporated world knowledge for Sequence Labeling (He et al. 2020), they come with certain limitations, which we dis- cuss ahead. First, as words in a language can be polysemous (Lin et al. 2002), entities and relations in a knowledge graph can be polysemous too (Xiao, Huang, and Zhu 2016). To introduce Knowledge Graph Embeddings (KGEs), we no- ticed that previously proposed approaches have primarily used pre-trained static embeddings obtained from extensive sources such as Wikidata. KGEs in these models fundamen- tally relies on the assumption that the tail entity is a linear transformation of the head entity and the relation, making them non-contextualized in nature. Second, we noticed that prior work only considered head-entity and relation embed- ding to get the knowledge graph embedding and ignored the tail-entity of the triplet completely. Dropping the tail entity entirely could lead to a potential loss of information. We observed that in addition to carrying information about the triplet itself, the head-relation-tail also helps in understanding and extracting implicit relationships existing between entities across triplets. Therefore, the model must know where the head and the relation are leaning towards to achieve accurate embedding estimation. The final limitation lies in applying a Recurrent architecture to obtain KGEs, introducing time inef- ficiency and a high computation cost (Annervaz, Chowdhury, and Dukkipati 2018). To further understand the importance and our motivation behind using world knowledge for NER, consider a couple of examples mentioned below. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly.model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ÃÂ¢ÃÂÃÂGoogleÃÂ¢ÃÂÃÂ is an organization and Ã¢ÂÂPichaiÃ¢ÂÂ indicates a Person. A: Berlin died in Season 2 of Money Heist. B: Messi saved Barcelona with an equalizer. In the first sentence above, Ã¢ÂÂBerlinÃ¢ÂÂ refers to a person, whereas in the second sentence, Ã¢ÂÂBarcelonaÃ¢ÂÂ refers to an organization. There are reasonable chances of misclassifi- cation in these two sentences because of a high probability of training data missing such nuance differences in all the possible entity tags for a named entity. From the examples mentioned above, we can infer that for the model to be aware of such subtle differences, we should provide it with the ability to look up relevant details from a re- liable source. Therefore, world knowledge can open the gates for the model to access such information and learn details about entities that it might never come across in the train- ing data. In addition to this, with access to structured world knowledge, far better applicability to a real-world setting can be expected. Setting these points as our objective, in this work, we pro- pose Knowledge Aware Representational Learning Network for Named Entity Recognition using Transformer (KARL- Trans-NER), which 1. Encodes the entities and relations existing in a knowledge base using a self-attention network to obtain Knowledge Graph Embeddings (KGEs). The embeddings thus ob- tained are dynamic and fully contextualized in nature. 2. Takes the encoded contextualized representations for enti- ties and relations and generates a knowledge-aware repre- sentation for words. The representation obtained, which we also call Ã¢ÂÂGlobal RepresentationÃ¢ÂÂ for words, can be augmented with the other underlying features to boost the NER modelÃ¢ÂÂs performance. 3. Generates sentence embeddings using BERT by fusing task-specific information through NER tag embeddings. 4. And lastly, relies on a Transformer as its context encoder incorporating direction-aware, distance-aware, and un- scaled attention for enhanced encoder representation learn- ing. To verify the effectiveness of our proposed model, we conduct our experiments on three publicly available datasets for NER. These are CoNLL 2003 (Sang and Meulder 2003), CoNLL++ (Wang et al. 2019) and OntoNotes v5 (Pradhan et al. 2013). Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the three NER datasets. Also, to validate the modelÃ¢ÂÂs generalizability and applicability in a real-world setting, we generate the modelÃ¢ÂÂs prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence. Related work The research community in NER moved from approaches us- ing character and word representations (Yao et al. 2015; Zhou et al. 2017; Kuru, Can, and Yuret 2016) to sentence-level con- textual representations (Yang, Zhang, and Dong 2017; Zhang, Liu, and Song 2018), and recently to document-level rep- resentations as proposed by Qian et al. (2018) and Akbik, Blythe, and Vollgraf (2018). Expanding the scope of embed- dings from character and word level to document level has shown significant improvements in the results for many NLP tasks, including NER (Luo, Xiao, and Zhao 2019). To expand the scope further, researchers have explored external knowl- edge bases to learn facts existing in the universe that may not be present in the training data (Annervaz, Chowdhury, and Dukkipati 2018; He et al. 2020). Incorporating information present in Knowledge Graph is an emerging research topic in NLP. While some methods focus on graph structure encoding (Lin et al. 2015; Das et al. 2017), others focus on learning entity-relation embeddings (Wang et al. 2020a; Jiang, Wang, and Wang 2019). Zhong et al. (2015) proposed anmodel for jointly embedding a knowledge base and a text corpus that achieved better or comparable performance on four NLP tasks: link prediction, triplet classification, relational fact ex- traction, and analogical reasoning. Xiao, Huang, and Zhu (2016) proposed a generative embedding model, TransG, which can discover the latent semantics of a relation and leverage a mixture of related components for generating em- bedding. They also reported substantial improvements over the state-of-the-art baselines on the task of link prediction. Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the four NLP tasks. Also, to validate the modelÃ¢ÂÂs generalizability and applicability in a real-world setting, we generate the modelÃ¢ÂÂs prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence. Related work The research community in NER moved from approaches us- ing character and word representations (Yao et al. 2015; Zhou et al. 2017; Kuru, Can, and Yuret 2016) to sentence-level con- textual representations (Yang, Zhang, and Dong 2017; Zhang, Liu, and Song 2018), and recently to document-level rep- resentations as proposed by Qian et al. (2018) and Akbik, Blythe, and Vollgraf (2018). Expanding the scope of embed- dings from character and word level to document level has shown significant improvements in the results for many NLP tasks, including NER (Luo, Xiao, and Zhao 2019). To expand the scope further, researchers have explored external knowl- edge bases to learn facts existing in the universe that may not be present in the training data (Annervaz, Chowdhury, and Dukkipati 2018; He et al. 2020). Incorporating information present in Knowledge Graph is an emerging research topic in NLP. While some methods focus on graph structure encoding (Lin et al. 2015; Das et al. 2017), others focus on learning entity-relation embeddings (Wang et al. 2020a; Jiang, Wang, and Wang 2019). Zhong et al. (2015) proposed an alignment model for jointly embedding a knowledge base and a text corpus that achieved better or comparable performance on four NLP tasks: link prediction, triplet classification, relational fact ex- traction, and analogical reasoning. Xiao, Huang, and Zhu (2016) proposed a generative embed- dings model, TransG, which can discover the latent semantics of a relation and leverage a mixture of related components for generating em- bedding. They also reported substantial improvements over the state-of-the-art baselines on the task of link prediction. Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the four NLP tasks. Also, to validate the modelÃ¢ÂÂs generalizability and applicability in a real-world setting, we generate the modelÃ¢ÂÂs prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence.",0
"Abstract In this work, we extensively redesign the newly introduced method of token mixing us- ing Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementa- tion on a long document summarization task (¿ 512 tokens). As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summariza- tion task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture. Introduction Abstractive summarization has made significant strides since the introduction of the transformer based model in NLP (Vaswani et al., 2017). How- ever, the quadratic computational and memory com- plexities of large transformers have limited their scalability for long document summarization as the token length for a standard transformer is limited to 512 tokens. One can try extractive summarization to reduce the length of the document while retain- ing the key elements of the article then taking an abstractive approach on the reduced document. In the extractive step, only the most important sentences are chosen to reduce the size of the document to fit within the token limits of the transformer model. Another way is to use extractive summarization to summarize the document thus retaining only the salient information. This approach is compu- tationally very expensive. Alternative transformer approaches such as the longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2021) alleviate the computation burden of the self-attention mech- anism by limiting the attention window each token has access to. Longformer was created for this pur- pose, using a pluggable sparse attention mechanism that combines dilated windowed attention for local context with full global attention on some tokens, of which the latter varies per task. This introduces an attention mechanism that grows linearly with sequence length using a sliding window of size w allowing for dealing documents in excess of 8000 tokens. More recently, a group at Google (Lee-Thorp et al., 2021) has introduced a new implementa- tion that replaces the entire self-attention heads in the transformer encoder with a non-parameterized Fourier transform mixing of the tokens that does not suffer from this quadratic computation penalty. We propose to extend this architecture to the long document summarization problem and compare the results to the two current baseline practices: Extracting the salient information then applying ab- stractive summarization using PEGASUS (Zhang et al., 2020) and using a Longformer implementa- tion. On both baseline approaches, we investigated multiple hyperparameter optimization and evalu- ated the summaries relative to their corresponding abstracts. This becomes the method for comparing the performance of each methodology. The primary dataset used for this work is the PubMed dataset (Dernoncourt and Lee, 2017) as there exists several prior work on long document summarization with it that we can compare to. Ac- cording to Zaheer et al. (2021), this dataset has a median token length of 2,715 with the 90th per- centile token length being 6,101. Dernoncourt and Lee (2017) shows how extensive this dataset is, with close to 200,00 articles. We decided to use the most common evaluation technique for document summarization – ROUGE scores (Lin, 2004). In our analyses, we include F1-scores for Rouge-1, Rouge-2, Rouge-3, and Rouge-l scores for completeness. Conclusion We have demonstrated for the first time that the recently proposed FNET architecture can be ex- tended to a full transformer model on an abstractive summarization task with a PubMed dataset. Even with a toy implementation, we have shown several novel architectural changes to the original proposal that can be used for a variety of tasks requiring low computational cost while maintaining reasonable accuracy. Our toy architecture yields lower Rouge scores than the baseline for two main reasons. First because the transformer model is much smaller and also because we did not have a pretrained FNET transformer as a starting point. The contribution of this work is the investigation of alternative im- plementation of the Fourier token mixing idea in a transformer on a summarization task. With a fully configured large implementation of these ar- chitectures, we believe that we can get competitive Rouge scores on the summarization task without the computation overhead of a full self-attention implementation. We believe that the extractive summarization prepossessing techniques used in this paper would generalize well with larger tun- ing datasets. Although extractive summarization is the most computationally expensive of the tech- niques used in this paper, it showed great promise as a tuning instrument for PEGASUSLARGE. Using more tuning examples, adjusting the tuning param- eters, and adding more computational power would make extractive prepossessing competitive with the state-of-the-art techniques for long documents sum- marization with a limited attention capacity of 512 tokens.",1
"Abstract In this work, we extensively redesign the newly introduced method of token mixing us- ing Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementa- tion on a long document summarization task (ÃÂÃÂ¿ 512 tokens). As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summariza- tion task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture. Introduction Abstractive summarization has made significant strides since the introduction of the transformer based model in NLP (Vaswani et al., 2017). How- ever, the quadratic computational and memory com- plexities of large transformers have limited their scalability for long document summarization as the token length for a standard transformer is limited to 512 tokens. One can try extractive summarization to reduce the length of the document while retain- ing the key elements of the article then taking an abstractive approach on the reduced document. In the extractive step, only the most important sentences are chosen to reduce the size of the document to fit within the token limits of the transformer model. Another way is to use extractive summarization to summarize the document thus retaining only the salient information. This approach is compu- tationally very expensive. Alternative transformer approaches such as the longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2021) alleviate the computation burden of the self-attention mech- anism by limiting the attention window each token has access to. Longformer was created for this pur- pose, using a pluggable sparse attention mechanism that combines dilated windowed attention for local context with full global attention on some tokens, of which the latter varies per task. This introduces an attention mechanism that grows linearly with sequence length using a sliding window of size w allowing for dealing documents in excess of 8000 tokens. More recently, a group at Google (Lee-Thorp et al., 2021) has introduced a new implementa- tion that replaces the entire self-attention heads in the transformer encoder with a non-parameterized Fourier transform mixing of the tokens that does not suffer from this quadratic computation penalty. We propose to extend this architecture to the long document summarization problem and compare the results to the two current baseline practices: Extracting the salient information then applying ab- stractive summarization using PEGASUS (Zhang et al., 2020) and using a Longformer implementa- tion. On both baseline approaches, we investigated multiple hyperparameter optimization and evalu- ated the summaries relative to their corresponding abstracts. This becomes the method for comparing the performance of each methodology. The primary dataset used for this work is the PubMed dataset (Dernoncourt and Lee, 2017) as there exists several prior work on long document summarization with it that we can compare to. Ac- cording to Zaheer et al. (2021), this dataset has a median token length of 2,715 with the 90th per- centile token length being 6,101. Dernoncourt and Lee (2017) shows how extensive this dataset is, with close to 200,00 articles. We decided to use the most common evaluation technique for document summarization Ã¢ÂÂ ROUGE scores (Lin, 2004). In our analyses, we include F1-scores for Rouge-1, Rouge-2, Rouge-3, and Rouge-l scores for completeness. Conclusion We have demonstrated for the first time that the recently proposed FNET architecture can be ex- tended to a full transformer model on an abstractive summarization task with a PubMed dataset. Even with a toy implementation, we have shown several novel architectural changes to the original proposal that can be used for a variety of tasks requiring low computationalwhile maintaining reasonable accuracy. Our toy architecture yields lower Rouge scores than the baseline for two main reasons. First because the transformer model is much smaller and also because we did not have a pretrained FNET transformer as a starting point. The contribution of this work is the investigation of alternative transformer approaches to the original FNET architecture. === FNET As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge scores (red) and performance on a summariza- tion task (green).",0