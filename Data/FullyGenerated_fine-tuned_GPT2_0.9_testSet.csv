text,#label,
"Abstract We present the first openly available cor- pus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based mod- els. We achieve a 77.53% accuracy with a Thai BERT model in detecting depres- sion. This establishes a good baseline for future researcher on the same corpus. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuﬀicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see Hämäläinen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. Related work In this section we present some of the recent related work in more detail. While there have been several digital humanities driven research efforts in better understanding depression in text (Girard and Cohn, 2015; Abd Yusof et al., 2017; Loveys et al., 2018), we only focus on work that has been conducted on depression detection. There are also several approaches to depression detection in other languages (Pi- rina and Çöltekin, 2018; Husseini Orabi et al., 2018; Song et al., 2018). An onset of major depression can be char- acterized and detected by investigating social media data such as Twitter (De Choudhury et al., 2013b,a). By exploring a large cor- pus of Twitter posting by using crowd-sourcing methodology. An SVM classifier was built by divesting a variety of social media measures such as social activity, egonetwork, style, user engagement, emotion, and language. The clas- sifier model predicted with high accuracy (70% and 73 % respectively) predicting ahead of the reported onset of depression and whether or not a post on Twitter could be depressive- indicative postings. Depression levels is de- tected by a proposed social media depression index. Computerized analysis of various kind of texts related to depression reveals signals of psychiatric disorders. Depression is measured by self-reported symptoms (Rude et al., 2002), by clinical interview (Rude et al., 2003). The Scrambled Sentences Test (SST) (Wenzlaff, 1993) was used to measure of cognitive process- ing bias of a large sample of college students. Negative cognitive processing biases in resolv- ing ambiguous verbal information can predict depression and subsequent depression symp- toms. Depression has been detected automatically before in Thai Facebook users (Katchapakirin et al., 2018). The authors train several models on RapidMiner. The models rely on metadata for activity on Facebook such as the number of posts posted on a given week day, the number of day-time and night-time posts and the num- ber of shared posts. The authors did not train the models on text, but rather used numerical features extracted from Facebook posts such as the number of first person pronouns and number of positive words. A screening text-based classification model was also applied to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). Similarly to the pre- vious approach, the authors apply several tra- ditional machine learning techniques to Thai social media text with pre-extracted features. Kumnunt and Sornil, 2020 present a CNN (convolutional neural network) based approach to depression detection in Thai social media posts. The authors crawl posts tagged with a depression hashtag to build their dataset. Contrary to any of the current work on Thai depression detection, our approach does not deal with social media posts but lengthier blog posts. We also make our dataset available un- like the existing work. Discussion and conclusions In this paper, we have presented a new ex- pert curated dataset for depression detection in Thai blog posts. Our sentence level results are promising and we are sure we can use our models to snowball more depressed blog posts from platforms such as Storylog for further linguistic analysis as a part of our interdisci- plinary research project. Based on our results, we can identify one easy future direction for enhancing the results obtained by our models. Currently, all freely available pretrained Thai embbeddings have been trained on Wikipedia. This is not opti- mal for several reasons, one being the ency- clopedic genre of Wikipedia, the other being the fact that while Wikipedia is written in for- mal ”correct” Thai, blog posts are written in a more colloquial language variety. This means that the vocabulary coverage of Wikipedia data is poor when compared to blog posts. Our blog corpus consists of 21,002 unique tokens while the word2vec model trained on Wikipedia has embeddings for 51,358 words. The blog corpus (training, testing and vali- dation combined) contains 6,488 words that are not present in the word2vec model, this means that around 31% of the words present in our blog depression corpus are simply not in a Wikipedia based model. In the future, it is clear that Thai language calls for openly available models that are trained on a larger and more varied internet corpus than solely on Wikipedia.",1,
"Abstract We present the first openly available cor- pus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based mod- els. We achieve a 77.53% accuracy with a Thai BERT model in detecting depres- sion. This establishes a good baseline for future researcher on the same corpus. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see Hamalainen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. Related work In this section we present some of the recent related work in more detail. While there have been several digital humanities driven research efforts in better understanding depression in text (Girard and Cohn, 2015; Abd Yusof et al., 2017; Loveys et al., 2018), we only focus on work that has been conducted on depression detection. There are also several approaches to depression detection in other languages (Pi- rina and Cltekin, 2018; Husseini Orabi et al., 2018; Song et al., 2018). An onset of major depression can be char- acterized and detected by investigating social media data such as Twitter (De Choudhury et al., 2013b,a). By exploring a large cor- pus of Twitter posting by using crowd-sourcing methodology. An SVM classifier was built by divesting a variety of social media measures such as social activity, egonetwork, style, user engagement, emotion, and language. The clas- sifier model predicted with high accuracy (70% and 73 % respectively) predicting ahead of the reported onset of depression and whether or not a post on Twitter could be depressive- indicative postings. Depression levels is de- tected by a proposed social media depression index. Computerized analysis of various kind of texts related to depression reveals signals of psychiatric disorders. Depression is measured by self-reported symptoms (Rude et al., 2002), by clinical interview (Rude et al., 2003). The Scrambled Sentences Test (SST) (Wenzlaff, 1993) was used to measure of cognitive process- ing bias of a large sample of college students. Negative cognitive processing biases in resolv- ing ambiguous verbal information can predict depression and subsequent depression symp- toms. Depression has been detected automatically before in Thai Facebook users (Katchapakirin et al., 2018). The authors train several models on RapidMiner. The models rely on metadata for activity on Facebook such as the number of posts posted on a given week day, the number of day-time and night-time posts and the num- ber of shared posts. The authorsnot train the models on text, but rather used numerical features extracted from Facebook posts such as the number of first person pronouns and number of positive words. A screening text-based classification model was also applied to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). A future researcher on the same corpus, you can apply the same techniques to Thai Facebook posts. An infor- sion scenario, a suicide attempt may be mistaken for depression. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see Hamalainen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2.",0,
"Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination – wherein a model generates factually inaccurate statements – has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our model’s output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response model’s task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the model’s output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models – without any fine-tuning – with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval models (Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as “hallucination”, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate hallu- cination by using recent advancements in retrieval- augmented generative models developed for open- domain QA tasks (Lewis et al., 2020b; Izacard and Grave, 2021). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions (Khot et al., 2020; Jiang and Bansal, 2019; Gupta et al., 2020). Khot et al. (2020) learn a generative model that decomposes the task in the language of existing QA models for HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019). Herzig et al. (2021) solve text-to-SQL tasks with intermediate text representations. For storytelling, hierarchical generation procedures have been pro- posed (Fan et al., 2018). In reinforcement learning settings, generating natural language has been used as an intermediate planning step (Sharma et al., 2021; Hu et al., 2019), and in particular in goal- oriented dialogue (Yarats and Lewis, 2018) and open-domain QA (Adolphs et al., 2021) as well. For summarization tasks, the work of Baziotis et al. (2019) proposes an intermediate autoencoder latent representation. Similarly, West et al. (2019) ap- ply the information bottleneck principle to find an intermediate compressed sentence that can best pre- dict the next sentence. For knowledge-grounded dialogue, an approach using internet search can also be seen as a modular intermediate step, where the search query is first generated (Komeili et al., 2021). In that sense retrieval-based QA has also been seen as a modular technique in many studies (Chen et al., 2017; Yan et al., 2019). Previous work has also explored the intersection of QA and dialogue models from multiple different angles. The DREAM dataset (Sun et al., 2019) con- sists of multiple-choice questions about a conver- sation. Yang and Choi (2019) propose a question- answering task based on dialogue histories of the TV show Friends. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) datasets are de- signed to have the questions asked in the conver- sational flow, with possibly, multiple follow-ups. However, while these datasets require a model to understand a dialogue’s history, the target re- sponses are short-form answers. Therefore, these tasks do not train a dialogue model that gener- ates an engaging, conversationally appropriate re- sponse; instead, they result in a QA model that understands dialogue-structured context. Conclusion In this work, we presented K2R: a modular ap- proach for knowledge-based dialogue models. We showed that by decomposing the knowledge step and response generation into explicit sequence-to- sequence subtasks, we could improve dialogue sys- tems by incorporating knowledge or turning short QA model answers into an appropriate conversa- tional form. In detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answer- ing questions, and improves generation metrics on open-domain dialogue. Furthermore, it allows for more interpretable results and supports knowledge injection. Future work should continue to investi- gate methods with modular reasoning steps to help in difficult language tasks.",1,
"Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this reasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination wherein a model generates factually inaccurate statements has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our model's output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response model's task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the model's output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models without any fine-tuning with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval(Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as hallucination, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate hallu- cination by using recent advancements in retrieval- augmented generative models developed for open- domain QA tasks (Lewis et al., 2020b; Izacard and Grave, 2021). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions (Khot et al., 2020; Jiang and Bansal, 2019; Gupta et al., 2020). Khot et al. (2020) learn a generative model that decomposes the task in the language of existing QA models for HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019). Herzig et al. (2021) solve text-to-SQL tasks with intermediate text representations. For storytelling, hierarchical generation procedures have been pro- posed (Fan et al., 2018). In reinforcement learning settings, generating natural language has been used as an intermediate planning step (Sharma et al., 2021; Hu et al., 2019), and in particular in goal- oriented dialogue (Yarats and Lewis, 2018) and open-domain QA (Dinan et al., 2019) as well. For summarization tasks, the work of Baziotis et al. (2019) proposes an intermediate autoencoder latent representation. Similarly, West et al. (2019) ap- ply the information bottleneck principle to find an intermediate compressed sentence that can best pre- dict the next sentence. For knowledge-grounded dialogue, an approach using internet search can also be seen as a modular intermediate step, where the search query is first generated (Komeili et al., 2021). In that sense retrieval-based QA has also been seen as a modular technique in many studies (Chen et al., 2017; Yan et al., 2019). Previous work has also explored the intersection of QA and dialogue sys- tems together to generate dialogue responses. The DREAM dataset (Sun et al., 2019) con- sists of multiple-choice questions about a conver- sation. Yang and Choi (2019) propose a question- answering task based on dialogue histories of the TV show Friends. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) datasets are de- signed to have the questions asked in the conver- sational flow, with possibly, multiple follow-ups. However, while these datasets require a model to understand a dialogue's history, the target re- sponses are short-form answers. Therefore, these tasks do not train a dialogue model that gener- ates an engaging, conversationally appropriate re- sponse; instead, they result in a QA model that understands dialogue-structured context. Conclusion In this work, we presented K2R: a modular ap- proach for knowledge-based dialogue models. We showed that by decomposing the knowledge step and response generation into explicit sequence-to- sequence subtasks, we could improve dialogue sys- tems by incorporating knowledge or turning short QA model answers into an appropriate conversa- tional form. In detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answer- ing questions, and improves generation metrics on open-domain dialogue. Furthermore, it allows for more interpretable results and supports knowledge injection. Future work should continue to investi- gate methods with modular reasoning steps to help in difficult language tasks.",0,
"Abstract Crowdsourcing requesters on Amazon Me- chanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they ex- pect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workers’ forums. Some of the issues causing the mass rejections stem from the requesters not tak- ing the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourc- ing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records in- formation from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation is- sues such as missing instructions or HITs that are not doable. 1 Introduction The rise of artificial intelligence has led to a surge in the need for data. Models are becoming increas- ingly more complex, needing more fine-grained data. In order to quickly collect such data, many researchers have turned to crowdsourcing. The crowdsourcing platform that is most familiar to re- questers is Amazon Mechanical Turk (AMT). Any- one in many countries can create an Amazon ac- count and offer work, in the form of HITs, to AMT workers. Desiring rapid results, requesters often post HITs quickly, not taking into account that the way in which a HIT is presented has a direct effect on the quality of the data they obtain. They may also not be aware that their actions as requesters affect the quality of the data they gather. For ex- ample, a HIT could have unclear instructions, low payment, or in some cases might not even function. The two former cases result can lead to lower data quality. Lately, this has lead requesters, seeing how to revise their HIT for better results, to refuse to pay for their earlier errorful HIT. Thus they send out mass rejections (rejecting all of the work on a given HIT regardless of its quality). While for the requester this is just restarting a HIT, the effect of this mass rejection is felt on both sides. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workers’ forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.1 Worker Payment A major issue affecting both the interaction be- tween the requester and the worker and the data quality is worker payment. Hara et al (Hara et al., 2018) have shown that the mean wage for a worker on AMT is very low, $3.13 per hour at the time, while requesters generally pay on average $11.58 per hour. This apparent disparity is due to the fact that the most abundant HITs tend to be the lower paid ones. Researchers may believe that AMT workers are willing to work for very little, and that the amount of compensation does not af- fect data quality (Buhrmester et al., 2016) (Mason and Watts, 2009). Indeed, a good worker will try to work on the better paying HITs when possible. But if it is the end of the month and rent is due, workers are obliged to take whatever HITs are available at that time even if they don’t pay well. This may result in the worker spending less time reading in- structions or in actually working on a HIT, in order to make the meager payment cover less work time. This usually results in lower quality HIT data. 2.2 Communication In addition to worker payment, the quality of com- munication between requesters and workers is fre- quently mentioned on forums such as Turkopticon (Hanrahan et al., 2021). Good communication is important when a worker wants to tell a requester about issues they have when attempting to do a HIT or when they want to find out why their work has been rejected. Good communication can lead to better data quality since it not only provides the worker with a better idea of what the requester re- ally wants, but also helps the requester to be aware of any issues with a given HIT. 2.3 Rejecting Work Rejections are a problem for both the requester and the worker. If a requester rejects work and does not use some subset of the data they collected, they may need to re-publish their HIT so that they can fulfill the originally desired quantity of data. This delays their work on their results. For the worker, if their work is rejected, their reputation suffers and they may not meet certain qualifications like HIT approval percentage for well-paying HITs (Hara et al., 2018). Even though rejections constitute a small percentage of work (Hara et al., 2018), real workers who have honestly put effort into their work should be paid for that time. On the other hand, requesters feel justified in rejecting work that has obviously come from bots. While this seems reasonable, lately the creators of those bots have started to write requester reviews, thus tarnishing the reputation of the requester. AMT suggests that when a requester is sure that a specific worker ac- count is a bot, they should pay them and at the same time report them to AMT. There are several other factors that affect the quality of the work, such as HITs that do not work (buggy), HITs that have no standard for evaluation (McInnis et al., 2016) (Schmidt and Jettinghoff, 2016) and poor instructions. The aforementioned websites and worker forums are used, by about 60% of all workers (Yang et al., 2018). Thus it is important to be attentive to worker feedback. Conclusion This paper reveals that, for a significant amount of HITs, there is a disconnect between the workers and the requesters. Payment is often not fair in both the workers’ eyes and according to federal wage standards. HIT instructions, communication, and understanding of rejections need improvement for requesters to express what they are actually looking to obtain and thus for them to find that their data is of the quality they were expecting.",1,
"Abstract Crowdsourcing requesters on Amazon Me- chanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they ex- pect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workers' forums. Some of the issues causing the mass rejections stem from the requesters not tak- ing the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourc- ing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records in- formation from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation is- sues such as missing instructions or HITs that are not doable. 1 Introduction The rise of artificial intelligence has led to a surge in the need for data. Models are becoming increas- ingly more complex, needing more fine-grained data. In order to quickly collect such data, many researchers have turned to crowdsourcing. The crowdsourcing platform that is most familiar to re- questers is Amazon Mechanical Turk (AMT). Any- one in many countries can create an Amazon ac- count and offer work, in the form of HITs, to AMT workers. Desiring rapid results, requesters often post HITs quickly, not taking into account that the way in which a HIT is presented has a direct effect on the quality of the data they obtain. They may also not be aware that their actions as requesters affect the quality of the data they gather. For ex- ample, a HIT could have unclear instructions, low payment, or in some cases might not even function. The two former cases result can lead to lower data quality. Lately, this has lead requesters, seeing how to revise their HIT for better results, to refuse to pay for their earlier errorful HIT. Thus they send out mass rejections (rejecting all of the work on a given HIT regardless of its quality). While for the requester this is just restarting a HIT, the effect of this mass rejection is felt on both sides. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workers forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.1 Worker Payment A major issue affecting both the interaction be- tween the requester and the worker and the data quality is worker payment. Hara et al (Hara et al., 2018) have shown that the mean wage for a worker on AMT is very low, $3.13 per hour at the time, while requesters generally pay on average $11.58 per hour. This apparent disparity is due to the fact that the most abundant HITs tend to be the lower paid ones. Researchers may believe that AMT workers are willing to work for very little, and that the amount of compensation does not af- fect data quality (Buhrmester et al., 2016) (Mason and Watts, 2009). Indeed, a good worker will try to work on the better paying HITs when possible. But if it is the end of the month and rent is due, workers are obliged to take whatever HITs are available attime even if they don't pay well. This may result in the worker spending less time reading in- structions or in actually working on a HIT, in order to make the meager payment cover less work time. This usually results in lower quality HIT data. 2.2 Communication In addition to its regular applications, communication is also a source of conflict. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workers' forums such as Turkop- ticon (Irani and Silberman, 2013) 1 2, TurkerView 3, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. Requesters can monitor their reputation on these sites and use worker feedback to improve their HITs and thus eventually their reputations. Background Recently, recruiting workers and bringing them into a lab has become much less desirable due to the high cost and the lack of diversity of the workers. In response to this issue AMT has grown in popularity amongst researchers (Paolacci et al., 2010). 2.3 Communication Channels In addition to its regular applications, communication is also a source of conflict. The workers are not paid for the time they spent regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs. The requester also gets a bad reputation on the quality of their HITs and their payments on the workers' forums such as Turkopticon (Irani and Silberman, 2013) 1 2, Turker Nation 4, and MTurk Crowd 5. These sites are regularly visited by many of the workers to determine which HITs are considered to be a reliable source of income. They also use the in- formation to avoid certain requesters. AMT has grown in popularity amongst researchers (Paolacci et al., 2010). Forest et al (2013) have shown that the quality of the workers' forums such as AMT that were most familiar to re- questers is the quality of the data they gather. They are not paid for the time they spend regard- less of the quality of their work. Also, since their work has been rejected, their individual rating goes down, thus making it hard for them to qualify in the future for other HITs.",0,
"Abstract Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models’ weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model’s size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scripts to reproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning.1 Both methods suggest pruning BERT-Base while fine-tuning to downstream tasks paired with model distillation, and present results showing 90% sparsity for several tasks. However, both methods require a long fine-tuning time as well as tuning pruning related hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we present better or comparable results at a much lower computation budget at the transfer learning phase. Gordon et al. [2020] explored the effect of weight pruning during transfer learning and concluded that pruning BERT-Base at the pre-training phase does not degrade the performance of the model compared to pruning at fine-tuning. We improve upon the suggested method and present better results at a much higher sparsity ratio. Chen et al. [2020] explored the Lottery Ticket Hypothesis [Frankle and Carbin, 2018] for BERT pre-trained models. More specifically, they analyzed the possibility of finding winning tickets in a BERT-Base pre-trained model that transfer to other downstream tasks. The authors concluded that winning tickets found while pre-training on a Masked-LM task, transfer well to other downstream tasks. Lagunas et al. [2021] presented a structured pruning method, removing rows, columns and attention heads, while achieving less than 1% loss in F1 for a BERT architecture on SQuADv1.1. Mishra et al. [2021] performed structured 2:4 pruning on BERT while further pre-training BERT; The method produced a 50% sparse model which can be fine-tuned without accuracy loss. Michel et al. [2019] explored the significance of each head in the multi-head attention mechanism of BERT and presented a method for pruning attention heads with their associated weights. Other works propose knowledge distillation to compress Transformer models to a smaller dense counter part that can be tuned to downstream tasks [Sanh et al., 2019, Jiao et al., 2020, Sun et al., 2020]. Quantization of Transformer-based language models is also a well known method for compression. Shen et al. [2020] proposes a method to quantize BERT at a different bit-width per layer. Other works implement quantization-aware training to quantize BERT to 8bits [Kim et al., 2021, Zafrir et al., 2019]. Zhang et al. [2020] created a method of producing a ternary weight BERT. Kim and Hassan [2020] presented a compression pipeline for Transformer models that includes model distillation, quantization and head pruning. Conclusion and future work We introduced Prune OFA, an architecture-agnostic method for producing sparse pre-trained language models. We also showed how these sparse models can be used to obtain fine-tuned sparse models without the burden of task-specific pruning. Our results suggest that using these sparse pre-trained models for transfer learning produces results with minimal performance degradation loss w.r.t their dense counterpart for a variety of NLP tasks. We further demonstrated that integrating quantization can lead to more efficient sparse and quantized models at a small cost to the model’s accuracy. A possible direction for future research is to explore whether a large and sparse pre-trained model is better at capturing and transferring natural language knowledge than a smaller dense model of the same architecture with similar non-zero parameters count. We hope that the release of our code and sparse pre-trained models to the community will help develop more efficient models.",1,
"Abstract Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models' weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model's size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scriptsreproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pro method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Transformer-based pre-trained models on target hardware. The method uses sparse pre-trained models to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the method's accuracy loss can be reduced in these sparse models for a wide range of tasks while maintaining their sparsity pattern. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model's size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3).",0,
"Abstract Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker Factorization—a recent approach that relaxes independence assumptions—to prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters. Introduction Creating a single model that performs well across multiple domains is often desirable, especially in production systems. Relying on multiple (task-specific) systems necessitates storing and managing corresponding collections of parameters. Multi-task models Caruana [1997] obviate this need by performing well on inputs from all tasks, simplifying deployment. In the medical domain especially, new data is constantly becoming available, and it is necessary to keep models up to date with this deluge. In medical entity linking—where the goal is to link mentions in clinical text to corresponding entities in an ontology—the underlying ontologies are frequently updated, and the new terms are put into use quickly. For example, in the past year or so many codes related to COVID-19 Guan et al. [2020] were added to the International Classification of Diseases (ICD) lexicon, therefore updating models to incorporate new codes without losing performance on older knowledge is of particular importance. The language in the medical domain also brings additional challenges because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are cases—particularly when dealing with medical data—where data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson d’ Autume et al. [2019] maintains performance when training on new tasks by “replaying” examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used. Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017] is an alternative, constraint-based technique that regularizes parameters such that they are encouraged to maintain optimal weights learned for prior tasks. By placing a prior involving the Hessian from previous tasks over network parameters, EWC affords flexibility with respect to changing parameters in different dimensions. Critically, EWC does not require continued access to data from ‘past’ tasks once key statistics are computed over a task’s data. However, to scale to even relatively small neural networks, EWC must assume independence between all parameters. This assumption allows one to drop off-diagonal terms in the Fisher Information Matrix (FIM), which is used to approximate the Hessian; calculating the full matrix would be intractable. Recent work Ritter et al. [2018b] has proposed Kronecker Factorization (KF) — which the optimization community uses to compute Hessians in neural networks — to perform a version of EWC with a relaxed independence assumption on networks of linear layers operating on small vision datasets. Our contribution here is the extension of EWC and KF to the large-scale neural models now common in NLP. In particular, modern NLP tends to rely on large-scale models with hundreds of millions of parameters Devlin et al. [2019], Liu et al. [2019], Radford et al. [2019], and CF is problematic across many of its sub-domains. Though EWC has been successfully applied to NLP in recent work (Section 7), we demonstrate that there is room for substantial gains. In particular, we have observed that the independence assumption over parameters significantly and negatively affects EWC’s ability to mitigate CF, as compared to what is achieved using the full covariance matrix of parameters. As far as we are aware, this work is the first application of the Kronecker Factorization method— which relaxes the assumption of independence between parameters—for continuous learning in large-scale networks. Though we do not model all elements of the Fisher Information Matrix, we approximate block diagonals corresponding to layers, which is less damaging than assuming completely independent parameters. Specifically, we apply Kronecker Factorization in two large-scale neural networks for Entity Linking: (1) a convolutional and (2) a transformer-based architecture Vaswani et al. [2017]. Our primary contributions are as follows: (1) We combine and extend prior work in EWC and Kronecker Factorization to modern large-scale NLP models. (2) We use Kronecker Factorization to train on multiple biomedical ontology entity linking tasks sequentially without access to previous data, and show that it significantly outperforms baseline methods. Related Work CF Mitigation in Continuous Learning. There is a large body of work on CF mitigation, much of which focuses on constraining model parameters during training. From this category, we use learning rate control from UMLFit Howard and Ruder [2018] and EWC Kirkpatrick et al. [2017] as baselines. Path Integral Zenke et al. [2017] is similar to EWC but calculates weight variances from the optimization path instead of at the end of training. Chaudhry et al. [2018] combines these methods and generalizes the fisher by replacing it with a KL divergence between output distributions of the previous task weights and those of the current weights. Progressive networks Rusu et al. [2016] do not constrain training but use features of previously trained networks as additional input to subsequently trained ones. Progress and Compress Schwarz et al. [2018] extends this by using EWC to create one model that performs well on all tasks. Previous work has also focused on preventing CF by introducing a working memory that allows past examples to be replayed. We use experience replay from de Masson d’ Autume et al. [2019] as an additional data-dependent baseline for our methods, but there are many other methods in this category Sprechmann et al. [2018], Wang et al. [2019], and many which combine these methods with regularization methods Lopez-Paz and Ranzato [2017], Chaudhry et al. [2019]. Kronecker Factorization for Neural Networks. Martens and Grosse [2015] first introduce Kro- necker Factorization as an approximation of blocks of the FIM of neural networks. They use this to perform second-order optimization techniques on linear neural networks, and then extend this to opti- mizing convolutional architectures Grosse and Martens [2016]. More recently, Ritter et al. [2018a] show that the Fisher approximation can be used as a posterior on network weights, and then expand EWC to use off-diagonal elements of the FIM in its regularization term with this approximation Ritter et al. [2018b]. This last paper mainly focuses on small vision datasets. Continuous Learning in NLP. NLP seems to be particularly susceptible to CF Howard and Ruder [2018], Yogatama et al. [2019]. Recent work has therefore focused on developing continuous learning techniques in NLP to mitigate this issue Moeed et al. [2020], Pilault et al. [2020], Chen et al. [2020]. There has also been work that applies previous techniques to specific domains (e.g. machine translation Thompson et al. [2019], sentiment analysis Madasu and Rao [2020], and reading comprehension Xu et al. [2020]). Many of these methods focus on avoiding over-fitting to new tasks during fine-tuning, whereas we focus on maintaining high performance on old tasks. None of these use Kronecker Factorization, which has not yet been scaled to prevent CF in large NLP models. CF mitigation is particularly important in clinical NLP given that many clinical datasets are quite different from generic domains and from each other. Arumae et al. [2020] explore CF in language modeling when transferring between the generic, clinical, and biomedical domains and compare learning rate control, experience replay, and EWC. Conclusions We have demonstrated the effectiveness of Kroneker Factorization (KFC) for preventing catastrophic forgetting in modern large-scale neural architectures commonly used in NLP, improving on the results of Elastic Weight Consolidation. We showed that KFC can be used to create a unified model on multiple domains of Medical Entity Linking with good performance across tasks after a continuous (sequential) learning process. We highlighted strengths and weaknesses of the adopted approximation used in KFC, pointing to potential future directions. Future work might consider alternatives to the block diagonal structure on the covariance matrix used in KFC. Another promising line of inquiry concerns reducing the difficulty in selecting the λ hyper-parameter, which controls the strength of the regularization, without requiring a robust grid-search.",1,
"Abstract Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker Factorization a recent approach that relaxes independence assumptions' to prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters. Introduction Creating a single model that performs well across multiple domains is often desirable, especially in production systems. Relying on multiple (task-specific) systems necessitates storing and managing corresponding collections of parameters. Multi-task models Caruana [1997] obviate this need by performing well on inputs from all tasks, simplifying deployment. In the medical domain especially, new data is constantly becoming available, and it is necessary to keep models up to date with this deluge. In medical entity linking where the goal is to link mentions in clinical text to corresponding entities in an ontology the underlying ontologies are frequently updated, and the new terms are put into use quickly. For example, in the past year or so many codes related to COVID-19 Guan et al. [2020] were added to the International Classification of Diseases (ICD) lexicon, therefore updating models to incorporate new codes without losing performance on older knowledge is of particular importance. The language in the medical domain also brings additional challenges because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are cases particularly when dealing with medical data where data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson d' Autume et al. [2019] maintains performance when training on new tasks by ""replaying"" examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used. Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017] is an alternative, constraint-based technique that regularizes parameters such that they are encouraged to maintain optimal weights learned for prior tasks. By placing a prior involving the Hessian from previous tasks over network parameters, EWC affords flexibility with respect to changing parameters in different dimensions. Critically, EWC does not require continued access to data from ""past"" tasks once key statistics are computed over a task's data. However, to scale to even relatively small neural networks, EWC must assume independence between all parameters. This assumption allows one to drop off-diagonal terms in the Fisher Information Matrix (FIM), which is used to approximate the Hessian; calculating the full matrix would be intractable. Recent work Ritter et al. [2018b] has proposed Kronecker(KF) which the optimization community uses to compute Hessians in neural networks to perform a version of EWC with a relaxed independence assumption on networks of linear layers operating on small vision datasets. Our contribution here is the extension of EWC and KF to the large-scale neural models now common in NLP. In particular, modern NLP tends to rely on large-scale models with hundreds of millions of parameters Devlin et al. [2019], Liu et al. [2019], Radford et al. [2019], and CF is problematic across many of its sub-domains. Though EWC has been successfully applied to NLP in recent work (Section 7), we demonstrate that there is room for substantial gains. In particular, we have observed that the independence assumption over parameters significantly and negatively affects EWC's ability to mitigate CF, as compared to what is achieved using the full covariance matrix of parameters. As far as we are aware, this work is the first application of the Kronecker Factorization method which relaxes the assumption of independence between parameters for continuous learning in large-scale networks. Though we do not model all elements of the Fisher Information Matrix, we approximate block diagonals corresponding to layers, which is less damaging than assuming completely independent parameters. Specifically, we apply Kronecker Factorization in two large-scale neural networks for Entity Linking: (1) a convolutional and (2) a transformer-based architecture Vaswani et al. [2017]. Our primary contributions are as follows: (1) We combine and extend prior work in EWC and Kronecker Factorization to modern large-scale NLP models. (2) We use Kronecker Factorization to train on multiple biomedical ontology entity linking tasks sequentially without access to previous data, and show that it significantly outperforms baseline methods. Related Work CF Mitigation in Continuous Learning. There is a large body of work on CF mitigation, much of which focuses on constraining model parameters during training. From this category, we use learning rate control from UMLFit Howard and Ruder [2018] and EWC Kirkpatrick et al. [2017] as baselines. Path Integral Zenke et al. [2017] is similar to EWC but calculates weight variances from the optimization path instead of at the end of training. Chaudhry et al. [2018] combines these methods and generalizes the fisher by replacing it with a KL divergence between output distributions of the previous task weights and those of the current weights. Progressive networks Rusu et al. [2016] do not constrain training but use features of previously trained networks as additional input to subsequently trained ones. Progress and Compress Schwarz et al. [2018] extends this by using EWC to create one model that performs well on all tasks. Previous work has also focused on preventing CF by introducing a working memory that allows past examples to be replayed. We use experience replay from de Masson d'Autume et al. [2019] as an additional data-dependent baseline for our methods, but there are many other methods in this category Sprechmann et al. [2018], Wang et al. [2019], and many which combine these methods with regularization methods Lopez-Paz and Ranzato [2017], Chaudhry et al. [2019]. Kronecker Factorization for Neural Networks. Martens and Grosse [2015] first introduce Kro- necker Factorization as an approximation of blocks of the FIM of neural networks. They use this to perform second-order optimization techniques on linear neural networks, and then extend this to opti- mizing convolutional architectures Grosse and Martens [2016]. More recently, Ritter et al. [2018a] show that the Fisher approximation can be used as a posterior on network weights, and then expand EWC to use off-diagonal elements of the FIM in its regularization term with this approximation Ritter et al. [2018b]. This last paper mainly focuses on small vision datasets. Continuous Learning in NLP. NLP seems to be particularly susceptible to CF Howard and Ruder [2018], Yogatama et al. [2019]. Recent work has therefore focused on developing continuous learning techniques in NLP to mitigate this issue Moeed et al. [2020], Pilault et al. [2020], Chen et al. [2020]. There has also been work that applies previous techniques to specific domains (e.g. machine translation Thompson et al. [2019], sentiment analysis Madasu and Rao [2020], and reading comprehension Xu et[2020]). Many of these methods focus on avoiding over-fitting to new tasks during fine-tuning, whereas we focus on maintaining high performance on old tasks. None of these use Kronecker Factorization, which has not yet been scaled to prevent CF in large NLP models. CF mitigation is particularly important in clinical text messaging (particularly when dealing with medical data) because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are cases particularly when dealing with medical data where data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks.",0,
"Abstract Virtual Adversarial Training (VAT) has been effective in learning robust models under su- pervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se- mEval2018 multilabel and multilingual emo- tion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models. Introduction Emotion recognition is an active and crucial area of research, especially for social media platforms. Un- derstanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, ha- tred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text under- standing has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, an- notating emotion categories is expensive and time consuming as emotion categories are highly cor- related and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like ""anger"" and ""sadness"" are co- related and co-occur more frequently than ""anger"" and ""happiness"" (Plutchik, 1980). In a multilingual setup, the annotation becomes even more challeng- ing as annotator team are expected to be familiar with different languages and culture for understand- ing the emotions accurately. Imbalance in availabil- ity of the data across languages further creates prob- lems, especially in case of resource impoverished languages. In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework. We formulate semi-supervised Virtual Adversarial Training (VAT) (Miyato et al.,2018) for multilabel emotion classification using contextual models and perform extensive exper- iments to demonstrate that unlabelled data from other languages Lul = {L1, L2, . . . , Ln} improves the classification on the target language Ltgt. We obtain competitive performance by reducing the amount of annotated data demonstrating cross- language learning. To effectively leverage the multilingual content, we use multilingual contex- tual models for representing the text across lan- guages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following: We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classifi- cation on multilingual corpus. Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Ara- bic, Spanish and English by leveraging un- labelled data of other languages while using 10% of labelled samples. Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respec- tively. Experiments showcasing the advantages of domain-adaptive and task-adaptive pre- training. Related Work Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and un- supervised learning by leveraging the informa- tion hidden in large amount of unlabelled data along with small amount of labelled data (Yang et al., 2021), (Van Engelen and Hoos, 2020). Early approaches used self-training for leveraging the model’s own predictions on unlabelled data to obtain additional information during training (Yarowsky,1995)(McCloskyetal.,2006). Clark et al. (2018) proposed cross-view training (CVT) for various tasks like chunking, dependency pars- ing, machine translation and reported state-of-the- art results. CVT forces the model to make consis- tent predictions when using the full input or partial input. Ladder networks (Laine and Aila, 2016), Mean Teacher networks (Tarvainen and Valpola, 2017) are another way for semi-supervised learn- ing where temporal and model-weights are ensem- bled. Another popular direction towards semi- supervised learning is adversarial training where the data point is perturbed with random or care- fully tuned perturbations to create an adversarial sample. The model is then encouraged to main- tain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models (Goodfellow et al., 2014), (Xiao et al., 2018), (Saadatpanah et al., 2020) to pre- vent attacks. Miyato et al. (2016), Cheng et al. (2019), Zhu et al. (2019) showed that adversarial training can improve both robustness and gener- alization for classification tasks, machine transla- tion and GLUE benchmark respectively. Miyato et al. (2016), Sachan et al. (2019), Miyato et al. (2018) then applied the adversarial training for semi-supervised image and text classification show- ing substantial improvements. Emotion recognition is an important problem and has received lot of attention from the com- munity (Yadollahi et al., 2017), (Sailunaz et al., 2018). The taxonomies of emotions suggested by Plutchik wheel of emotions (Plutchik, 1980) and (Ekman, 1984) have been used by the majority of the previous work in emotion recognition. Emo- tion recognition can be formulated as a multiclass problem (Scherer and Wallbott, 1994), (Moham- mad, 2012) or a multilabel problem (Mohammad et al., 2018), (Demszky et al., 2020). In the multi- class formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emo- tion can be present in the text instance. Binary relevance approach (Godbole and Sarawagi, 2004) is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation be- tween emotions. Seq2Seq approaches (Yang et al., 2018), (Huang et al., 2021) solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis (Felbo et al., 2017), hash- tags (Mohammad, 2012) or pretraining emotion specific embeddings and language models (Barbi- eri et al., 2021), (Ghosh et al., 2017). With the emergence of contextual models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) etc., the field of NLP and text classifi- cation has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different lan- guages and domains (Hassan et al., 2021), (Bar- bieri et al., 2021). Social media content contains linguistic errors, idiosyncratic styles, spelling mis- takes, grammatical inconsistency, slangs, hashtags, emoticons etc. (Barbieri et al., 2018), (Derczynski et al., 2013) due to which off-the-shelf contextual models may not be optimum. We use language- adaptive, domain-adaptive and task-adaptive pre- training which has shown performance gains (Pe- ters et al., 2019), (Gururangan et al., 2020), (Barbi- eri et al., 2021), (Howard and Ruder, 2018), (Lee et al., 2020) for different tasks and domains. Conclusion In this work, we explored semi-supervised learning using Virtual Adversarial Training (VAT) for multi- label emotion classification in a multilingual setup and showed performance improvement by leverag- ing unlabelled data from different languages. We used Mean Square Error (MSE) as the divergence measure for leveraging VAT for multilabel emotion classification. We also evaluated the performance of monolingual, multilingual and domain-adaptive and task-adaptive multilingual contextual models across three languages - English, Spanish and Ara- bic for multilabel and multilingual emotion recog- nition and obtained state-of-the-art results. We also performed probing experiments for understanding the impact of different layers of contextual models.",1,
"Abstract Virtual Adversarial Training (VAT) has been effective in learning robust models under su- pervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se- mEval2018 multilabel and multilingual emo- tion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models. Introduction Emotion recognition is an active and crucial area of research, especially for social media platforms. Un- derstanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, ha- tred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text under- standing has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, an- notating emotion categories is expensive and time consuming as emotion categories are highly cor- related and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like ""anger"" and ""sadness"" are co- related and co-occur more frequently than ""anger"" and ""happiness"" (Plutchik, 1980). In a multilingual setup, the annotation becomes even more challeng- ing as annotator team are expected to be familiar with different languages and culture for understand- ing the emotions accurately. Imbalance in availabil- ity of the data across languages further creates prob- lems, especially in case of resource impoverished languages. In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework. We formulate semi-supervised Virtual Adversarial Training (VAT) (Miyato et al.,2018) for multilabel emotion classification using contextual models and perform extensive exper- iments to demonstrate that unlabelled data from other languages Lul = {L1, L2, . . . , Ln} improves the classification on the target language Ltgt. We obtain competitive performance by reducing the amount of annotated data demonstrating cross- language learning. To effectively leverage the multilingual content, we use multilingual contex- tual models for representing the text across lan- guages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following: We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classifi- cation on multilingual corpus. Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Ara- bic, Spanish and English by leveraging un- labelled data of other languages while using 10% of labelled samples. Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish,and English respec- tively. Experiments showcasing the advantages of domain-adaptive and task-adaptive pre- training. Related Work Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and un- supervised learning by leveraging the informa- tion hidden in large amount of unlabelled data along with small amount of labelled data (Yang et al., 2021), (Van Engelen and Hoos, 2020). Early approaches used self-training for leveraging the model's own predictions on unlabelled data to obtain additional information during training (Yarowsky,1995)(McCloskyetal.,2006). Clark et al. (2018) proposed cross-view training (CVT) for various tasks like chunking, dependency pars- ing, machine translation and reported state-of-the- art results. CVT forces the model to make consis- tent predictions when using the full input or partial input. Ladder networks (Laine and Aila, 2016), Mean Teacher networks (Tarvainen and Valpola, 2017) are another way for semi-supervised learn- ing where temporal and model-weights are ensem- bled. Another popular direction towards semi- supervised learning is adversarial training where the data point is perturbed with random or care- fully tuned perturbations to create an adversarial sample. The model is then encouraged to main- tain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models (Goodfellow et al., 2014), (Xiao et al., 2018), (Saadatpanah et al., 2020) to pre- vent attacks. Miyato et al. (2016), Cheng et al. (2019), Zhu et al. (2019) showed that adversarial training can improve both robustness and gener- alization for classification tasks, machine transla- tion and GLUE benchmark respectively. Miyato et al. (2016), Sachan et al. (2019), Miyato et al. (2018) then applied the adversarial training for semi-supervised image and text classification show- ing substantial improvements. Emotion recognition is an important problem and has received lot of attention from the com- munity (Yadollahi et al., 2017), (Sailunaz et al., 2018). The taxonomies of emotions suggested by Plutchik wheel of emotions (Plutchik, 1980) and (Ekman, 1984) have been used by the majority of the previous work in emotion recognition. Emo- tion recognition can be formulated as a multiclass problem (Scherer and Wallbott, 1994), (Moham- mad, 2012) or a multilabel problem (Mohammad et al., 2018), (Demszky et al., 2020). In the multi- class formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emo- tion can be present in the text instance. Binary relevance approach (Godbole and Sarawagi, 2004) is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation be- tween emotions. Seq2Seq approaches (Yang et al., 2018), (Huang et al., 2021) solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis (Felbo et al., 2017), hash- tags (Mohammad, 2012) or pretraining emotion specific embeddings and language models (Barbi- eri et al., 2021), (Ghosh et al., 2017). With the emergence of contextual models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) etc., the field of NLP and text classifi- cation has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different lan- guages and domains (Hassan et al., 2021), (Bar- bieri et al., 2021). Social media content contains linguistic errors, idiosyncratic styles, spelling mis- takes, grammatical inconsistency, slangs, hashtags, emoticons etc. (Barbieri et al., 2018), (Derczynski et al., 2013) due to which off-the-shelfmodels may not be optimum. We use language- adaptive, domain-adaptive and task-adaptive pre- training which has shown performance gains (Pe- ters et al., 2019), (Gururangan et al., 2020), (Barbi- eri et al., 2021), (Howard and Ruder, 2018), (Lee et al., 2020) for different tasks and domains. Conclusion In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework.",0,
"Abstract African languages still lag in the advances of Natu- ral Language Processing techniques, one reason be- ing the lack of representative data, having a tech- nique that can transfer information between lan- guages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to cre- ate cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where se- mantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evalu- ate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic qual- ity of the embeddings to determine if there is im- provement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation Introduction Many African languages have insufficient language resources (data, tools, people) (Abbott & Martinus 2019, Martinus & Abbott 2019, Nekoto et al. 2020, Sefaraetal. 2021) and fall into the classification of low resource languages (Ranathunga et al. 2021) in the Natural Language Processing (NLP) field. This lack of resources makes it harder to capi- talise on the recent advances in many NLP down- stream tasks such as Neural Machine Transla- tion (Choetal. 2014), Large Language Models (Devlin et al. 2018, Howard & Ruder 2018), Q&A systems (Kwiatkowskietal. 2019), etc. There may be more downstream approaches to deal with some of these challenges such as Transfer Learning (Ruder et al. 2019), Data Augmentation (Marivate & Sefara 2020a), Multilingual Models (Hedderich et al. 2020), etc. Additionally, the lack of research attention to existing NLP tech- niques results in difficulties finding a benchmark (Abbott & Martinus 2019). In this work, we focus on word representations through word embeddings and how we can leverage one language to assist in the representation of another related language. These embeddings can then be used to develop tools for other downstream tasks. Word Embeddings are a mathematical technique to learn general language vector representations from a large amount of unlabelled text using co-occurring statistics. In recent years, monolingual word em- beddings techniques are increasingly becoming an important resource in NLP. Word embeddings are widely used in NLP problems such as sen- timent analysis (Socher et al. 2013), named-entity- recognition (Guo et al. 2014), parts-of-speech tag- ging, and document retrieval. Word2Vec is a vector training model proposed by Mikolov et al. (2013). Word2Vec produces a low-dimensional real-value vector representing the meaning of a word. The word vector represents grammatical and semantic properties, which results in words with similar se- mantic relations being close to each other. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). Cross-lingual word embeddings have been receiv- ing more and more attention from the NLP com- munity, mainly because it has provided a path to effectively align two disjoint monolingual embed- dings with no bilingual dictionary for unsuper- vised techniques or no more than a small bilingual dictionary for supervised techniques (Lample et al. 2018, Artetxeetal. 2020). Cross-lingual tech- niques also enable knowledge transfer between lan- guages with rich resources and low resources. For languages lacking bilingual parallel corpus with other languages, cross-lingual embeddings can be utilised to train high-quality cross-lingual embed- dings (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Artetxe et al. (2018) created the cross- lingual unsupervised or supervised word embed- ding (VecMap library) approach for training cross- lingual word embedding models. The approaches can be used to construct cross-language word vec- tors with or without a bilingual dictionary. The majority of South African languages lag bilin- gual parallel corpus with other languages. In this work, we aim to investigate how cross-lingual em- beddings could be used to improve the state of one or both languages. We used data (corpus) from different domains to train Word2Vec and fastText (Bojanowski et al. 2016) monolingual embeddings. When using VecMap, the two embeddings are aligned. VecMap requires two monolingual word vectors from source and target (Artetxe et al. 2018). To evaluate the effectiveness of the cross-lingual em- bedding for Setswana and Sepedi, we use intrinsic evaluation (Bakarov 2018) through Setswana and Sepedi versions of WordSim (Finkelstein et al. 2001) and Simlex (Hill et al. 2015). This is following on an approach that has been used for Yoruba and Twi (Alabi et al. 2019). We also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment. Conneau et al. (2018) show that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: ”neighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list.” In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsic embedding was assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLWEs are resilient even when target language training data is scaled-down and that initialisation of NMLM parameters leads to good performance. Artetxe & Schwenk (2019) introduced an architec- ture that can be used to learn multilingual sentence representations for more than 90 languages. The languages belonged to 30 different families. The re- search used a single BiLSTM encoder with a shared Byte Pair Encoding (BPE) vocabulary coupled with an auxiliary decoder and trained on parallel corpora. They learn a classifier using English annotated data only and transfer it to any language without modification. The research mainly focused on vector rep- resentations of sentences that are general for the in- put language and the NLP task. Alabi et al. (2019) worked on massive vs. curated embeddings for low-resourced languages: the case of Yoru` ba ́ and Twi. Authors compare two types of word embeddings obtained from curated cor- pora and a language-dependent processing. They move further to collect high quality and noisy data for the two languages. They quantify that im- provements that is based on the quality of data and not only on the amount of data. In their ex- periments, they use different architectures to learn word representations both from characters and sur- face forms. They evaluate multilingual BERT on a down stream task, specifically named entity recog- nition and WordSim-353 word pairs dataset. Feng et al. (2018) investigates a cross-lingual knowl- edge transfer technique to improve the seman- tic representation of low-resourced languages and improving low resource named-entity recognition. In their research, neural networks are used to do knowledge transfer from high resource language us- ing bilingual lexicons to improve low resource word representation. They automatically learn semantic projections using a lexicon extension strategy that is designed to address out-of lexicon problem. Fi- nally, they regard word-level entity type distribu- tion features as an external language independent knowledge and incorporate them into their neural architecture. The experiment is done on two low resource languages (Dutch and Spanish) to demon- strate the effectiveness of these additional semantic representations. Banerjee et al. (2021) show that initialising the em- bedding layer of Unsupervised Neural Machine Translation (UNMT) models with cross-lingual embeddings shows significant improvements in BLEU score. Authors show that freezing the em- bedding layer weights lead to better gains com- pared to updating the embedding layer weights during training. They experimented using De- noising Autoencoder (DAE) and Masked Sequence to Sequence (MASS) for three different unrelated language pairs (for English-Hindi, English-Bengali, and English-Gujarati). The analysis shows the im- portance of using cross-lingual embedding as com- pared to other techniques. The literature shows that there is a substantial amount of work done on cross-lingual transfer and empirical proof that the method improves the per- formance of models. The literature does not relay solely on intrinsic evaluation but the solutions are applied to some downstream tasks. In the next sec- tion, we detail the data used for conducting experi- ments. Conclusion In this paper, VecMap was used to align Setswana- Sepedi to the same vector space. Through this work, we wanted to use cross-lingual (VecMap) technique to enable knowledge transfer between languages with rich resources and low resources. The results show that it is possible to align two monolingual embeddings to get cross-lingual embeddings. We mapped Setswana to Sepedi and used Spearman’s to check correlation. Interestingly we get different re- sults on fastText and word2Vec-based embeddings though we used the same data to train the embed- dings. In future work, it would be interesting to use the cross-lingual embedding on a downstream task like translation or sentiment analysis specifically for low-resourced languages.",1,
"Abstract African languages still lag in the advances of Natu- ral Language Processing techniques, one reason be- ing the lack of representative data, having a tech- nique that can transfer information between lan- guages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to cre- ate cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where se- mantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evalu- ate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic qual- ity of the embeddings to determine if there is im- provement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation Introduction Many African languages have insufficient language resources (data, tools, people) (Abbott & Martinus 2019, Martinus & Abbott 2019, Nekoto et al. 2020, Sefaraetal. 2021) and fall into the classification of low resource languages (Ranathunga et al. 2021) in the Natural Language Processing (NLP) field. This lack of resources makes it harder to capi- talise on the recent advances in many NLP down- stream tasks such as Neural Machine Transla- tion (Choetal. 2014), Large Language Models (Devlin et al. 2018, Howard & Ruder 2018), Q&A systems (Kwiatkowskietal. 2019), etc. There may be more downstream approaches to deal with some of these challenges such as Transfer Learning (Ruder et al. 2019), Data Augmentation (Marivate & Sefara 2020a), Multilingual Models (Hedderich et al. 2020), etc. Additionally, the lack of research attention to existing NLP tech- niques results in difficulties finding a benchmark (Abbott & Martinus 2019). In this work, we focus on word representations through word embeddings and how we can leverage one language to assist in the representation of another related language. These embeddings can then be used to develop tools for other downstream tasks. Word Embeddings are a mathematical technique to learn general language vector representations from a large amount of unlabelled text using co-occurring statistics. In recent years, monolingual word em- beddings techniques are increasingly becoming an important resource in NLP. Word embeddings are widely used in NLP problems such as sen- timent analysis (Socher et al. 2013), named-entity- recognition (Guo et al. 2014), parts-of-speech tag- ging, and document retrieval. Word2Vec is a vector training model proposed by Mikolov et al. (2013). Word2Vec produces a low-dimensional real-value vector representing the meaning of a word. The word vector represents grammatical and semantic properties, which results in words with similar se- mantic relations being close to each other. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). Cross-lingual word embeddings havereceiv- ing more and more attention from the NLP com- munity, mainly because it has provided a path to effectively align two disjoint monolingual embed- dings with no bilingual dictionary for unsuper- vised techniques or no more than a small bilingual dictionary for supervised techniques (Lample et al. 2018, Artetxeetal. 2020). Cross-lingual tech- niques also enable knowledge transfer between lan- guages with rich resources and low resources. For languages lacking bilingual parallel corpus with other languages, cross-lingual embeddings can be utilised to train high-quality cross-lingual embed- dings (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Artetxe et al. (2018) created the cross- lingual unsupervised or supervised word embed- ding (VecMap library) approach for training cross- lingual word embedding models. The approaches can be used to construct cross-language word vec- tors with or without a bilingual dictionary. The majority of South African languages lag bilin- gual parallel corpus with other languages. In this work, we aim to investigate how cross-lingual em- beddings could be used to improve the state of one or both languages. We used data (corpus) from different domains to train Word2Vec and fastText (Bojanowski et al. 2016) monolingual embeddings. When using VecMap, the two embeddings are aligned. VecMap requires two monolingual word vectors from source and target (Artetxe et al. 2018). To evaluate the effectiveness of the cross-lingual em- bedding for Setswana and Sepedi, we use intrinsic evaluation (Bakarov 2018) through Setswana and Sepedi versions of WordSim (Finkelstein et al. 2001) and Simlex (Hill et al. 2015). This is following on an approach that has been used for Yoruba and Twi (Alabi et al. 2019). We also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment. Conneau et al. (2018) show that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: neighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list. In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsicwas assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLM is resilient even when target language training data is scaled-down and that re- search methods are biased. Artetxe & Schwenk (2019) introduced an architec- ture that can be used to learn multilingual sentence representations for more than 90 languages. The languages belonged to 30 different families. The re- search used a single BiLSTM encoder with a shared Byte Pair Encoding (BPE) vocabulary coupled with an auxiliary decoder and trained on parallel corpora. They learn a classifier using English annotated data only and transfer it to any language without modification. The research focused on determining if re- search methods are resilient even when target language training data is scaled-down and if re- search methods are biased. They compare two types of CLM known as Neural Machine Transla- tion (Choetal. 2014), which is based on the idea that tornata are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). The research mainly focused on vector rep- resentations of sentences that are general for the in- put language and the sentence- construction task. Word representations can also be used to improve the state of one or both languages. Artetxe & Schwenk (2019) worked on massive vs. curated embeddings for low-dimensional real-value data (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Nakanishkowsa et al. (2018) created a cross-lingual word vector representation method that can be used to improve the state of one or both languages. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). The research mainly focused on word vectors and how diversional programming can improve the seman- tic representation of low-resourced languages. Alabi et al. (2018) worked on massive vs. curated word embeddings for low-resourced languages. This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Banerjee et al. (2018) investigates a cross-lingual word vector representation method for Setswana-Sepedi monolingual word vector. Author: Brennan et al. (2018). In this paper, they investigate a cross-lingual word vector representation method for Sepedi monolingual word vector. They use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word representation method. They also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment.",0,
"Abstract. Communication is compositional if complex signals can be represented as a combi- nation of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experi- mentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence. Introduction In emergent communication studies, one often considers agents who can share information about a set of objects described by the common features. Such a situation is common in multi-agent systems with partial observation (Foerster et al. (2016), Lazaridou et al. (2017), Jaques et al. (2019), Raczaszek-Leonardi et al. (2018)) and it is the major theme in signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). In a signaling game, one agent (a sender) conveys information about an object to another agent (a receiver), which then has to infer the object’s features. Typically, agents are rewarded if some of the features are correctly identified. During this process, the agents develop a communication protocol. A recent line of work has studied conditions under which compositionality emerges (Batali (1998); Kottur et al. (2017); Choi et al. (2018); Korbak et al. (2019); Li and Bowling (2019); Słowik et al. (2020b,a); Guo et al. (2020)). Compositionality is a crucial feature of natural languages and it has been investigated extensively in cognitive science (see e.g. Chomsky (1957) Fodor and Pylyshyn (1988)). It is often measured using dedicated metrics such as topographic similarity (Brighton and Kirby (2006); Lazaridou et al. (2018); Kriegeskorte (2008); Bouchacourt and Baroni (2018)), context independence Bogin et al. (2018), conflict count Kucin ́ski et al. (2020), or positional disentanglement (Chaabouni et al. (2020)). In signaling games it bears a strong resemblance to the concept of disentangled representations, see (Higgins et al. (2017), Kim and Mnih (2018), Locatello et al. (2019)). In machine learning context, compositionality is perceived as a generalization mechanism (Lake et al. (2017)) and has been used e.g. for goal composition (Jiang et al. (2019)) or knowledge transfer (Li and Bowling (2019)). In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for compositionality to emerge. A similar observation has been made by Kottur et al. (2017); however, our result is more fundamental and points out a common misconception that compositionality can be learned in a purely unsupervised way. Such a result can be perceived as a discrete analog of Locatello et al. (2019), applicable in the communication context. We then prove that adding an inductive bias in the loss function coupled with communication over a noisy channel leads to the spontaneous emergence of compositionality. This shows the catalytic role of noise in this process. Intuitively, this can be attributed to the (partial) robustness of compositional language with respect to message corruption caused by a noisy channel. We experimentally verify that a certain range of noise levels, dependent on the model and the data, promotes compositionality. We provide a wide range of experiments that illustrate the influence of different priors. For the inductive biases in the training framework, we look into the impact of the network architecture as well as implementation and temporal variation in noise. On the data side, we study the effect of scrambling visual input or its description. We also study the generalization properties of the proposed training framework. Related work The topic of communication is actively studied in multi-agent RL, see Hernandez-Leal et al. (2020, Table 2) for a recent survey. Compositionality is often investigated in the context of signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). Recent research has shown that strong inductive biases or grounding of communication protocols are necessary for the protocol to be compositional (see e.g. Kottur et al. (2017), Słowik et al. (2020b)). The inductive bias can be imposed into the architecture of the agents or the training procedure. For instance, Das et al. (2017) place pressure on agents, to use symbols consistently across varying contexts, by a frequent reset of the agent’s memory. A model-based approach was proposed by Choi et al. (2018) and Bogin et al. (2018), who build upon the obverter algorithm (Oliphant and Batali (1997), Batali (1998)). Słowik et al. (2020a) explore games with hierarchical inputs and shows how agents implemented as graph convolutional networks obtain good generalization. Korbak et al. (2019) implemented the idea of template transfer (Barrett and Skyrms, 2017) by pre-training the agents on simpler subtasks before the target task. Kirby (2001) studied the iterative learning paradigm, where each generation of agents learns the language spoken by the previous generation before starting to communicate. In the machine learning literature, this idea was explored by Li and Bowling (2019), Cogswell et al. (2019) and Ren et al. (2020) with the generation transfer typically implemented as reinitializing the weights of agents’ neural networks. Such an approach inevitably introduces noise into the learning process. This naturally leads to a question of whether the noise itself may be a sufficient mechanism of compositionality, which we will try to address in this paper. Guo et al. (2020) have shown that the choice of a game has a large impact on the properties of a communication protocol emerging in that game, foreshadowing what we call grounding. The noisy channel model of communication was famously introduced by Shannon (1948). The idea of noise as a driving force in the emergence of communication was first proposed by Nowak and Krakauer (1999), who showed that word-level compositionality is the optimal solution to the problem of communication in a noisy environment under a particular fitness function. Noise is also used in deep learning, e.g. as a regularizer (see e.g. dropout (Srivastava et al., 2014)) or a mechanism allowing backpropagation through a discrete latent (see e.g. Salakhutdinov and Hinton (2009), Kaiser and Bengio (2018)). Noise in the latter context was used in Foerster et al. (2016) in order to learn to communicate. The authors observed that it is essential for successful training. Conclusions In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for the compositionality to emerge spontaneously in signaling games. We then formulate inductive biases in the loss function and prove that they are sufficient to achieve compositionality when coupled with communication over a noisy channel. Consequently, we highlight the catalytic role of noise in the emergence of compositionality. We perform a series of experiments in order to understand different aspects of the proposed framework better. We empirically validate that, indeed, a certain range of noise levels, dependent on the model and the data, promotes compositionality. Our work is foundational research and does not lead to any direct negative applications.",1,
"Abstract. Communication is compositional if complex signals can be represented as a combi- nation of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experi- mentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence. Introduction In emergent communication studies, one often considers agents who can share information about a set of objects described by the common features. Such a situation is common in multi-agent systems with partial observation (Foerster et al. (2016), Lazaridou et al. (2017), Jaques et al. (2019), Raczaszek-Leonardi et al. (2018)) and it is the major theme in signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). In a signaling game, one agent (a sender) conveys information about an object to another agent (a receiver), which then has to infer the object's features. Typically, agents are rewarded if some of the features are correctly identified. During this process, the agents develop a communication protocol. A recent line of work has studied conditions under which compositionality emerges (Batali (1998); Kottur et al. (2017); Choi et al. (2018); Korbak et al. (2019); Li and Bowling (2019); Sowik et al. (2020b,a); Guo et al. (2020)). Compositionality is a crucial feature of natural languages and it has been investigated extensively in cognitive science (see e.g. Chomsky (1957) Fodor and Pylyshyn (1988)). It is often measured using dedicated metrics such as topographic similarity (Brighton and Kirby (2006); Lazaridou et al. (2018); Kriegeskorte (2008); Bouchacourt and Baroni (2018)), context independence Bogin et al. (2018), conflict count Kucin ski et al. (2020), or positional disentanglement (Chaabouni et al. (2020)). In signaling games it bears a strong resemblance to the concept of disentangled representations, see (Higgins et al. (2017), Kim and Mnih (2018), Locatello et al. (2019)). In machine learning context, compositionality is perceived as a generalization mechanism (Lake et al. (2017)) and has been used e.g. for goal composition (Jiang et al. (2019)) or knowledge transfer (Li and Bowling (2019)). In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for compositionality to emerge. A similar observation has been made by Kottur et al. (2017); however, our result is more fundamental and points out a common misconception that compositionality can be learned in a purely unsupervised way. Such a result can be perceived as a discrete analog of Locatello et al. (2019), applicable in the communication context. We then prove that adding an inductive bias in the loss function coupled with communication over a noisy channel leads to the spontaneous emergence of compositionality. This shows the catalytic role of noise in this process. Intuitively, this can be attributed to the (partial) robustness of compositional language with respect to message corruption caused by a noisy channel. We experimentally verify that a certain range of noise levels, dependent on the model and the data, promotes compositionality. We provide a wide range of experiments that illustrate the influence of different priors. For the inductive biases in the training framework, we look into the impact of the network architecture as well as implementation and temporal variation in noise. On the data side, we study the effect of scrambling visual input or its description. We also study the generalization properties of the proposed training framework. Related work The topic of communication is actively studied in multi-agent RL, see Hernandez-Leal et al. (2020, Table 2) for a recent survey. Compositionality is often investigated in the context of signaling games (Fudenberg and Tirole (1991),(1969), Skyrms (2010), Lazaridou et al. (2018)). Recent research has shown that strong inductive biases or grounding of communication protocols are necessary for the protocol to be compositional (see e.g. Kottur et al. (2017), Sowik et al. (2020b)). The inductive bias can be imposed into the architecture of the agents or the mechanisms(s)in which the biases are imposed. For instance, Das et al. (2017) place pressure on agents, to use symbols consistently across varying contexts, by a frequent reset of the agent's memory. A model-based approach was proposed by Choi et al. (2018) and Bogin et al. (2018), who build upon the obverter algorithm (Oliphant and Batali (1998)). Sowik et al. (2020a) explore games with hierarchical inputs and shows how agents implemented as graph convolutional networks obtain good generalization. Korbak et al. (2019) implemented the idea of template transfer (Barrett and Skyrms (2010), Lazaridou et al. (2018)). In machine learning context, compositionality is perceived as a generalization mechanism (Lake et al. (2017)) and has been used e.g. for goal composition (Jiang et al. (2019)) or knowledge transfer (Li and Bowling (2019)). In this paper, we theoretically show that inductive biases on both the training framework and the data are needed for compositionality to emerge. A similar observation has been made by Kottur et al. (2017); however, our result is more fundamental and points out a common misconception that compositionality can be learned in a purely unsupervised way. Such a result can be perceived as a discrete analog of Locatello et al. (2019), applicable in the communication context. We then prove that adding an inductive bias in the loss function coupled with communication over a noisy channel leads to the spontaneous emergence of compositionality. This shows the catalytic role of noise in this process. Intuitively, this can be attributed to the (partial) robustness of compositional language with respect to message corruption caused by a noisy channel. We experimentally verify that a certain range of noise levels, dependent on the model and the data, promotes compositionality. We provide a wide range of experiments that illustrate the influence of different priors. For the inductive biases in the training framework, we look into the impact of the network architecture as well as implementation and temporal variation in noise. On the data side, we study the effect of scrambling visual input or its description. We also study the generalization properties of the proposed training framework. Related work The topic of communication is actively studied in multi-agent RL, see Hernandez-Leal et al. (2020, Table 2) for a recent survey. Compositionality is often investigated in the context of signaling games (Fudenberg and Tirole (1991), Lewis (1969), Skyrms (2010), Lazaridou et al. (2018)). Recent research has shown that strong inductive biases or grounding of communication protocols are necessary for the protocol to be compositional (see e.g. Kottur et al. (2017), Sowik et al. (2020b)). The inductive bias can be imposed into the architecture of the agents or the mechanismsin which the biases are imposed.",0,
"Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Lee et al., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, Epsilon Non-Negative rectification (ENN) and Prox- imal Alternating Linearized Minimization rectification (PALM). We also propose the Low-rank Anchor Word algorithm (LAW) that learns the latent topics and their cor- relations only from the compressed statistics, guaranteeing the same performance as the original Anchor Word algo- rithm under a certain condition. Our experiments show that applying LAW after ENN learns topics of quality compara- ble to using AW after AP based on the full co-occurrence. We then introduce the Low-Rank Joint Stochastic Matrix Factorization pipeline (LR-JSMF) that first adopts a ran- domized algorithm to construct a low-rank approximation of the full co-occurrence C directly from the raw data; then performs ENN and LAW. While PALM needs access to the full co-occurrence, ENN can work solely with a low-rank initialization, eliminating the burden to ever construct a full co-occurrence matrix. This new pipeline scales to large vocabularies that were previously intractable for spectral inference, and offers a 10x∼100x speedup over previous methods on various textual and non-textual datasets. Note that second-order spectral topic models often rely on the separability assumption that forces at least one anchor word for each topic. This has led to criticism in theory despite their superior performance in practice compared to probabilistic counterparts (Lee et al., 2017) and third-order tensor models (Lee et al., 2019). As most topic models with large vocabularies are proven separable (Ding et al., 2015), we show that our capability to process large vocabularies not only fits for modern datasets, but also alleviates the theoreti- cal limitation. In addition, we also develop a new approach that helps better interpretation of topics by jointly reading characteristic words as well as traditional prominent words. By defining the characteristic words as the terms that are highly associated with each anchor word, we design a graph- based metric that can measure the degree of incoherence in individual topics. To the best of our knowledge, this work makes the first principled attempt to utilize anchor words for quantitative and qualitative interpretations of topics with the prominent words. Given our on-the-fly methods, users are now capable of efficiently understanding latent topics and their correlations from noisy co-occurrence statistics within time and space complexity linear in the size of vocabulary. Conclusion Spectral algorithms provide appealing alternatives for iden- tifying interpretable low-rank subspaces by simple factoriza- tions of higher-order co-occurrence data. But this simplicity is also a weakness: the size of the co-occurrence limit us to small vocabularies, and these methods perform poorly without rectifications that previously suffered quadratic scal- ing. Anchor words are guaranteed to be exclusive to the corresponding topics, but they are rarely used for topic inter- pretations because they are often chosen as too rare terms. We develop a robust and scalable pipeline: Low-Rank Joint Stochastic Matrix Factorization based on our two comple- mentary on-the-fly rectification methods (ENN/PALM) and a sufficiently general low-rank inference algorithm (LAW). These methods simultaneously compress and rectify the co- occurrence from raw data; learn high-quality topics from the compressed matrix factorization; and achieve low-rank non- negative approximations without quadratic blowup. They also provide orders of magnitude speedups for rectification even on small vocabularies. In addition, we verify that us- ing large vocabularies benefits inference quality by better satisfying the separability assumption. It also improves model interpretability by jointly understanding the promi- nent words with the characteristic words, and by measuring our MST-Incoherence metric for individual topics. Given all these new development, we can now learn and evalu- ate useful low-dimensional structures in high-dimensional datasets on laptop-grade hardware, massively increasing the applicability and potential use of the spectral algorithms.",1,
"Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Leeal., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, Epsilon Non-Negative rectification (ENN) and Prox- imal Alternating Linearized Minimization rectification (PALM). We also propose the Low-rank Anchor Word algorithm (LAW) that learns the latent variables LAW and HAW, and scales well with the size of LAW and HAW. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Linearized Minimization rectification (ALM) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Lee et al., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. ALM learns low-dimensional rep- resentations of individual items, which are useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appearin contexts (Pennington et al., 2014; Levy & Goldberg, 2014).",0,
"Abstract While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sen- tence. Second, we test not only models’ ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- els’ treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like “this sentence contains an anomaly”, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomalies—here our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating “this sentence contains an anomaly”, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders. We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generality—however, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddity—which appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Rogers et al., 2020; Jawahar et al., 2019). Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- ness (Dasgupta et al., 2018; Poliak et al., 2018a,b; White et al., 2017; Belinkov et al., 2017b; Yang et al., 2015; Rajpurkar et al., 2016; Jia and Liang, 2017; Rajpurkar et al., 2018). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Conneau et al., 2018; Belinkov et al., 2017a; Ettinger et al., 2016, 2018; Tenney et al., 2019; Klafka and Ettinger, 2020). We expand on this work by designing analyses to shed light on encod- ing of syntactic anomaly information in sentence embeddings. A growing body of work has examined syntactic sensitivity in language model outputs (Chowdhury and Zamparelli, 2018; Futrell et al., 2019; Lakretz et al., 2019; Marvin and Linzen, 2018; Ettinger, 2020), and our Agree-Shift task takes inspiration from the popular number agreement task for lan- guage models (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Like this work, we fo- cus on syntax in designing our tests, but we differ from this work in focusing on model representa- tions rather than outputs, and in our specific focus on understanding how models encode information about anomalies. Furthermore, as we detail below, our Agree-Shift task differs importantly from the LM number agreement tests, and should not be compared directly to results from those tests. Our work relates most closely to studies involv- ing anomalous or erroneous sentence information (Warstadt et al., 2019; Yin et al., 2020; Hashemi and Hwa, 2016). Some work investigates impacts from random shuffling or other types of distor- tion of input text (Pham et al., 2020; Gupta et al., 2021) or of model pre-training text (Sinha et al., 2021) on downstream tasks—but this work does not investigate models’ encoding of these anoma- lies. Warstadt et al. (2019) present and test with the CoLA dataset for general acceptability detec- tion, and among the probing tasks of Conneau et al. (2018) there are three that involve analyz- ing whether sentence embeddings can distinguish erroneous modification to sentence inputs: SOMO, BShift, and CoordInv. Yin et al. (2020) also gen- erate synthetic errors based on errors from non- native speakers, showing impacts of such errors on downstream tasks, and briefly probing error sen- sitivity. More recently, Li et al. (2021) conduct anomaly detection with various anomaly types at different layers of transformer models, using train- ing of Gaussian models for density estimation, and finding different types of anomaly sensitivity at different layers. We build on this line of work in anomaly detection with a fine-grained exploration of models’ detection of word-content-controlled perturbations at different levels of syntactic hierar- chy. Our work is complementary also in exploring generality of models’ anomaly encoding by exam- ining transfer performance between anomalies. Conclusion We have undertaken a direct study of anomaly en- coding in sentence embeddings, finding impacts of hierarchical differences in anomaly type, but find- ing evidence of generalized anomaly encoding only in BERT and RoBERTa. Follow-up analyses sup- port the conclusion that these embeddings encode a combination of generalized and anomaly-specific cues in these embeddings, with models appearing to leverage both finer-grained and coarser-grained information for anomaly detection. These results contribute to our understanding of the nature of encoding of linguistic input in embeddings from recent models. Future work can further explore the relationship between naturalness-oriented training and cultivation of abstract anomaly awareness, and how these insights can be leveraged for more robust and human-like processing of language inputs.",1,
"Abstract While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sen- tence. Second, we test not only models' ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- els' treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like this sentence contains an anomaly, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomalies here our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating this sentence contains an anomaly, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders. We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generality however, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddity which appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Rogers et al., 2020; Jawahar et al., 2019). Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- nesset al., 2018; Poliak et al., 2018a,b; White et al., 2017; Belinkov et al., 2017b; Yang et al., 2015; Rajpurkar et al., 2016; Jia and Liang, 2017; Rajpurkar et al., 2018). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sentence. Second, we test not only models' ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- else treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like this sentence contains an anomaly, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomalies here our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating this sentence contains an anomaly, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders.",0,
"Abstract Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive– definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extrac- tion and generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT- based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl- ative definitional information (i.e., definitions of related terms). For generation, we design a BART- based definition generator to produce the final defi- nition by incorporating the extracted knowledge. From another perspective, we propose to reform the problem of definition modeling, which is pre- viously mainly defined as generating definitions of terms conditioned with a target term and a given context. Instead, we restudy this problem as defin- ing terms with extracted knowledge. This setting is in line with human behavior: to understand a term, compared to reading the given sentence it is used in, it is more straightforward and helpful to search and read its relevant content on the Internet. Our framework for definition modeling is simple and flexible that can easily be further expanded by leveraging more advanced language models. Ex- perimental results demonstrate our simple model outperforms state-of-the-art models significantly (e.g., BLEU score from 8.76 to 22.66, human an- notated score from 2.34 to 4.04), with several inter- esting findings: 1) for computer science terms, our extractive model can achieve performance compa- rable to (even better than) state-of-the-art abstrac- tive models; 2) both self- and correlative defini- tional information are significant to define a term; 3) the quality of definitions generated by our best model is high, while the state-of-the-art models suffer severely from hallucinations, i.e., generating irrelevant or contradicted facts. Our contributions are summarized as follows:  As far as we know, we are the first to connect and combine definition extraction and definition generation– a simple idea that can significantly improve the performance of definition modeling.  We propose to restudy definition modeling as generating definitions of terms with extracted knowledge. We design a novel framework for definition modeling by incorporating both self- and correlative definitional information of terms. We publish two datasets for technical terms, along with definitions of ~75,600 computer science terms generated by our model. Related Work Definition Extraction. Existing works for defini- tion extraction can be roughly divided into three cat- egories: 1) rule-based, which extracts definitions with defined linguistic rules and templates (Klavans and Muresan, 2001; Cui et al., 2004; Fahmi and Bouma, 2006); 2) machine learning-based, which extracts definitions by statistical machine learning with carefully designed features (Westerhout, 2009; Jin et al., 2013); 3) deep learning-based, the state- of-the-art approach for definition extraction, which is based on deep learning models such as CNN, LSTM, and BERT (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020). Definition Generation. Definition generation, or definition modeling, was first introduced in (No- raset et al., 2017), which aims to generate defi- nitions of words with word embeddings. Later works on definition generation put more empha- sis on generating definitions of words/phrases with given contexts (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019; Li et al., 2020;Reid et al., 2020; Bevilacqua et al., 2020). There are also recent works on definition modeling for other languages, e.g., Chinese, by incorporating the special properties of the specific language (Yang et al., 2020; Zheng et al., 2021). Conclusion In this paper, we combine extraction and gener- ation for definition modeling. We show that, by incorporating extracted self- and correlative def- initional information, the generator can produce high-quality definitions for technical terms. Ex- perimental results demonstrate the effectiveness of our framework. As future work, we plan to apply our methods to more domains and construct several online domain dictionaries.",1,
"Abstract Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extrac- tion and generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT- based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl- ative definitional information (i.e., definitions of related terms). For generation, we design a BART- based definition generator to produce the final defi- nition by incorporating the extracted knowledge. From another perspective, we propose to reform the problem of definition modeling, which is pre- viously mainly defined as generating definitions of terms conditioned with a target term and a given context. Instead, we restudy this problem as defin- ing terms with extracted knowledge. This setting is in line with human behavior: to understand a term, compared to reading the given sentence it is used in, it is more straightforward and helpful to search and read its relevant content on the Internet. Our framework for definition modeling is simple and flexible that can easily be further expanded by leveraging more advanced language models. Ex- perimental results demonstrate our simple model outperforms state-of-the-art models significantly (e.g., BLEU score from 8.76 to 22.66, human an- notated score from 2.34 to 4.04), with several inter- esting findings: 1) for computer science terms, our extractive model can achieve performance compa- rable to (even better than) state-of-the-art abstrac- tive models; 2) both self- and correlative defini- tional informationsignificant to define a term; 3) the quality of definitions generated by our best model is high, while the state-of-the-art models suffer severely from hallucinations, i.e., generating irrelevant or contradicted facts. Our contributions are summarized as follows: As far as we know, we are the first to connect and combine definition extraction and definition generation a simple idea that can significantly improve the performance of definition modeling. We propose to combine definition extraction and definition generation for definition modeling evaluation. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT-based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl useful definitional information (i.e., definitions of related terms). For generation, we design a BART-based definition generator to produce the final defi- nition by incorporating the most advanced language models. We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl useful definitional information (i.e., definitions of related terms). For definition modeling, we design a BART-based definition generator to produce the final defi- nition by incorporating the most advanced language models. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling evaluation. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information.",0,
"Abstract When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting tempo- ral misalignment can degrade end-task perfor- mance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and re- views) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today’s pretraining-finetuning paradigm, this misalignment can affect a pretrained language model—a situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021; Peters et al., 2018; Raffel et al., 2020; Röttger and Pierrehumbert, 2021)—or the finetuned task model, or both. We suspect that the effects of temporal misalignment will vary depending on the genre or domain of the task’s text, the nature of that task or application, and the specific time periods. We focus primarily on measuring the extent of temporal misalignment on task performance. We consider eight tasks, each with datasets that span at least five years (§2.4), ranging from summarization to entity typing, a subproblem of entity recognition (Grishman and Borthwick, 1999). Notably, these task datasets span four different domains: social media, scientific articles, news, and reviews. We introduce an easily interpretable metric that summa- rizes the rate at which task performance degrades as function of time. Our research questions are:(Q1) how does temporal misalignment affect downstream tasks over time? (Q2) how does sensitivity to temporal misalign- ment vary with text domain and task? (Q3 )how does temporal misalignment affect lan- guage models across domains and spans of time? (Q4) how effective is temporal adaptation, or ad- ditional pretraining on a target year, in miti- gating temporal misalignment? We find that temporal misalignment affects both language model generalization and task perfor- mance. We find considerable variation in degra- dation across text domains (§3.2) and tasks (§3.1). Over 5 years, classifiers’ F1 score can deteriorate as much as 40 points (political affiliation in Twitter) or as little as 1 point (Yelp review ratings). Two distinct tasks defined on the same domain can show different levels of degradation over time. We explore domain adaptation of a language model, using temporally selected (unannotated) data, as a way to curtail temporal misalignment (Röttger and Pierrehumbert, 2021). We find that this does not offer much benefit, especially relative to performance that can be achieved by finetuning on temporally suitable data (i.e., from the same time period as the test data). We conclude that tem- poral adaptation should not be seen as a substitute for finding temporally aligned labeled data. The evidence and benchmarks we offer motivate careful attention to temporal misalignment in many applications of NLP models, and further research on solutions to this problem. Contributions. To facilitate the study of tempo- ral misalignment phenomenon on downstream ap- plications, we compile a suite of eight diverse tasks across four important language domains. We de- fine an interpretable metric that summarizes tempo- ral misalignment of a model on a task with times- tamped data. Our experiments reveal key factors in how temporal misalignment affects NLP model performance. Conclusion Changes in language use over time, and how lan- guage relates to other quantities of interest in NLP applications, has clear effects on the performance of those applications. We have explored how tem- poral misalignment between training data—both data used to train LMs and annotated data used to finetune them—affects performance across a range of NLP tasks and domains, taking advantage of datasets where timestamps are available. We com- pile these datasets as a benchmark for future re- search as well. We also introduced a summary metric, TD score, that makes it easier to compare models in terms of their temporal misalignment. Our experiments revealed considerable variation in temporal degradation accross tasks, more so than found in previous studies (Röttger and Pierrehum- bert, 2021). These findings motivate continued study of temporal misalignment across applica- tions of NLP, its consideration in benchmark evalu- ations,13 and vigilance on the part of practitioners able to monitor live system performance over time. Notably, we observed that continued training of LMs on temporally aligned data does not have much effect, motivating further research to find effective temporal adaptation methods that are less costly than ongoing collection of annotated/labeled datasets over time.",1,
"Abstract When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting tempo- ral misalignment can degrade end-task perfor- mance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and re- views) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today's pretraining-finetuning paradigm, this misalignment can affect a pretrained language model a situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021; Peters et al., 2018; Raffel et al., 2020; Rottger and Pierrehumbert, 2021) or the finetuned task model, or both. We suspect that the effects of temporal misalignment will vary depending on the genre or domain of the task's text, the nature of that task or application, and the specific time periods. We focus primarily on measuring the extent of temporal misalignment on task performance. We consider eight tasks, each with datasets that span at least five years (§2.4), ranging from summarization to entity typing, a subproblem of entity recognition (Grishman and Borthwick, 1999). Notably, these task datasets span four different domains: social media, scientific articles, news, and reviews. We introduce an easily interpretable metric that summa- rizes the rate at which task performance degrades as function of time. Our research questions are:(Q1) how does temporal misalignment affect downstream tasks over time? (Q2) how does sensitivity to temporal misalign- ment vary with text domain and task? (Q3 )how does temporal misalignment affect lan- guage models across domains and spans of time? (Q4) how effective is temporal adaptation, or ad- ditional pretraining on a target year, in miti- gating temporal misalignment? We find that temporal misalignment affects both language model generalization and task perfor- mance. We find considerable variation in degra- dation across text domains (§3.2) and tasks (§3.1). Over 5 years, classifiers' F1 score can deteriorate as much as 40 points (political affiliation in Twitter) or as little as 1 point (Yelp review ratings). Two distinct tasks defined on the same domain can show different levels of degradation over time. We explore domain adaptation of a language model, using temporally selected (unannotated) data, as a way to curtail temporal misalignment (Rottger and Pierrehumbert, 2021). We find that this does not offer much benefit, especially relative to performance that can be achieved by finetuning on temporally suitable data (i.e., from the same time period as the test data). We conclude that tem- poral adaptation should not be seen as a substitute for finding temporally aligned labeled data. The evidence and benchmarks we offer motivate careful attention to temporal misalignment in many applications of NLP models, and further research on solutions to this problem. Contributions. To facilitate the study of tempo- ral misalignment phenomenon on downstream ap- plications, we compile a suite of eight diverse tasks across four important language domains. We de- fine an interpretable metric that summarizes tempo- ral misalignment of a model on a task withtamped data. Our experiments reveal key factors in how temporal misalignment affects NLP model performance. Conclusion Changes in language use over time, and how lan- guage relates to other quantities of interest in NLP applications, has clear effects on the performance of those applications. We have explored how tem- poral misalignment between data from one time period and data from another, using datasets that span multiple years, classifies tasks across different domains, and periods of time, does not favor one side or the other's point of view. We also observed that, while temporal misalignment can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in language use over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today's pretraining-finetuning paradigm, this misalignment can affect a pretrained language model a situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021).",0,
"Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learners’ writing ability, many corpus studies compare L2 learners’ writing with native experts’ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native experts’ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing students’ academic writing. However, native experts’ publication has been proofread concerning linguistic use, while students’ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of students’ writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papers’ abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liu (2017) also proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic analysis systems by Anthony and Lashkia (2003) and Dayrell et al. (2012). Hence, the micro-level linguistic features of high-impact English academic papers should be considered and analyzed to provide a greater number of dimensions for future automatic analysis and feedback systems of research articles. Thanks to the rapid development of NLP technology, many emerging methods can help automatically extract features with bigger weights beyond corpus-based studies, for example, feature engineering. It has been widely applied in fault detection (e.g., Li et al., 2021), image detection (e.g., Cai, Nee, & Loh, 1996; Cai & Chen, 2011), etc., but has not been tried in the micro-level linguistic feature extraction of research articles. Our study will employ this method to extract and validate the selected features to provide consistent and accurate predictions for journal articles with different academic impacts. Conclusion In summary, we used four feature selection methods to reduce the feature dimensions. We verified the selected 24 features, including word imagery, third-person plural pronouns, all sentence actual word overlap, adjacent sentence actual word overlap, the semantic overlap of all sentences, the semantic overlap of adjacent sentences, the ratio of old information to new information in all sentences, the overlap of all sentence nouns, text ease of vocabulary actuality, content word meaning, content word concretization, all sentence argument overlap, adjacent sentence noun overlap, modal verb, focus past, focus present, focus future, articles, numbers, positive emotions, causal vocabulary can better distinguish journal articles with different impacts. Besides, we found that based on these 24 language features, the random forest model can be more consistent and accurate to classify high- and moderate-impact journal articles. The selected linguistic features at the micro-level and machine learning model with better performance can provide reference for future automatic analysis and feedback systems of English research articles.",1,
"Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learners' writing ability, many corpus studies compare L2 learners' writing with native experts' writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native experts' writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing students' academic writing. However, native experts' publication has been proofread concerning linguistic use, while students' academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of students' writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papers abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liualso proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic evaluation systems by the Cotos team of Iowa State University in the same study. The result is much higher than the accuracy rate of automatic evaluation systems by the Cotos team of Iowa State University in the same study (independent of impact factors) about which independent studies do not mention words such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learners' writing ability, many corpus studies compare L2 learners' writing with native experts' writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native experts' writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing students' academic writing. However, native experts' publication has been proofread concerning linguistic use, while students' academic writings have more or less grammatical problems or spelling errors. ",0,
"Abstract The development of neural networks for clini- cal artificial intelligence (AI) is reliant on in- terpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clin- ical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes. The transparency and explainability of neural networks is critical to trust and acceptance in clinical environments as useful tools. Research has sought to provide improved inter- pretability of NLP models using methods to derive extractive rationales for neural networks predictions by the model itself (Bastings et al., 2019; Lei et al., 2016; Sha et al., 2020). The aim of rationale produc- tion is to increase explainability of models by extract- ing the minimal crucial input required to make a class prediction. In NLP, the rationales are subsets of the input text which maintain predictive power. One of the ways rationales have been achieved is by creating a two-module network, i.e., a selector followed by a predictor, which are trained jointly (Lei et al., 2016). Given an input x, the selector picks a subset of the input features r(x) (the rationale) by specifying a dis- tribution over the possible rationales. The predictor acts as a standard classifier, taking as input r(x) and predicting a class yˆ to compare with the ground truth class y. Sha et al. (2020) proposed InfoCal, an improved type of selector-predictor model that uses an information calibration technique and an additional guider module trained jointly with the selector and predictor in an adversarial manner. InfoCal achieves the current state-of-the-art in rationale extraction on tasks such as sentiment analysis and legal judgment prediction, hence we are interested to see how it per- forms in the medical domain. In this work, we apply InfoCal to the task of predicting hospital readmission from EHRs (John- son et al., 2016). We compare InfoCal with clinical domain Bidirectional Encoder Transformer (BERT) models, ClinicalBERT (Huang et al., 2019) and Bio- ClinicalBERT (Alsentzer et al., 2019). Additionally we compare InfoCal exrractive rationales with impor- tance features in the BERT models via self-attention and layerwise relevance propagation (LRP). We find that the BERT models outperform InfoCal on the classification task, but has a relatively limited mech- anism for interpretability in the form of self-attention. InfoCal was able to produce extractive rationales which reach baseline performance on the classifica- tion task, and we argue the difficulty lies in the do- main expertise created by pretraining present in the BERT based models. Related Work With the advent of big data and machine learning, research is beginning to glean insights from the many types of EHRs data (Li et al., 2020; Huang et al., 2019; Kormilitzin et al., 2020; Weng et al., 2017; Wang et al., 2018; Kuruvilla and Gunavathi, 2014; Barak-Corren et al., 2017; Johnson et al., 2016). Data within EHRs can be either structured (following a pre-defined data structure and type, such as ECG recordings, x-ray images, laboratory results, and de- mographics) or unstructured data (lacking formal rules, type, and bounds, such as the free-text clin- ical notes which remain in a natural language for- mat). The frequency and volume in which clinical notes are recorded for individual patients surpass that of any other data type within EHRs, with some pa- tients having hundreds of individual notes over their entire history of care (Huang et al., 2019; Naumann and Boag, 2020; Barak-Corren et al., 2017; Wang et al., 2018; Weng et al., 2017). Consequently, the information within these clinical notes arguably pro- vides a nuanced, rich picture of a patient’s symptoms and trajectory to augment other measures. Other research has applied similar selector-predictor style models to medical text for certain tasks, such as diagnosis code prediction,and treatment success inference (Mullenbach et al., 2018; Lehman et al., 2019). To our knowledge this work is the first to apply InfoCal to a clinical domain task, and to compare rationales with self-attention and layerwise relevance propaga- tion (LRP) in BERT models.",1,
"Abstract The development of neural networks for clini- cal artificial intelligence (AI) is reliant on in- terpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clin- ical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes. The transparency and explainability of neural networks is critical to trust and acceptance in clinical environments as useful tools. Research has sought to provide improved inter- pretability of NLP models using methods to derive extractive rationales for neural networks predictions by the model itself (Bastings et al., 2019; Lei et al., 2016; Sha et al., 2020). The aim of rationale produc- tion is to increase explainability of models by extract- ing the minimal crucial input required to make a class prediction. In NLP, the rationales are subsets of the input text which maintain predictive power. One of the ways rationales have been achieved is by creating a two-module network, i.e., a selector followed by a predictor, which are trained jointly (Lei et al., 2016). Given an input x, the selector picks a subset of the input features r(x) (the rationale) by specifying a dis- tribution over the possible rationales. The predictor acts as a standard classifier, taking as input r(x) and predicting a class y to compare with the ground truth class y. Sha et al. (2020) proposed InfoCal, an improved type of selector-predictor model that uses an information calibration technique and an additional guider module trained jointly with the selector and predictor in an adversarial manner. InfoCal achieves the current state-of-the-art in rationale extraction on tasks such as sentiment analysis and legal judgment prediction, hence we are interested to see how it per- forms in the medical domain. In this work, we apply InfoCal to the task of predicting hospital readmission from EHRs (John- son et al., 2016). We compare InfoCal with clinical domain Bidirectional Encoder Transformer (BERT) models, ClinicalBERT (Huang et al., 2019) and Bio- ClinicalBERT (Alsentzer et al., 2019). Additionally we compare InfoCal exrractive rationales with impor- tance features in the BERT models via self-attention and layerwise relevance propagation (LRP). We find that the BERT models outperform InfoCal on the classification task, but has a relatively limited mech- anism for interpretability in the form of self-attention. InfoCal was able to produce extractive rationales which reach baseline performance on the classifica- tion task, and we argue the difficulty lies in the do- main expertise created by pretraining present in the BERT based models. Related Work With the advent of big data and machine learning, research is beginning to glean insights from the many types of EHRs data (Li et al., 2020; Huang et al., 2019;et al., 2020; Weng et al., 2017; Wang et al., 2018; Kuruvilla and Gunavathi, 2014; Barak-Corren et al., 2017; Johnson et al., 2016). Data within EHRs can be either structured (following a pre-defined data structure and type, such as ECG recordings, x-ray images, laboratory results, and de- mographics) or unstructured data (lacking formal rules, such as the free-text clin- ical notes that remain in a natural language for human airdresser. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes.",0,
"Abstract: From both human translators (HT) and machine translation (MT) researchers’ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA). Keywords: Translation Quality Evaluation, Quality Estimation, Post-editing Distance, Confidence Intervals, Monte Carlo Modeling, Bernoulli Statistics Introduction Machine Translation (MT) is one of the pioneering artificial intelligence (AI) tasks dating back from the 1950s (Weaver, 1955). It emphasizes the interaction of Language and Machine, and how machine can learn human languages with cognitive knowledge. Before MT, human translation (HT) of written text and documents has always played an important role in science and literature communication between different language speakers, breaking the language barriers. From both MT and HT perspectives, trans- lation quality evaluation (TQE), sometimes incorrectly referred as translation quality assessment (TQA)i, is an important task to reflect how well the source text is translated into the target languages (Han et al., 2021b). On the one hand, for low resource language pair scenarios, human translators still play the dom- inant role in translation production. The translated text and documents can contain unavoidable errors due to personal bias, input efforts, or the training level of the translators. On the other hand, for high resource language pair situation, neural MT (NMT) has achieved remarkable improvement especially on fluency level, compared to conventional rule-based and statistical phrase-based MT models; however, NMT still has “poisoned cookie” problem struggling to achieve real human parity, for instance, on ad- equacy level, meaning preservation, and on idiomatic expression translations (Sag et al., 2002; Han et al., 2020b; Johnson et al., 2016; Han et al., 2020a). Translation service providers (TSPs) relying on both MT, HT, and human post-editing of MT output (TPE) carry out translation and editing tasks with the high demand and harsh constraints nowadays. Thus, TQE role in this workflow remains to be critical. However, it is tedious, costly, and time-consuming to check through the entire translated text given the huge amount of data TSP and customers process. One obvious solution, to this point, is to extract a sub-set of the translated text and make a conclusion about the overall translation quality by results of TQE of the sample, which has always been done in real practice. However, one question arises here: how large the sample size shall be to estimate the overall translation quality of the entire material reliably? In other words, what is the confidence interval of such evaluation on certain desired confidence level (which is commonly taken as 95%) with the samples we choose to estimate the overall translation quality? In this work, we carry out such a motivated experimental investigation on confidence evaluation of translation quality evaluation. To take advantage of statistical modeling techniques, we start with the assumption that error distribution is uniform across the entire material, and minimum unit where the error occurs is one sentence (since errors can be between words, in the form of punctuation and conjugation, etc.). This assumption is the best case scenario which potentially ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation when we need to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of “errors” (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation. Another disadvantage for automatic evaluation is that we can not get in-depth view of what kind of errors the candidate translations present in the studied context, except for an overall evaluation score, or segment-level scores, not to mention that most metrics do not even allow for clear interpretation of what does the score exactly mean. Regarding this aspect, professional translators can always do a much better job by giving transparent error analysis and categorization on the candidate translations, such as idiomatic expressions (Han et al., 2020a; Han et al., 2020b). However, another issue arrives at this point, that is how to correctly chose a confident sample from the candidate translation, instead of just take a random sample for granted and try to blindly extrapolate its result to the entire material? Actually this is not a brand new challenge in natural language processing (NLP) field. Having it in mind that randomly chosen samples may contain model bias against a proper evaluation, (Prabhu et al., 2019) proposed an uncertainty sampling approach for text classification task, and their statistical models can reduce the bias effectively with smaller size of data in comparison to confessional models; Similarly, (Haertel et al., 2008) carried out work on how statistical sampling models can help reduce the high cost for Penn Tree-bank annotation while maintaining the higher accuracy; (Nadeem et al., 2020) carried out one systematic comparison of several sampling algorithms used for language generation task, including top-k, nucleus and tempered sampling, looking into quality-diversity trade-off. Sampling method was also applied into confident level evaluation of MT. For instance, (Koehn, 2004) proposed to use bootstrap re-sampling methods to test the significance level of automatic metric BLEU, but using a fixed number of sentences, i.e. 300. Like many other research work, the chosen number of sentences for evaluation was never explained or justified with any statistical validation. In contrast, we carry out statistical sampling modelling to estimate the number of sentences that is confident enough to achieve reliable quality evaluation, which means a better representation and generalization of the overall candidate translations in question, with an confidence-cost tread-off. Discussions and Conclusions In this article, we proposed the research question on confident sample size of translation documents to estimate the overall material quality, which is a crucial question for both academic research and industrial applications, such as for clients and language service providers. We started the experimental investigation of translation quality evaluation (TQE) by assuming that the errors in translated text are evenly distributed, with errors being rare (7 errors per 100 sentences, on average). This assumption is placed as a random seed of our statistical Bernoulli modelling which does not affect the overall model behavior. To simulate the practical situation where the errors can come from different translators and different types, and span into a different weight across the translated text and documents, we applied the Monte Carlo Simulation analysis, using a sample size of 2000 sentences and 95% confidence level. We also applied MCS into confidence estimation of post-editing distance measurement which is currently widely adopted evaluation metric for translation assessment, and gained very valuable findings from empirical investigations regarding practical situations when translation qual- ity evaluation (TQE) is deployed. Furthermore, we suggest that, ideally, a reliability level of analytic sample quality measurement can be added to every analytic TQE scorecard in the form of confidence interval at certain confidence level as one important indicator of the level of certainty of measurement results. In the future work, we plan to compare different sampling methods, as well as apply the confidence estimation model into broader TQE metrics.",1,
"Abstract: From both human translators (HT) and machine translation (MT) researchers' point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA). Keywords: Translation Quality Evaluation, Quality Estimation, Post-editing Distance, Confidence Intervals, Monte Carlo Modeling, Bernoulli Statistics Introduction Machine Translation (MT) is one of the pioneering artificial intelligence (AI) tasks dating back from the 1950s (Weaver, 1955). It emphasizes the interaction of Language and Machine, and how machine can learn human languages with cognitive knowledge. Before MT, human translation (HT) of written text and documents has always played an important role in science and literature communication between different language speakers, breaking the language barriers. From both MT and HT perspectives, trans- lation quality evaluation (TQE), sometimes incorrectly referred as translation quality assessment (TQA)i, is an important task to reflect how well the source text is translated into the target languages (Han et al., 2021b). On the one hand, for low resource language pair scenarios, human translators still play the dom- inant role in translation production. The translated text and documents can contain unavoidable errors due to personal bias, input efforts, or the training level of the translators. On the other hand, for high resource language pair situation, neural MT (NMT) has achieved remarkable improvement especially on fluency level, compared to conventional rule-based and statistical phrase-based MT models; however, NMT still has ""poisoned cookie"" problem struggling to achieve real human parity, for instance, on ad- equacy level, meaning preservation, and on idiomatic expression translations (Sag et al., 2002; Han et al., 2020b; Johnson et al., 2016; Han et al., 2020a). Translation service providers (TSPs) relying on both MT, HT, and human post-editing of MT output (TPE) carry out translation and editing tasks with the high demand and harsh constraints nowadays. Thus, TQE role in this workflow remains to be critical. However, it is tedious, costly, and time-consuming to check through the entire translated text given the huge amount of data TSP and customers process. One obvious solution, to this point, is to extract a sub-set of the translated text and make a conclusion about the overall translation quality by results of TQE of the sample, which has always been done in real practice. However, one question arises here: how large the sample size shall be to estimate the overall translation quality of the entire material reliably? In other words, what is the confidence interval of such evaluation on certain desired confidence level (which is commonly taken as 95%) with the samples we choose to estimate the overall translation quality? In this work, we carry out such a motivated experimental investigation on confidence evaluation of translation quality evaluation. To take advantage of statistical modeling techniques, we start with the assumption that error distribution is uniform across the entire material, and minimum unit where the error occurs is one sentence (since errors can be between words, in the form of punctuation and conjugation, etc.). This assumption is the best case scenario which potentially ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation whenneed to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of ""errors"" (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation. Another disadvantage for automatic evaluation is that we can not get in-depth view of what kind of errors the candidate translations present in the studied context, except for an overall evaluation score, or segment-level scores, not to mention that most metrics do not even allow for clear interpretation of what does the score exactly mean. Regarding this aspect, professional translators can always do a much better job by giving transparent error analysis and categorization on the candidate translations, such as idiomatic expressions (Han et al., 2020a; Han et al., 2020b). However, another issue arrives at this point, that is how to correctly chose a confident sample from the candidate translation, instead of just take a random sample for granted and try to blindly extrapolate its result to the entire material? Actually this is not a brand new challenge in natural language processing (NLP) field. Having it in mind that randomly chosen samples may contain model bias against a proper evaluation, (Prabhu et al., 2019) proposed an uncertainty sampling approach for text classification task, and their statistical models can reduce the bias effectively with smaller size of data in comparison to confessional models; Similarly, (Haertel et al., 2008) carried out work on how statistical sampling models can help reduce the high cost for Penn Tree-bank annotation while maintaining the higher accuracy; (Nadeem et al., 2020) carried out one systematic comparison of several sampling algorithms used for language generation task, including top-k, nucleus and tempered sampling, looking into quality-diversity trade-off. Sampling method was also applied into confident level evaluation of MT. For instance, (Koehn, 2004)to use bootstrap re-sampling methods to test the significance level of automatic metric BLEU, but using a fixed number of sentences, i.e. 300. Like many other research work, the chosen number of sentences for evaluation was never explained or justified with any statistical validation. In contrast, we carry out statistical statistical modelling to estimate the confidence intervals (Brown et al., 2001) which ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation when we need to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of ""errors"" (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text.",0,
"Abstract Developing speech technologies is a challenge for low-resource lan- guages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such lan- guages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to de- termine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model. Keywords Data Augmentation; Speech Recognition; Maltese Language; Unsupervised Transcriptions; Multilingual Training; Synthesized Speech Introduction In recent years, the field of Natural Language Processing has seen renewed in- terest in multilingual models. New techniques of transfer and multi-task learn- ing, as well as the availability of multilingual datasets, have motivated the de- velopment of large-scale reusable models (Devlin et al., 2019; Conneau et al., 2020) and NLP benchmarks (e.g. Hu et al., 2020; Liang et al., 2020). Many of these models rely on large quantities of data for pretraining. As a result, ‘under-resourced’ languages, for which such data is less easy to come by, remain under-represented in these developments. To take an example, the large-scale language models currently in use in many language understanding tasks, such as Multilingual BERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are pretrained on multilingual data available in online resources such as Wikipedia or CommonCrawl (Wenzek et al., 2020). This, however, also means that lan- guages with small speaker populations, which also tend to be under-represented on the web, are not represented in these models. Similar developments are being seen in Automatic Speech Recognition (ASR) (Baevski et al., 2020a). There has been a lot of work focused on transfer learning-based improvements to speech recognition systems in order to make many modern architectures accessible to a wider set of languages. This has taken a number of different forms. For example Wang et al. (2020) use high-to- low resource language machine translation as an intermediate task to create a language model for a low-resource target language. More in line with parallel developments in NLP, recent research has also turned towards large, multilin- gual, pretrained ASR architectures. For example, Pratap et al. (2020) show that great benefits for low-resource speech recognition can accrue from a multilingual acoustic model. However, the majority of languages used in this work have over 100 hours of training data available. A different line of research has been pur- sued in Wav2Vec2 (Baevski et al., 2020b), which builds on the earlier Wav2Vec model Schneider et al. (2019). This architecture can learn robust speech repre- sentations of sufficient quality to perform recognition even for target languages with a very low volume of labelled data (in the order of one hour), reaching the same performance as previous state-of-the-art systems requiring 100 times more data (100 hours). Wav2Vec2 can also provide good results with just ten minutes of labelled speech data, reaching word error rates as low as 4.8, given substantial pretraining on 53,000 hours of unlabelled data. This work therefore demonstrates that semi- or self-supervised pretraining, similar in spirit to what we have seen in recent years with language models in NLP, can result in ro- bust representations that lower the requirements on labelled data. Once again, however, the key is in ensuring that sufficient data is available for pretraining, in this case in the form of audio. Indeed, Baevski et al. (2020b) describe two settings, one with 960 hours of unlabelled data, and another with 60,000 hours. In summary, research in NLP in both the textual and speech modalities has convincingly shown the benefits of pretraining with minimal supervision. Yet, from the perspective of under-resourced languages with low presence on the web, in the form of either text or speech, their feasibility is far from guar- anteed. Maltese provides a good example of this kind of scenario. Web-scale textual or speech data is harder to obtain for Maltese compared to languages such as English or Mandarin and we expect that similar challenges arise for many other under-resourced languages (Besacier et al., 2014). Furthermore, as recent critiques of large-scale pretraining approaches have emphasised, even where web-scale data is available, there are significant risks arising from its ‘unfathomable’ nature, not least that it is likely to be extremely noisy, while not guaranteeing representativeness across demographic or ethnic groups, and/or across language varieties Bender et al. (2021); Rogers (2021). Lastly, the com- putational resources needed for such experiments are not available to all research teams. Given these challenges, this paper presents an exploratory analysis of a set of related techniques for improving ASR for Maltese using different data augmen- tation strategies, relying on the use of smaller, curated datasets in the target language, the use of larger datasets in related languages, and the contribution of artificial data augmentation. Briefly, the scenario in which these experiments were designed consists of the following: Approximately7hoursoflabelledspeechdatainthetargetlanguage(here: Maltese); A small amount of unlabelled speech data in the target language; A medium sized corpus of Maltese text (250m tokens); A concatenative speech synthesis system whose output, while not of high quality, can be exploited for data augmentation purposes; Substantial data in a number of other languages – notably Arabic, Italian and English – which are typologically and historically related to Maltese. In the remainder of this section, we first introduce some salient features of the Maltese language itself, followed by an explanation of the motivation for these experiments. The section concludes with an outline of the rest of the paper. Maltese and other languages Maltese is spoken as a first language by just under half a million citizens of the Maltese islands (Mena et al., 2020); it is also taught as a second language in a few countries. It is the national language of Malta, but co-exists with two other official languages (English and Maltese Sign Language). In recent years, there has been significant gain in digital support for the Maltese language. A large (250m tokens) annotated text corpus is available, as are tools for segmentation and morphosyntactic labelling (Gatt and Cˇ ́eplo ̈, 2013) and electronic lexical repositories (Camilleri, 2013).1 In addition, some studies have focused on natural language analysis tasks such as morphologi- cal labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) and dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Despite these advances, Maltese remains under-resourced on a number of fronts, especially where speech is concerned. Concatenative speech synthesis systems have been developed using classical (e.g. diphone synthesis) methods (Micallef, 1997; Borg et al., 2011), but there are currently no tools for ASR, except for some preliminary experiments done by Mena et al. (2020). The most important reason for this is a lack of resources. In the absence of large reposito- ries of paired speech and text samples, it is not realistic to use current end-to-end methods for speech recognition. As argued above, it is also hard to obtain large samples of unlabelled speech for pretraining purposes. The present paper focuses on a variety of approaches designed to overcome this bottleneck, conducted in the context of the project MASRI - Maltese Auto- matic Speech Recognition2. Starting from a relatively small dataset of Maltese text and speech, we describe experiments deploying a variety of techniques for data augmentation. The broad question we address is therefore the following: Given an under-resourced language for which a relatively small speech-to-text dataset is available, what data augmentation methods work best to bring ASR performance to a level which provides a suitable, competitive baseline for future development? In our experiments, we maintain a focus on data augmentation for end-to- end ASR. While our focus is on the Maltese language, the findings presented here are of relevance to speech technology researchers working on other under- resourced languages. While a variety of data augmentation methods can be envisaged, our focus on Maltese makes two lines of inquiry particularly relevant. Here, we give an outline of the motivation for each one. Typological relatedness Maltese bears strong historical relationships to three major languages. From a historical perspective, it has been characterised as having a Semitic/Arabic stratum, a Romance (Italian/Sicilian) superstratum, and an English adstratum (Brincat, 2011). This characterisation reflects the his- torical development of Maltese, originally a variety of Arabic, which came into intensive contact with Italian due to its geographical proximity and historical relations with that country. This was followed by a period of intensive contact with English (Malta was a British colony from 1800 to 1964), as a result of which, the Maltese population is largely bilingual. The impact of this linguistic history is clearly evident in the language at many levels of analysis, including the lexical and morphological (Borg and Gatt, 2017). From the perspective of the present experiments, this suggests that a promising way to approach the data augmentation problem is to exploit the (much larger) resources available for these languages. We do this by transcribing speech data from one of these languages with a baseline Maltese acoustic model trained on a very small corpus (Section 5.2); and using a ‘language mixture’ approach (Section 5.3), in which an end-to-end system is pretrained on data using mixtures of such languages. In both cases, pretraining is followed by a fine-tuning step. Synthesis Given the existence of a concatenative speech synthesis system for Maltese, we also discuss data augmentation experiments involving artificially synthesised training data (Section 5.4). Outline of the paper The rest of this paper is structured as follows. Section 2 reviews data augmen- tation methods, especially for speech. In Section 3 we describe the datasets we use whilst Section 4 describes the experimental methodology. Section 5 presents a variety of experiments and results, in which we aim to identify the best data augmentation methods for ASR in Maltese. Section 6 concludes with a general discussion and some pointers to future work. Conclusions and future work The work presented in this paper had one over-arching theme - the analysis of whether ASR performance for under-resourced languages can be improved by various data augmentation methods. We elicited a detailed taxonomy of what types of augmentation exist in the literature, and we also devised a consistent set of experiments that sift through the many possibilities, whilst providing clear results for each. The results obtained are specific to ASR for the Maltese language, but we postulate that similar approaches would yield similar improve- ments for other under-resourced languages. In order to assess that we were indeed testing for improvements in acoustic modelling based on data augmentation, this work specifically makes no use of language modelling as part of the ASR pipeline. We do however show that language model rescoring does indeed have a big effect on WER (section 4.6), and in future experimentation, we intend to combine the best methods from this work with further language modelling experiments to obtain state-of-the- art performance for Maltese ASR. That is however, a different research question altogether. As part of our efforts in maintaining consistency in our experiments, ques- tions were also raised as to whether augmentation is helpful for pretraining methodologies, whilst keeping the network architectures unchanged. To this end we employ training cycles, and show that some measure of performance improvement is obtained, as shown in Section 5.2. We also postulate that this approach is generic enough to be proposed for any ASR setup for an under- resourced target language. The one aspect of the work we present here that is probably dependent on the target language is in section 5.3. The ancillary language data chosen is not random. This augmentation is based on languages which are somehow related to Maltese - either historically, culturally or linguistically. The results show that supervised transcriptions of some closely related languages provide a substantial improvement in WER (Section 5.3.1). The choice of these languages, however, has to be assessed individually based on the target language. Furthermore, this paper then analyzed the use of synthesized speech for ASR training (Section 5.4). Whilst data from a speech synthesis system might not always be readily available for under-resourced languages, we highlight the fact that the synthetic speech quality need not be of very good quality for immediate benefits in WER. In fact, the synthesis system used is produces highly unnatural speech with many pronunciation errors, yet we gained the largest improvements from using this method of data augmentation. In summary, the conclusions we can draw from the work presented is as follows: Both gold and noisy transcriptions can be used as data augmentation techniques, up until a limit is reached after various training cycles. Noisy transcriptions have poorer performance with respect to gold tran- scriptions. In the absence of sufficient gold transcription quantities, how- ever, the utilization of noisy transcriptions of the target language shows substantial improvements in WER. Mixing pretraining data from non-target languages is useful, especially with a small data batch from the target language. There are however limits to this and adding more data beyond a certain point yields diminishing returns. The use of synthesized speech in the target language, as training data, outperforms the use of gold or noisy transcriptions in languages different to the target, even when the synthesized speech has a low quality. The results we obtain from this work are very encouraging, as we have ob- served an absolute reduction of 15% in WER from our baseline systems (63.71% WER) to the system resulting from the best setup based on pretraining and fine tuning from an augmented dataset (48.97% WER). This is a remarkable improvement in light of the fact that they revolve around a curated Maltese ASR corpus of less than 7 hours of speech. We believe this work sets a starting point for all future work in ASR for the Maltese language, and propose that all languages which are similarly under- resourced could follow the methodology we outline in this paper to assess and capitalize on data augmentation methods. A promising direction for future work is to explore the augmentation tech- niques presented here with a different neural architecture. Recent research in multilingual NKP has showing the remarkable performance of models in the Transformer paradigm (Vaswani et al., 2017), both for textual, language- understanding tasks with models such as multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) and for speech recognition with Wav2Vec- 2 Baevski et al. (2020b). These approaches provide impetus for further experi- mentation, with a view to identifying the best strategies for data augmentation to make ASR more feasible for low-resource languages.",1,
"Abstract Developing speech technologies is a challenge for low-resource lan- guages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such lan- guages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to de- termine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model. Keywords Data Augmentation; Speech Recognition; Maltese Language; Unsupervised Transcriptions; Multilingual Training; Synthesized Speech Introduction In recent years, the field of Natural Language Processing has seen renewed in- terest in multilingual models. New techniques of transfer and multi-task learn- ing, as well as the availability of multilingual datasets, have motivated the de- velopment of large-scale reusable models (Devlin et al., 2019; Conneau et al., 2020) and NLP benchmarks (e.g. Hu et al., 2020; Liang et al., 2020). Many of these models rely on large quantities of data for pretraining. As a result, ""under-resourced"" languages, for which such data is less easy to come by, remain under-represented in these developments. To take an example, the large-scale language models currently in use in many language understanding tasks, such as Multilingual BERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are pretrained on multilingual data available in online resources such as Wikipedia or CommonCrawl (Wenzek et al., 2020). This, however, also means that lan- guages with small speaker populations, which also tend to be under-represented on the web, are not represented in these models. Similar developments are being seen in Automatic Speech Recognition (ASR) (Baevski et al., 2020a). There has been a lot of work focused on transfer learning-based improvements to speech recognition systems in order to make many modern architectures accessible to a wider set of languages. This has taken a number of different forms. For example Wang et al. (2020) use high-to- low resource language machine translation as an intermediate task to create a language model for a low-resource target language. More in line with parallel developments in NLP, recent research has also turned towards large, multilin- gual, pretrained ASR architectures. For example, Pratap et al. (2020) show that great benefits for low-resource speech recognition can accrue from a multilingual acoustic model. However, the majority of languages used in this work have over 100 hours of training data available. A different line of research has been pur- sued in Wav2Vec2 (Baevski et al., 2020b), which builds on the earlier Wav2Vec model Schneider et al. (2019). This architecture can learn robust speech repre- sentations of sufficient quality to perform recognition even for target languages with a very low volume of labelled data (in the order of one hour), reaching the same performance as previous state-of-the-art systems requiring 100 times more data (100 hours). Wav2Vec2 can also provide good results with just ten minutes of labelled speech data, reaching word error rates as low as 4.8, given substantial pretraining on 53,000 hours of unlabelled data. This work therefore demonstrates that semi- or self-supervised pretraining, similar in spirit to what we have seen in recent years with language models in NLP, can result in ro- bust representations that lower the requirements on labelled data. Once again, however, the key is in ensuring that sufficient data is available for pretraining, in this case in the form of audio. Indeed, Baevski et al. (2020b) describe two settings, one with 960 hours of unlabelled data, and another with 60,000 hours. In summary, research in NLP in both the textual and speech modalities has convincingly shown the benefits of pretraining with minimal supervision. Yet, fromperspective of under-resourced languages with low presence on the web, in the form of either text or speech, their feasibility is far from guar- anteed. Maltese provides a good example of this kind of scenario. Web-scale textual or speech data is harder to obtain for Maltese compared to languages such as English or Mandarin and we expect that similar challenges arise for many other under-resourced languages (Besacier et al., 2014). Furthermore, as recent critiques of large-scale pretraining approaches have emphasised, even where web-scale data is available, there are significant risks arising from its ""unfathomable"" nature, not least that it is likely to be extremely noisy, while not guaranteeing representativeness across demographic or ethnic groups, and/or across language varieties Bender et al. (2021); Rogers (2021). Lastly, the com- putational resources needed for such experiments are not available to all research teams. Given these challenges, this paper presents an exploratory analysis of a set of related techniques for improving ASR for Maltese using different data augmen- tation strategies, relying on the use of smaller, curated datasets in the target language, the use of larger datasets in related languages, and the contribution of artificial data augmentation. Briefly, the scenario in which these experiments were designed consists of the following: Approximately7hoursoflabelledspeechdatainthetargetlanguage(here: Maltese); A small amount of unlabelled speech data in the target language; A medium sized corpus of Maltese text (250m tokens); A concatenative speech synthesis system whose output, while not of high quality, can be exploited for data augmentation purposes; Substantial data in a number of other languages notably Arabic, Italian and English which are typologically and historically related to Maltese. In the remainder of this section, we first introduce some salient features of the Maltese language itself, followed by an explanation of the motivation for these experiments. The section concludes with an outline of the rest of the paper. Maltese and other languages Maltese is spoken as a first language by just under half a million citizens of the Maltese islands (Mena et al., 2020); it is also taught as a second language in a few countries. It is the national language of Malta, but co-exists with two other official languages (English and Maltese Sign Language). In recent years, there has been significant gain in digital support for the Maltese language. A large (250m tokens) annotated text corpus is available, as are tools for segmentation and morphosyntactic labelling (Gatt and Ceplo, 2013) and electronic lexical repositories (Camilleri, 2013).1 In addition, some studies have focused on natural language analysis tasks such as morphologi- cal labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) and dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Despite these advances, Maltese remains under-resourced on a number of fronts, especially where speech is concerned. Concatenative speech synthesis systems have been developed using classical (e.g. diphone synthesis) methods (Micallef, 1997; Borg et al., 2011), but there are currently no tools for ASR, except for some preliminary experiments done by Mena et al. (2020). The most important reason for this is a lack of resources. In the absence of large reposito- ries of paired speech and text samples, it is not realistic to use current end-to-end methods for speech recognition. As argued above, it is also hard to obtain large samples of unlabelled speech for pretraining purposes. The present paper focuses on a variety of approaches designed to overcome this bottleneck, conducted in the context of the project MASRI - Maltese Auto- matic Speech Recognition2. Starting from a relatively small dataset of Maltese text and speech, we describe experiments deploying a variety of techniques for data augmentation. The broad question we address is therefore the following: Given an under-resourced language for which a relatively small speech-to-text dataset is available, what data augmentation methods work best to bring ASR performance to a level which provides a suitable, competitive baseline for future development? In our experiments, we maintain a focus on data augmentation for end-to- end ASR. While our focus is on the Maltese language, the findings presented here are of relevance to speech technology researchers working on other under- resourced languages. Whilevariety of data augmentation methods can be envisaged, our focus on Maltese makes two lines of inquiry particularly relevant. Here, we give an outline of the motivation for each one. Typological relatedness Maltese bears strong historical relationships to three major languages. From a historical perspective, it has been characterised as having a Semitic/Arabic stratum, a Romance (Italian/Sicilian) superstratum, and an English adstratum (Brincat, 2011). This characterisation reflects the his- torical development of Maltese, originally a variety of Arabic, which came into intensive contact with Italian due to its geographical proximity and historical relations with that country. This was followed by a period of intensive contact with English (Malta was a British colony from 1800 to 1964), as a result of which, the Maltese population is largely bilingual. The impact of this linguistic history is clearly evident in the language at many levels of analysis, including the lexical and morphological (Borg and Gatt, 2017). From the perspective of the present experiments, this suggests that a promising way to approach the data augmentation problem is to exploit the (much larger) resources available for these languages. We do this by transcribing speech data from one of these languages with a baseline Maltese acoustic model trained on a very small corpus (Section 5.2); and using a ""language mixture"" approach (Section 5.3), in which an end-to-end system is pretrained on data using mixtures of such languages. In both cases, pretraining is followed by a fine-tuning step. Synthesis Given the existence of a concatenative speech synthesis system for Maltese, we also discuss data augmentation experiments involving artificially synthesised training data (Section 5.4). Outline of the paper The rest of this paper is structured as follows. Section 2 reviews data augmen- tation methods, especially for speech. In Section 3 we describe the datasets we use whilst Section 4 describes the experimental methodology. Section 5 presents a variety of experiments and results, in which we aim to identify the best data augmentation methods for ASR in Maltese. Section 6 concludes with a general discussion and some pointers to future work. Conclusions and future work The work presented in this paper had one over-arching theme - the analysis of whether ASR performance for under-resourced languages can be improved by various data augmentation methods. We elicited a detailed taxonomy of what types of augmentation exist in the literature, and we also devised a consistent set of experiments that sift through the many possibilities, whilst providing clear results for each. The results obtained are specific to ASR for the Maltese language, but we postulate that similar approaches would yield similar improve- ments for other under-resourced languages. In order to assess that we were indeed testing for improvements in acoustic modelling based on data augmentation, this work specifically makes no use of language modelling as part of the ASR pipeline. We do however show that language model rescoring does indeed have a big effect on WER (section 4.6), and in future experimentation, we intend to combine the best methods from this work with further language modelling experiments to obtain state-of-the- art performance for Maltese ASR. That is however, a different research question altogether. As part of our efforts in maintaining consistency in our experiments, ques- tions were also raised as to whether augmentation is helpful for pretraining methodologies, whilst keeping the network architectures unchanged. To this end we employ training cycles, and show that some measure of performance improvement is obtained, as shown in Section 5.2. We also postulate that this approach is generic enough to be proposed for any ASR setup for an under- resourced target language. The one aspect of the work we present here that is probably dependent on the target language is in section 5.3. The ancillary language data chosen is not random. This augmentation is based on languages which are somehow related to Maltese - either historically, culturally or linguistically. The results show that supervised transcriptions of some closely related languages provide a substantial improvement in WER (Section 5.3.1). The choice of these languages, however, has to be assessed individually based on the target language. Furthermore, this paper then analyzed the use of synthesized speech for ASR training (Section 5.4). Whilst data from a speech synthesis system might not always be readily available for under-resourced languages, we highlight the fact that the synthetic speech quality need not be of very good quality for immediate benefits in WER. In fact, the synthesis system used is produces highly unnatural speech with many pronunciationyet we gained the largest improvements from using this method of data augmentation. In summary, the conclusions we can draw from the work presented is as follows: Both gold and noisy transcriptions can be used as data augmentation techniques, up until a limit is reached after various training cycles. Noisy transcriptions have poorer quality as training data. Mixing pretraining data from non-agile sources, as mixing training data is harder to obtain for Maltese compared to languages such as English or Mandarin and we expect that similar challenges arise for many other under-resourced languages (Besacier et al., 2014). Furthermore, as recent critiques of large-scale pretraining approaches have emphasised, even where web-scale data is available, there are significant risks arising from its ""fathomable"" nature, not least that it is likely to be extremely noisy, while not guaranteeing representativeness across demographic or ethnic groups, and/or across language varieties Bender et al. (2021); Rogers (2021). Lastly, the com- putational resources needed for such experiments are not available to all research teams. Given these challenges, this paper presents an exploratory analysis of a set of related techniques for improving ASR for Maltese using different data augmen- tation strategies, relying on the use of smaller, curated datasets in the target language, the use of larger datasets in related languages, and the contribution of artificial data augmentation. Briefly, the scenario in which these experiments were designed consists of the following: Approximately7hoursoflabelledspeechdatainthetargetlanguage(here: Maltese); A small amount of unlabelled speech data in the target language; A medium sized corpus of Maltese text (250m tokens); A concatenative speech synthesis system whose output, while not of high quality, can be exploited for data augmentation purposes; Substantial data in a number of other languages notably Arabic, Italian and English which are typologically and historically related to Maltese. In the remainder of this section, we first introduce some salient features of the Maltese language itself, followed by an explanation of the motivation for these experiments. The section concludes with an outline of the rest of the paper. Maltese and other languages Maltese is spoken as a first language by just under half a million citizens of the Maltese islands (Mena et al., 2020); it is also taught as a second language in a few countries.",0,
"Abstract Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. How- ever, most current dialogue personalization ap- proaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona infor- mation based on the dialogue history to per- sonalize the dialogue agent without relying on any explicit persona descriptions during infer- ence. Experimental results on the PersonaChat dataset show that the proposed method can im- prove the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated re- sponses when conditioning on the predicted persona of the dialogue partner (i.e. “their per- sona”). We also find that a trained persona pre- diction model can be successfully transferred to other datasets and help generate more rele- vant responses. Introduction Recently, end-to-end dialogue response genera- tion models (Sordoni et al., 2015; Serban et al., 2016; Bordes et al., 2017) based on recent ad- vances of neural sequence-to-sequence learning models (Sutskever et al., 2014; Vaswani et al., 2017) have gained increasing popularity as they can generate fluent responses. However, as the dialogue agent is trained with datasets contain- ing dialogues from many different speakers, it can not generate personalized responses for the current speaker, making the generated responses less rele- vant and engaging (Li et al., 2016b). To address this problem, recent studies attempt to personalize dialogue systems by generating di- alogue responses conditioning on given persona descriptions have been shown to help dialogue agents perform better (Zhang et al., 2018; Mazare ́ et al., 2018). However, a major drawback of the current dialogue agent personalization approaches is that they require explicit persona descriptions in both training and inference stages, which severely limits their application in real-world scenarios be- cause detailed persona descriptions for current speakers are not available in most scenarios. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speaker’s persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a “persona approximator” which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a “persona generator” which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both “self-persona” which is the persona information of the dialogue agent, and “their- persona” which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred “self- persona” can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches. Our experiments on dialogue datasets with and without the persona description demonstrate the effectiveness of the proposed approach and show that a trained persona detection model can be suc- cessfully transferred to datasets without persona description. Related Work Preliminary study on dialogue personalization (Li et al., 2016b) attempts to use a persona-based neu- ral conversation model to capture individual char- acteristics such as background information and speaking style. However, it requires the current speaker during inference to have sufficient dialogue utterances included in the training set, which is quite restricted by the cold-start problem. More recently, Zhang et al. (2018) released the PersonaChat dataset which incorporates per- sona of two speakers represented as multiple sen- tences of profile description to personalize dia- logue agents. They propose a profile memory net- work by considering the dialogue history as in- put and then performing attention over the per- sona to be combined with the dialogue history. Mazare ́ et al. (2018) proposed to train a persona encoder and combine the encoded persona em- bedding with context representation by concate- nation. The combined representation is then fed into the dialogue decoder to generate personal- ized responses. (Yavuz et al., 2019) designed the DeepCopy model, which leverages copy mech- anism to incorporate persona texts and Madotto et al. (2019) propose to use meta-learning to adapt to the current speaker quickly, their approach also requires several dialogues of the speaker to per- form dialogue personalization, which is different from our approach. Wellecketal.(2019)propose a dialogue natural language inference dataset and use it to measure and improve the consistency of the dialogue system. More recently, Zheng et al. (2019) propose personalized dialogue generation with diversified traits. Song et al. (2020) introduce a multi-stage response generation stage to improve the personalization of generated responses. Wu et al. (2020) propose a variational response gener- ator to better exploit persona information. Differ- ent from the aforementioned works, our approach does not require persona information during test time, which makes it more generally applicable. Conclusion In this paper, we propose a novel dialogue per- sonalization approach that automatically infers the current speakers’ persona based on the dialogue history, which enables neural dialogue systems to generate personalized dialogue responses without using persona description at test time. Our exper- iments on the PersonaChat dataset show that the proposed models can improve the model’s con- sistency and engagingness when conditioning on the inferred persona information of the dialogue agent itself or the dialogue partner. We also con- duct experiments on the Dailydialog dataset where persona description is not available and find that pre-trained persona detection models can be suc- cessfully transferred to other datasets without an- notated persona descriptions. This further demon- strates the potential of our approach to enable per- sonalized dialogue response generation for various domains where persona descriptions are not avail- able or expensive to collect.",1,
"Abstract Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. How- ever, most current dialogue personalization ap- proaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona infor- mation based on the dialogue history to per- sonalize the dialogue agent without relying on any explicit persona descriptions during infer- ence. Experimental results on the PersonaChat dataset show that the proposed method can im- prove the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. self persona), and improve the engagingness of the generated re- sponses when conditioning on the predicted persona of the dialogue partner (i.e. their per- sona). We also find that a trained persona pre- diction model can be successfully transferred to other datasets and help generate more rele- vant responses. Introduction Recently, end-to-end dialogue response genera- tion models (Sordoni et al., 2015; Serban et al., 2016; Bordes et al., 2017) based on recent ad- vances of neural sequence-to-sequence learning models (Sutskever et al., 2014; Vaswani et al., 2017) have gained increasing popularity as they can generate fluent responses. However, as the dialogue agent is trained with datasets contain- ing dialogues from many different speakers, it can not generate personalized responses for the current speaker, making the generated responses less rele- vant and engaging (Li et al., 2016b). To address this problem, recent studies attempt to personalize dialogue systems by generating di- alogue responses conditioning on given persona descriptions have been shown to help dialogue agents perform better (Zhang et al., 2018; Mazare et al., 2018). However, a major drawback of the current dialogue agent personalization approaches is that they require explicit persona descriptions in both training and inference stages, which severely limits their application in real-world scenarios be- cause detailed persona descriptions for current speakers are not available in most scenarios. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speaker's persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a ""persona approximator"" which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a ""persona generator"" which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both ""self-persona"" which is the persona information of the dialogue agent, and ""their- persona"" which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred ""self- persona"" can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches. Our experiments on dialogue datasets with and without the persona description demonstrate the effectiveness of the proposed approach and show that a trained persona detection model can be suc- cessfully transferred to datasets withoutdescription. Related Work Preliminary study on dialogue personalization (Li et al., 2016b) attempts to use a persona-based neu- ral conversation model to capture individual char- acteristics such as background information and speaking style. However, it requires the current speaker during inference to have sufficient dialogue utterances included in the training set, which is quite restricted by the cold-start problem. More recently, Zhang et al. (2018) released the PersonaChat dataset which incorporates per- sona of two speakers represented as multiple sen- tences of profile description to personalize dia- logue agents. They propose a profile memory net- work by considering the dialogue history as in- put and then performing attention over the per- sona to be combined with the dialogue history. Mazare et al. (2018) proposed to train a persona-based neu- ral conversation model with multi-task learning by sharing layers and training jointly with the dialogue agent in both scenarios. The combined representation of fluent responses is then fed into the dialogue decoder to generate personal- ized responses. (Yavuz et al., 2019) designed the DeepCopy model, which leverages copy mech- anism to incorporate persona texts and Madotto et al. (2019) propose to use meta-learning to adapt to the current speaker quickly, their approach also requires several dialogues of the speaker to per- form dialogue personalization, which is different from our approach. Wellecketal.(2019)propose a dialogue natural language inference dataset and use it to measure and improve the consistency of the dialogue system. More recently, Zheng et al. (2019) propose personalized dialogue generation with diversified traits. Song et al. (2020) introduce a multi-stage response generation stage to improve the personalization of generated responses. Wu et al. (2020) propose a variational response gener- ator to better exploit persona information. Differ- ent from the aforementioned works, our approach does not require persona information during test time, which makes it more generally applicable. Conclusion In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speaker's persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our exper- iments on the PersonaChat dataset show that a trained persona pre- diction model can be suc- cessfully transferred to other datasets and help generate more rele- vant responses. We also con- duct experiments on the Dailydialog dataset where persona description is not available and find that pre- diction training can be suc- cessfully transferred to other datasets without an- notated persona descriptions. This further demon- strates the potential of our approach to help dialogue agents perform better (i.e. ""their- persona"").",0,
"Abstract The inception of modeling contextual information using mod- els such as BERT, ELMo, and Flair has significantly im- proved representation learning for words. It has also given SOTA results in almost every NLP task — Machine Trans- lation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these domi- nant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities. Introduction Named Entity Recognition (NER) is the task of locating and classifying named entities in a given piece of text into pre- defined entity categories such as Person (PER), Location (LOC), Organisation (ORG), etc. NER is considered an es- sential preprocessing step that can benefit many downstream applications in Natural Language Processing (NLP), such as Machine Translation (Babych and Hartley 2003), Informa- tion Retrieval (Antony and G S 2015) and Text Classification (Armour, Japkowicz, and Matwin 2005). Over the past few years, Deep Learning has been the key to solving not only NER but many other NLP applications (Le et al. 2018; Kouris, Alexandridis, and Stafylopatis 2019). On the downside, these models also demand a lot of well- structured and annotated data for their training. This restricts the applicability of trained models to a real-world scenario as the model’s behavior and predictions become very specific to the type of data they are trained on. To conquer this, many studies have recently evolved that focus on building models that can incorporate world knowledge for enhanced modeling and inference on the task at hand, such as He et al. (2020) for NER, Denk and Peleteiro Ramallo (2020) for Representation Learning and Kim et al. (2015) for Dependency Parsing, etc. Although recent works in literature have successfully in- corporated world knowledge for Sequence Labeling (He et al. 2020), they come with certain limitations, which we dis- cuss ahead. First, as words in a language can be polysemous (Lin et al. 2002), entities and relations in a knowledge graph can be polysemous too (Xiao, Huang, and Zhu 2016). To introduce Knowledge Graph Embeddings (KGEs), we no- ticed that previously proposed approaches have primarily used pre-trained static embeddings obtained from extensive sources such as Wikidata. KGEs in these models fundamen- tally relies on the assumption that the tail entity is a linear transformation of the head entity and the relation, making them non-contextualized in nature. Second, we noticed that prior work only considered head-entity and relation embed- ding to get the knowledge graph embedding and ignored the tail-entity of the triplet completely. Dropping the tail entity entirely could lead to a potential loss of information. We observed that in addition to carrying information about the triplet itself, the head-relation-tail also helps in understanding and extracting implicit relationships existing between entities across triplets. Therefore, the model must know where the head and the relation are leaning towards to achieve accurate embedding estimation. The final limitation lies in applying a Recurrent architecture to obtain KGEs, introducing time inef- ficiency and a high computation cost (Annervaz, Chowdhury, and Dukkipati 2018). To further understand the importance and our motivation behind using world knowledge for NER, consider a couple of examples mentioned below. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ”Google” is an organization and ”Pichai” indicates a Person. A: Berlin died in Season 2 of Money Heist. B: Messi saved Barcelona with an equalizer. In the first sentence above, ”Berlin” refers to a person, whereas in the second sentence, ”Barcelona” refers to an organization. There are reasonable chances of misclassifi- cation in these two sentences because of a high probability of training data missing such nuance differences in all the possible entity tags for a named entity. From the examples mentioned above, we can infer that for the model to be aware of such subtle differences, we should provide it with the ability to look up relevant details from a re- liable source. Therefore, world knowledge can open the gates for the model to access such information and learn details about entities that it might never come across in the train- ing data. In addition to this, with access to structured world knowledge, far better applicability to a real-world setting can be expected. Setting these points as our objective, in this work, we pro- pose Knowledge Aware Representational Learning Network for Named Entity Recognition using Transformer (KARL- Trans-NER), which 1. Encodes the entities and relations existing in a knowledge base using a self-attention network to obtain Knowledge Graph Embeddings (KGEs). The embeddings thus ob- tained are dynamic and fully contextualized in nature. 2. Takes the encoded contextualized representations for enti- ties and relations and generates a knowledge-aware repre- sentation for words. The representation obtained, which we also call ”Global Representation” for words, can be augmented with the other underlying features to boost the NER model’s performance. 3. Generates sentence embeddings using BERT by fusing task-specific information through NER tag embeddings. 4. And lastly, relies on a Transformer as its context encoder incorporating direction-aware, distance-aware, and un- scaled attention for enhanced encoder representation learn- ing. To verify the effectiveness of our proposed model, we conduct our experiments on three publicly available datasets for NER. These are CoNLL 2003 (Sang and Meulder 2003), CoNLL++ (Wang et al. 2019) and OntoNotes v5 (Pradhan et al. 2013). Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the three NER datasets. Also, to validate the model’s generalizability and applicability in a real-world setting, we generate the model’s prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence. Related work The research community in NER moved from approaches us- ing character and word representations (Yao et al. 2015; Zhou et al. 2017; Kuru, Can, and Yuret 2016) to sentence-level con- textual representations (Yang, Zhang, and Dong 2017; Zhang, Liu, and Song 2018), and recently to document-level rep- resentations as proposed by Qian et al. (2018) and Akbik, Blythe, and Vollgraf (2018). Expanding the scope of embed- dings from character and word level to document level has shown significant improvements in the results for many NLP tasks, including NER (Luo, Xiao, and Zhao 2019). To expand the scope further, researchers have explored external knowl- edge bases to learn facts existing in the universe that may not be present in the training data (Annervaz, Chowdhury, and Dukkipati 2018; He et al. 2020). Incorporating information present in Knowledge Graph is an emerging research topic in NLP. While some methods focus on graph structure encoding (Lin et al. 2015; Das et al. 2017), others focus on learning entity-relation embeddings (Wang et al. 2020a; Jiang, Wang, and Wang 2019). Zhong et al. (2015) proposed an alignment model for jointly embedding a knowledge base and a text corpus that achieved better or comparable performance on four NLP tasks: link prediction, triplet classification, relational fact ex- traction, and analogical reasoning. Xiao, Huang, and Zhu (2016) proposed a generative embedding model, TransG, which can discover the latent semantics of a relation and leverage a mixture of related components for generating em- bedding. They also reported substantial improvements over the state-of-the-art baselines on the task of link prediction. Lukovnikov et al. (2017) presented a neural network to an- swer simple questions over large-scale knowledge graphs using a hierarchical word and character-level question en- coder. Annervaz, Chowdhury, and Dukkipati (2018) lever- aged world knowledge in training task-specific models and proposes a novel convolution-based architecture to reduce the attention space over entities and relations. It outperformed other models on text classification and natural language in- ference tasks. Despite producing state-of-the-art results in many NLP tasks, Knowledge Graphs are relatively unexplored for NER. He et al. (2020) introduced a Knowledge-Graph Augmented Word Representation (KAWR). The proposed model encoded the prior knowledge of entities from an external knowledge base into the representation. Though KAWR performed better than its benchmark BERT (Devlin et al. 2018), the model underperformed compared to the SOTA models for NER. Conclusion and Future Work This work proposed a novel world knowledge augmentation technique that leveraged large knowledge bases represented as fact triplets and successfully extracted relevant informa- tion for word-level augmentation. The model was trained and tested in an NER setting. Experimental results showed that knowledge level representation learning outperformed most NER systems in literature and made the model highly applicable to a real-world scenario by accurately predicting entities in random pieces of text. Since we augmented features at the word level, we be- lieve our method could facilitate many other NLP tasks, such as Chunking, Word Sense Disambiguation, Question An- swering, etc. Therefore, as future work, we plan to test the applicability of the proposed methods on other NLP tasks as well. Our intuition says that any system can leverage the pro- posed system as a general knowledge representation learning tool. Moreover, being among the very few works in this di- rection, we see an ample scope of improvement. For instance, the Knowledge Graph Embedding model was trained sepa- rately on a Masked Language Modelling task, and then the trained model was used on the task at hand. This restricted the model from interacting and learning from the task at hand, NER in our case. We believe that a technique to incorporate and train the NER model with the knowledge representation module can be more beneficial. Another improvement that could be made lies in the entity shortlisting step. Although the technique is quite reliable, it does not consider any se- mantic information about the entities. Different entities in a knowledge base can be highly correlated to each other and yet have different names. Therefore, we plan to improve the entity shortlisting technique further for more accurate and robust shortlisting.",1,
"Abstract The inception of modeling contextual information using mod- els such as BERT, ELMo, and Flair has significantly im- proved representation learning for words. It has also given SOTA results in almost every NLP task Machine Trans- lation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these domi- nant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities. Introduction Named Entity Recognition (NER) is the task of locating and classifying named entities in a given piece of text into pre- defined entity categories such as Person (PER), Location (LOC), Organisation (ORG), etc. NER is considered an es- sential preprocessing step that can benefit many downstream applications in Natural Language Processing (NLP), such as Machine Translation (Babych and Hartley 2003), Informa- tion Retrieval (Antony and G S 2015) and Text Classification (Armour, Japkowicz, and Matwin 2005). Over the past few years, Deep Learning has been the key to solving not only NER but many other NLP applications (Le et al. 2018; Kouris, Alexandridis, and Stafylopatis 2019). On the downside, these models also demand a lot of well- structured and annotated data for their training. This restricts the applicability of trained models to a real-world scenario as the model's behavior and predictions become very specific to the type of data they are trained on. To conquer this, many studies have recently evolved that focus on building models that can incorporate world knowledge for enhanced modeling and inference on the task at hand, such as He et al. (2020) for NER, Denk and Peleteiro Ramallo (2020) for Representation Learning and Kim et al. (2015) for Dependency Parsing, etc. Although recent works in literature have successfully in- corporated world knowledge for Sequence Labeling (He et al. 2020), they come with certain limitations, which we dis- cuss ahead. First, as words in a language can be polysemous (Lin et al. 2002), entities and relations in a knowledge graph can be polysemous too (Xiao, Huang, and Zhu 2016). To introduce Knowledge Graph Embeddings (KGEs), we no- ticed that previously proposed approaches have primarily used pre-trained static embeddings obtained from extensive sources such as Wikidata. KGEs in these models fundamen- tally relies on the assumption that the tail entity is a linear transformation of the head entity and the relation, making them non-contextualized in nature. Second, we noticed that prior work only considered head-entity and relation embed- ding to get the knowledge graph embedding and ignored the tail-entity of the triplet completely. Dropping the tail entity entirely could lead to a potential loss of information. We observed that in addition to carrying information about the triplet itself, the head-relation-tail also helps in understanding and extracting implicit relationships existing between entities across triplets. Therefore, the model must know where the head and the relation are leaning towards to achieve accurate embedding estimation. The final limitation lies in applying a Recurrent architecture to obtain KGEs, introducing time inef- ficiency and a high computation cost (Annervaz, Chowdhury, and Dukkipati 2018). To further understand the importance and our motivation behind using world knowledge for NER, consider a couple of examples mentioned below. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly.model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" is an organization and an organization. ""Google"" refers to a person, whereas in the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" refers to a person, whereas in the model's behavior when trained on an out-of-domain NER dataset, the model would have hardly received any information. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any informationthe train- ing data. Not all training data are created equal. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ""Google"" is an organization and ""Pichai"" indicates a Person. A: Berlin died in Season 2 of Money Heist. B: Messi saved Barcelona with an equalizer. In the first sentence above, ""Berlin"" refers to a person, whereas in the second sentence, ""Barcelona"" refers to an organization. There are reasonable chances of misclassifi- cation in these two sentences because of a high probability of training data missing such nuance differences in all the possible entity tags for a named entity. From the examples mentioned above, we can infer that for the model to be aware of such subtle differences, we should provide it with the ability to look up relevant details from a re- liable source. Therefore, world knowledge can open the gates for the model to access such information and learn details about entities that it might never come across in the train- ing data. In addition to this, with access to structured world knowledge, far better applicability to a real-world setting can be expected. Setting these points as our objective, in this work, we pro- pose Knowledge Aware Representational Learning Network for Named Entity Recognition using Transformer (KARL- Trans-NER), which 1. Encodes the entities and relations existing in a knowledge base using a self-attention network to obtain Knowledge Graph Embeddings (KGEs). The embeddings thus ob- tained are dynamic and fully contextualized in nature. 2. Takes the encoded contextualized representations for enti- ties and relations and generates a knowledge-aware repre- sentation for words. The representation obtained, which we also call ""Global Representation"" for words, can be augmented with the other underlying features to boost the NER model's performance. 3. Generates sentence embeddings using BERT by fusing task-specific information through NER tag embeddings. 4. And lastly, relies on a Transformer as its context encoder incorporating direction-aware, distance-aware, and un- scaled attention for enhanced encoder representation learn- ing. To verify the effectiveness of our proposed model, we conduct our experiments on three publicly available datasets for NER. These are CoNLL 2003 (Sang and Meulder 2003), CoNLL++ (Wang et al. 2019) and OntoNotes v5 (Pradhan et al. 2013).",0,
"Abstract In this work, we extensively redesign the newly introduced method of token mixing us- ing Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementa- tion on a long document summarization task (¿ 512 tokens). As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summariza- tion task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture. Introduction Abstractive summarization has made significant strides since the introduction of the transformer based model in NLP (Vaswani et al., 2017). How- ever, the quadratic computational and memory com- plexities of large transformers have limited their scalability for long document summarization as the token length for a standard transformer is limited to 512 tokens. One can try extractive summarization to reduce the length of the document while retain- ing the key elements of the article then taking an abstractive approach on the reduced document. In the extractive step, only the most important sentences are chosen to reduce the size of the document to fit within the token limits of the transformer model. Another way is to use extractive summarization to summarize the document thus retaining only the salient information. This approach is compu- tationally very expensive. Alternative transformer approaches such as the longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2021) alleviate the computation burden of the self-attention mech- anism by limiting the attention window each token has access to. Longformer was created for this pur- pose, using a pluggable sparse attention mechanism that combines dilated windowed attention for local context with full global attention on some tokens, of which the latter varies per task. This introduces an attention mechanism that grows linearly with sequence length using a sliding window of size w allowing for dealing documents in excess of 8000 tokens. More recently, a group at Google (Lee-Thorp et al., 2021) has introduced a new implementa- tion that replaces the entire self-attention heads in the transformer encoder with a non-parameterized Fourier transform mixing of the tokens that does not suffer from this quadratic computation penalty. We propose to extend this architecture to the long document summarization problem and compare the results to the two current baseline practices: Extracting the salient information then applying ab- stractive summarization using PEGASUS (Zhang et al., 2020) and using a Longformer implementa- tion. On both baseline approaches, we investigated multiple hyperparameter optimization and evalu- ated the summaries relative to their corresponding abstracts. This becomes the method for comparing the performance of each methodology. The primary dataset used for this work is the PubMed dataset (Dernoncourt and Lee, 2017) as there exists several prior work on long document summarization with it that we can compare to. Ac- cording to Zaheer et al. (2021), this dataset has a median token length of 2,715 with the 90th per- centile token length being 6,101. Dernoncourt and Lee (2017) shows how extensive this dataset is, with close to 200,00 articles. We decided to use the most common evaluation technique for document summarization – ROUGE scores (Lin, 2004). In our analyses, we include F1-scores for Rouge-1, Rouge-2, Rouge-3, and Rouge-l scores for completeness. Conclusion We have demonstrated for the first time that the recently proposed FNET architecture can be ex- tended to a full transformer model on an abstractive summarization task with a PubMed dataset. Even with a toy implementation, we have shown several novel architectural changes to the original proposal that can be used for a variety of tasks requiring low computational cost while maintaining reasonable accuracy. Our toy architecture yields lower Rouge scores than the baseline for two main reasons. First because the transformer model is much smaller and also because we did not have a pretrained FNET transformer as a starting point. The contribution of this work is the investigation of alternative im- plementation of the Fourier token mixing idea in a transformer on a summarization task. With a fully configured large implementation of these ar- chitectures, we believe that we can get competitive Rouge scores on the summarization task without the computation overhead of a full self-attention implementation. We believe that the extractive summarization prepossessing techniques used in this paper would generalize well with larger tun- ing datasets. Although extractive summarization is the most computationally expensive of the tech- niques used in this paper, it showed great promise as a tuning instrument for PEGASUSLARGE. Using more tuning examples, adjusting the tuning param- eters, and adding more computational power would make extractive prepossessing competitive with the state-of-the-art techniques for long documents sum- marization with a limited attention capacity of 512 tokens.",1,
"Abstract In this work, we extensively redesign the newly introduced method of token mixing us- ing Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementa- tion on a long document summarization task ( 512 tokens). As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summariza- tion task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture. Introduction Abstractive summarization has made significant strides since the introduction of the transformer based model in NLP (Vaswani et al., 2017). How- ever, the quadratic computational and memory com- plexities of large transformers have limited their scalability for long document summarization as the token length for a standard transformer is limited to 512 tokens. One can try extractive summarization to reduce the length of the document while retain- ing the key elements of the article then taking an abstractive approach on the reduced document. In the extractive step, only the most important sentences are chosen to reduce the size of the document to fit within the token limits of the transformer model. Another way is to use extractive summarization to summarize the document thus retaining only the salient information. This approach is compu- tationally very expensive. Alternative transformer approaches such as the longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2021) alleviate the computation burden of the self-attention mech- anism by limiting the attention window each token has access to. Longformer was created for this pur- pose, using a pluggable sparse attention mechanism that combines dilated windowed attention for local context with full global attention on some tokens, of which the latter varies per task. This introduces an attention mechanism that grows linearly with sequence length using a sliding window of size w allowing for dealing documents in excess of 8000 tokens. More recently, a group at Google (Lee-Thorp et al., 2021) has introduced a new implementa- tion that replaces the entire self-attention heads in the transformer encoder with a non-parameterized Fourier transform mixing of the tokens that does not suffer from this quadratic computation penalty. We propose to extend this architecture to the long document summarization problem and compare the results to the two current baseline practices: Extracting the salient information then applying ab- stractive summarization using PEGASUS (Zhang et al., 2020) and using a Longformer implementa- tion. On both baseline approaches, we investigated multiple hyperparameter optimization and evalu- ated the summaries relative to their corresponding abstracts. This becomes the method for comparing the performance of each methodology. The primary dataset used for this work is the PubMed dataset (Dernoncourt and Lee, 2017) as there exists several prior work on long document summarization with it that we can compare to. Ac- cording to Zaheer et al. (2021), this dataset has a median token length of 2,715 with the 90th per- centile token length being 6,101. Dernoncourt and Lee (2017) shows how extensive this dataset is, with close to 200,00 articles. We decided to use the most common evaluation technique for document summarization ROUGE scores (Lin, 2004). In our analyses, we include F1-scores for Rouge-1, Rouge-2, Rouge-3, and Rouge-l scores for completeness. Conclusion We have demonstrated for the first time that the recently proposed FNET architecture can be ex- tended to a full transformer model on an abstractive summarization task with a PubMed dataset. Even with a toy implementation, we have shown several novel architectural changes to the original proposal that can be used for a variety of tasks requiring low computationalwhile maintaining reasonable accuracy. Our toy architecture yields lower Rouge scores than the baseline for two main reasons. First because the transformer model is much smaller and also because we did not have a pretrained FNET transformer as a starting point. The contribution of this work is the investigation of alternative transformer approaches that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge performance on a summariza- tion task. ",0,
"Abstract Named entity recognition (NER) models gen- erally perform poorly when large training datasets are unavailable for low-resource do- mains. Recently, pre-training a large-scale language model has become a promising di- rection for coping with the data scarcity is- sue. However, the underlying discrepancies between the language modeling and NER task could limit the models’ performance, and pre- training for the NER task has rarely been stud- ied since the collected NER datasets are gen- erally small or large but with low quality. In this paper, we construct a massive NER cor- pus with a relatively high quality, and we pre- train a NER-BERT model based on the cre- ated dataset. Experimental results show that our pre-trained model can significantly outper- form BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenar- ios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the “miscellaneous” entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has “corporation” and “group” entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the “organization” entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training. Instead of utilizing manually annotated NER datasets, a few previous studies (Cao et al., 2019; Mengge et al., 2020) have focused on leverag- ing weakly-labeled NER data constructed from Wikipedia to enhance the model’s performance. Cao et al. (2019) generated the weakly-labeled data based on Wikipedia anchors and a taxonomy, but the quality of the produced data is relatively low and the number of entity categories is limited. To cope with these issues, Mengge et al. (2020) lever- aged a gazetteer to obtain coarse-grained entities and k-means clustering to further mine the fine-grained entities. However, obtaining fine-grained labels based on clustering algorithms is not stable, which could limit the effectiveness of pre-training. In this work, we first aim to construct a large- scale NER dataset with a relatively high quality and abundant entity categories. After that, our goal is to prove that using the created dataset to pre-train an entity tagging model can outperform pre-trained language models on the low-resource NER task. Similar to Cao et al. (2019), we build the NER dataset based on the Wikipedia corpus. To improve the quality and increase the number of entity cate- gories, we utilize the DBpedia Ontology (Mendes et al., 2012) to assist in categorizing entities in the Wikipedia corpus. Eventually, we obtain around 16 million NER training examples, and then we continue pre-training BERT on the NER task using the constructed data to build NER-BERT. We emphasize that the focus of this paper is not to achieve state-of-the-art results, but to show the effectiveness of entity tagging-based pre-training using our constructed corpus, since current state- of-the-art NER models are constructed on top of pre-trained language models (e.g., BERT (Devlin et al., 2019)) which can be easily replaced by our NER-BERT. Therefore, we simply add a linear layer instead of many complex components on top of the pre-trained models when fine-tuning them on the downstream NER task. We evaluate our model and baselines on nine diverse domains (e.g., litera- ture, biomedical, and Twitter) of the NER task and show that our model can surpass BERT and other strong baselines such as cross-domain language modeling (Jia et al., 2019) and domain-adaptive pre-training (Gururangan et al., 2020; Liu et al., 2021b). Furthermore, we conduct extensive ex- periments in terms of different low-resource levels across multiple diverse target domains and demon- strate that NER-BERT has a powerful few-shot adaptation ability to target domains when only a few training data are available. Additionally, we visualize the entity representations for NER-BERT and baselines to further prove the effectiveness of our NER pre-training. Moreover, we will release our constructed dataset and pre-trained model to facilitate future research in this area. Conclusion In this paper, we first incorporate Wikipedia an- chors and DBpedia Ontology to build a large-scale NER dataset with a relatively high quality. Then, we utilize the constructed dataset to pre-train NER- BERT. Results illustrate that it is essential to lever- age various entity categories for pre-training, and NER-BERT is able to significantly outperform BERT as well as other strong baselines across nine diverse domains. Additionally, we show that NER-BERT is especially effective when only a few pre- training examples are available in target domains. Moreover, the visualization further indicates that NER-BERT possesses good pre-learned knowledge for categorizing a variety of entities.",1,
"Abstract Named entity recognition (NER) models gen- erally perform poorly when large training datasets are unavailable for low-resource do- mains. Recently, pre-training a large-scale language model has become a promising di- rection for coping with the data scarcity is- sue. However, the underlying discrepancies between the language modeling and NER task could limit the models' performance, and pre- training for the NER task has rarely been stud- ied since the collected NER datasets are gen- erally small or large but with low quality. In this paper, we construct a massive NER cor- pus with a relatively high quality, and we pre- train a NER-BERT model based on the cre- ated dataset. Experimental results show that our pre-trained model can significantly outper- form BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenar- ios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the ""miscellaneous"" entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has ""corporation"" and ""group"" entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the ""organization"" entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training. Instead of utilizing manually annotated NER datasets, a few previous studies (Cao et al., 2019; Mengge et al., 2020) have focused on leverag- ing weakly-labeled NER data constructed from Wikipedia to enhance the model's performance. Cao et al. (2019) generated the weakly-labeled data based on Wikipedia anchors and a taxonomy, but the quality of the produced data is relatively low and the number of entity categories is limited. To cope with these issues, Mengge et al. (2020) lever- aged a gazetteer to obtain coarse-grained entities and k-means clustering to further mine the fine-grained entities. However, obtaining fine-grained labels based on clustering algorithms is not stable, which could limit the effectiveness of pre-training. In this work, we first aim to construct a large- scale NER dataset with a relatively high quality and abundant entity categories. After that, our goal is to prove that using the created dataset to pre-train an entitymodel can outperform pre-trained language models on the low-resource NER task. Similar to Cao et al. (2019), we build the NER dataset based on the Wikipedia corpus. To improve the quality and increase the number of entity cate- gories, we utilize the DBpedia Ontology (Mendes et al., 2012) to assist in categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the ""miscellaneous"" entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has ""corporation"" and ""group"" entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the ""organization"" entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training.",0,