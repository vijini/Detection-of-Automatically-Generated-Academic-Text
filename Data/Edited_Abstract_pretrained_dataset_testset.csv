text,#label
"Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation of the administering agency and the COVID-19 vaccines themselves. This paper proposes a semi-supervised ma- chine learning pipeline to perform topic modeling, sentiment analysis, and an analysis of vaccine brand reputation to obtain an in-depth understanding of national public opinion of Filipinos on Facebook. The methodology makes use of a multilingual version of Bidirectional Encoder Representations from Transformers or BERT for topic modeling, hierarchical clustering, five different classifiers for sentiment analysis, and cosine similarity of BERT topic embeddings for vaccine brand reputation analysis. Results suggest that any type of COVID-19 misinformation is an emergent property of COVID-19 public opinion, and that the detection of COVID-19 misinformation can be an unsupervised task. Sentiment analysis aided by hierarchical clustering reveal that 21 of the 25 topics extrapolated by topic modeling are negative topics. Such negative comments spike in count whenever the Department of Health in the Philippines posts about the COVID-19 situation in other countries. Additionally, the high numbers of laugh reactions on the Face- book posts by the same agency—without any humorous content—suggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase negative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4. Sputnik V, AstraZeneca, and Sinovac suffer from a negative public reputation 5. Ministries of health and stakeholders in vaccination campaigns are recommended to employ interventions that correct misinformation, engage people and use local narratives of success",1
"Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation  of the vaccine.
Vaccine Hesitance and Other Concerns and Complaints
There are a number of reasons why people are hesitant to vaccinate their children. Some of these reasons include the following:
1. Vaccines are expensive. The cost of a vaccine can range from $100 to $1,000, depending on the type of vaccine and the age of the child. 
2. There is a lack of scientific evidence to support the safety and efficacy of vaccines. In the United States, there is no evidence that vaccines cause autism or other neurological disorders. However, some studies have suggested that certain vaccines may be associated with an increased risk of autism in children.
3. Many parents are concerned about the potential side effects of vaccinations. For example, a recent study found that children who received the measles-mumps-rubella (MMR) vaccine were more likely to have a fever, cough.",0
"Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue systems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user’s input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation.",1
"Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue systems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user’s input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data processing system. Our results show that the engine is able to perform the following tasks: (i) provide a high-level representation of the natural language; (ii) extract the relevant information from the user's input; and (iii) generate a query that can be used to answer the question. DOI:https://doi.org/10.1103/PhysRevLett.117.120501",0
"We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo.",1
We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT mode. The results of our study are presented in the following table.,0
"Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.",1
"Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating both of these challenges into a single language model. We show that this model can be used to generate conversational dialogue in a variety of languages, including English, French, German, Italian, Japanese, Korean, Spanish, and Chinese. Our results demonstrate that the model is capable of producing accurate and coherent discourse in all languages.",0
"Crowdsourcing requesters on Amazon Mechanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they expect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workers’ forums. Some of the issues causing the mass rejections stem from the requesters not tak- ing the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourc- ing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records in- formation from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation is- sues such as missing instructions or HITs that are not doable.",1
"Crowdsourcing requesters on Amazon Mechanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they are looking for. In response to these concerns, Amazon has announced that it will begin testing a new system that will allow workers to submit their work directly to the company, rather than relying on a third-party requester. Workers will be able to choose whether they want to accept or reject the work, and they will have the option to opt in or out at any time. They will also have access to feedback on the quality of work they receive, as well as the ability to share their experiences with other workers in the same organization. Amazon says that these changes will make it easier for employees to communicate with one another and will help them to improve their performance.",0
"Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models’ weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT.",1
"Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new compression algorithm based on the LZMA compression scheme, which is efficient and easy to implement. We demonstrate that the new algorithm can be used to compress a large number of text files in less than a minute.",0
"Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker Factorization—a recent approach that relaxes independence assumptions—to prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters.",1
"Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is prohibitively expensive, or because the model is already well-suited for the task at hand. In this paper, we present a novel approach to multi-tasking learning, which is based on the idea that a model should be able to learn to perform multiple tasks at the same time. We show that this approach can be applied to a wide variety of real-world tasks, including speech recognition, machine translation, and natural language processing (NLP). We demonstrate that the approach is robust to multiple training conditions and that it outperforms previous approaches in terms of learning speed and accuracy.",0
"Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labeled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models.",1
"Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a large sample of English- and Spanish-speaking participants. We find that VAT is highly effective for the classification of emotional expressions in English, but not in Spanish. This finding is consistent with previous studies that have found VAT to be effective when used in conjunction with other methods, such as word embeddings. Our results also suggest VAT may be more effective than previous methods in the detection of emotion in text.",0
"African languages still lag in the advances of Natural Language Processing techniques, one reason being the lack of representative data, having a technique that can transfer information between languages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to create cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where semantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lingual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evaluate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic quality of the embeddings to determine if there is improvement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation.",1
"African languages still lag in the advances of Natural Language Processing techniques, one reason being the lack of representative data, having a technique that can transfer information between languages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses them to train an artificial neural network. The network is able to learn to recognize and classify words in a wide range of languages, and it can also be used to perform word-by-word translation between two languages.",0
"Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.",1
"Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, and that it is not a consequence of the fact that the signal-to-noise ratio is high.",0
"Across many data domains, co-occurrence statistics about the joint appearance of objects are powerfully informative. By transforming unsupervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occurrence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that simultaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vocabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data.",1
"Across many data domains, co-occurrence statistics about the joint appearance of objects are powerfully informative. By transforming unsupervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes increasingly important to understand how they are related to each other and to the environment in which they occur. In this paper, we present a novel approach to decomposition of object-to-object relationships using spectral analysis. We show that spectral methods can be used to identify the relationships between objects in a data domain, and that these relationships are highly correlated with the properties of the objects themselves. Our approach is based on the notion of spectral coherence, which posits that the spectral structure of an object is determined by its spectral properties, i.e., by the degree to which the object's spectral features are coherent with those of its environment.",0
"While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierarchical level at which anomalies occur in a sentence. Second, we test not only models’ ability to detect a given anomaly, but also the generality of the detected anomaly signal, by examining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anomalies, and only representations from more recent transformer models show signs of generalized knowledge of anomalies. Follow-up analyses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position information is likely also a contributor to the ob- served anomaly detection.",1
"While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore the extent to which sentences are syntactically anomalous, and second we discuss the implications of these anomalies for the interpretation of sentences. Syntactic Anomalies in Representations of Sentences Syntactical anomalies are the absence or presence of a word or phrase in a sentence. For example, in the sentence ""I am a man"", the word ""man"" does not appear in either the first or the second clause, but in both clauses. The presence or absence of an anomaly can be inferred from the context in which the anomaly occurs. An example of such a context would be the following sentence: ""A man is not a woman"". The first clause of this sentence is ambiguous, as it refers to both men and women.  However, it is possible to infer that 'a man' and 'woman' refer to the same person (i.e. a male and a female). The second sentence, however, is ambiguous, as it does not specify whether the person is male or female.",0
"Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive– definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extraction and generation for definition modeling: first extract self- and correlative definitional information of target terms from the Web and then generate the final definitions by incorporating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly.",1
"Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive– definitions are either extracted from a corpus or generated by a language generation model. In this paper, we  velop a novel approach to define terms in a natural language. We propose a new approach, which is based on the concept of a lexicon, that is, a collection of lexical terms that can be defined in terms of each other. This approach allows us to extract and generate definitions from the corpus of natural languages, without having to resort to the extraction and generation of definitions manually. The resulting lexicons can then be used to generate new definitions for the same or different terms. Our approach is applicable to a wide range of topics, such as the meaning of words, the structure of sentences, and so on.",0
"When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adaptation through continued pre training can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.",1
"When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews)  that can be used to train and test a model that can detect and correct for time-related misalignments. We demonstrate that the model can perform well in both real-time and near-real time environments. Our results suggest that a robust and scalable model for detecting and correcting for misaligned temporal data is possible.",0
"English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students.",1
"English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs. In this paper, we present a novel approach to the study of language features using feature extraction and feature analysis. We show that the extraction of feature features from a large corpus of English-language articles can be used to identify features that are associated with a particular language.   Our approach is based on the use of a feature-extraction algorithm, which we describe in detail in a subsequent paper. In this paper, we introduce a new feature extraction algorithm that can be used to extract features from a large set of images.",0
"The development of neural networks for clini- cal artificial intelligence (AI) is reliant on interpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clinical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces extractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extractive rationales produced by InfoCal to competitive transformer-based models pre-trained on clinical text data and for which the attention mechanism can be used for interpretation. We find each presented model with selected interpretability or feature importance methods yield varying results, with clinical language domain expertise and pretraining critical to performance and subsequent interpretability.",1
"The development of neural networks for clini- cal artificial intelligence (AI) is reliant on interpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being able to respond to an AI-based intervention. In this paper, we present a novel approach to this task, based on the use of machine learning to predict the probability that a given intervention will be successful. We demonstrate that this approach can be applied to a wide range of clinical interventions, including cognitive behavioral therapy (CBT), psychosocial rehabilitation (PTR), and neurofeedback (NBT). Our results show that the predictive power of our approach is significantly higher than that of previous approaches,   such as Bayesian inference. Our approach also outperforms other approaches in several ways. First, we use Bayes' Theorem, which is a generalization of the theorem that the probability of an event is proportional to the sum of its components. Second, our approach is based on the idea that there is no such thing as a random event, but rather that events are distributed according to a set of rules. Third, and most importantly, it uses the concept of probability to describe the distribution of events.",0
"From both human translators (HT) and machine translation (MT) researchers’ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA).",1
"From both human translators (HT) and machine translation (MT) researchers’ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers  have to evaluate the quality of translation services provided by translation service provider (TSP) on the basis of a variety of factors, such as the number of users, the complexity of the translation, and the amount of time it takes to translate a given text. In this article, we will discuss some of these factors and how they can be improved in order to improve the performance of TQEs. We will also look at the current state of research in this area.1. Quality of Translation Service Providers. There are many factors that can affect translation performance. The most important of which is the type of service provided to the user. For example, if a user wants to read a text in a language other than English, he or she will need to pay a fee for the service. This fee can range from a few cents to several hundred dollars. However, this fee is  not included in the price of the product. This fee will be added to the total cost of your order. If you have any questions, please feel free to contact us.",0
"Developing speech technologies is a challenge for low-resource languages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such languages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to de- termine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model.",1
"Developing speech technologies is a challenge for low-resource languages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we present a novel approach to synthesizing and processing speech in a language with limited resources. We show that the approach can be used to generate and synthesize speech from raw data, and that it can also be applied to annotate and process speech.   Our approach is based on the concept of a synthesis tree, which we define as a set of nodes that describe the syntactic structure of the language. In this paper, we use the term ""syntactic tree"" to refer to such a tree. Syntactic trees can be thought of as tree-like structures in which each node represents a different type of information.",0
"Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on the dialogue history to personalize the dialogue agent without relying on any explicit persona descriptions during inference. Experimental results on the PersonaChat dataset show that the proposed method can improve the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated responses when conditioning on the predicted persona of the dialogue partner (i.e. “their per- sona”). We also find that a trained persona prediction model can be successfully transferred to other datasets and help generate more relevant responses.",1
"Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on context. We demonstrate that this approach can be applied to a wide range of human-computer interaction (HCI) systems, including voice recognition, text-to-speech (TTS), and video conferencing (VCS). We show that our approach is robust and robustly discriminates between human and computer-generated speech.  Our results suggest that the use of context-aware personalizing agents can improve the accuracy of self-assessments. In addition to the above, there are a number of other ways in which the game can be played. For example, you can play it as a stand-alone game,  as part of a series, or as an expansion pack for the original game.",0
"Named entity recognition (NER) models generally perform poorly when large training datasets are unavailable for low-resource domains. Recently, pre-training a large-scale language model has become a promising direction for coping with the data scarcity issue. However, the underlying discrepancies between the language modeling and NER task could limit the models’ performance, and pre-training for the NER task has rarely been studied since the collected NER datasets are generally small or large but with low quality. In this paper, we construct a massive NER corpus with a relatively high quality, and we pre-train a NER-BERT model based on the created dataset. Experimental results show that our pre-trained model can significantly outperform BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenarios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities.",1
"Named entity recognition (NER) models generally perform poorly when large training datasets are unavailable for low-resource domains. Recently, pre-training a large-scale language model has become a promising direction for coping with the data scarcity issue. However, the underlying discrepancies between the language modeling and NER task could limit the applicability of these approaches to real-world problems. Here, we present a novel approach to the problem of training a high-quality NERS model. We show that the training of a model that is trained on a small subset of the available training data is sufficient to achieve high performance in a wide variety of domains, including natural language processing, speech recognition, and machine translation. Our approach is based on the use of including natural language processing, speech recognition, and machine translation. Our approach is based on the use of deep neural networks (DNNs), which have been shown to be able to learn from large amounts of training data.",0