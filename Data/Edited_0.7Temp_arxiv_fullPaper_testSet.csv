text,#label
"Abstract This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language’s parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016; Johnson et al., 2017). Multilingual NMT is a compelling approach in production, as one only needs to train, deploy and maintain one model (instead of 2 × N ones, where N is the number of languages). It has also been shown to improve MT quality for low-resource languages (at the cost of a slight degradation for high-resource languages) and it can allow translation between languages that have no aligned data (“zero-shot translation”). However, such models can be costly to train, as they usually involve larger architectures and large datasets. Moreover, because they are trained jointly on all the languages, they require to know in advance the full set of languages. Adding a new language to an existing model usually means retraining the model on the full multilingual dataset. Naively fine-tuning the original model on the new language’s data is not an option because of vocabulary mismatch (the shared vocabulary needs to be modified to include the new language’s tokens) and catastrophic forgetting (the model will quickly forget how to translate in the other languages). In this paper, we study the problem of multilingual NMT incremental training or continual learning and propose a novel way to efficiently add a new source or target language. Some desirable properties of an incremental training method are: • No degradation on the existing language pairs; • Efficient training (e.g., no re-training on the existing language pairs); • Minimal amount of added parameters: the approach should scale to many languages and the model fit on a single GPU; • Minimal degradation in inference speed; • Good zero-shot performance: when training with X-EN (or EN-X) data, where X is a new language, we would like the model to be able to translate from X to any known language Y (resp. from Y to X). We propose a novel technique for incrementally adding a new source or target language, which consists in substituting the shared embedding matrix with a language-specific embedding matrix, which is fine-tuned on the new language’s data only while freezing the other parameters of the model. In some cases (e.g., when the new language is on the target size), a small number of additional parameters (e.g., adapter modules) have to be trained to match the performance of the re-training baseline. We perform two sets of experiments, with a 20-language Transformer Base trained on TED Talks, and a 20- language Transformer Big (with deep encoder and shallow decoder) trained on ParaCrawl; and show that this approach is fast and parameter-efficient and that it performs as well or better as the more costly alternatives. We propose a new technique for incrementally training multilingual NMT models on a new source or target language. It consists in creating a new monolingual BPE vocabulary for that language, substituting the shared embedding matrix with language-specific embeddings, and training those while freezing the other model parameters. At inference, translating in any of the initial languages is done by using the initial shared embeddings, and translating in the new language is done by using the newly trained embeddings. This approach does not change performance on the initial languages as the initial parameters are kept aside and not modified. For new source languages, it can achieve close performance to the more costly and less flexible bilingual and re-training baselines. For new target languages, this technique can be combined with language-specific parameters (finetuned Transformer layers or adapter modules) to match baseline performance at a small parameter cost. We validate this technique on two sets of experiments: small-scale on TED Talks and largescale on ParaCrawl; and show that it is compatible with two architectures: Transformer Base 6-6 and Big 12-2. We also show that incremental training on data aligned with English is enough to learn to translate between the new language and any of the initial languages. Translation between a new source and a new target language is also possible by combining their respective parameters at inference.",1
"Abstract This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new languageÃÂ¢ÃÂÃÂs parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016; Johnson et al., 2017). Multilingual NMT is a compelling approach in production, as one only needs to train, deploy and maintain one model (instead of 2 ÃÂ N ones, where N is the number of languages). It has also been shown to improve MT quality for low-resource languages (at the cost of a slight degradation for high-resource languages) and it can allow translation between languages that have no aligned data (Ã¢ÂÂzero-shot translationÃ¢ÂÂ). However, such models can be costly to train, as they usually involve larger architectures and large datasets. Moreover, because they are trained jointly on all the languages, they require to know in advance the full set of languages. Adding a new language to an existing model usually means retraining the model on the full multilingual dataset. Naively fine-tuning the original model on the new language is not an option because of vocabulary mismatch (the shared vocabulary needs to be modified to include the new language) and catastrophic forgetting (the model will quickly forget how to translate in the other languages). In this paper, we study the problem of multilingual NMT incremental training or continual learning and propose a novel way to efficiently add a new source or target language. Some desirable properties of an incremental training method are: Ã¢ÂÂ¢ No degradation on the existing language pairs; Ã¢ÂÂ¢ Efficient training (e.g., no re-training on the existing language pairs); Ã¢ÂÂ¢ Minimal amount of added parameters: the approach should scale to many languages and the model fit on a single GPU; Ã¢ÂÂ¢ Minimal degradation in inference speed; Ã¢ÂÂ¢ Good zero-shot performance: when training with X-EN (or EN-X) data, where X is a new language, we would like the model to be able to translate from X to any known language Y (resp. from Y to X). We propose a novel technique for incrementally adding a new source or target language, which consists in substituting the shared embedding matrix with a language-specific embedding matrix, which is fine-tuned on the new languageÃÂ¢ÃÂÃÂs data only while freezing the other parameters of the model. In some cases (e.g., when the new language is on the target size), a small number of additional parameters (e.g., adapter modules) have to be trained to match the performance of the re-training baseline. We perform two sets of experiments, with a 20-language Transformer Base trained on TED Talks, and a 20- language Transformer Big (with deep encoder and shallow decoder) trained on ParaCrawl; and show that this approach is fast and parameter-efficient and that it performs as well or better as the more costly alternatives. We propose a new technique for incrementally training multilingual NMT models on a new source or target language. It consists in creating a new monolingual BPE vocabulary for that language, substituting the shared embedding matrix with language-specific embeddings, and training those while freezing the other model parameters. At inference, translating in any of the initial languages is done by using the initial shared embeddings, and translating in the new language is done by using the newly trained embeddings. This approach does not change performance on the initial languages as the initial parameters are kept aside and not modified. For new source languages, it can achieve close performance to the more costly and less flexible bilingual and re-training baselines. For new target languages, this technique can be combined with language-specific parameters (finetuned Transformer layers or adapter modules) to match baseline performance at a small parameter cost. We validate this technique onsets of experiments: small-scale on TED Talks and largescale on ParaCrawl; and show that it is compatible with two architectures: Transformer Base 6-6 and Big 12-2. We also show that incremental training on data aligned with English is enough to learn to translate between the new language and any of the new languages.",0
"Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST. Introduction Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregressive generation. However, low latency is required by an increasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simultaneously. Compared with autoregressive models, non-autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly finetune an off-the-shelf pre-trained encoder model for nonautoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a nontrivial problem. To fully exploit the power of pre-trained models, we propose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding methods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacrifice the inference speed. MIST is a simple and effective iterative training strategy that works during the training stage and has no effect on inference speed. During the training stage, the model predicts the entire target sequence first, then we treat the generated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data augmentation method, which needs to prepare data before training, MIST enables dynamical data augmentation in the training stage. The term “dynamic” in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model overfitting with static pseudo targets generated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo targets also enables the conditional dependence to help convergence. As our experiments show, we evaluate our method on three generation tasks including question generation, summarization and paraphrase generation. Our method achieves significant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further evaluate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: • We propose a new paradigm, adopting pre-trained encoder for non-autoregressive generation without modifying model architectures. • We propose a simple and novel training method to improve the performance. • We empirically verify the effectiveness of our method in different generation tasks and pre-trained models. In this paper, we proposed a new paradigm to adopt pretrained encoders to NAR tasks. Our method uses a transformer encoder as model architecture with a new iterative decoding method called MIST to improve the generation quality without additional latency in decoding. Our method achieves state of the art results among the NAR and semi-NAR models on three different NLG tasks. We also demonstrated that our method can successfully utilize the pre-trained models to achieve better results than large-scale NAR pre-trained models like BANG.",1
"Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST. Introduction Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregressive generation. However, low latency is required by an increasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simultaneously. Compared with autoregressive models, non-autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly finetune an off-the-shelf pre-trained encoder model for nonautoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a nontrivial problem. To fully exploit the power of pre-trained models, we propose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding methods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacrifice the inference speed. MIST is a simple and effective iterative training strategy that works during the training stage and has no effect on inference speed. During the training stage, the model predicts the entire target sequence first, then we treat the generated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data augmentation method, which needs to prepare data before training, MIST enables dynamical data augmentation in the training stage. The term Ã¢ÂÂdynamicÃ¢ÂÂ in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model overfitting with static pseudo targets generated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo targets also enables the conditional dependence to help convergence. As our experiments show, we evaluate our method on three generation tasks including question generation, summarization and paraphrase generation. Our method achieves significant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further evaluate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: Ã¢ÂÂ¢ We propose a new paradigm, adopting pre-trained encoder for non-autoregressive generation without modifyingarchitectures. ÃÂ¢ÃÂÃÂ¢ We propose a simple and novel training method to improve the performance. ÃÂ¢ÃÂÃÂ¢ We empirically verify the effectiveness of our method in different generation tasks and pre-trained models. In this paper, we proposed a new paradigm to adopt pretrained encoders to NAR tasks. Our method uses a transformer pre-trained transformer model called MIx Source and pseudo Target (MIST). Unlike other pre-trained models, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our method uses a transformer pre-trained model to transform a transformer model into a transformer transformer model.",0
"Abstract Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehen- sibility and maintainability of the law. With five intuitive law smells as running examples—namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsession—, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting. Keywords Refactoring · Software Engineering · Law · Natural Language Process- ing · Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host alone accounting for 250 million repositories between 2008 and 2020.1 Over the course of its growth, this codebase has become increasingly interconnected when viewed through the lens of network analy- sis (Lima et al. 2014). Naturally, software engineers have turned to technology to keep track of this development and manage code interdependencies. While the challenges for law and software engineering and the constraints within which these challenges must be addressed are not identical, both domains share three important characteris- tics: Materially, their subject matters, legal rules on the one hand and code fragments on the other hand, contain commands intended to control (human or machine) behav- ior in order to achieve specific outcomes. Procedurally, output creation in both law and software engineering is distributed in time and space, and thus, both domains are subject to the challenges of dynamic multi-agent systems. Methodologically, lawyers and software engineers alike use abstractions to accommodate problems that are not fully known in the present. The similarities between the law and software engineering domains suggest possi- bilities for knowledge transfer between them. In this paper, we explore one such pos- sibility for the software engineering subfield of refactoring. Introduced into academic discourse by Opdyke and Johnson (1990) and popularized by Becker et al. (1999) (Second Edition: Fowler 2018), refactoring “describes a disciplined technique for re- structuring an existing body of code, altering its internal structure without changing its external behavior.”2 In software engineering, refactoring is indispensable for en- suring software quality and maintainability, and it is also subject to vivid academic discourse, inspiring detailed analyses of large code bases and even dedicated confer- ences.3 This has resulted in an actionable understanding of how various variables— e.g., programming language, team size, project size, or commit size (Ray et al. 2017), repetition and duplicated code (Lopes et al. 2017), or component size and open source model (Stamelos et al. 2002)—impact code quality. In this paper, we demonstrate how concepts from refactoring can be used in law, focusing on the example of code smells. At a high level, a code smell is a characteris- tic of (a part of) the source code that may indicate a deeper problem in its design and implementation, highlighting a need for refactoring (Tufano et al. 2015). We port this idea to the legal domain, introducing the concept of law smells. Law smells constitute the first step towards both (semi-)automatically detecting problematic parts of exist- ing codifications and (semi-)automatically improving codification quality. They also pave the path towards reproducible, quantitative quality measures for collections of legal rules, which allow us to assess the overall law climate and facilitate automatic code review and quality assurance. The remainder of this paper is structured as follows. In Section 2, we present related literature from law and computer science, along with interdisciplinary work in the intersection of both fields. We develop the concept of law smells, guided by five illustrative examples, in Section 3, and describe methods to detect law smells in Section 4. In Section 5, we demonstrate the utility of our methods for detecting five example smells, deliberately showcasing a variety of ways to present law smell detection results to end users. We discuss the limitations of our approach along with opportunities for future work in Section 6, and conclude in Section 7. Conclusion We have initiated the systematic study of law smells, i.e., patterns in legal texts that might impair the comprehensibility and maintainability of the law and indicate the need for legal refactoring. Guided by five illustrative examples, we have developed a comprehensive law smell taxonomy, introduced a methodological toolkit for law smell detection, and confirmed the utility of this toolkit in practice by applying it to 22 years of codified statutory legislation in the United States Code (1998–2019). Our work demonstrates how ideas from software engineering can be ported to the legal domain, and it creates numerous opportunities for further research on defining, detecting, and deodorizing law smells in all types of legal texts. Thus, we hope to have contributed a building block for the road towards truly computational legal drafting.",1
"Abstract Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehen- sibility and maintainability of the law. With five intuitive law smells as running examplesÃÂ¢ÃÂÃÂnamely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsessionÃ¢ÂÂ, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting. Keywords Refactoring ÃÂ· Software Engineering ÃÂ· Law ÃÂ· Natural Language Process- ing ÃÂ· Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host alone accounting for 250 million repositories between 2008 and 2020.1 Over the course of its growth, this codebase has become increasingly interconnected when viewed through the lens of network analy- sis (Lima et al. 2014). Naturally, software engineers have turned to technology to keep track of this development and manage code interdependencies. While the challenges for law and software engineering and the constraints within which these challenges must be addressed are not identical, both domains share three important characteris- tics: Materially, their subject matters, legal rules on the one hand and code fragments on the other hand, contain commands intended to control (human or machine) behav- ior in order to achieve specific outcomes. Procedurally, output creation in both law and software engineering is distributed in time and space, and thus, both domains are subject to the challenges of dynamic multi-agent systems. Methodologically, lawyers and software engineers alike use abstractions to accommodate problems that are not fully known in the present. The similarities between the law and software engineering domains suggest possi- bilities for knowledge transfer between them. In this paper, we explore one such pos- sibility for the software engineering subfield of refactoring. Introduced into academic discourse by Opdyke and Johnson (1990) and popularized by Becker et al. (1999) (Second Edition: Fowler 2018), refactoring Ã¢ÂÂdescribes a disciplined technique for re- structuring an existing body of code, altering its internal structure without changing its external behavior.Ã¢ÂÂ2 In software engineering, refactoring is indispensable for en- suring software quality and maintainability, and it is also subject to vivid academic discourse, inspiring detailed analyses of large code bases and even dedicated confer- ences.3 This has resulted in an actionable understanding of how various variablesÃ¢ÂÂ e.g., programming language, team size, project size, or commit size (Ray et al. 2017), repetition and duplicated code (Lopes et al. 2017), or component size and open source model (Stamelos et al. 2002)Ã¢ÂÂimpact code quality. In this paper, we demonstrate how concepts from refactoring can be used in law, focusing on the example of code smells. At a high level, a code smell is a characteris- tic of (a part of) the source code that may indicate a deeper problem in its design and implementation, highlighting a need for refactoring (Tufano et al. 2015). We port this idea to the legal domain, introducing the concept of law smells. Law smells constitute the first step towards both (semi-)automatically detecting problematic parts of exist- ing codifications and (semi-)automatically improving codification quality. They also pave the path towards reproducible, quantitative quality measures for collections of legal rules, which allow us to assess the overall law climate and facilitate automatic code review andassurance. The remainder of this paper is structured as follows. In Section 2, we present related literature from law and computer science, along with interdisciplinary work in the intersection of both fields. We develop the concept of law smells, guided by five illustrative examples, in Section 3, and describe methods to detect law smells in Section 4. We discuss the limitations of our approach along with opportunities for future work. Keywords Refactoring ÃÂ· Software Engineering ÃÂ· Law ÃÂ· Natural Language Process- ing ÃÂ· Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host alone accounting for 250 million repositories between 2008 and 2020.1 Over the course of its growth, this codebase has become increasingly interconnected when viewed through the lens of network analy- sis (Lima et al. 2014).",0
"Abstract This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such ap- proaches to handle productive UGC phenom- ena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of han- dling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this be- havior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. Introduction. Neural Machine Translation (NMT) models fall far short from being able to translate noisy User- Generated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addi- tion to ambiguous grammatical constructs and pro- fusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out- of-vocabulary tokens (OOVs) resulting from mis- spelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti et al., 2020). This is why, focusing more on the noise axis, char-based mod- els appear to offer a natural solution to this prob- lem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of- the-box character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Be- linkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BPE models even when translating noisy UGC. To understand this counter-intuitive result, we con- duct several experiments and analyses. In partic- ular, we manually annotated 400 sentences at the token level using a fine-grained typology, to per- form our analyses. These sentences correspond to the worst and the best translated utterances of two MT systems (a char-based and a transformer-based model). Our results highlight the extreme sensi- bility of character-based models to the vocabulary size, a parameter often overlooked in the literature. Using a simple set of experiments, we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robust- ness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set that enables in-depth evaluations of the impact of UGC idiosyncrasies; • we demonstrate that char-based neural machine translation models are extremely sensi- tive to unknown and rare characters on both synthetic data and noisy user-generated content; • we show how an overlooked hyper-parameter drastically improve char-based MT models ro- bustness to natural noise while maintaining the in-domain level of performance. Conclusion We showed that in zero-shot scenarios, char-based models are not robust to UGC idiosyncrasies. We presented several experiments that explained this counter-intuitive result by an over-sensibility of these models to the vocabulary size. We demonstrated that drastically lowering this param- eter increased the robustness of our char-based models when facing noisy UGC while maintaining almost the same level of performance for in-domain and out-of-domain canonical datasets. Interestingly, we noticed a lack in the literature regarding the importance of vocabulary considerations when training character-based NMTmodels. However, our results show that a path for improvement exists, leading to more generic and stable models and making us believe that further research on the matter can bring promising alternatives to domain-adaptation via target domain data addition.",1
"Abstract This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such ap- proaches to handle productive UGC phenom- ena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of han- dling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this be- havior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. Introduction. Neural Machine Translation (NMT) models fall far short from being able to translate noisy User- Generated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales NuÃÂnÃÂez et al., 2019). In addi- tion to ambiguous grammatical constructs and pro- fusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out- of-vocabulary tokens (OOVs) resulting from mis- spelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti et al., 2020). This is why, focusing more on the noise axis, char-based mod- els appear to offer a natural solution to this prob- lem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of- the-box character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Be- linkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BPE models even when translating noisy UGC. To understand this counter-intuitive result, we con- duct several experiments and analyses. In partic- ular, we manually annotated 400 sentences at the token level using a fine-grained typology, to per- form our analyses. These sentences correspond to the worst and the best translated utterances of two MT systems (a char-based and a transformer-based model). Our results highlight the extreme sensi- bility of character-based models to the vocabulary size, a parameter often overlooked in the literature. Using a simple set of experiments, we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robust- ness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: Ã¢ÂÂ¢ we provide an annotated data set that enables in-depth evaluations of the impact of UGC idiosyncrasies; Ã¢ÂÂ¢ we demonstrate that char-based neural machine translation models are extremely sensi- tive to unknown and rare characters on both synthetic data and noisy user-generated content; Ã¢ÂÂ¢ we show how an overlooked hyper-parameter drastically improve char-based MT models ro- bustness to natural noise while maintaining the in-domain level of performance. Conclusion We showed that in zero-shot scenarios, char-based models are not robust to UGC idiosyncrasies. We presented several experiments that explained this counter-intuitive result by an over-sensibility of these models to the vocabulary size. We demonstrated that drastically lowering this param- eterthe robustness of our char-based models when facing noisy UGC while maintaining almost the same level of performance for in-domain and out-of-domain canonical datasets. Interestingly, we noticed a lack in the literature regarding the importance of vocabulary considerations when training character-based NMTmodels.",0
"Abstract Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reason- ing process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decom- position. The Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (Velicˇkovic ́ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence, such as DFGN (Xiao et al., 2019). There is no evidence to show that such additional discrim- inators are strongly correlated with the reasoning results of the end-to-end model, which means not faithful. Secondly, although graph structure is help- ful to multi-hop reasoning in theory, but recent work (Shao et al., 2020) shows that the existing graph neural network is only a special attention mechanism (Bahdanau et al., 2014), and it’s not necessary for multi-hop QA, with the experiments that better results can be achieved by using only transformer network instead of graph neural net- work, as long as the same additional adjacency matrix information is provided. We observed that human reasoning about com- plex questions is not accomplished overnight and it’s usually divided into the steps of question de- composition, answering sub-questions, summariz- ing and comparing. For example, for the complex question, ""whose candidate will get more votes in the 2020 U.S. election, Democrats and Repub- licans?"" People will not think about the whole question, but firstly decompose the complex ques- tion. Realizing that the subject of the question is ""Democrats and Republicans"", and the question is about ""candidates"" and ""number of votes"", peo- ple can answer those sub-questions progressively– ""who is the Democratic candidate?"" and ""how many votes does ANS get?"" The same thinking process was performed for another question sub- ject, ""Republican Party"". Finally, the two votes were compared to obtain the answer to the entire complex question. Inspired by the way humans answer complex multi-hop questions, in this work we abandoned the end-to-end model structure, but imitated the human reasoning mechanism to propose a three- stage Relation Extractor-Reader and Comparator (RERC) model1. We first build a Relation Extrac- tor, which can automatically extract the subject and key relations of the question from the com- plex unstructured textual representation. For the Relation Extractor, we use two different structures, one is classification-type (CRERC), where the evi- dence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; the other is span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Next, we use the advanced ALBERT model (Lan et al., 2020) as the Reader, which reads the corre- sponding paragraphs and answer each sub-question composed of the subject and relations of the ques- tion in turn. Finally, for comparison type questions, our Comparator module compares the magnitude of each subject’s final answer, and then get the entire answer. Our contributions are summarized as follows: We propose a novel RERC model for multi- hop text-based QA and evidence path search tasks. We propose a Query-aware Entity Tree Para- graph Screening (QETPS) method to filter valid paragraphs from a large number of doc- uments before Reader module, which is more efficiently than previous paragraph selecting methods. We provide an experimental study on a pub- lic multi-hop dataset (2WikiMultiHopQA) to demonstrate that our proposed RERC model has the state-of-the-art performance in both answering multi-hop questions and extracting evidence at the same time. Conclusion and future work We propose a three-stage framework of Relation Extractor-Reader and Comparator (RERC), which solves the multi-hop QA task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2WikiMultiHopQA dataset, which is close to human performance. Our RERC framework can also provide faithful evi- dence with excellent interpretability. Multiple future research directions according to our proposed RERC model may be envisioned. First of all, benefiting to the three-stage structure, the RERC model has the potential to leverage the network structure of the Relation Extractor to gain generalization capabilities for more complex ques- tions. Moreover, we expect that collaborative error correction mechanism applied in Reader module will largely avoid accumulation of errors in multi- hop reasoning.",1
"Abstract Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machineÃÂ¢ÃÂÃÂs reason- ing process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decom- position. The Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (VelicÃÂkovic ÃÂ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence, such as DFGN (Xiao et al., 2019). There is no evidence to show that such additional discrim- inators are strongly correlated with the reasoning results of the end-to-end model, which means not faithful. Secondly, although graph structure is help- ful to multi-hop reasoning in theory, but recent work (Shao et al., 2020) shows that the existing graph neural network is only a special attention mechanism (Bahdanau et al., 2014), and itÃ¢ÂÂs not necessary for multi-hop QA, with the experiments that better results can be achieved by using only transformer network instead of graph neural net- work, as long as the same additional adjacency matrix information is provided. We observed that human reasoning about com- plex questions is not accomplished overnight and itÃ¢ÂÂs usually divided into the steps of question de- composition, answering sub-questions, summariz- ing and comparing. For example, for the complex question, ""whose candidate will get more votes in the 2020 U.S. election, Democrats and Repub- licans?"" People will not think about the whole question, but firstly decompose the complex ques- tion. Realizing that the subject of the question is ""Democrats and Republicans"", and the question is about ""candidates"" and ""number of votes"", peo- ple can answer those sub-questions progressivelyÃ¢ÂÂ ""who is the Democratic candidate?"" and ""how many votes does ANS get?"" The same thinking process was performed for another question sub- ject, ""Republican Party"". Finally, the two votes were compared to obtain the answer to the entire complex question. Inspired by the way humans answer complex multi-hop questions, in this work we abandoned the end-to-end model structure, but imitated the human reasoning mechanism to propose a three- stage Relation Extractor-Reader and Comparator (RERC) model1. We first build a Relation Extrac- tor, which can automatically extract the subject and key relations of the question from the com- plex unstructured textual representation. For the Relation Extractor,use two different structures, one is classification-type (CRERC), where the evi- dence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; the other is span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (VelicÃÂkovic ÃÂ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. ",0
"ABSTRACT The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification performance, and many use data that contain potential confounds, which makes it difficult to determine their validity. Two possible confounds are data-origin (i.e., the dataset is composed of more than one source) and product ownership (i.e., reviews written by individuals who own or do not own the reviewed product). In the present study, we investigate the effect of both confounds for fake review detection. Using an experimental design, we manipulate data-origin, product ownership, review polarity, and veracity. Supervised learning analysis suggests that review veracity (60.26 - 69.87%) is somewhat detectable but reviews additionally confounded with product-ownership (66.19 - 74.17%), or with data-origin (84.44 - 86.94%) are easier to classify. Review veracity is most easily classified if confounded with product-ownership and data-origin combined (87.78 - 88.12%), suggesting overestimations of the true performance in other work. These findings are moderated by review polarity. Introduction Online shopping is not new, but it is increasing in popularity as seen by the growth of companies such as Amazon and eBay [Palmer, 2020, Soper, 2021, Weise, 2020]. Previous work shows that consumers rely heavily on product reviews posted by other people to guide their purchasing decisions [Anderson and Magruder, 2012, Chevalier and Mayzlin, 2006, Watson, 2018]. While sensible, this has created the opportunity and market for deceptive reviews, which are currently among the most critical problems faced by online shopping platforms and those who use them [Dwoskin and Timberg, 2018, Nguyen, 2018]. Research suggests that for a range of deception detection tasks (e.g. identifying written or verbal lies about an individual’s experience, biographical facts, or any non-personal events), humans typically perform at the chance level [DePaulo et al., 2003, Kleinberg and Verschuere, 2021]. Furthermore, in the context of considering online reviews, the sheer volume of reviews [Woolf, 2014] makes the task of deception detection implausible for all but the most diligent consumers. With this in mind, the research effort has shifted towards the use and calibration of automated approaches. For written reviews, which are the focus of this article, such approaches typically rely on text mining and supervised machine learning algorithms [Newman et al., 2003, Pérez-Rosas et al., 2018, Ott et al., 2011, 2013]. However, while the general approach is consistent, classification performance varies greatly between studies, as do the approaches to constructing the datasets used. Higher rates of performance are usually found in studies for which data are constructed from several different sources such as a crowdsourcing platform and an online review platform [Ott et al., 2011, 2013], while lower rates of performance are typically found in studies for which data is extracted from a single source and for which greater experimental control is exercised (e.g., Kleinberg and Verschuere [2021]). Such findings suggest that confounds associated with the construction of datasets may explain some of the variation in classification performance between studies and highlights the need for the exploration of such issues. In the current study, we will explore two possible confounds and estimate their effects on automated classification performance. In what follows, we first identify and explain the two confounds. Next, we provide an outline of how we control for them through a highly controlled data collection procedure. Lastly, we will run six analyses on subsets of the data to demonstrate the pure and combined effects of the confounds in automated veracity classification tasks. Confounding factors In an experiment, confounding variables can lead to an omitted variable bias, in which the omitted variables affect the dependent variable, and the effects are falsely attributed to the independent variables(s). In the case of the detection of fake reviews, two potential confounds might explain why some studies report higher and possibly overestimated automated classification performances than others. The first concerns the provenance of the data used. For example, deceptive reviews are often collected from participants recruited through crowdsourcing platforms, while truthful reviews are scraped from online platforms Ott et al. [2011, 2013], such as TripAdvisor, Amazon, Trustpilot, or Yelp. Creating datasets in this way is efficient but introduces a potential confound. That is, not only do the reviews differ in veracity but also their origin. If origin and veracity were counterbalanced so that half of the fake (and genuine) reviews were generated using each source this would be unproblematic but unfortunately in some existing studies, the two are confounded. A second potential confound concerns ownership. In existing studies, participants who write fake reviews are asked to write about products (or services) that they do not own. In contrast, in the case of the scraped reviews – assuming that they are genuine (which is also a problematic assumption) – these will be written by those who own the products (or have used the services). As such, ownership and review veracity (fake or genuine) will also be confounded. Confounds in fake review detection Studies of possible confounding factors in deception detection tasks that involve reviews are scarce. In their study, Salvetti et al. [2016] investigated whether a machine learning classifier could disentangle the effects of two different types of deception – lies vs. fabrications. In the case of the former, participants recruited using Amazon’s Mechanical Turk (AMT) were asked to write a truthful and deceptive review about an electronic product or a hotel they knew. In the case of the latter, a second group of AMT participants was asked to write deceptive reviews about the same products or hotels. However, this time they were required to do this for products or hotels they had no knowledge of, resulting in entirely fabricated reviews. Salvetti et al. [2016] found that the classifier was able to differentiate between truthful reviews and fabricated ones but not particularly well. However, it could not differentiate between truthful reviews and lies – classification performance was around the chance level. These findings suggest that product ownership (measured here in terms of fabrications vs truthful reviews) is a potentially important factor in deceptive review detection. A different study examined the ability of a classifier to differentiate truthful and deceptive reviews from Amazon [Fornaciari et al., 2020] using the ""DeRev"" dataset [Fornaciari and Poesio, 2014]. The dataset contains fake Amazon book reviews that were identified through investigative journalism [Flood, 2012, Streitfeld, 2011]. Truthful reviews were selected from Amazon about other books, from famous authors such as Arthur Conan Doyle, Rudyard Kipling, Ken Follett, or Stephen King for which it was assumed that it would not make sense for someone to write fake reviews about them. A second corpus of fake reviews – written about the same books – was then generated by participants recruited through crowdsourcing to provide a comparison with the ""DeRev"" reviews. The authors then compared the performance of a machine learning classifier in distinguishing between different classes of reviews (e.g., crowdsourced-fake vs. Amazon-fake, crowdsourced-fake vs. Amazon-truthful). Most pertinent here was the finding that the authors found the crowdsourced-fake reviews differed from the Amazon-fake reviews. Both studies [Fornaciari et al., 2020, Salvetti et al., 2016] hint at the problems of confounding factors in deception detection tasks. However, a combination of both factors (ownership and data-origin) or how the two interact for true and deceptive reviews has not been tested yet. Aims of this paper Confounding variables have the potential to distort the findings of studies, leading researchers to conclude that a classifier can distinguish between truthful and fake reviews when, in reality, it is actually distinguishing between other characteristics of the data, such as the way in which it was generated. Such confounds would mean that the real-world value of the research is limited (at best). In the current study, we employ an experimental approach to systematically manipulate these possible confounders and to measure their effects for reviews of smartphones. Specifically, we estimate the effect of product-ownership by collecting truthful and deceptive reviews from participants who do and do not own the products they were asked to review. To examine the effect of data-origin we also use data (for the same products) scraped from an online shopping platform. We first examine how well reviews can be differentiated by veracity alone (i.e., without confounds), and if classification performance changes when this is confounded with product-ownership, data-origin, or both. If ownership or data-origin do influence review content (we hypothesize that they do), reviews should be easier to differentiate when either of the two confounds is present in veracity classification, but reviews should be most easily classifiable if both confounds (ownership, data-origin) are present at the same time. Conclusion Through careful experimental control, we found that product ownership and data-origin are confounding fake review detection resulting in overestimations of model performances. Especially data-origin seems to boost classification performance, and this could easily be misattributed to classifying veracity alone. Our findings suggest an overestimation of 24.89-46.23% when data is sourced from different platforms. More effort and experimental control are necessary to create datasets when investigating complex concepts such as deception.",1
"ABSTRACT The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification performance, and many use data that contain potential confounds, which makes it difficult to determine their validity. Two possible confounds are data-origin (i.e., the dataset is composed of more than one source) and product ownership (i.e., reviews written by individuals who own or do not own the reviewed product). In the present study, we investigate the effect of both confounds for fake review detection. Using an experimental design, we manipulate data-origin, product ownership, review polarity, and veracity. Supervised learning analysis suggests that review veracity (60.26 - 69.87%) is somewhat detectable but reviews additionally confounded with product-ownership (66.19 - 74.17%), or with data-origin (84.44 - 86.94%) are easier to classify. Review veracity is most easily classified if confounded with product-ownership and data-origin combined (87.78 - 88.12%), suggesting overestimations of the true performance in other work. These findings are moderated by review polarity. Introduction Online shopping is not new, but it is increasing in popularity as seen by the growth of companies such as Amazon and eBay [Palmer, 2020, Soper, 2021, Weise, 2020]. Previous work shows that consumers rely heavily on product reviews posted by other people to guide their purchasing decisions [Anderson and Magruder, 2012, Chevalier and Mayzlin, 2006, Watson, 2018]. While sensible, this has created the opportunity and market for deceptive reviews, which are currently among the most critical problems faced by online shopping platforms and those who use them [Dwoskin and Timberg, 2018, Nguyen, 2018]. Research suggests that for a range of deception detection tasks (e.g. identifying written or verbal lies about an individualÃ¢ÂÂs experience, biographical facts, or any non-personal events), humans typically perform at the chance level [DePaulo et al., 2003, Kleinberg and Verschuere, 2021]. Furthermore, in the context of considering online reviews, the sheer volume of reviews [Woolf, 2014] makes the task of deception detection implausible for all but the most diligent consumers. With this in mind, the research effort has shifted towards the use and calibration of automated approaches. For written reviews, which are the focus of this article, such approaches typically rely on text mining and supervised machine learning algorithms [Newman et al., 2003, PeÃÂrez-Rosas et al., 2018, Ott et al., 2011, 2013]. However, while the general approach is consistent, classification performance varies greatly between studies, as do the approaches to constructing the datasets used. Higher rates of performance are usually found in studies for which data are constructed from several different sources such as a crowdsourcing platform and an online review platform [Ott et al., 2011, 2013], while lower rates of performance are typically found in studies for which data is extracted from a single source and for which greater experimental control is exercised (e.g., Kleinberg and Verschuere [2021]). Such findings suggest that confounds associated with the construction of datasets may explain some of the variation in classification performance between studies and highlights the need for the exploration of such issues. In the current study, we will explore two possible confounds and estimate their effects on automated classification performance. In what follows, we first identify and explain the two confounds. Next, we provide an outline of how we control for them through a highly controlled data collection procedure. Lastly, we will run six analyses on subsets of the data to demonstrate the pure and combined effects of the confounds in automated veracity classification tasks. Confounding factors In an experiment, confounding variables can lead to an omitted variable bias, in which the omitted variables affect the dependent variable, and the effects are falsely attributed to the independent variables(s). In the case of the detection of fake reviews, two potential confounds might explain why some studies report higher and possibly overestimated automated classification performances than others. The first concerns the provenance of the data used. For example, deceptive reviews are often collected from participants recruited through crowdsourcing platforms, while truthful reviews are scraped from online platforms Ott et al. [2011, 2013], such as TripAdvisor, Amazon, Trustpilot, or Yelp. Creating datasets in this way is efficient but introduces a potential confound. That is,only do the reviews differ in veracity but also their origin. If origin and veracity were counterbalanced so that half of the fake (and genuine) reviews were generated using each source this would be unproblematic but unfortunately in some existing studies, the two are confounded. A second potential confound concerns ownership. In existing studies, participants who write fake reviews are asked to write about products (or services) that they do not own. In contrast, in the case of the scraped reviews Ã¢ÂÂ assuming that they are genuine (which is also a problematic assumption) Ã¢ÂÂ these will be written by those who own the products (or have used the services). As such, ownership and review veracity (fake or genuine) will also be confounded. Confounds in fake review detection Studies of possible confounding factors in deception detection tasks that involve reviews are scarce. In their study, Salvetti et al. [2016] investigated whether a machine learning classifier could disentangle the effects of two different types of deception Ã¢ÂÂ lies vs. fabrications. In the case of the former, participants recruited using AmazonÃ¢ÂÂs Mechanical Turk (AMT) were asked to write a truthful and deceptive review about an electronic product or a hotel they knew. In the case of the latter, a second group of AMT participants was asked to write deceptive reviews about the same products or hotels. However, this time they were required to do this for products or hotels they had no knowledge of, resulting in entirely fabricated reviews. Salvetti et al. [2016] found that the classifier was able to differentiate between truthful reviews and fabricated ones but not particularly well. However, it could not differentiate between truthful reviews and lies Ã¢ÂÂ classification performance was around the chance level. These findings suggest that product ownership (measured here in terms of fabrications vs truthful reviews) is a potentially important factor in deceptive review detection. A different study examined the ability of a classifier to differentiate truthful and deceptive reviews from Amazon [Fornaciari et al., 2020] using the ""DeRev"" dataset [Fornaciari and Poesio, 2014]. The dataset contains fake Amazon book reviews that were identified through investigative journalism [Flood, 2012, Streitfeld, 2011]. Truthful reviews were selected from Amazon about other books, from famous authors such as Arthur Conan Doyle, Rudyard Kipling, Ken Follett, or Stephen King for which it was assumed that it would not make sense for someone to write fake reviews about them. A second corpus of fake reviews Ã¢ÂÂ written about the same books Ã¢ÂÂ was then generated by participants recruited through crowdsourcing to provide a comparison with the ""DeRev"" reviews. The authors then compared the performance of a machine learning classifier in distinguishing between different classes of reviews (e.g., crowdsourced-fake vs. Amazon-fake, crowdsourced-fake vs. Amazon-truthful). Most pertinent here was the finding that the authors found the crowdsourced-fake reviews differed from the Amazon-fake reviews. Both studies [Fornaciari et al., 2020, Salvetti et al., 2016] hint at the problems of confounding factors in deception detection tasks. However, a combination of both factors (ownership and data-origin) or how the two interact for true and deceptive reviews has not been tested yet. Aims of this paper Confounding variables have the potential to distort the findings of studies, leading researchers to conclude that a classifier can distinguish between truthful and fake reviews when, in reality, it is actually distinguishing between other characteristics of the data, such as the way in which it was generated. Such confounds would mean that the real-world value of the research is limited (at best). In the current study, we employ an experimental approach to systematically manipulate these possible confounders and to measure their effects for reviews of smartphones. Specifically, we estimate the effect of product-ownership by collecting truthful and deceptive reviews from participants who do and do not own the products they were asked to review. To examine the effect of data-origin we also use data (for the same products) scraped from an online shopping platform. We first examine how well reviews can be differentiated by veracity alone (i.e., without confounds), and if classification performance changes when this is confounded with product-ownership, data-origin, or both. If ownership or data-origin do influence review content (we hypothesize that they do), reviews should be easier to differentiate when either of the two confounds is present in veracity classification, but reviews should be most easily classifiable if both confounds (ownership, data-origin) are present at the same time. Conclusion Throughexperimental control, we found that product ownership and data-origin are confounding fake review detection resulting in overestimations of model performances. Especially data-origin seems to boost classification performance, and this could easily be misattributed to classifying veracity alone.",0
"Abstract For each goal-oriented dialog task of inter- est, large amounts of data need to be col- lected for end-to-end learning of a neural dia- log system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a related dialog task. Naively learning from related data fails to improve performance as the related data can be inconsistent with the target task. We de- scribe a meta-learning based method that selec- tively learns from the related dialog task data. Our approach leads to significant accuracy im- provements in an example dialog task. Introduction One key benefit of goal-oriented dialog systems that are trained end-to-end is that they only re- quire examples of dialog for training. Avoiding the modular structure of pipeline methods removes the human effort involved in creating intermediate annotations for data to train the modules. The end- to-end structure also enables automatic adaptation of the system, with different components of the model changing together. This flexibility is partic- ularly valuable when applying the system to a new domain. However, end-to-end systems currently require significantly more data, increasing the human effort in data collection. The most common method for training is Supervised Learning (SL) using a dataset of dialogs of human agents performing the task of interest (Bordes et al., 2017; Eric and Manning, 2017; Wen et al., 2017). To produce an effective model, the dataset needs to be large, high quality, and in the target domain. That means for each new dialog task of interest large amounts of new data has to be collected. The time and money involved in that collection process limits the potential appli- cation of these systems. We propose a way to reduce this cost by selec- tively learning from data from related dialog tasks: tasks that have parts/subtasks that are similar to the new task of interest. Specifically, we describe a method for learning which related task examples to learn from. Our approach uses meta-gradients to automatically meta-learn a scalar weight ∈ (0, 1) for each of the related task data points, such that learning from the weighted related task data points improves the performance of the dialog system on the new task of interest. These weights are dynami- cally adjusted over the course of training in order to learn most effectively. We still learn from data for the target task, but do not need as much to achieve the same results. To demonstrate this idea, we considered two ex- periments. First, we confirmed that the method can work in an ideal setting. We constructed a classifi- cation task where the related task data is actually from the same task, but with the incorrect label for 75% of examples, and there is an input feature that indicates whether the label is correct or not. Our approach is able to learn to ignore the misleading data, achieving close to the performance of a model trained only on the correct examples. Second, we evaluated the approach on a per- sonalized restaurant reservation task with limited training data. Here, the related task is also restau- rant reservation, but without personalization and with additional types of interactions. We compare our approach to several standard alternatives, in- cluding multi-task learning and using the related data for pre-training only. Our approach is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dia- log policy using Reinforcement Learning (RL) in a multi-domain setting which can then be trans- ferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learn- ing (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge to a new test task by learning how to do transfer on different training tasks. While each task only has a limited amount of dialog data, they need a lot of tasks during training. In contrast, we look at a setting where the task from which we want to transfer knowledge from and the task that we want to transfer knowledge to are the only tasks that we have access to at training time. Any learning about how to transfer knowledge has to happen from just these two tasks. None of the above methods are applicable to this setting. Learning a task, while simultaneously meta- learning certain aspects of the learning process has been done successfully in some SL and RL set- tings recently. Wu et al. (2018); Wichrowska et al. (2017) use meta-learning to adapt hyper parameters such as learning rate and and even learn entire op- timizers themselves during training for SL tasks such as image classification. Given a single task, Zheng et al. (2018) successfully meta-learn intrin- sic rewards that help the agent perform well on that task. Xu et al. (2018) use meta-gradients to learn RL training hyperparameters such as the discount factor and bootstrapping parameters. The meta- gradient technique used in our proposed method is closely related to Rajendran et al. (2020). They learn intrinsic rewards for an RL agent acting in given domain, such that learning with those intrin- sic rewards improves the performance of the agent in the task of interest in a different domain. While we use a meta-learning based method for learning the weights for the related task data points in this work, there are other techniques in the ma- chine learning literature, especially in the computer vision literature, that can potentially be used to learn the weights. A large section of these recent techniques are based on learning an adversarially trained discriminator for estimating the weights of related image classification task data points (Zhao et al., 2018; Cao et al., 2018; Sankaranarayanan et al., 2018; Wang et al., 2019). Jiang and Zhai (2007) use a combination of several domain adap- tation heuristics to assign weights and evaluate on NLP tasks. Moon and Carbonell (2017) cluster the related task data points and learn attention weights for the clusters. An interesting future direction would be to study which weighting methods are best suited for end-to-end learning of neural goal- oriented dialog systems using related tasks and under what conditions. Conclusion End-to-end learning of neural goal-oriented dialog systems requires large amounts of data for training. Collecting data is a costly and time consuming pro- cess. In this work we showed on an example dialog task we can utilise a related task’s data to improve the performance of a new task of interest with only a limited amount of data. Our proposed method uses meta-learning to automatically learn which of the related task data points to selectively learn from. An important future work is to evaluate/extend the proposed method to more challenging and complex dialog datasets/tasks. Data useful for a dialog task of interest (related data) could be present in different formats. The related data could include, for example, natural language instructions on how to perform the task of interest, or a description of how the new dialog task of interest is different from a related dialog task. An interesting future direction is to investi- gate methods to successfully utilise such related data.",1
"Abstract For each goal-oriented dialog task of inter- est, large amounts of data need to be col- lected for end-to-end learning of a neural dia- log system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a related dialog task. Naively learning from related data fails to improve performance as the related data can be inconsistent with the target task. We de- scribe a meta-learning based method that selec- tively learns from the related dialog task data. Our approach leads to significant accuracy im- provements in an example dialog task. Introduction One key benefit of goal-oriented dialog systems that are trained end-to-end is that they only re- quire examples of dialog for training. Avoiding the modular structure of pipeline methods removes the human effort involved in creating intermediate annotations for data to train the modules. The end- to-end structure also enables automatic adaptation of the system, with different components of the model changing together. This flexibility is partic- ularly valuable when applying the system to a new domain. However, end-to-end systems currently require significantly more data, increasing the human effort in data collection. The most common method for training is Supervised Learning (SL) using a dataset of dialogs of human agents performing the task of interest (Bordes et al., 2017; Eric and Manning, 2017; Wen et al., 2017). To produce an effective model, the dataset needs to be large, high quality, and in the target domain. That means for each new dialog task of interest large amounts of new data has to be collected. The time and money involved in that collection process limits the potential appli- cation of these systems. We propose a way to reduce this cost by selec- tively learning from data from related dialog tasks: tasks that have parts/subtasks that are similar to the new task of interest. Specifically, we describe a method for learning which related task examples to learn from. Our approach uses meta-gradients to automatically meta-learn a scalar weight Ã¢ÂÂ (0, 1) for each of the related task data points, such that learning from the weighted related task data points improves the performance of the dialog system on the new task of interest. These weights are dynami- cally adjusted over the course of training in order to learn most effectively. We still learn from data for the target task, but do not need as much to achieve the same results. To demonstrate this idea, we considered two ex- periments. First, we confirmed that the method can work in an ideal setting. We constructed a classifi- cation task where the related task data is actually from the same task, but with the incorrect label for 75% of examples, and there is an input feature that indicates whether the label is correct or not. Our approach is able to learn to ignore the misleading data, achieving close to the performance of a model trained only on the correct examples. Second, we evaluated the approach on a per- sonalized restaurant reservation task with limited training data. Here, the related task is also restau- rant reservation, but without personalization and with additional types of interactions. We compare our approach to several standard alternatives, in- cluding multi-task learning and using the related data for pre-training only. Our approach is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod-as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. In this work, we compare our approach to several standard alternatives, in- cluding multi-task learning and using the related data for pre-training only. Our approach is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dia- log policy using Reinforcement Learning (RL) in a multi-domain setting which can then be trans- ferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learn- ing (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge to a new test task by learning how to do transfer on different training tasks. While each task only has a limited amount of dialog data, they need a lot of tasks during training. In contrast, we look at a setting where the task from which we want to transfer knowledge from and the task that we want to transfer knowledge to are the only tasks that we have access to at training time. Any learning about how to transfer knowledge has to happen from just these two tasks. None of the above methods are applicable to this setting. Learning a task, while simultaneously meta- learning certain aspects of the learning process has been done successfully in some SL and RL set- tings recently. Wu et al. (2018); Wichrowska et al. (2017) use meta-learning to adapt hyper parameters such as learning rate and and even learn entire op- timizers themselves during training for SL tasks such as image classification. Given a single task, Zheng et al. (2018) successfully meta-learn intrin- sic rewards that help the agent perform well on that task. Xu et al. (2018) use meta-gradients to learn RL training hyperparameters such as the discount factor and bootstrapping parameters. The meta- gradient technique used in our proposed method is closely related to Rajendran et al. (2020). They learn intrinsic rewards for an RL agent acting in given domain, such that learning with those intrin- sic rewards improves the performance of the agent in the task of interest in a different domain.",0
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Pérez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, accept it as given that certain legal clauses are fair and others unfair, for which they implement ML methods to draw a classification boundary. Testing their API (“CLAUDETTE” 2021) with the sentence ‘the boy will hit the girl’, for example, produces the result: ‘Claudette found no potentially unfair clause’. Which may be a reflection of out-of-domain knowledge limitations. Work done by (Schramowski et al. 2019) and (Jentzsch et al. 2019) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2019) hold implicit representations of moral values. They do so using vector comparisons based on a template of Do’s and Don’ts. Furthermore (Schramowski et al. 2019) replicates the moral choices found by (Jentzsch et al. 2019), then computes the variance explained by another LM, the Universal Sentence Encoder (USE) (Cer et al. 2018) with respect to Yes/No question templates on moral choices. Further work in (Izzidien 2021) replicates the finding that word embeddings contain implicit biases, and proposes to use them to make assessments of verbs. Building on these studies, we propose to harness these implicit social biases to act as a metric for an explainable assessment of sentences, specifically those related to fairness. However, in order to extract this bias, and instead of using a template of Do’s and Don’ts, we build on work in the social psychology literature on determining, which factors are able to explain acts of fairness made by humans, from which we extract a template that represents the principal perceptions, humans typically engage when making a fairness assessment. In doing so, we aim to approximate those perceptions, which we hypothesise will allow sentences to be classed according to which perception they are closer to, being fair or unfair. Although the paper does not set out to produce a fully validated and verified fairness measurement tool, it contributes to the development of one based on an approximation of the factors, humans engage when making such measurements. In effect, the metric may be considered a proxy for fairness perceptions. As such, we do not claim to be measuring a specific fairness type, e.g., distributional/outcome. However, as will be discussed, fairness evaluations engage a number of principal psychological factors. It is these factors that we attempt to approximate using a method of word embeddings. While the ML techniques used in this paper are well established, our approach to digitising the factors, and the theory behind their use in this domain is new. Not least as no such measure exists in the literature. The paper is organised as follows. The methods section is presented next, this incorporates a detailed study to determine the most explanatory psychological factors present in fairness assessments. The paper then details two approaches to digitise these psychological factors using word embeddings and ML. The results are then presented. A short discussion is followed by improvements, limitations, and the conclusion. This paper contributes originally with the following: 1) A new literature review to determine the principal factors that best explain pro-social acts, employing the dictator game (DG) to remove the confound of strategic intentions – i.e., where a pro-social act is engaged not out of fairness, but due to fear of punishment. 2) The use of the factors found in the above literature review to act as measures in multidimensional vector space. We implement an under-utilised method of vector linear algebra to define an ontological approximation of fairness perceptions. 3) The use of the above vectors as a measure of test sentences; are they closer to being: fair, or unfair, and to what degree?",1
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and PÃÂ©rez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, accept it as given that certain legal clauses are fair and others unfair,which they implement ML methods to draw a classification boundary. Testing their API (ÃÂ¢ÃÂÃÂCLAUDETTEÃÂ¢ÃÂÃÂ 2021) with the sentence ÃÂ¢ÃÂÃÂthe boy will hit the girlÃÂ¢ÃÂÃÂ, for example, produces the result: ÃÂ¢ÃÂÃÂClaudette found no potentially unfair clauseÃÂ¢ÃÂÃÂ. Which may be a reflection of out-of-domain knowledge limitations. Work done by (Schramowski et al. 2021) and (Jentzsch et al. 2021) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2021) hold implicit representations of moral values. They do so using vector comparisons based on a template of DoÃ¢ÂÂs and DonÃ¢ÂÂts. Furthermore, a second approach, using BFTs (Schramowski et al. 2021) and ML based on the said fairness template produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and PÃÂ©rez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020).",0
"ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation · Machine Translation ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) system’s translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE “is concerned about predicting the quality of a system’s output for a given input, without any information about the expected output” [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as ‘good for nothing’ [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in O’Connell [2001] which assert that round-trip translation relies heavily on a model accurately translating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentences back to their original language so that errors in one translation system are reduced by the collection of models used, however, in Zaanen and Zwarts [2006], they still concluded that round-trip translation was not a good way to measure machine translation quality. One common theme we saw throughout these research papers was that prior research mainly focused on the round-trip translation and did not focus too heavily on how they were evaluating the similarity between these sentences. Evaluation metrics like BLEU are limited as they only measure the lexical similarity and do not take the semantics of the sentences into consideration. Maintaining the semantic information of a sentence is something we should want to ensure that a machine translation system does when evaluating its translations. While Somers [2005] does conclude that the results given by his RTT experiment do not look promising, he also recognises in his conclusion that there is a big reliance on BLEU and F-Score throughout his paper and he says that he would like to replicate his experiments using human rating of intelligibility. During the course of our work, Moon et al. [2020] was published which very closely resembles our approach. They report a Pearson R correlation of 0.95 on the WMT19 metrics task evaluation set for English-German sentence pairs. We were unable to compare our approach on this data, as the ground truth direct assessment scores were not made publicly available. Conclusion In this paper, we have investigated the efficacy of round-trip translation as an estimate of translation quality and proposed a novel use of sentence embeddings to measure the similarity between source and round-trip translated sentences. The aim of our approach was to use the semantic information retained through round-trip translation as a proxy for translation quality. Experimentation shows increased performance of sentence embedding based similarity measures over traditional lexical metrics. We also highlight current pitfalls of the round-trip translation approach and suggest directions for future research on this topic.",1
"ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation ÃÂ· Machine Translation ÃÂ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) systemÃ¢ÂÂs translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE Ã¢ÂÂis concerned about predicting the quality of a systemÃ¢ÂÂs output for a given input, without any information about the expected outputÃ¢ÂÂ [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as Ã¢ÂÂgood for nothing [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in OÃ¢ÂÂConnell [2001] which assert that round-trip translation relies heavily on a modeltranslating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentence back into the original language, generating sentence embedding vectors for both the original and round-trip translated sentences, and comparing the two vectors to create this reference. However, some researchers have attempted to address the previous pitfalls found with the approach and have attempted to reconcile the two vectors using a system of vector machines. One downfall of this approach is that it requires a lot more data and takes far longer to train. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation ÃÂ· Machine Translation ÃÂ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) systemÃ¢ÂÂs translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE Ã¢ÂÂis concerned about predicting the quality of a systemÃ¢ÂÂs output for a given input, without any information about the expected outputÃ¢ÂÂ [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. ",0
"Abstract We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices. INTRODUCTION In this paper, we describe and analyze an as far as we know theoretically un-examined kind of bias, which we call interpretive blindness(IB). IB is exemplified by humans (and perhaps soon by sophisticated machine learning algorithms) whose beliefs are guided and shaped by testimony. When learning through testimony—perhaps the primary way that most people acquire information nowadays—an agent acquires beliefs through conversations with other agents, or from books, newspapers or social networks, and so on. Typically, such people lack direct access to the phenomena described via that testimony. Typically too, humans only pay attention to a restricted set of bodies of testimony from a limited number of sources for their information—which makes sense in terms of an agent’s limited resources and attention span. Our paper is about the strategic consequences of opinion diffusion through testimony and the distortions on learning and information that can result. IB results from this restriction to few sources of testimony and a natural co-dependence between beliefs and interpretation (Asher and Paul, 2018). Relying on testimony T from a restricted set of sources to update one’s beliefs leads to the mutual reinforcement of our confidence in the source and our belief in T; this creates a bias that can preclude learning when an agent tries to exploit new data that are incompatible with or simply distinct from T. Agents who are interpretively blind will discount any evidence that challenges their beliefs. We use Wolpert’s 2018 extended Bayesian framework to prove our results. While IB is problematic for a standard Bayesian framework, it also poses problems for hierarchical Bayesian approaches (Gelman et al., 2013), because testimony from sources on social media like Facebook, 24/7 media outlets and web interest groups is often argumentatively complete, a notion we analyze precisely in Section 4; in an argumentatively complete body of testimony T, the authors of that testimony can respond to and argue with any doubts raised by other data or arguments in a body T ′ that might threaten T’s credibility. A skillful climate denier, for example, will always find a way to undercut the most scientifically careful argument. Argumentatively complete testimony thus can undermine higher order constraints and good epistemic practices that should guide first order learning. Our paper starts in Section ?? by discussing testimony. We then introduce the codependence of belief and interpretation and apply it to the situation of testimony and the sources that support it. In Section 3 we formally show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings.  Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and Capp´e (2011). It is also related to work on generalization in machine learning. Epistemic biases affect generalization and learning capacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016) show that standard techniques in machine learning for promoting good epistemic biases and generalization—training error minimization, regularization techniques like weight decay or dropout, or complexity measures used to minimize generalization error (the difference between training error and test error)—do not necessarily lead to good generalization and test performance. Argumentatively complete testimony T incorporates an adversarial attack mechanism against any good epistemic practices that might discount T. It’s this mechanism that guarantees IB. The argumentation literature (Amgoud and Demolombe, 2014; Dung, 1995) is also relevant to IB. If testimony T is argumentatively complete, then T always provides a counterargument to an attack against T–much like an acceptable argument in Dung (1995). In addition, however, an argumentatively complete T also supports higher order evaluation hypotheses that support hypotheses that support T. There are also important connections to the literature on trust (Castelfranchi and Falcone, 2010); in our set up learning agents trust certain sources over others, and our higher order setting invokes a hierarchy of reasons. Nevertheless, the argumentation and trust-based work of which we are aware is complementary to our approach. An argumentation framework takes a possibly inconsistent belief base and imposes a static constraint on inference in such a setting. Similarly, trust is typically modeled in some sort of static modal framework. By contrast, ME learning games and the whole Bayesian framework are dynamic, with beliefs evolving under evidence and game strategies evolving under agent interaction. It is this dynamic evolution that is crucial to our approach and, we think, to modeling agents and learning. In sum, we are not looking at the problem of consistency, but rather the problems of entrenchment and bias. Conclusions Interpretive blindness results from a dynamic, iterative process whereby a learner’s background beliefs and biases lead her to update her beliefs based on a body of testimony T, and then biases inherent in T come back to reinforce her beliefs and her trust in T’s source(s), further biasing her towards these sources for future updates. We have introduced and formally characterized IB. We have shown that IB can prevent learning even in higher order Bayesian frameworks for learning from argumentatively complete testimony, despite the presence of constraints designed to promote good epistemic practices. We also shown that IB is computationally complex as a co-r.e. set via a game theoretic analysis, and that an agent may rationally remain in IB in the face of epistemic arguments. Our game theoretic analysis can also be extended to cases where the agent falls out of IB but then is a recidivist and becomse a prisoner once more. We leave that for future work. Investigating IB alas is not just an academic enterprise. IB really does happen, with sometimes tragic or dangerous results. We think a careful formal analysis is urgent for society. Finally, we note that while we have focused on IB as a problem for learning from testimony, the problem it raises for learning extends to any case in which we do not have unmediated access to ground truth and our data is “theory laden” Hanson (1958).",1
"Abstract We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices. INTRODUCTION In this paper, we describe and analyze an as far as we know theoretically un-examined kind of bias, which we call interpretive blindness(IB). IB is exemplified by humans (and perhaps soon by sophisticated machine learning algorithms) whose beliefs are guided and shaped by testimony. When learning through testimonyÃ¢ÂÂperhaps the primary way that most people acquire information nowadaysÃ¢ÂÂan agent acquires beliefs through conversations with other agents, or from books, newspapers or social networks, and so on. Typically, such people lack direct access to the phenomena described via that testimony. Typically too, humans only pay attention to a restricted set of bodies of testimony from a limited number of sources for their informationÃ¢ÂÂwhich makes sense in terms of an agentÃ¢ÂÂs limited resources and attention span. Our paper is about the strategic consequences of opinion diffusion through testimony and the distortions on learning and information that can result. IB results from this restriction to few sources of testimony and a natural co-dependence between beliefs and interpretation (Asher and Paul, 2018). Relying on testimony T from a restricted set of sources to update oneÃ¢ÂÂs beliefs leads to the mutual reinforcement of our confidence in the source and our belief in T; this creates a bias that can preclude learning when an agent tries to exploit new data that are incompatible with or simply distinct from T. Agents who are interpretively blind will discount any evidence that challenges their beliefs. We use WolpertÃ¢ÂÂs 2018 extended Bayesian framework to prove our results. While IB is problematic for a standard Bayesian framework, it also poses problems for hierarchical Bayesian approaches (Gelman et al., 2013), because testimony from sources on social media like Facebook, 24/7 media outlets and web interest groups is often argumentatively complete, a notion we analyze precisely in Section 4; in an argumentatively complete body of testimony T, the authors of that testimony can respond to and argue with any doubts raised by other data or arguments in a body T Ã¢ÂÂ² that might threaten TÃ¢ÂÂs credibility. A skillful climate denier, for example, will always find a way to undercut the most scientifically careful argument. Argumentatively complete testimony thus can undermine higher order constraints and good epistemic practices that should guide first order learning. Our paper starts in Section ?? by discussing testimony. We then introduce the codependence of belief and interpretation and apply it to the situation of testimony and the sources that support it. In Section 3 we formally show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings. Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and CappÃÂ´e (2011). It is also related to work on generalization in machine learning. Epistemic biases affect generalization andcapacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016) show that standard techniques in machine learning for promoting good epistemic biases and generalizationÃÂ¢ÃÂÃÂtraining error minimization, regularization techniques like weight decay or dropout, or complexity measures used to minimize bias (the difference between training error and test error, which can preclude learning when an agent tries to exploit new data that are incompatible with or simply distinct from T. Agents who are interpretively blind will discount any evidence that challenges their beliefs. We use WolpertÃ¢ÂÂs 2018 extended Bayesian framework to prove our results. While IB is problematic for a standard Bayesian framework, it also poses problems for hierarchical Bayesian approaches (Gelman et al., 2013), because testimony from sources on social media like Facebook, 24/7 media outlets and web interest groups is often argumentatively complete, a notion we analyze precisely in Section 4; in an argumentatively complete body of testimony T, the authors of that testimony can respond to and argue with any doubts raised by other data or arguments in a body T Ã¢ÂÂ² that might threaten TÃ¢ÂÂs credibility. A skillful climate denier, for example, will always find a way to undercut the most scientifically careful argument. Argumentatively complete testimony thus can undermine higher order constraints and good epistemic practices that should guide first order learning. Our paper starts in Section ?? by discussing testimony. We then introduce the codependence of belief and interpretation and apply it to the situation of testimony and the sources that support it. In Section 3 we formally show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings. Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and CappÃÂ´e (2011).",0
"Abstract Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on trans- fer learning to broaden its impact. Bench- marks are dominated by a small set of fre- quent phenomena, leaving a long tail of in- frequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently ad- dressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualita- tive meta-analysis of 100 representative pa- pers on transfer learning for NLU. Our anal- ysis asks three questions: (i) Which long tail dimensions do transfer learning studies tar- get? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction “There is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora.” (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to Zipf’s law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented. For example, since most popu- lar coreference benchmarks focus on English nar- ratives, they do not contain many instances of zero anaphora, a phenomenon quite common in other languages (e.g., Japanese, Chinese). In such sit- uations, model performance on benchmark cor- pora may not be truly reflective of expected perfor- mance on micro-level long tail phenomena, raising questions about the ability of state-of-the-art mod- els to generalize to the long tail. Most benchmarks do not explicitly catalogue the list of micro-level language phenomena that are included or excluded in the sample, which makes it non-trivial to construct a list of long tail micro-level language phenomena. Hence, we formalize an alternate conceptualization of the long tail: undersampled portions of the macro- level space that can be treated as proxies for long tail micro-level phenomena. These undersampled long tail macro-level dimensions highlight gaps and present potential new challenging directions for the field. Therefore, periodically taking stock of research to identify long tail macro-level di- mensions can help in highlighting opportunities for progress that have not yet been tackled. This idea has been gaining prominence recently; for ex- ample, Joshi et al. (2020) survey languages stud- ied by NLP papers, providing statistical support for the existence of a macro-level long tail of low- resource languages. In this work, our goal is to attempt to charac- terize the macro-level long tail in NLU and ef- forts that have tried to address it from research on transfer learning. Large benchmarks have driven much of the recent methodological progress on NLU (Bowman et al., 2015; Rajpurkar et al., 2016; McCann et al., 2018; Talmor et al., 2019; Wang et al., 2019c,b), but the generalization abilities of benchmark-trained models to the long tail have been unclear. In tandem, the NLP community has been successfully developing transfer learn- ing methods to improve generalization of models trained on NLU benchmarks (Ruder et al., 2019). The goal of transfer learning research is to tackle the macro-level long tail in NLU, leading to the question: how far has transfer learning addressed performance of benchmark models on the NLU long tail, and where do we still fall behind? Probing further, we perform a qualitative meta- analysis of a representative sample of 100 pa- pers on domain adaptation and transfer learning in NLU. We sample these papers based on citation counts and publication venues (§2.1), and docu- ment 7 facets for each paper such as tasks and do- mains studied, adaptation settings evaluated, etc. (§2.2). Adaptation methods proposed (or applied) are documented using a hierarchical categoriza- tion described in §2.3, which we develop by ex- tending the hierarchy from Ramponi and Plank (2020). With this information, our analysis fo- cuses on three questions: Q1: What long tail macro-level dimensions do transfer learning studies target? Here di- mensions include tasks, domains, languages and adaptation settings covered in transfer learning research. Q2: Which properties help adaptation meth- ods improve performance on long tail dimen- sions? Q3: Which methodological gaps have great- est negative impact on long tail performance? The rest of the paper presents thorough answers to these questions, laying out avenues for future research on transfer learning that more effectively address the macro-level long tail in NLU. We also present a case study to demonstrate that our meta- analysis framework can be use to systematically design and conduct experiments that provide in- sights that enable us to make progress along these avenues. Conclusion This work presents a qualitative meta-analysis of 100 representative papers on domain adaptation and transfer learning in NLU, with the aim of understanding performance of adaptation methods on the long tail. Through this analysis, we assess current trends and highlight methodological gaps that we consider to be major avenues for future re- search in transfer learning for the long tail. We ob- serve that current research has a tendency to side- line certain types of tasks, languages, domains, and adaptation settings, indicating that long tail coverage is far from comprehensive. We also iden- tify two properties that help long tail performance: (i) incorporating source-target domain distance, and (ii) incorporating a nuanced view of domain variation. Additionally, we identify three major gaps that must be addressed to improve long tail performance: (i) combining adaptation methods, (ii) incorporating extra-linguistic knowledge and (iii) application to data-scarce adaptation settings. Finally, we demonstrate the utility of our meta- analysis framework and observations in guiding the design of systematic meta-experiments to ad- dress prevailing open questions by conducting a systematic evaluation of popular adaptation meth- ods for high-expertise domains in a data-scarce setting. This case study reveals interesting insights about the adaptation methods evaluated and shows that significant progress can be made towards de- veloping a better understanding of adaptation for the long tail by conducting such experiments.",1
"Abstract Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on trans- fer learning to broaden its impact. Bench- marks are dominated by a small set of fre- quent phenomena, leaving a long tail of in- frequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently ad- dressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualita- tive meta-analysis of 100 representative pa- pers on transfer learning for NLU. Our anal- ysis asks three questions: (i) Which long tail dimensions do transfer learning studies tar- get? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction Ã¢ÂÂThere is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora.Ã¢ÂÂ (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to ZipfÃ¢ÂÂs law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented. For example, since most popu- lar coreference benchmarks focus on English nar- ratives, they do not contain many instances of zero anaphora, a phenomenon quite common in other languages (e.g., Japanese, Chinese). In such sit- uations, model performance on benchmark cor- pora may not be truly reflective of expected perfor- mance on micro-level long tail phenomena, raising questions about the ability of state-of-the-art mod- els to generalize to the long tail. Most benchmarks do not explicitly catalogue the list of micro-level language phenomena that are included or excluded in the sample, which makes it non-trivial to construct a list of long tail micro-level language phenomena. Hence, we formalize an alternate conceptualization of the long tail: undersampled portions of the macro- level space that can be treated as proxies for long tail micro-level phenomena. These undersampled long tail macro-level dimensions highlight gaps and present potential new challenging directions for the field. Therefore, periodically taking stock of research to identify long tail macro-level di- mensions can help in highlighting opportunities for progress that have not yet been tackled. This idea has been gaining prominence recently; for ex- ample, Joshi et al. (2020) survey languages stud- ied by NLP papers, providing statistical support for the existence of a macro-level long tail of low- resource languages. In this work, our goal is to attempt to charac- terize the macro-level long tail in NLU and ef- forts that have tried to address it from research on transfer learning. Large benchmarks have driven much of the recent methodological progress on NLU (Bowman et al., 2015; Rajpurkar et al., 2016; McCann et al., 2018; Talmor et al., 2019; Wang et al., 2019c,b), but the generalization abilities of benchmark-trained modelsthe long tail have been unclear. In tandem, the NLP community has been successfully developing transfer learn- ing methods to improve generalization of models trained on NLU benchmarks (Ruder et al., 2019). The goal of transfer learning research is to tackle the macro-level long tail in NLU, leading to the question: how far has transfer learning addressed performance of benchmark-trained models on the macro-level long tail, and where do we still fall behind? Probing further, we perform a qualitative meta- analysis of a representative sample of 100 pa- pers on domain adaptation and transfer learning for NLU. We sample these papers based on citation counts and publication venues (ÃÂ§2.1), and docu- ment 7 facets for each paper such as tasks and do- mains studied, adaptation settings evaluated, etc. (ÃÂ§2.2). Adaptation methods proposed (or applied) are documented using a hierarchical categoriza- tion described in ÃÂ§2.3, which we develop by ex- tending the hierarchy from Ramponi and Plank (2020). With this information, our analysis fo- cuses on three questions: Q1: What long tail dimensions do transfer learning studies target? Here di- mensions include tasks, domains, languages and adaptation settings covered in transfer learning research. Q2: Which properties help adaptation meth- ods improve performance on the long tail? Q3: Which methodological gaps have great- est negative impact on long tail performance? The rest of the paper presents thorough answers to these questions, laying out avenues for future research on transfer learning that more effectively address the long tail. We also present a case study to demonstrate that our meta- analysis framework can be use to systematically design and conduct experiments that provide in- sights that enable us to make progress along these avenues. Conclusion This work presents a qualitative meta-analysis of 100 representative papers on domain adaptation and transfer learning for NLU, with the aim of understanding performance of adaptation methods on the long tail. Through this analysis, we assess current trends and highlight methodological gaps that we consider to be major avenues for future re- search in transfer learning for the long tail. We ob- serve that current research has a tendency to side- line certain types of tasks, languages, domains, and adaptation settings, indicating that long tail coverage is far from comprehensive. We also iden- tify two properties that help long tail performance: (i) incorporating source-target domain distance, and (ii) incorporating a nuanced view of domain variation. Additionally, we identify three major gaps that must be addressed to improve long tail performance: (i) combining adaptation methods, (ii) incorporating extra-linguistic knowledge and (iii) application to data-scarce adaptation settings. Finally, we demonstrate the utility of our meta- analysis framework and observations in guiding the design of systematic meta-experiments to ad- dress prevailing open questions by conducting a systematic evaluation of popular adaptation meth- ods for high-expertise domains in a data-scarce setting. This case study reveals interesting insights about the adaptation methods evaluated and shows that significant progress can be made towards de- veloping a better understanding of adaptation for the long tail by conducting such experiments.",0
"Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): ● Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. ● Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1​ ​: ●  DSTC6 human evaluation data​ (Hori et al., 2017) ●  DSTC7 human evaluation data​ (Galley et al., 2019) ●  Persona-Chatlog dataset​ (See et al., 2019) ●  USR dataset​ (Mehri & Eskenazi, 2020) ●  FED dataset​ (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without even realizing of that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of generative approaches to repeat words from the user utterances or dialogue history. In this task, our goal is to evaluate the capability of generative dialogue systems to generate appropriate answers that can go beyond detecting toxicity and moderate the conversation by producing appropriate and correct answers that allow the system to continue with the dialogue. For this task a dataset of pairs of 100K messages (training and validation set) will be automatically collected with the following characteristics: ●  A toxic user sends a Tweet message using one or several of the most common swear words found on the Internet. The Tweet message must be directed to one of the customer service channels. ●  A toxic user writes a Tweet message using one or several swear words and the message is replied by another user. ●  A toxic user posts a message in Reddit using one or several swear words and the message is replied by another user. In all cases, for a pair to be considered as part of the dataset, the replied message must not contain any swear word or being classified as toxic or inappropriate. In addition, since many platforms forbid the usage of swear words we will be considering different variants of the swear words by using obfuscation techniques (Rojas-Galeano, 2017). Scripts will be provided to the participants to collect desired tweet training/validation splits based on the tweet IDs. A hidden test set will be reserved to evaluate the performance of the submitted systems. The desired system should generate responses that are polite, specific and semantically appropriate. Participants will be evaluated based on the objective similarity between the generated response and the original response (e.g. sentence embedding similarity, Deep AM-FM (Zhang et al., 2020), BLEU, ROUGE, etc). For the top-3 submission systems, a set of 100 responses will be manually evaluated allowing a subjective analysis of the best system and analysis of correlations between the human evaluations and the automatic scores.",1
"Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Ã¢ÂÂ Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Ã¢ÂÂ Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1Ã¢ÂÂ Ã¢ÂÂ: Ã¢ÂÂ DSTC6 human evaluation dataÃ¢ÂÂ (Hori et al., 2017) Ã¢ÂÂ DSTC7 human evaluation dataÃ¢ÂÂ (Galley et al., 2019) Ã¢ÂÂ Persona-Chatlog datasetÃ¢ÂÂ (See et al., 2019) Ã¢ÂÂ USR datasetÃ¢ÂÂ (Mehri & Eskenazi, 2020) Ã¢ÂÂ FED datasetÃ¢ÂÂ (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without evenof that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of chatbots to repeat words from the user utterances or dialogue history. In this project we are attempting to provide safe responses to toxic comments, using standard safe utterance detection and safe chatbot reply phrases generation functions. Though these functions help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Ã¢ÂÂ Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Ã¢ÂÂ Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. ",0
"Abstract Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. Introduction Tremendous effort has gone into the research and development of task-oriented dialogue agents for English and a few other major languages in recent years. A methodology that can transfer the effort to other languages automatically will greatly benefit the large population of speakers of the many other languages in the world. Underlying an effective TOD agent is dialogue state tracking, the task of predicting a formal representation of the conversation sufficient for the dialogue agent to reply, in the form of slots and values. However, DST currently remains restricted to a few popular languages (Razumovskaia et al., 2021). Traditional DST agents require large handannotated Wizard-of-Oz (Kelley, 1984) datasets for training, which are prohibitively labor-intensive to produce in most languages (Gunasekara et al., 2020). Large, multi-domain WOZ datasets are only available in English and Chinese (Quan et al., 2020; Ye et al., 2021a). The contributions of this paper are as follows: 1. We propose an automatic technique to build high-quality multilingual data sets using machine translation. Machine translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too. 2. We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 3. We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA on RiSAWOZ (Quan et al., 2020) and CrossWOZ (Zhu et al., 2020), two large-scale multi-domain WoZ dialogue datasets, by 10.7% and 17% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective on translated data as evident by bigger performance improvement: on RiSAWOZ-EN-auto and RiSAWOZ-DE-auto datasets, automatically translated versions of RiSAWOZ, BART-CSP improves SOTA by 32.4% and 52.5%. Related Work 2.1 Dialogue State Tracking Dialogue state tracking (DST) refers to the task of predicting a formal state of a dialogue at its current turn, as a set of slot-value pairs at every turn. State-of-the-art approaches apply large transformer networks (Peng et al., 2020; Hosseini-Asl et al., 2020) to encode the full dialogue history in order to predict slot values. Other approaches include question-answering models (Gao et al., 2019), ontology matching in the finite case (Lee et al., 2019), or pointer-generator networks (Wu et al., 2019). Both zero-shot cross-lingual DST transfer (Ponti et al., 2018; Chen et al., 2018) and multilingual knowledge distillation (Hinton et al., 2015; Tan et al., 2019) have been investigated; however, training with translated data is the dominant approach, outperforming zero-shot and few-shot methods. 2.2 Contextual Semantic Parsing Alternatively to encoding the full dialogue history, previous work has proposed including the state as context (Lei et al., 2018; Heck et al., 2020; Ye et al., 2021b) together with the last agent and user utterance. Recently, Cheng et al. (2020b) proposed replacing the agent utterance with a formal representation as well. Existing models rely on custom encoder architectures and loss functions for the state (Heck et al., 2020). Our formulation of CSP is different since we encode the formal dialogue state directly as text, which simplifies the architecture and makes better use of the pretrained model’s understanding of natural text. Previous work also applied rule-based state trackers that compute the state based on the agent and user dialogue acts (Schulz et al., 2017; Zhong et al., 2018; Zhu et al., 2020). Such techniques cannot handle state changes outside of a state machine defined ahead of time and do not achieve state-of-the-art accuracy on WOZ dialogues. 2.3 Multilingual Dialogues Several multilingual dialogue benchmarks have been created over the past few years. Dialogue State Tracking Challenge (DSTC) has released several datasets (Kim et al., 2016; Hori et al., 2019; Gunasekara et al., 2020), covering only a few domains and languages. CrossWOZ (Zhu et al., 2020) and RiSAWOZ (Quan et al., 2020) are Chinese datasets collected through crowdsourcing which is expensive and human errors degrade quality. The creation of affordable high-quality datasets for other languages still remains a challenge (Razumovskaia et al., 2021). Conclusion Given a dialogue dataset in one language, this paper shows how to build contextual semantic parsers for a new language using automatically machinetranslated data. We propose an improved alignment approach for dialogues to ensure faithful translation of slot values. This removes the need for costly human-post editing used in all previous benchmarks. We show that the compounding effects of translation noise across turns can be mitigated with a CSP model for dialogue state tracking. By leveraging pretrained seq2seq models such as BART, training with CSP can outperform state-of-the-art results on RiSAWOZ, CrossWOZ, and MultiWOZ-ZH while remaining competitive on MultiWOZ despite not encoding any previous conversation turns or having access to a predefined ontology. Using our methodology, we create RiSAWOZ English and German, the first automatically created high-quality translated datasets for dialogue state tracking with no human-in-the-loop. We have implemented our methodology as a toolkit which developers can use to create a new multilingual dialogue datasets and contextual semantic parsers for it. Our new datasets and software will be released open-source upon publication.",1
"Abstract Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. Introduction Tremendous effort has gone into the research and development of task-oriented dialogue agents for English and a few other major languages in recent years. A methodology that can transfer the effort to other languages automatically will greatly benefit the large population of speakers of the many other languages in the world. Underlying an effective TOD agent is dialogue state tracking, the task of predicting a formal representation of the conversation sufficient for the dialogue agent to reply, in the form of slots and values. However, DST currently remains restricted to a few popular languages (Razumovskaia et al., 2021). Traditional DST agents require large handannotated Wizard-of-Oz (Kelley, 1984) datasets for training, which are prohibitively labor-intensive to produce in most languages (Gunasekara et al., 2020). Large, multi-domain WOZ datasets are only available in English and Chinese (Quan et al., 2020; Ye et al., 2021a). The contributions of this paper are as follows: 1. We propose an automatic technique to build high-quality multilingual data sets using machine translation. Machine translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too. 2. We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 3. We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA on RiSAWOZ (Quan et al., 2020) and CrossWOZ (Zhu et al., 2020), two large-scale multi-domain WoZ dialogue datasets, by 10.7% and 17% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective on translated data as evident by bigger performance improvement: onand RiSAWOZ-DE-auto datasets, automatically translated versions of RiSAWOZ, BART-CSP improves SOTA by 32.4% and 52.5%. Related Work 2.1 Dialogue State Tracking Dialogue state tracking (DST) refers to the task of predicting a formal state of a dialogue at its current turn, as a set of slot-value pairs at every turn. State-of-the-art approaches apply large transformer networks (Peng et al., 2020; Hosseini-Asl et al., 2020) to encode the full dialogue history in order to predict slot values. Other approaches include question-answering models (Gao et al., 2019), ontology matching in the finite case (Lee et al., 2019), or pointer-generator networks (Wu et al., 2019). Both zero-shot cross-lingual DST transfer (Ponti et al., 2018; Chen et al., 2018) and multilingual knowledge distillation (Hinton et al., 2015; Tan et al., 2019) have been investigated; however, training with translated data is the dominant approach, outperforming zero-shot and few-shot methods. 2.2 Contextual Semantic Parsing Alternatively to encoding the full dialogue history, previous work has proposed including the state as context (Lei et al., 2018; Heck et al., 2020; Ye et al., 2021b) together with the last agent and user utterances. Recently, Cheng et al. (2020b) proposed replacing the agent utterance with a formal representation as well. Existing models rely on custom encoder architectures and loss functions for the state (Heck et al., 2020). Our formulation of contextual semantic parsing (Cheng et al., 2020b) is different since we encode the formal dialogue state directly as text, which simplifies the architecture and makes better use of the pretrained modelÃ¢ÂÂs understanding of natural text. Previous work also applied rule-based state trackers that compute the state based on the agent and user dialogue acts (Schulz et al., 2017; Zhong et al., 2018; Zhu et al., 2019). Such techniques cannot handle state changes outside of a state machine defined ahead of time and do not achieve state-of-the-art accuracy on the RiSAWOZ benchmark. 2.3 Multilingual Dialogues Several multilingual dialogue benchmarks have been created over the past few years. Dialogue State Tracking Challenge (DSTC) has released several datasets (Kim et al., 2016; Hori et al., 2019; Gunasekara et al., 2020), covering only a few domains and languages. Cross-lingual DST Distillation (Ponti et al., 2018; Heck et al., 2020) and MultiWOZ (Hinton et al., 2015; Tan et al., 2019) have been named for their accuracy; however, no other approach is named for one language. 3. Domain-Level Translation (Gao et al., 2019) and MultiWOZ (Hinton et al., 2015; Tan et al., 2019) Tropes and DSTs are automatically assigned to slot values using machine translation. Domain-level translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too.",0
"Abstract Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine transla- tion systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of gram- matical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to uti- lize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmen- tation method, our model achieves strong per- formances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results. Introduction Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sen- tences. Due to the growing number of language learners of English, there has been increasing atten- tion to the English GEC in the past few years. Considering the outstanding performance of neu- ral network models in machine translation tasks, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhao et al., 2019; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by Velicˇkovic ́ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model. Exper- iments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Among models without using the large pre-trained models, our model achieves the best F-score on all benchmarks. Comparing with models which incorporate the large pre-trained models, our model achieves very competitive per- formance as well. In general, our model achieves strong performance without using any large pre- trained models. Our contributions are summarized as follows: 1. To the best of our knowledge, we introduce syntactic knowledge into neural GEC model for the first time, by applying graph attention mechanism to utilize the dependency tree. 2. We propose a dependency tree correction task to deal with the problem that the dependency trees of grammatically incorrect source sentences might provide incorrect syntactic knowledge. 3. Without using any large pre-trained model, our SG-GEC model achieves strong performances on public GEC benchmarks. Related Work Early published works in GEC developed models based on manually designed grammar rules (Mu- rata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). After Han et al. (2006) pointed out the limitation of rule-based method, some researchers turned their attention to the statistical machine learning method (Knight and Chander, 1994; Min- nen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed various neural network models to solve GEC task. Some regarded the GEC task as a translation problem and applied cutting-edge neural machine translation model to deal with it (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018). Many recent works (Junczys-Dowmunt et al., 2018; Zhao et al., 2019) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Considering the tremendous performance of pre-trained methods, pre-trained language model, such as BERT(Devlin et al., 2019),RoBERTa(Liu et al., 2019) and XLNet(Yang et al., 2019), have been adopted in GEC models(Kaneko et al., 2020; Omelianchuk et al., 2020a). A challenge in applying neural machine transla- tion models to GEC task is the requirement of the large training data. Given that, many works incor- porated data augmentation methods to address this problem. Many works adopted pre-defined rules to generate synthetic samples with grammatical er- rors. (Grundkiewicz et al., 2019; Lichtarge et al., 2018; Choe et al., 2019). Kiyono et al. (2019) fur- ther studied the data augmentation methods and showed the efficacy of back-translation procedure. Recently, dependency parsing has been further developed with neural network(Dozat and Man- ning, 2016; Li et al., 2018). Benefiting from it, models could receive syntactic knowledge with higher accuracy. Many works showed the po- tential of using syntactic knowledge in various tasks(Zhang et al., 2020; Wang et al., 2020; Jin et al., 2020). Inspired by previous works, we pro- posed the SG-GEC model to utilize the syntactic knowledge within dependency trees. Conclusion In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to utilize the syntactic knowledge of dependency tree. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmentation method, our model achieves strong performances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results.",1
"Abstract Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine transla- tion systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of gram- matical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to uti- lize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmen- tation method, our model achieves strong per- formances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results. Introduction Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sen- tences. Due to the growing number of language learners of English, there has been increasing atten- tion to the English GEC in the past few years. Considering the outstanding performance of neu- ral network models in machine translation tasks, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhao et al., 2019; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by VelicÃÂkovic ÃÂ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model. Exper- iments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Among models without using the large pre-trained models, our model achieves the best F-score on all benchmarks. Comparing with models which incorporate the large pre-trained models, our model achieves very competitive per- formance as well. In general, our model achieves strong performance without using any large pre- trained models. Our contributions are summarized as follows: 1. To the best of our knowledge, we introduce syntactic knowledge into neural GEC model for the first time, by applying graph attention mechanism to utilize the dependency tree. 2. We propose a dependency tree correction task to deal with the problem that the dependency trees of grammatically incorrect source sentences might provide incorrect syntactic knowledge. 3. Without using any large pre-trained model, our SG-GEC model achieves strong performances on public GEC benchmarks. Related Work Early published works in GEC developed models based on manually designed grammar rules (Mu- rata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). After Han et al. (2006) pointed out the limitation of rule-based method, some researchers turned their attention to the statistical machine learning method (Knight and Chander, 1994; Min- nen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed various neural network models to solve GEC task. Some regarded the GEC task as aproblem and applied cutting-edge neural machine translation model to deal with it (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018). Many recent works (Junczys-Dowmunt et al., 2018; Zhao et al., 2019) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Considering the tremendous performance of pre-trained models, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhang et al., 2020; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by VelicÃÂkovic ÃÂ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model.",0
"ABSTRACT Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for the NER tasks to minimize the annotation cost without sacrificing model performance. However, heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose AL sentence query evaluation functions which pay more attention to possible positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize too long or too short sentences. Our experiments on three datasets from different domains reveal that the proposed approaches reduce the number of annotated tokens while achieving better or comparable prediction performance with conventional methods. Keywords Active learning, Named entity recognition, Annotation cost, Semi-supervised clustering Introduction Name entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes (e.g., person names, organizations, locations). NER is one of the fundamental natural language processing (NLP) tasks and is used in other NLP tasks such as entity linking, event extraction, and question answering. Although deep learning-based pre-trained language models [Devlin et al., 2019, Yang et al., 2019, Liu et al., 2019, Raffel et al., 2020] have advanced the state-of-the-art performance in NER [Torfi et al., 2020, Peters et al., 2019], a sufficient amount of labeled data is still necessary for achieving satisfactory prediction performance in most domains [Tikhomirov et al., 2020, Wang et al., 2020]. Since acquiring labeled data is both time and budget-consuming, efficient label acquisition for NER remains a challenge. A general framework for tackling the labeled data acquisition problem is active learning, in which the learner strategically chooses the most valuable instances as opposed to selecting a random sample for labeling [Thompson et al., 1999]. In the pool-based active learning setup, the active learner selects the most useful examples from an unlabeled pool of samples, queries them to an annotator for labeling; upon receiving the labels for the queried examples, the model is retrained with the augmented labeled set. These query selection, annotation, and retraining steps are iterated multiple times until either the desired performance is achieved or the labeling budget is exhausted [Settles, 2011]. The goal is to reduce the annotation cost by creating a smaller labeled set while still achieving good predictive performance. Active learning (AL) has demonstrated success in various sequence annotation tasks such as part-of-speech tagging [Ringger et al., 2007], dependency parsing [Li et al., 2016b] and semantic parsing [Thompson et al., 1999]. AL has been used to tackle the label acquisition problem in the NER task as well. Shen et al. [2017] demonstrated that AL combined with deep learning achieves nearly the same performance on standard datasets with just 25% of the original training data. Chen et al. [2015] developed and evaluated both existing and new AL methods for a clinical NER task. Their results showed that AL methods, particularly uncertainty-sampling approaches, provide significant savings in the annotation cost. In active learning, the most critical step is selecting the useful query examples for manual annotation. This step becomes more challenging for sequence labeling tasks, especially for named entity recognition, for two reasons. The first challenge of applying active learning to NER arises due to imbalanced data distribution. In NER annotation, a token is either labeled with its corresponding named entity class if it is part of a named entity or with the “other” class if it is not part of a named entity. The other class is generally referred as negative annotation or negative token, and all other labels -named entity labels- are referred as positive annotations or positive tokens [Marcheggiani and Artières, 2014]. In NER datasets, negative tokens are usually over-abundant compared to positive tokens. The second challenge in applying active learning to NER is related to the varying length of sentences. In NER, tokens are annotated one by one, but the context, hence the corresponding sentence, is still required for accurate token annotation. Therefore, at each active learning iteration, sentences are queried instead of tokens. Active learners that select the informative sentences for querying by directly aggregating over all the tokens are biased towards longer sentences. In order to prevent this bias towards sentences with more terms, the aggregated sentence scores are normalized by the number of tokens in the sentence [Engelson and Dagan, 1996, Haertel et al., 2008, Settles and Craven, 2008]. This commonly used approach solves the problem only partially, since this time, the active learner starts to query ""too"" short sentences in the early, and intermediate rounds [Tomanek, 2010]. In this paper, we moderate these two extreme cases and propose a normalization which exploits the corresponding datasets’ token count distribution. The varying length of sentences also affects the cost evaluation of the active learning framework. Some studies [Settles and Craven, 2008, Yao et al., 2009, Kim et al., 2006, Liu et al., 2020a] treat all sentences equally and compare active learning methods directly with respect to the number of sentences queried. However, this is not realistic since the cost is not fixed across sentences as sentences differ in the number of tokens and the number of named entities they contain [Arora et al., 2009, Haertel et al., 2008]. Therefore, the number of annotated tokens should be incorporated into the active learning cost. In this regard, many studies in the literature [Shen et al., 2004, Settles and Craven, 2008, Reichart et al., 2008, Shen et al., 2017] measure the cost of the annotation by the number of tokens annotated even though they query the sentences. Using only the token count is also an imperfect strategy as the cost of annotating the same number of tokens distributed over multiple sentences is not equivalent to annotating these tokens within a single sentence [Settles et al., 2008, Tomanek and Hahn, 2010]. This is mainly because there is a cost factor associated with each new sentence that is independent of its content and length. Even though we do not propose a new cost calculation method that encompasses all these different aspects, we consider these two cost evaluation set-ups to analyze the existing and proposed approaches in detail. In this study, we propose an extension to the subset of the existing uncertainty sampling methods to handle the challenges associated with the over-abundance of the negative tokens. In our proposed approach, the query evaluation metrics are designed to pay less attention to the tokens that are predicted to have negative annotations. We identify potentially negative tokens through clustering of pre-trained BERT representations after a semi-supervised dimensionality reduction. To the best of our knowledge, the use of the BERT embeddings directly in the active learning querying step for NER has never been attempted before this paper. Last but not least, this paper proposes a better normalization strategy for aggregating token scores to attain a good sentence query metric. For a fair comparison, we evaluate different active learning query methods both under the assumption of fixed annotation cost per sentence and fixed annotation cost per token. Our experiments on three datasets from different domains illustrate that our proposed approach reduces the number of annotated tokens while maintaining the slightly better or same level of prediction performance with the compared methods. We also present an extensive investigation about the effects of different pre-trained language embeddings on the performance of our NER model. The rest of the paper is organized as follows: Section 2 presents the NER data collections used in the experiments together with additional details to motivate the reader for the proposed method, Section 3 summarizes the general active learning setup and the commonly used active learning strategies for the NER task, and Section 4 describes the proposed approach. We describe our experimental setting in Section 5 and detail our results in Section 6. Section 7 concludes with a summary of our findings. Conclusion In this work, we focus on active learning for NER. One challenge of applying NER in active learning is the abundance of negative tokens. Uncertainty-based sentence query functions aggregate the scores of the tokens in a sentence, and since the negative tokens’ uncertainty scores dominate the overall score, they shadow the informative, positive tokens. In this work, we propose strategies to overcome this by focusing on the possible positive tokens. To identify positive tokens, we use a semi-supervised clustering strategy of tokens’ BERT embeddings. We experiment with several strategies where the sentence uncertainty score focuses on positive tokens and show empirically on multiple datasets that this is useful. A second challenge of querying sentences with NER is related to the length of the sentences. Longer sentences that contain more tokens can bring more information at once; however, the annotation cost is higher. Normalizing sentences with the tokens they contain, on the other hand, yields to querying too short sentences. We propose to normalize the scores such that sentences with the typical length for the dataset are queried more often. We evaluate the suggested methods based on both sentence and token-based cost analysis. Overall, we believe the work presented here can support active learning efforts in the NER task.",1
"ABSTRACT Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for the NER tasks to minimize the annotation cost without sacrificing model performance. However, heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose AL sentence query evaluation functions which pay more attention to possible positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize too long or too short sentences. Our experiments on three datasets from different domains reveal that the proposed approaches reduce the number of annotated tokens while achieving better or comparable prediction performance with conventional methods. Keywords Active learning, Named entity recognition, Annotation cost, Semi-supervised clustering Introduction Name entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes (e.g., person names, organizations, locations). NER is one of the fundamental natural language processing (NLP) tasks and is used in other NLP tasks such as entity linking, event extraction, and question answering. Although deep learning-based pre-trained language models [Devlin et al., 2019, Yang et al., 2019, Liu et al., 2019, Raffel et al., 2020] have advanced the state-of-the-art performance in NER [Torfi et al., 2020, Peters et al., 2019], a sufficient amount of labeled data is still necessary for achieving satisfactory prediction performance in most domains [Tikhomirov et al., 2020, Wang et al., 2020]. Since acquiring labeled data is both time and budget-consuming, efficient label acquisition for NER remains a challenge. A general framework for tackling the labeled data acquisition problem is active learning, in which the learner strategically chooses the most valuable instances as opposed to selecting a random sample for labeling [Thompson et al., 1999]. In the pool-based active learning setup, the active learner selects the most useful examples from an unlabeled pool of samples, queries them to an annotator for labeling; upon receiving the labels for the queried examples, the model is retrained with the augmented labeled set. These query selection, annotation, and retraining steps are iterated multiple times until either the desired performance is achieved or the labeling budget is exhausted [Settles, 2011]. The goal is to reduce the annotation cost by creating a smaller labeled set while still achieving good predictive performance. Active learning (AL) has demonstrated success in various sequence annotation tasks such as part-of-speech tagging [Ringger et al., 2007], dependency parsing [Li et al., 2016b] and semantic parsing [Thompson et al., 1999]. AL has been used to tackle the label acquisition problem in the NER task as well. Shen et al. [2017] demonstrated that AL combined with deep learning achieves nearly the same performance on standard datasets with just 25% of the original training data. Chen et al. [2015] developed and evaluated both existing and new AL methods for a clinical NER task. Their results showed that AL methods, particularly uncertainty-sampling approaches, provide significant savings in the annotation cost. In active learning, the most critical step is selecting the useful query examples for manual annotation. This step becomes more challenging for sequence labeling tasks, especially for named entity recognition, for two reasons. The first challenge of applying active learning to NER arises due to imbalanced data distribution. In NER annotation, a token is either labeled with its corresponding named entity class if it is part of a named entity or with the Ã¢ÂÂotherÃ¢ÂÂ class if it is not part of a named entity. The other class is generally referred as negative annotation or negative token, and all other labels -named entity labels- are referred as positive annotations or positive tokens [Marcheggiani and ArtieÃÂres, 2014]. In NER datasets, negative tokens are usually over-abundant compared to positive tokens. The second challenge in applying active learning to NER is related to the varying length of sentences. In NER, tokens are annotated one by one, but the context, hence the corresponding sentence, is still required for accurate token annotation. Therefore, at each active learning iteration, sentences are queried instead of tokens. Active learners that select thesentences for querying by directly aggregating over all the tokens are biased towards longer sentences. In order to prevent this bias towards sentences with more terms, the aggregated sentence scores are normalized by the number of tokens in the sentence [Engelson and Dagan, 1996, Haertel et al., 2008, Settles and Craven, 2008]. This commonly used approach solves the problem only partially, since this time, the active learner starts to query ""too"" short sentences in the early, and intermediate rounds [Tomanek, 2010]. In this paper, we moderate these two extreme cases and propose a normalization which exploits the corresponding datasetsÃ¢ÂÂ token count distribution. The varying length of sentences also affects the cost evaluation of the active learning framework. Some studies [Settles and Craven, 2008, Yao et al., 2009, Kim et al., 2006, Liu et al., 2020a] treat all sentences equally and compare active learning methods directly with respect to the number of sentences queried. However, this is not realistic since the cost is not fixed across sentences as sentences differ in the number of tokens and the number of named entities they contain [Arora et al., 2009, Haertel et al., 2008]. Therefore, the number of annotated tokens should be incorporated into the active learning cost. In this regard, many studies in the literature [Shen et al., 2004, Settles and Craven, 2008, Reichart et al., 2008, Shen et al., 2017] measure the cost of the annotation by the number of tokens annotated even though they query the sentences. Using only the token count is also an imperfect strategy as the cost of annotating the same number of tokens distributed over multiple sentences is not equivalent to annotating these tokens within a single sentence [Settles et al., 2008, Tomanek and Hahn, 2010]. This is mainly because there is a cost factor associated with each new sentence that is independent of its content and length. Even though we do not propose a new cost calculation method that encompasses all these different aspects, we consider these two cost evaluation set-ups to analyze the existing and proposed approaches in detail. In this study, we propose an extension to the subset of the existing uncertainty sampling methods to handle the challenges associated with the over-abundance of the negative tokens. In our proposed approach, the query evaluation metrics are designed to pay less attention to the tokens that are predicted to have negative annotations. We identify potentially negative tokens through clustering of pre-trained BERT representations after a semi-supervised dimensionality reduction. To the best of our knowledge, the use of the BERT embeddings directly in the active learning querying step for NER has never been attempted before this paper. Last but not least, this paper proposes a better normalization strategy for aggregating token scores to attain a good sentence query metric. For a fair comparison, we evaluate different active learning query methods both under the assumption of fixed annotation cost per sentence and fixed annotation cost per token. Our experiments on three datasets from different domains illustrate that our proposed approach reduces the number of annotated tokens while maintaining the slightly better or same level of prediction performance with the compared methods. We also present an extensive investigation about the effects of different pre-trained language embeddings on the performance of our NER model. The rest of the paper is organized as follows: Section 2 presents the NER data collections used in the experiments together with additional details to motivate the reader for the proposed method, Section 3 summarizes the general active learning setup and the commonly used active learning strategies for the NER task, and Section 4 describes the proposed approach. We describe our experimental setting in Section 5 and detail our results in Section 6. Section 7 concludes with a summary of our findings. Conclusion In this work, we focus on active learning for NER. One challenge of applying NER in active learning is the abundance of negative tokens. Uncertainty-based sentence query functions aggregate the scores of the tokens in a sentence, and since the negative tokensÃ¢ÂÂ uncertainty scores dominate the overall score, they shadow the informative, positive tokens. In this work, we propose strategies to overcome this by focusing on the possible positive tokens. To identify positive tokens, we use a semi-supervised clustering strategy of tokensÃ¢ÂÂ BERT embeddings. We experiment with several strategies where the sentence uncertainty score focuses on positive tokens and show empirically on multiple datasets that this is useful. A second challenge of querying sentences with NER is related to the length of the sentences. Longer sentences that contain more tokens can bring more information at once; however, the annotation cost is higher. Normalizing sentences with the tokens they contain, on the otheryields to querying too short sentences. We propose to normalize the scores such that sentences with the typical length for the dataset are queried more often. We evaluate the suggested methods based on both sentence and token-based cost analysis. Overall, we believe the work presented here can support active learning to NER.",0
"Abstract. Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue sys- tems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user’s input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation. Keywords: natural language understanding, ontology, Ontolex, data virtualization. Introduction The corporate automated system users expect them to be “intellectual” enough to give precise answers to their questions. It implies that a system must give meaningful answers in the dialogue with a user and ask clarifying questions as a human would do. To achieve that, the system must deal with the structured representation of each ques- tion and answer of a dialogue, as well as with a structured data required to give an answer. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets – for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1)  Conceptual model construction for representing the users’ domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2)  Disparate corporate data sets into a virtual Knowledge Graph. 3)  The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of user’s natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a user’s question into a query to the graph. The func- tional requirements to this system are: 1)  It must find the class or property which instances would be an answer to the user’s question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2)  The system’s answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the user’s query, includ- ing visualization of the relations chain that led to these objects. 3)  If there is no unambiguous answer to the user’s query, the system should ask clarifying questions. 4)  The system should answer user’s clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with the big graphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a KG, which also contains data on the telemetry sensors and the parameters they measure. We have created a compact ontology for this domain, which offers diversity of the re- lations between objects and the playground for making queries involving 3-4 related graph vertices. In the real use case such a system will include a much extensive set of facts on the various aspects of the enterprise activity. The facts will be gathered into KG from the variety of data sources, such as corporate applications’ databases, the or- ganizational and administrative documents, and will be consolidated by a data virtual- ization platform. The resulting graph should be available with SPARQL interface. Related works We have used some well-known technologies in our NL processing pipeline. Named entities recognition techniques developed over long time [Shen, 2015]. POS tagging is considered in [Mikic, 2009; Wu, 2020; Huang, 2015; Le, 2018; Piccinno, 2014]. Since our ideas are based on moving from syntactical relations to semantical relations (see: [Melchuk, 1999; Gerd, 2005; Kolshansky, 1980; Banarescu, 2013; Fensel, 2003]), we need a morphological and syntactical analysis of sentences and coreference clusters (chains) finding. Morpho-syntactical properties of tokens, analysis of syntactic struc- ture of a phrase, syntactic relationships discovery between words are considered in [Ju- rafsky, 2008]. Co-reference resolution techniques review is given in [Zheng, 2011]. Pre-trained language models such as BERT [Kuratov, 2019] can be used to deter- mine the context-depending word meaning. Transformer models allow vectorization of the word sequences, which can be used to interpret their meanings [Kalyan, 2021]. The ability to retrieve an entity from a Knowledge Base given a textual input is a key com- ponent for several applications (see: [Ferrucci, 2012; Slawski, 2015; Yang, 2018]). We have used Ontolex1 ontology, developed by W3C Ontology-Lexica Community Group2, to formalize lexical model. This ontology was published in 20163 and it is well documented. Its key features are described in its developer’s publications, for example [Cimiano, 2011], [McCrae, 2017]. Ontolex is often used in the computational linguis- tics tasks, for examples see: [Declerck, 2019, Abgaz, 2020]. It is also used in the inter- disciplinary projects, particularly it was used in the European Commission PMKI pro- ject (Public Multilingual Knowledge Infrastructure) in 20184. Conclusions In the quest of the most effective combination of the machine learning and KG tools we have developed the architecture of the natural language understanding pipeline. The algorithms of establishing links between tokens of the recognized text and the domain ontology play the key role in it. The lexical ontology layer, describing words usage to denote domain concepts, is necessary to make them work. It allows semantic ambigui- ties resolution considering semantic fields in which the words are included. The practical value of the developed architecture significantly increases if the KG queries it generates to answer user questions are processed by the data virtualization platform which can access the huge arrays of the data disparate in the corporate stor- ages. It opens the way to create a dialogue system which allows user to discover previ- ously hidden, implicit, or virtually unavailable information from these storages, and involves it in the business processes including decision making support. This way of KG usage can be described as the valuable component of the true Knowledge Manage- ment System of an enterprise. The further work on our NL solution improvement includes quantitative question answering implementation (object counting, searching for maximal/minimal values, summation, and other aggregation methods), work with the date and numeric intervals, temporal relations recognition. These functions are valuable in the corporate data pro- cessing.",1
"Abstract. Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue sys- tems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms userÃÂ¢ÃÂÃÂs input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation. Keywords: natural language understanding, ontology, Ontolex, data virtualization. Introduction The corporate automated system users expect them to be Ã¢ÂÂintellectualÃ¢ÂÂ enough to give precise answers to their questions. It implies that a system must give meaningful answers in the dialogue with a user and ask clarifying questions as a human would do. To achieve that, the system must deal with the structured representation of each ques- tion and answer of a dialogue, as well as with a structured data required to give an answer. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets Ã¢ÂÂ for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1) Conceptual model construction for representing the usersÃ¢ÂÂ domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2) Disparate corporate data sets into a virtual Knowledge Graph. 3) The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of userÃ¢ÂÂs natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a userÃ¢ÂÂs question into a query to the graph. The func- tional requirements to this system are: 1) It must find the class or property which instances would be an answer to the userÃ¢ÂÂs question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2) The systemÃ¢ÂÂs answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the userÃ¢ÂÂs query, includ- ing visualization of the relations chain that led to these objects. 3) If there is no unambiguous answer to the userÃ¢ÂÂs query, the system should ask clarifying questions. 4) The system should answer userÃ¢ÂÂs clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with thegraphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a virtual Knowledge Graph, which can be populated with information by data mining from text sources or struc- tured data sets Ã¢ÂÂ for example, the business application databases in the corporate envi- ronment. The domain ontology composition and the virtualization projects are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of userÃ¢ÂÂs natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a userÃ¢ÂÂs question into a query to the graph. The func- tional requirements to this system are: 1) It must find the class or property which instances would be an answer to the userÃ¢ÂÂs question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2) The systemÃ¢ÂÂs answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the userÃ¢ÂÂs query, includ- ing visualization of the relations chain that led to these objects. 3) If there is no unambiguous answer to the user query, the system should ask clarifying questions. 4) The system should answer userÃ¢ÂÂs clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with the big graphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a virtual Knowledge Graph, which can be populated with information by data mining from text sources or struc- tured data sets Ã¢ÂÂ for example, the business application databases in the corporate envi- ronment. The domain ontology composition and the virtualization projects are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of userÃ¢ÂÂs natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. ",0
"Abstract Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker Factorization—a recent approach that relaxes independence assumptions—to prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters. Introduction Creating a single model that performs well across multiple domains is often desirable, especially in production systems. Relying on multiple (task-specific) systems necessitates storing and managing corresponding collections of parameters. Multi-task models Caruana [1997] obviate this need by performing well on inputs from all tasks, simplifying deployment. In the medical domain especially, new data is constantly becoming available, and it is necessary to keep models up to date with this deluge. In medical entity linking—where the goal is to link mentions in clinical text to corresponding entities in an ontology—the underlying ontologies are frequently updated, and the new terms are put into use quickly. For example, in the past year or so many codes related to COVID-19 Guan et al. [2020] were added to the International Classification of Diseases (ICD) lexicon, therefore updating models to incorporate new codes without losing performance on older knowledge is of particular importance. The language in the medical domain also brings additional challenges because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are cases—particularly when dealing with medical data—where data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson d’ Autume et al. [2019] maintains performance when training on new tasks by “replaying” examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used. Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017] is an alternative, constraint-based technique that regularizes parameters such that they are encouraged to maintain optimal weights learned for prior tasks. By placing a prior involving the Hessian from previous tasks over network parameters, EWC affords flexibility with respect to changing parameters in different dimensions. Critically, EWC does not require continued access to data from ‘past’ tasks once key statistics are computed over a task’s data. However, to scale to even relatively small neural networks, EWC must assume independence between all parameters. This assumption allows one to drop off-diagonal terms in the Fisher Information Matrix (FIM), which is used to approximate the Hessian; calculating the full matrix would be intractable. Recent work Ritter et al. [2018b] has proposed Kronecker Factorization (KF) — which the optimization community uses to compute Hessians in neural networks — to perform a version of EWC with a relaxed independence assumption on networks of linear layers operating on small vision datasets. Our contribution here is the extension of EWC and KF to the large-scale neural models now common in NLP. In particular, modern NLP tends to rely on large-scale models with hundreds of millions of parameters Devlin et al. [2019], Liu et al. [2019], Radford et al. [2019], and CF is problematic across many of its sub-domains. Though EWC has been successfully applied to NLP in recent work (Section 7), we demonstrate that there is room for substantial gains. In particular, we have observed that the independence assumption over parameters significantly and negatively affects EWC’s ability to mitigate CF, as compared to what is achieved using the full covariance matrix of parameters. As far as we are aware, this work is the first application of the Kronecker Factorization method— which relaxes the assumption of independence between parameters—for continuous learning in large-scale networks. Though we do not model all elements of the Fisher Information Matrix, we approximate block diagonals corresponding to layers, which is less damaging than assuming completely independent parameters. Specifically, we apply Kronecker Factorization in two large-scale neural networks for Entity Linking: (1) a convolutional and (2) a transformer-based architecture Vaswani et al. [2017]. Our primary contributions are as follows: (1) We combine and extend prior work in EWC and Kronecker Factorization to modern large-scale NLP models. (2) We use Kronecker Factorization to train on multiple biomedical ontology entity linking tasks sequentially without access to previous data, and show that it significantly outperforms baseline methods. Related Work CF Mitigation in Continuous Learning. There is a large body of work on CF mitigation, much of which focuses on constraining model parameters during training. From this category, we use learning rate control from UMLFit Howard and Ruder [2018] and EWC Kirkpatrick et al. [2017] as baselines. Path Integral Zenke et al. [2017] is similar to EWC but calculates weight variances from the optimization path instead of at the end of training. Chaudhry et al. [2018] combines these methods and generalizes the fisher by replacing it with a KL divergence between output distributions of the previous task weights and those of the current weights. Progressive networks Rusu et al. [2016] do not constrain training but use features of previously trained networks as additional input to subsequently trained ones. Progress and Compress Schwarz et al. [2018] extends this by using EWC to create one model that performs well on all tasks. Previous work has also focused on preventing CF by introducing a working memory that allows past examples to be replayed. We use experience replay from de Masson d’ Autume et al. [2019] as an additional data-dependent baseline for our methods, but there are many other methods in this category Sprechmann et al. [2018], Wang et al. [2019], and many which combine these methods with regularization methods Lopez-Paz and Ranzato [2017], Chaudhry et al. [2019]. Kronecker Factorization for Neural Networks. Martens and Grosse [2015] first introduce Kro- necker Factorization as an approximation of blocks of the FIM of neural networks. They use this to perform second-order optimization techniques on linear neural networks, and then extend this to opti- mizing convolutional architectures Grosse and Martens [2016]. More recently, Ritter et al. [2018a] show that the Fisher approximation can be used as a posterior on network weights, and then expand EWC to use off-diagonal elements of the FIM in its regularization term with this approximation Ritter et al. [2018b]. This last paper mainly focuses on small vision datasets. Continuous Learning in NLP. NLP seems to be particularly susceptible to CF Howard and Ruder [2018], Yogatama et al. [2019]. Recent work has therefore focused on developing continuous learning techniques in NLP to mitigate this issue Moeed et al. [2020], Pilault et al. [2020], Chen et al. [2020]. There has also been work that applies previous techniques to specific domains (e.g. machine translation Thompson et al. [2019], sentiment analysis Madasu and Rao [2020], and reading comprehension Xu et al. [2020]). Many of these methods focus on avoiding over-fitting to new tasks during fine-tuning, whereas we focus on maintaining high performance on old tasks. None of these use Kronecker Factorization, which has not yet been scaled to prevent CF in large NLP models. CF mitigation is particularly important in clinical NLP given that many clinical datasets are quite different from generic domains and from each other. Arumae et al. [2020] explore CF in language modeling when transferring between the generic, clinical, and biomedical domains and compare learning rate control, experience replay, and EWC. Conclusions We have demonstrated the effectiveness of Kroneker Factorization (KFC) for preventing catastrophic forgetting in modern large-scale neural architectures commonly used in NLP, improving on the results of Elastic Weight Consolidation. We showed that KFC can be used to create a unified model on multiple domains of Medical Entity Linking with good performance across tasks after a continuous (sequential) learning process. We highlighted strengths and weaknesses of the adopted approximation used in KFC, pointing to potential future directions. Future work might consider alternatives to the block diagonal structure on the covariance matrix used in KFC. Another promising line of inquiry concerns reducing the difficulty in selecting the λ hyper-parameter, which controls the strength of the regularization, without requiring a robust grid-search.",1
"Abstract Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker FactorizationÃ¢ÂÂa recent approach that relaxes independence assumptionsÃ¢ÂÂto prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters. Introduction Creating a single model that performs well across multiple domains is often desirable, especially in production systems. Relying on multiple (task-specific) systems necessitates storing and managing corresponding collections of parameters. Multi-task models Caruana [1997] obviate this need by performing well on inputs from all tasks, simplifying deployment. In the medical domain especially, new data is constantly becoming available, and it is necessary to keep models up to date with this deluge. In medical entity linkingÃ¢ÂÂwhere the goal is to link mentions in clinical text to corresponding entities in an ontologyÃ¢ÂÂthe underlying ontologies are frequently updated, and the new terms are put into use quickly. For example, in the past year or so many codes related to COVID-19 Guan et al. [2020] were added to the International Classification of Diseases (ICD) lexicon, therefore updating models to incorporate new codes without losing performance on older knowledge is of particular importance. The language in the medical domain also brings additional challenges because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are casesÃ¢ÂÂparticularly when dealing with medical dataÃ¢ÂÂwhere data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson dÃ¢ÂÂ Autume et al. [2019] maintains performance when training on new tasks by Ã¢ÂÂreplayingÃ¢ÂÂ examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used. Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017] is an alternative, constraint-based technique that regularizes parameters such that they are encouraged to maintain optimal weights learned for prior tasks. By placing a prior involving the Hessian from previous tasks over network parameters, EWC affords flexibility with respect to changing parameters in different dimensions. Critically, EWC does not require continued access to data from Ã¢ÂÂpastÃ¢ÂÂ tasks once key statistics are computed over a taskÃ¢ÂÂs data. However, to scale to even relatively small neural networks, EWC must assume independence between all parameters. This assumption allows one to drop off-diagonal terms in the Fisher Information Matrix (FIM), which is used to approximate the Hessian; calculating the full matrix would be intractable. Recent work Ritter et al. [2018b] has proposed Kronecker(KF) ÃÂ¢ÃÂÃÂ which the optimization community uses to compute Hessians in neural networks ÃÂ¢ÃÂÃÂ to perform a version of EWC with a relaxed independence assumption on networks of linear layers operating on small vision datasets. Our contribution here is the extension of EWC and KF to the large-scale neural models now common in NLP. In particular, modern NLP tends to rely on large-scale models with hundreds of millions of parameters Devlin et al. [2019], Liu et al. [2019], Radford et al. [2019], and CF is problematic across many of its sub-domains. Though EWC has been successfully applied to NLP in recent work (Section 7), we demonstrate that there is room for substantial gains. In particular, we have observed that the independence assumption over parameters significantly and negatively affects EWCÃ¢ÂÂs ability to mitigate CF, as compared to what is achieved using the full covariance matrix of parameters. As far as we are aware, this work is the first application of the Kronecker Factorization methodÃ¢ÂÂ which relaxes the assumption of independence between parametersÃ¢ÂÂfor continuous learning in large-scale networks. Though we do not model all elements of the Fisher Information Matrix, we approximate block diagonals corresponding to layers, which is less damaging than assuming completely independent parameters. Specifically, we apply Kronecker Factorization in two large-scale neural networks for Entity Linking: (1) a convolutional and (2) a transformer-based architecture Vaswani et al. [2017]. Our primary contributions are as follows: (1) We combine and extend prior work in EWC and Kronecker Factorization to modern large-scale NLP models. (2) We use Kronecker Factorization to train on multiple biomedical ontology entity linking tasks sequentially without access to previous data, and show that it significantly outperforms baseline methods. Related Work CF Mitigation in Continuous Learning. There is a large body of work on CF mitigation, much of which focuses on constraining model parameters during training. From this category, we use learning rate control from UMLFit Howard and Ruder [2018] and EWC Kirkpatrick et al. [2017] as baselines. Path Integral Zenke et al. [2017] is similar to EWC but calculates weight variances from the optimization path instead of at the end of training. Chaudhry et al. [2018] combines these methods and generalizes the fisher by replacing it with a KL divergence between output distributions of the previous task weights and those of the current weights. Progressive networks Rusu et al. [2016] do not constrain training but use features of previously trained networks as additional input to subsequently trained ones. Progress and Compress Schwarz et al. [2018] extends this by using EWC to create one model that performs well on all tasks. Previous work has also focused on preventing CF by introducing a working memory that allows past examples to be replayed. We use experience replay from de Masson dÃ¢ÂÂ Autume et al. [2019] as an additional data-dependent baseline for our methods, but there are many other methods in this category Sprechmann et al. [2018], Wang et al. [2019], and many which combine these methods with regularization methods Lopez-Paz and Ranzato [2017], Chaudhry et al. [2019]. Kronecker Factorization for Neural Networks. Martens and Grosse [2015] first introduce Kro- necker Factorization as an approximation of blocks of the FIM of neural networks. They use this to perform second-order optimization techniques on linear neural networks, and then extend this to opti- mizing convolutional architectures Grosse and Martens [2016]. More recently, Ritter et al. [2018a] show that the Fisher approximation can be used as a posterior on network weights, and then expand EWC to use off-diagonal elements of the FIM in its regularization term with this approximation Ritter et al. [2018b]. This last paper mainly focuses on small vision datasets. Continuous Learning in NLP. NLP seems to be particularly susceptible to CF Howard and Ruder [2018], Yogatama et al. [2019]. Recent work has therefore focused on developing continuous learning techniques in NLP to mitigate this issue Moeed et al. [2020], Pilault et al. [2020], Chen et al. [2020]. There has also been work that applies previous techniques to specific domains (e.g. machine translation Thompson et al. [2019], sentiment analysis Madasu and Rao [2020], and reading comprehension Xu et[2020]). Many of these methods focus on avoiding over-fitting to new tasks during fine-tuning, whereas we focus on maintaining high performance on old tasks. None of these use Kronecker Factorization, which has not yet been scaled to prevent CF in large NLP models. CF mitigation is particularly important in clinical NLP given that many clinical datasets are quite different from generic domains and from each other quickly. For example, there are casesÃ¢ÂÂparticularly when dealing with medical dataÃ¢ÂÂwhere data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson dÃ¢ÂÂ Autume et al. [2019] maintains performance when training on new tasks by Ã¢ÂÂreplayingÃ¢ÂÂ examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used.",0
"Abstract While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sen- tence. Second, we test not only models’ ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- els’ treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like “this sentence contains an anomaly”, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomalies—here our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating “this sentence contains an anomaly”, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders. We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generality—however, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddity—which appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Rogers et al., 2020; Jawahar et al., 2019). Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- ness (Dasgupta et al., 2018; Poliak et al., 2018a,b; White et al., 2017; Belinkov et al., 2017b; Yang et al., 2015; Rajpurkar et al., 2016; Jia and Liang, 2017; Rajpurkar et al., 2018). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Conneau et al., 2018; Belinkov et al., 2017a; Ettinger et al., 2016, 2018; Tenney et al., 2019; Klafka and Ettinger, 2020). We expand on this work by designing analyses to shed light on encod- ing of syntactic anomaly information in sentence embeddings. A growing body of work has examined syntactic sensitivity in language model outputs (Chowdhury and Zamparelli, 2018; Futrell et al., 2019; Lakretz et al., 2019; Marvin and Linzen, 2018; Ettinger, 2020), and our Agree-Shift task takes inspiration from the popular number agreement task for lan- guage models (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Like this work, we fo- cus on syntax in designing our tests, but we differ from this work in focusing on model representa- tions rather than outputs, and in our specific focus on understanding how models encode information about anomalies. Furthermore, as we detail below, our Agree-Shift task differs importantly from the LM number agreement tests, and should not be compared directly to results from those tests. Our work relates most closely to studies involv- ing anomalous or erroneous sentence information (Warstadt et al., 2019; Yin et al., 2020; Hashemi and Hwa, 2016). Some work investigates impacts from random shuffling or other types of distor- tion of input text (Pham et al., 2020; Gupta et al., 2021) or of model pre-training text (Sinha et al., 2021) on downstream tasks—but this work does not investigate models’ encoding of these anoma- lies. Warstadt et al. (2019) present and test with the CoLA dataset for general acceptability detec- tion, and among the probing tasks of Conneau et al. (2018) there are three that involve analyz- ing whether sentence embeddings can distinguish erroneous modification to sentence inputs: SOMO, BShift, and CoordInv. Yin et al. (2020) also gen- erate synthetic errors based on errors from non- native speakers, showing impacts of such errors on downstream tasks, and briefly probing error sen- sitivity. More recently, Li et al. (2021) conduct anomaly detection with various anomaly types at different layers of transformer models, using train- ing of Gaussian models for density estimation, and finding different types of anomaly sensitivity at different layers. We build on this line of work in anomaly detection with a fine-grained exploration of models’ detection of word-content-controlled perturbations at different levels of syntactic hierar- chy. Our work is complementary also in exploring generality of models’ anomaly encoding by exam- ining transfer performance between anomalies. Conclusion We have undertaken a direct study of anomaly en- coding in sentence embeddings, finding impacts of hierarchical differences in anomaly type, but find- ing evidence of generalized anomaly encoding only in BERT and RoBERTa. Follow-up analyses sup- port the conclusion that these embeddings encode a combination of generalized and anomaly-specific cues in these embeddings, with models appearing to leverage both finer-grained and coarser-grained information for anomaly detection. These results contribute to our understanding of the nature of encoding of linguistic input in embeddings from recent models. Future work can further explore the relationship between naturalness-oriented training and cultivation of abstract anomaly awareness, and how these insights can be leveraged for more robust and human-like processing of language inputs.",1
"Abstract While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sen- tence. Second, we test not only modelsÃ¢ÂÂ ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- elsÃ¢ÂÂ treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like Ã¢ÂÂthis sentence contains an anomalyÃ¢ÂÂ, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomaliesÃ¢ÂÂhere our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating Ã¢ÂÂthis sentence contains an anomalyÃ¢ÂÂ, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders. We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generalityÃ¢ÂÂhowever, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddityÃ¢ÂÂwhich appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Rogers et al., 2020; Jawahar et al., 2019). Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- nesset al., 2018; Poliak et al., 2018a,b; White et al., 2017; Belinkov et al., 2017b; Yang et al., 2015; Rajpurkar et al., 2016; Jia and Liang, 2017; Rajpurkar et al., 2018). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Conneau et al., 2018; Belinkov et al., 2017a; Ettinger et al., 2016, 2018; Tenney et al., 2019; Klafka and Ettinger, 2020). We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generalityÃ¢ÂÂhowever, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddityÃ¢ÂÂwhich appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Klafka et al., 2020; Tenney et al., 2019; Ettinger et al., 2020), focusing on the domain of linguistic knowledge and detection performance. Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- ness (Dasgupta et al., 2018; Poliak et al., 2018a; Ettinger, 2019). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Ettinger, 2020; Klafka et al., 2020; Yang et al., 2015; Yang et al., 2016; White et al., 2017; Ettinger, 2018). We expand on this work by designing analyses to shed light on encod- ing of syntactic anomaly information in sentence embeddings. A growing body of work has examined syntactic sensitivity in language model outputs (Chowdhury and Zamparelli, 2018; Futrell et al., 2019; Lakretz et al., 2019; Marvin and Linzen, 2018; Ettinger, 2020), and our Agree-Shift task takes inspiration from the popular number agreement task for lan- guage models (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Like this work, we fo- cus on syntax in designing our tests, but we differ from this work in focusing on model representa- tions rather than outputs, and in our specific focus on understanding how models encode information about anomalies. Furthermore, as we detail below, our Agree-Shift task differs importantly from the LM number agreement tests, and should not be compared directly to results from those tests. Our work relates most closely to studies involv- ing anomalous or erroneous sentence information (Warstadt et al., 2019; Yin et al., 2020; Hashemi and Hwa, 2016). Some work investigates impacts from random shuffling or other types of distor- tion of input text (Pham et al., 2020; Gupta et al., 2021) or of model pre-training text (Sinha et al., 2021) on downstream tasksÃ¢ÂÂbut this work does not investigate modelsÃ¢ÂÂ encoding of these anoma- lies. Warstadt et al. (2019) present and test with the CoLA dataset for general acceptability detec- tion, and among the probing tasks of Conneau et al. (2018) there are three that involve analyz- ing whether sentence embeddings can distinguish erroneous modification to sentence inputs: SOMO, BShift, and CoordInv.",0
"Abstract: From both human translators (HT) and machine translation (MT) researchers’ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA). Keywords: Translation Quality Evaluation, Quality Estimation, Post-editing Distance, Confidence Intervals, Monte Carlo Modeling, Bernoulli Statistics Introduction Machine Translation (MT) is one of the pioneering artificial intelligence (AI) tasks dating back from the 1950s (Weaver, 1955). It emphasizes the interaction of Language and Machine, and how machine can learn human languages with cognitive knowledge. Before MT, human translation (HT) of written text and documents has always played an important role in science and literature communication between different language speakers, breaking the language barriers. From both MT and HT perspectives, trans- lation quality evaluation (TQE), sometimes incorrectly referred as translation quality assessment (TQA)i, is an important task to reflect how well the source text is translated into the target languages (Han et al., 2021b). On the one hand, for low resource language pair scenarios, human translators still play the dom- inant role in translation production. The translated text and documents can contain unavoidable errors due to personal bias, input efforts, or the training level of the translators. On the other hand, for high resource language pair situation, neural MT (NMT) has achieved remarkable improvement especially on fluency level, compared to conventional rule-based and statistical phrase-based MT models; however, NMT still has “poisoned cookie” problem struggling to achieve real human parity, for instance, on ad- equacy level, meaning preservation, and on idiomatic expression translations (Sag et al., 2002; Han et al., 2020b; Johnson et al., 2016; Han et al., 2020a). Translation service providers (TSPs) relying on both MT, HT, and human post-editing of MT output (TPE) carry out translation and editing tasks with the high demand and harsh constraints nowadays. Thus, TQE role in this workflow remains to be critical. However, it is tedious, costly, and time-consuming to check through the entire translated text given the huge amount of data TSP and customers process. One obvious solution, to this point, is to extract a sub-set of the translated text and make a conclusion about the overall translation quality by results of TQE of the sample, which has always been done in real practice. However, one question arises here: how large the sample size shall be to estimate the overall translation quality of the entire material reliably? In other words, what is the confidence interval of such evaluation on certain desired confidence level (which is commonly taken as 95%) with the samples we choose to estimate the overall translation quality? In this work, we carry out such a motivated experimental investigation on confidence evaluation of translation quality evaluation. To take advantage of statistical modeling techniques, we start with the assumption that error distribution is uniform across the entire material, and minimum unit where the error occurs is one sentence (since errors can be between words, in the form of punctuation and conjugation, etc.). This assumption is the best case scenario which potentially ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation when we need to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of “errors” (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation. Another disadvantage for automatic evaluation is that we can not get in-depth view of what kind of errors the candidate translations present in the studied context, except for an overall evaluation score, or segment-level scores, not to mention that most metrics do not even allow for clear interpretation of what does the score exactly mean. Regarding this aspect, professional translators can always do a much better job by giving transparent error analysis and categorization on the candidate translations, such as idiomatic expressions (Han et al., 2020a; Han et al., 2020b). However, another issue arrives at this point, that is how to correctly chose a confident sample from the candidate translation, instead of just take a random sample for granted and try to blindly extrapolate its result to the entire material? Actually this is not a brand new challenge in natural language processing (NLP) field. Having it in mind that randomly chosen samples may contain model bias against a proper evaluation, (Prabhu et al., 2019) proposed an uncertainty sampling approach for text classification task, and their statistical models can reduce the bias effectively with smaller size of data in comparison to confessional models; Similarly, (Haertel et al., 2008) carried out work on how statistical sampling models can help reduce the high cost for Penn Tree-bank annotation while maintaining the higher accuracy; (Nadeem et al., 2020) carried out one systematic comparison of several sampling algorithms used for language generation task, including top-k, nucleus and tempered sampling, looking into quality-diversity trade-off. Sampling method was also applied into confident level evaluation of MT. For instance, (Koehn, 2004) proposed to use bootstrap re-sampling methods to test the significance level of automatic metric BLEU, but using a fixed number of sentences, i.e. 300. Like many other research work, the chosen number of sentences for evaluation was never explained or justified with any statistical validation. In contrast, we carry out statistical sampling modelling to estimate the number of sentences that is confident enough to achieve reliable quality evaluation, which means a better representation and generalization of the overall candidate translations in question, with an confidence-cost tread-off. Discussions and Conclusions In this article, we proposed the research question on confident sample size of translation documents to estimate the overall material quality, which is a crucial question for both academic research and industrial applications, such as for clients and language service providers. We started the experimental investigation of translation quality evaluation (TQE) by assuming that the errors in translated text are evenly distributed, with errors being rare (7 errors per 100 sentences, on average). This assumption is placed as a random seed of our statistical Bernoulli modelling which does not affect the overall model behavior. To simulate the practical situation where the errors can come from different translators and different types, and span into a different weight across the translated text and documents, we applied the Monte Carlo Simulation analysis, using a sample size of 2000 sentences and 95% confidence level. We also applied MCS into confidence estimation of post-editing distance measurement which is currently widely adopted evaluation metric for translation assessment, and gained very valuable findings from empirical investigations regarding practical situations when translation qual- ity evaluation (TQE) is deployed. Furthermore, we suggest that, ideally, a reliability level of analytic sample quality measurement can be added to every analytic TQE scorecard in the form of confidence interval at certain confidence level as one important indicator of the level of certainty of measurement results. In the future work, we plan to compare different sampling methods, as well as apply the confidence estimation model into broader TQE metrics.",1
"Abstract: From both human translators (HT) and machine translation (MT) researchersÃÂ¢ÃÂÃÂ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA). Keywords: Translation Quality Evaluation, Quality Estimation, Post-editing Distance, Confidence Intervals, Monte Carlo Modeling, Bernoulli Statistics Introduction Machine Translation (MT) is one of the pioneering artificial intelligence (AI) tasks dating back from the 1950s (Weaver, 1955). It emphasizes the interaction of Language and Machine, and how machine can learn human languages with cognitive knowledge. Before MT, human translation (HT) of written text and documents has always played an important role in science and literature communication between different language speakers, breaking the language barriers. From both MT and HT perspectives, trans- lation quality evaluation (TQE), sometimes incorrectly referred as translation quality assessment (TQA)i, is an important task to reflect how well the source text is translated into the target languages (Han et al., 2021b). On the one hand, for low resource language pair scenarios, human translators still play the dom- inant role in translation production. The translated text and documents can contain unavoidable errors due to personal bias, input efforts, or the training level of the translators. On the other hand, for high resource language pair situation, neural MT (NMT) has achieved remarkable improvement especially on fluency level, compared to conventional rule-based and statistical phrase-based MT models; however, NMT still has Ã¢ÂÂpoisoned cookieÃ¢ÂÂ problem struggling to achieve real human parity, for instance, on ad- equacy level, meaning preservation, and on idiomatic expression translations (Sag et al., 2002; Han et al., 2020b; Johnson et al., 2016; Han et al., 2020a). Translation service providers (TSPs) relying on both MT, HT, and human post-editing of MT output (TPE) carry out translation and editing tasks with the high demand and harsh constraints nowadays. Thus, TQE role in this workflow remains to be critical. However, it is tedious, costly, and time-consuming to check through the entire translated text given the huge amount of data TSP and customers process. One obvious solution, to this point, is to extract a sub-set of the translated text and make a conclusion about the overall translation quality by results of TQE of the sample, which has always been done in real practice. However, one question arises here: how large the sample size shall be to estimate the overall translation quality of the entire material reliably? In other words, what is the confidence interval of such evaluation on certain desired confidence level (which is commonly taken as 95%) with the samples we choose to estimate the overall translation quality? In this work, we carry out such a motivated experimental investigation on confidence evaluation of translation quality evaluation. To take advantage of statistical modeling techniques, we start with the assumption that error distribution is uniform across the entire material, and minimum unit where the error occurs is one sentence (since errors can be between words, in the form of punctuation and conjugation, etc.). This assumption is the best case scenario which potentially ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation whenneed to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of Ã¢ÂÂerrorsÃ¢ÂÂ (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation. Another disadvantage for automatic evaluation is that we can not get in-depth view of what kind of errors the candidate translations present in the studied context, except for an overall evaluation score, or segment-level scores, not to mention that most metrics do not even allow for clear interpretation of what does the score exactly mean. Regarding this aspect, professional translators can always do a much better job by giving transparent error analysis and categorization on the candidate translations, such as idiomatic expressions (Han et al., 2020a; Han et al., 2020b). However, another issue arrives at this point, that is how to correctly chose a confident sample from the candidate translation, instead of just take a random sample for granted and try to blindly extrapolate its result to the entire material? Actually this is not a brand new challenge in natural language processing (NLP) field. Having it in mind that randomly chosen samples may contain model bias against a proper evaluation, (Prabhu et al., 2019) proposed an uncertainty sampling approach for text classification task, and their statistical models can reduce the bias effectively with smaller size of data in comparison to confessional models; Similarly, (Haertel et al., 2008) carried out work on how statistical sampling models can help reduce the high cost for Penn Tree-bank annotation while maintaining the higher accuracy; (Nadeem et al., 2020) carried out one systematic comparison of several sampling algorithms used for language generation task, including top-k, nucleus and tempered sampling, looking into quality-diversity trade-off. Sampling method was also applied into confident level evaluation of MT. For instance, (Koehn, 2004)to use bootstrap re-sampling methods to test the significance level of automatic metric BLEU, but using a fixed number of sentences, i.e. 300. Like many other research work, the chosen number of sentences for evaluation was never explained or justified with any statistical validation. In contrast, we carry out statistical validation by taking post-editing distance (PED) measurement for evaluation, and by taking post-editing distance (PED) value-based distance modeling techniques for detail. The results of this work are published as supporting information (TPE). Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation.",0
"Abstract Named entity recognition (NER) models gen- erally perform poorly when large training datasets are unavailable for low-resource do- mains. Recently, pre-training a large-scale language model has become a promising di- rection for coping with the data scarcity is- sue. However, the underlying discrepancies between the language modeling and NER task could limit the models’ performance, and pre- training for the NER task has rarely been stud- ied since the collected NER datasets are gen- erally small or large but with low quality. In this paper, we construct a massive NER cor- pus with a relatively high quality, and we pre- train a NER-BERT model based on the cre- ated dataset. Experimental results show that our pre-trained model can significantly outper- form BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenar- ios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the “miscellaneous” entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has “corporation” and “group” entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the “organization” entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training. Instead of utilizing manually annotated NER datasets, a few previous studies (Cao et al., 2019; Mengge et al., 2020) have focused on leverag- ing weakly-labeled NER data constructed from Wikipedia to enhance the model’s performance. Cao et al. (2019) generated the weakly-labeled data based on Wikipedia anchors and a taxonomy, but the quality of the produced data is relatively low and the number of entity categories is limited. To cope with these issues, Mengge et al. (2020) lever- aged a gazetteer to obtain coarse-grained entities and k-means clustering to further mine the fine-grained entities. However, obtaining fine-grained labels based on clustering algorithms is not stable, which could limit the effectiveness of pre-training. In this work, we first aim to construct a large- scale NER dataset with a relatively high quality and abundant entity categories. After that, our goal is to prove that using the created dataset to pre-train an entity tagging model can outperform pre-trained language models on the low-resource NER task. Similar to Cao et al. (2019), we build the NER dataset based on the Wikipedia corpus. To improve the quality and increase the number of entity cate- gories, we utilize the DBpedia Ontology (Mendes et al., 2012) to assist in categorizing entities in the Wikipedia corpus. Eventually, we obtain around 16 million NER training examples, and then we continue pre-training BERT on the NER task using the constructed data to build NER-BERT. We emphasize that the focus of this paper is not to achieve state-of-the-art results, but to show the effectiveness of entity tagging-based pre-training using our constructed corpus, since current state- of-the-art NER models are constructed on top of pre-trained language models (e.g., BERT (Devlin et al., 2019)) which can be easily replaced by our NER-BERT. Therefore, we simply add a linear layer instead of many complex components on top of the pre-trained models when fine-tuning them on the downstream NER task. We evaluate our model and baselines on nine diverse domains (e.g., litera- ture, biomedical, and Twitter) of the NER task and show that our model can surpass BERT and other strong baselines such as cross-domain language modeling (Jia et al., 2019) and domain-adaptive pre-training (Gururangan et al., 2020; Liu et al., 2021b). Furthermore, we conduct extensive ex- periments in terms of different low-resource levels across multiple diverse target domains and demon- strate that NER-BERT has a powerful few-shot adaptation ability to target domains when only a few training data are available. Additionally, we visualize the entity representations for NER-BERT and baselines to further prove the effectiveness of our NER pre-training. Moreover, we will release our constructed dataset and pre-trained model to facilitate future research in this area. Conclusion In this paper, we first incorporate Wikipedia an- chors and DBpedia Ontology to build a large-scale NER dataset with a relatively high quality. Then, we utilize the constructed dataset to pre-train NER- BERT. Results illustrate that it is essential to lever- age various entity categories for pre-training, and NER-BERT is able to significantly outperform BERT as well as other strong baselines across nine diverse domains. Additionally, we show that NER-BERT is especially effective when only a few pre- training examples are available in target domains. Moreover, the visualization further indicates that NER-BERT possesses good pre-learned knowledge for categorizing a variety of entities.",1
"Abstract Named entity recognition (NER) models gen- erally perform poorly when large training datasets are unavailable for low-resource do- mains. Recently, pre-training a large-scale language model has become a promising di- rection for coping with the data scarcity is- sue. However, the underlying discrepancies between the language modeling and NER task could limit the modelsÃ¢ÂÂ performance, and pre- training for the NER task has rarely been stud- ied since the collected NER datasets are gen- erally small or large but with low quality. In this paper, we construct a massive NER cor- pus with a relatively high quality, and we pre- train a NER-BERT model based on the cre- ated dataset. Experimental results show that our pre-trained model can significantly outper- form BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenar- ios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the Ã¢ÂÂmiscellaneousÃ¢ÂÂ entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has Ã¢ÂÂcorporationÃ¢ÂÂ and Ã¢ÂÂgroupÃ¢ÂÂ entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the Ã¢ÂÂorganizationÃ¢ÂÂ entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training. Instead of utilizing manually annotated NER datasets, a few previous studies (Cao et al., 2019; Mengge et al., 2020) have focused on leverag- ing weakly-labeled NER data constructed from Wikipedia to enhance the modelÃ¢ÂÂs performance. Cao et al. (2019) generated the weakly-labeled data based on Wikipedia anchors and a taxonomy, but the quality of the produced data is relatively low and the number of entity categories is limited. To cope with these issues, Mengge et al. (2020) lever- aged a gazetteer to obtain coarse-grained entities and k-means clustering to further mine the fine-grained entities. However, obtaining fine-grained labels based on clustering algorithms is not stable, which could limit the effectiveness of pre-training. In this work, we first aim to construct a large- scale NER dataset with a relatively high quality and abundant entity categories. After that, our goal is to prove that using the created dataset to pre-train an entitymodel can outperform pre-trained language models on the low-resource NER task. Similar to Cao et al. (2019), we build the NER dataset based on the Wikipedia corpus. To improve the quality and increase the number of entity cate- gories, we utilize the DBpedia Ontology (Mendes et al., 2012) to assist in categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the Ã¢ÂÂmiscellaneousÃ¢ÂÂ entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has Ã¢ÂÂcorporationÃ¢ÂÂ and Ã¢ÂÂgroupÃ¢ÂÂ entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the Ã¢ÂÂorganizationÃ¢ÂÂ entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training.",0