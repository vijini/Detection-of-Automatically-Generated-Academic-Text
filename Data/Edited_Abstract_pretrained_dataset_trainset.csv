text,#label
"An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question- asking model capable of producing polar (yes- no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf im- age captioner without requiring any supervised question-answer data. We demonstrate our model’s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.",1
"An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a vernacular-based approach to the repair process, in which machines learn to repair ambiguities and ambiguously word-for-word. In this paper, we present a natural-language-processing system that can repair ambiguity in natural languages. Our system is based on the concept of a lexicon, which is a collection of words and phrases that are used in a given context. For example, if we ask, ""What is the color of the sky?"", the system will learn that the word ""sky"" is used to refer to a color. The system then learns to use the phrase ""the sky is blue"" in the same context as ""blue sky"", and so on, until it is able to reconstruct the meaning of all of these phrases.",0
"While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest “I enjoyed the game this weekend”: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker’s broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive languagemodeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines.",1
"While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest “I enjoyed the game this weekend”: the correct answer depends heavily on where the questioner is located in the sentence. In this case, the answer is likely to depend on the context in which it is being asked, as well as the speaker's state of mind (e.g., whether they are smiling or frowning). To address these issues, we developed a language model that captures contextual information about the language being spoken. The model is trained on a large corpus of spoken English sentences, and is then used to predict whether a given sentence will be correct or incorrect based on its context. We then tested the model's predictions against a set of real sentences from the same corpus and found that it outperformed the current state-of-the-art models by an order of magnitude (Fig. 1a,b).",0
"Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.",1
"Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further,  developing a model that can be used to train a large number of models on a single dataset. The model can then be applied to a wide range of real-world tasks, including speech recognition, natural language processing, and machine translation. In this paper, we show that the model is able to perform well on the following tasks: (i) Recognition of words in text; (ii) Speech recognition; and (iii) Translation of text into another language. We also demonstrate that this model has the ability to outperform other models trained on similar data sets. Our results suggest that it may be possible to develop models that are capable of outperforming existing models.",0
"This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, named S-APIR, where outlier detection models are investigated to properly handle various genres of news articles. Moreover, we propose a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 years’ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time.",1
"This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, i.e. a measure of how people perceive the state of the economy. We find that the index is highly correlated with the stock market, with a correlation coefficient (r) of 0.78. Our results suggest that sentiment can be used as a reliable indicator of economic activity. Citations are extracted by the CitEc Project, subscribe to its RSS feed for this item.",0
"This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language’s parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages.",1
"This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language’s parallel vernaculars. We show that this technique can be applied to a wide range of multilanguage models, and that it has the potential to improve the performance of existing models.",0
"Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online.",1
"Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a topic model for a given text, which can then be viewed in the context of the text. Our results demonstrate that the use of topic embeddings can significantly improve the coherency of a text's content.",0
We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining.,1
We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the associated results to be of interest to researchers in the field of argument mining.,0
"Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic self attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks.",1
"Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic self attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this paper, we present a novel approach to model long-term memory in the context of a text-based model. We show that our approach can be applied to a wide range of documents, from simple text documents to complex multi-layered texts. Our approach is based on the idea that the model should be able to handle both short and long text, and that it should also be capable of representing complex multilayered text as well. In particular, our model can handle text that is both long and short, but not both at the same time. This allows us to represent text in a way that allows it to be represented as a sequence of words, rather than a single word.",0
"We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from two major U.S. journals to forecast the CBOE Volatility Index.",1
"We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into ids, and to extract sentiment values from text. Sentiment scores can then be used in a variety of applications, such as machine learning, machine translation, text classification, sentiment mining, etc.",0
"While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST.",1
"While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, the model is trained on a large corpus of English sentences. We show that this model can be used to generate sentences with a high degree of generality, and that it can also be applied to a wide range of natural languages.",0
"While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks.",1
"While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation  (SEM) is an approach to augmenting real-world data with synthetic data. The goal of SEM is to reduce the amount of data that needs to be processed in order to achieve the same level of performance as the real world. For example, if you want to train a neural network to recognize faces, you would need to process hundreds of thousands of images and millions of words per second. In contrast, SEM can reduce this processing time to a fraction of the time it would take for a human to perform the task. SEM has several advantages over traditional training methods. First, it can be applied to any type of training data, including images, text, audio, video, and more. Second, because SEM does not require any prior knowledge of how the data will be used in the training process, there is no need for the user to know anything about the underlying machine learning algorithm.",0
"A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that “summarizes” texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",1
"A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to vernacular documents (e.g., English, French, German, Spanish, Italian, Japanese, Korean, Chinese, etc.). We show that this model can be applied to a wide range of documents, including documents that are written in different languages, documents written by different authors, as well as documents from different time periods. We also demonstrate that the model is robust to changes in the language of the document being analyzed.",0
"We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent whitebox and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.",1
"We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision vernacular (e.g., visual acuity) and speech recognition. However, there is a lack of empirical evidence to support the use of such approaches in the context of text analysis. Here we present a novel approach to text-based text classification, which is based on the idea that text can be used as an input to a machine learning model. We show that this approach is robust to the fact that the input text is not necessarily the same as the output text, and that it is possible to generate text that is different from the original text. In addition, our approach allows us to use text from a wide range of languages, including English, French, German, Italian, Spanish, Portuguese, Russian, Chinese, Japanese, Korean, Arabic, Hebrew, Hindi, Urdu, Tamil, Telugu, Malayalam, Bengali, Gujarati, Punjabi, Kannada, Marathi, Nepali.",0
"A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually.",1
"A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the complexity of the topic, as well as the amount of time it takes to identify the topics. For example, let's say you have a list of topics that you want to learn more about. You can do this by simply typing in the name of each topic into the search box on the left side of this page, and then clicking the ""Learn More"" button at the top right of that page. Then, you can click on any of those topics and you will be taken to a page that lists all the articles about that topic that have been published so far. If you are looking for a specific topic in that list, then you simply need to type in its name and click ""Read More"". This will take you directly to the article that describes that particular topic. The article will then tell you more information about it, such as its title, author, date of publication, etc.",0
"Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehensibility and maintainability of the law. With five intuitive law smells as running examples—namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsession—, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting.",1
"Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehensibility and maintainability of the law. With five intuitive law smells as running examples—namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and ambiguous sentence—we explore the nature of these smells and how they can be used to understand the structure of a legal text. We conclude with a discussion of how we can use these laws to improve our understanding of legal language.",0
"Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today’s LMs limits the applicability of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.",1
"Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language features that are difficult to model in a general-purpose model. Here, we present a novel approach to the modeling of niche languages, which is based on the use of a model-based approach. We show that this approach can be applied to a wide range of text-processing tasks, such as text classification, text segmentation, and word embedding. Our results demonstrate that our approach is able to solve a large number of problems in niche-language modeling.",0
"We present Harmonic Memory Networks (HMem), a neural architecture for knowledge base completion that models entities as weighted sums of pairwise bindings between an entity’s neighbors and corresponding relations. Since entities are modeled as aggregated neighborhoods, representations of unseen entities can be generated on the fly. We demonstrate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows.",1
"We present Harmonic Memory Networks (HMem), a neural architecture for knowledge base completion that models entities as weighted sums of pairwise bindings between an entity’s neighbors and corresponding relations. Since entities are modeled as aggregated neighborhoods, representations of unseen entities can be generated on the fly. We demonstrate this with the use of a simple model of the human brain, which is used to train HMem models. Our results show that the model is robust to changes in the number of neighbors in a neighborhood, as well as the amount of information contained in each neighbor. In addition, we demonstrate that our model can learn to predict the presence of an unseen entity, even when the entity is hidden from view. These results provide a novel framework for understanding the evolution of human cognition.",0
"We present the task of Automated Punishment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Addressing APE will enable the identification of sentencing patterns and constitute an important stepping stone for many follow up legal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evaluation dataset, and implement rule-based and supervised models. We find that while supervised models can identify the sentence containing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models’ errors, indicating avenues for future work, such as distinguishing between probation and actual imprisonment punishment. We will make all our resources available upon request, including data, annotation, and first benchmark models.",1
"We present the task of Automated Punishment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Addressing APE will enable the identification of sentencing patterns and constitute an important stepping stone for many follow up legal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate  the results of the study and present them in the form of a report.",0
"Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Distant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstractifies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance.",1
"Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Distant supervision produces noisy labels and requires 𝒞-weighted inference. In this paper, we propose a novel approach to extract the relationship between two facts. Our approach is based on the idea that the relationships between facts can be represented as a graph, where each node represents a fact and the edges represent a relationship. The graph can then be used to infer the truth value of a given fact. We show that our approach can extract a large number of relationships from a single graph.
In this article, I will introduce the concept of relationship extraction and describe how it is used in biomedical research. Relationship extraction is the process of extracting information from data sets that are not fully labeled.",0
"This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation.",1
"This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we  present a novel approach to train a neural network model that is able to learn to recognize and translate noise generated by User Generated Text (UGT) in the context of a real-time video game. Our approach is based on the assumption that UGT can be represented as a sequence of characters, each of which is represented by a character vector, and that this sequence can then be transformed into a representation of the UGT sequence. In this way, our approach allows us to explore the potentials of neural networks to represent noise in a variety of contexts, such as video games, speech recognition, machine translation, etc. We also present an example of how we can use this approach in order to improve the performance of our model.",0
"Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub- graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks",1
"Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this   paper, we propose a new MDS method for summarizing the inter- and intra-documents of a document. We show that this new method outperforms the existing method by a factor of two, and that it can be applied to a wide range of documents. Our method is based on the idea that, in order to summarize an entire document, it is necessary to extract all the relevant information from the document and then combine it into a single summary that can then be used as a starting point for further analysis. This is the first time that such a method has been proposed.",0
"Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (∼40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience’s laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a “funniness” score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our “Open Mic” dataset is released for further research along with the code.",1
"Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (∼40 hours) using stand-up comedy clips. We  ute the humour data into a single dataset, which is then used to annotate the data. The resulting dataset is used as the basis for a machine learning algorithm to predict the funniest jokes in the dataset. Our results show that the machine-learning algorithm is able to correctly predict a large number of funny jokes. The aim of this paper is to present a novel approach to the problem of automatically annotating humour content in natural language processing (NLP) and computational linguistics (CL) datasets. This approach is based on the idea that humour is an important aspect of human communication, and that it can be used in NLP and CL datasets to improve the accuracy of machine translation.",0
"Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ‘shortcut learning’: relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Distillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning towards a less expressive but more robust model as the data becomes more OOD, while retaining its full context capability when operating in distribution. We apply our method to a GRU architecture, demonstrating improvements on multiple language modeling (LM) datasets",1
"Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ‘shortcut learning': relying on weak correlations over arbitrary large 𝒞 space. In this paper, we propose a novel approach to overcome this problem. We show that a neural network model can be trained to predict the distribution of words in a given language, and that this model is robust to changes in word order, word length, or word frequency. Our approach is based on the notion of a word embedding space, in which each word is represented by a set of embeddings, each of which is associated with a corresponding word in the training set. The model then learns to associate words with their corresponding words, using a learning algorithm that takes advantage of the fact that words are distributed over a wide range of word lengths and word frequencies. For example, the model learns that the word 'dog' has a frequency of 0.5 and a length of 1.0, while the words 'cat' and 'bird' have frequencies of 4 and 5, respectively.",0
"The fourth industrial revolution (4IR) is likely to have a substantial impact on the economy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occupations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Researchers could validate our findings through further analyses with micro data, and our dataset can serve as a source for more complex labor market analyses. Further, we compared the 4IR exposure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. We show that a combination of 4IR exposure with other automation measures may provide additional insights. For example, near-term automation might be driven by non-4IR technologies. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures.",1
"The fourth industrial revolution (4IR) is likely to have a substantial impact on the economy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological ills are needed to assess the impact of 4IR on employment and productivity.",0
"Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reasoning process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decomposition. The Relation Extractor decomposes the complex question, and then the Reader answers the sub-questions in turn, and finally the Comparator performs numerical comparison and summarizes all to get the final answer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path provided by our RERC framework has excellent readability and faithfulness.",1
"Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reasoning process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decomposition. The Relation Extractor decomposes the complex question, and then 〈reads〉 the answer to the question. In this way, the RERC can be used to extract information from a large number of questions. In this paper, we propose a new framework for the extraction of information in complex problem-solving tasks. This framework, which we refer to as a ""relation extractor"" or ""comparator"", allows us to solve complex problems in a way that is more efficient than traditional methods, such as linear regression or logistic regression, that rely on linear discriminant analysis (LDA). In addition, this framework can also be applied to other types of problems that are more complex than simple ones.",0
"Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language models and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we focus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data displaying a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings.",1
"Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language models and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we present a novel approach to the problem of building a language model for a large number of languages, using a combination of machine learning and supervised learning techniques. We show that our approach can be applied to a wide variety of language systems, including English, French, German, Italian, Japanese, Korean, Russian, Spanish, and Chinese. Our approach is also applicable to languages that are not widely spoken, such as Arabic, Arabic-Indonesian, Basque, Catalan, Chinese, Croatian, Czech, Danish, Dutch, Esperanto, Finnish, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Irish Gaelic, Latin, Malay, Maltese, Norwegian Bokmål, Polish, Portuguese, Romanian, Serbian, Slovak, Slovene, Swedish, Thai, Turkish, Ukrainian, Urdu, Vietnamese, Welsh, Xhosa, Yiddish, Zulu.",0
"Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multifaceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extraction. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. Index Terms— Self-Supervised Learning, Speech Pre- Training.",1
"Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multifaceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained neural network for speech-recognition tasks, which can be trained to recognize speech in a wide variety of speech contexts. We show that this network is able to perform well in both speech and non-speech contexts, and that it is capable of performing well even in the absence of any prior knowledge of the context in which the task is to be performed. Our results suggest that the use of a universal representation for a speech task may be an effective way to improve the performance of existing neural networks.",0
"Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi- automated fact-checking that can be used for either the general public but also useful for fact- checking organisations. FacTeR-Check enables retrieving fact-checked information, unchecked claims verification and tracking dangerous information over social media. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-checking tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI19-SP that is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes.",1
"Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to the functioning of our democratic society. In order to combat misinformation, we need to ensure that the information we receive is accurate and reliable. This is especially true when it comes to social media platforms such as Facebook, Twitter, YouTube, and other online social networks. In order for us to be able to make informed decisions about what information to share with our friends and family, it is essential that we have accurate information available to us at all times. It is also important to note that there is no such thing as ""fake news"" or ""alternative facts."" In fact, there are so many different types of misinformation that it can be difficult to keep track of all of them. For this reason, the U.S. Department of Homeland Security (DHS) has created the National Center for Information Analysis and Dissemination (NCID) to provide a comprehensive database of false and misleading information on the Internet.",0
"Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One factor that significantly affects the performance of NMT is the availability of high-quality parallel corpora. However, high-quality parallel corpora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently released seven types of parallel corpora for Korean. In this study, we conduct an in-depth verification of the quality of corresponding parallel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant experiments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictionary base. To the best of our knowledge, this study is the first to use LIWC to analyze parallel corpora in the field of NMT. Our findings suggest the direction of further research toward obtaining the improved quality parallel corpora through our correlation analysis in LIWC and NMT performance.",1
"Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One factor that significantly affects the performance of NMT is the availability of high-quality parallel corpora. However, high-quality parallel corpora concerning Korean are relatively scarce compared to other languages. In this paper, we present a novel approach to improve the speed and accuracy of the MT system for Korean. We show that we can significantly improve MT performance by using a combination of two approaches: (1) a parallel-based approach, and (2) the use of a neural network. Our results demonstrate that our approach is able to achieve a significant improvement in MT speed.",0
"The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification performance, and many use data that contain potential confounds, which makes it difficult to determine their validity. Two possible confounds are data-origin (i.e., the dataset is composed of more than one source) and product ownership (i.e., reviews written by individuals who own or do not own the reviewed product). In the present study, we investigate the effect of both confounds for fake review detection. Using an experimental design, we manipulate data-origin, product ownership, review polarity, and veracity. Supervised learning analysis suggests that review veracity (60.26 - 69.87%) is somewhat detectable but reviews additionally confounded with product-ownership (66.19 - 74.17%), or with data-origin (84.44 - 86.94%) are easier to classify. Review veracity is most easily classified if confounded with product-ownership and data-origin combined (87.78 - 88.12%), suggesting overestimations of the true performance in other work. These findings are moderated by review polarity.",1
"The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification of fake reviews and the methods used to detect them. Here, we present an automated method for detecting fake products reviews based on the type of product being reviewed. We demonstrate that this method can be applied to a wide range of products, including consumer electronics, medical devices, and pharmaceuticals.",0
"Despite great success on many machine learning tasks, deep neural networks are still vulnerable to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial samples. In this framework, gradient-based continuous perturbations are added to the embedding layer and are amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a mask language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by performing transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical adversarial samples compared to strong baseline methods. All the code and data will be made public.",1
"Despite great success on many machine learning tasks, deep neural networks are still vulnerable to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of text. In this paper, we present a novel approach to train a deep convolutional neural network (CNN) on a text-based text classification task. Our approach is based on the fact that text recognition is a task that requires a large amount of training data, which can be difficult to obtain. In order to overcome this problem, our approach uses a combination of supervised and unsupervised learning techniques. We show that our method is able to perform well on both text and non-text classification tasks. Furthermore, the results are robust to a wide range of network architectures and training parameters.",0
"The inception of modeling contextual information using models such as BERT, ELMo, and Flair has significantly improved representation learning for words. It has also given SOTA results in almost every NLP task — Machine Translation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these dominant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities.",1
"The inception of modeling contextual information using models such as BERT, ELMo, and Flair has significantly improved representation learning for words. It has also given SOTA results in almost every NLP task — Machine Translation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to improving the accuracy of the models, we have also improved the performance of these models. This work has been supported by the National Science Foundation (NSF).",0
"In this work, we extensively redesign the newly introduced method of token mixing using Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementation on a long document summarization task (¿ 512 tokens). As a baseline, we also carried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are currently the state of the art methods for these type of problems. The original FNET paper implemented this in an encoder only architecture while abstractive summarization requires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we decided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the individual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summarization task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture.",1
"In this work, we extensively redesign the newly introduced method of token mixing using Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementation on a long document summarization task (¿ 512 tokens). As a baseline, we also carried out long document summarization using established vernacular methods of summarizing documents. Our results show that the new method is much faster and more efficient than the old one, and that it can be applied to a wide range of document types.",0
"Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph recategorization.",1
"Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the use of a novel model-based approach to improve the performance of AMRs. Our approach is based on the notion of an abstract meaning representation (AQS), which is defined as a set of semantic entities that can be represented by a sequence of words. An AQS is composed of two components: a lexical entity and a syntactic entity. The lexicographical entity represents the semantic content of the sentence, while the syntactically semantic entity is used to represent the meaning of each word. We propose a new approach that combines the advantages of both approaches in order to achieve better performance than either approach alone. Specifically, we use a model that is trained on a large corpus of English-language sentences, and then uses this model to extract semantic information from each of these sentences.",0
"Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning.",1
"Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear whether these models can be used to train other types of models, such as neural networks. In this paper, we present a novel approach to training a neural network model to extract semantic information from a large corpus of English words. Our approach is based on the use of a deep convolutional network (DNN) trained on a corpus consisting of over 100,000 words from the English language. We show that the DNN model is capable of extracting semantic features from this corpus, and that this feature extraction is robust to the number of words in the corpus. In addition, our approach can also be applied to other large-scale corpora, for example, to understand the structure of the human brain.",0
"Natural Language Processing (NLP) is a branch of artificial intelligence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level accuracy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words consideration.",1
"Natural Language Processing (NLP) is a branch of artificial intelligence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a vernacular analysis of a corpus of English spoken in the United States. I found that the accuracy of POS tagging was significantly higher than expected based on previous work. In addition, my results showed that POS tags were more accurate than previous results. These results suggest that NLP techniques can be used to improve human language processing.",0
"For each goal-oriented dialog task of interest, large amounts of data need to be collected for end-to-end learning of a neural dialog system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a related dialog task. Naively learning from related data fails to improve performance as the related data can be inconsistent with the target task. We describe a meta-learning based method that selectively learns from the related dialog task data. Our approach leads to significant accuracy improvements in an example dialog task.",1
"For each goal-oriented dialog task of interest, large amounts of data need to be collected for end-to-end learning of a neural dialog system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a vernacular neural network, to learn a simple neural dialogue system from scratch. In this paper we present a novel approach to learning neural networks from small data sets. Our approach is based on the idea that neural systems can be trained to perform a variety of tasks, such as speech recognition, speech synthesis, and machine translation. We demonstrate that our approach works well for learning a speech-recognition neural system, as well as a machine-translation neural model. In addition, our method can also be applied to other types of learning tasks.",0
"The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.",1
"The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise ids. We show that the loss is robust to the number of training examples and that it can be applied to a wide range of tasks, including speech recognition, natural language processing, image classification, machine translation and speech synthesis.",0
"We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner similar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments.",1
"We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner similar to traditional local policy search methods. We show that our method  urns a significant improvement in the accuracy of skill policies, and that it can be applied to a wide range of situations.",0
"We propose BERMo, an architectural modification to BERT, which makes predictions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Models (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the downstream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67× and 1.15× faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task.",1
"We propose BERMo, an architectural modification to BERT, which makes predictions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Models (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1)   it allows us to predict the topological structure of the network, and (2) it allows for the prediction of semantic features that are not present in the original network. In particular, we show that we can predict features such as the number of words in a sentence, the order in which words appear in sentences, or whether a word is a noun or a verb. These predictions can be used to improve the accuracy of language models.",0
"Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources.",1
"Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too  much performance. In this paper, we present a novel approach to reduce the cost of kernel machine learning by using a combination of stochastic gradient descent (SGD) and convolutional neural networks (CNNs). We show that SGD and CNNs can be combined to significantly improve the performance of neural network models. We also demonstrate that this combination can also be applied to other types of deep learning models, such as reinforcement learning.",0
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness.",1
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, vernacular texts have been used to measure fairness. In this paper, we present a new measure of fairness that is based on the concept of 'fairness', which is defined as 'the degree to which an act is fair to all parties involved'. We use this measure to assess the fairness of a contract between two parties. We find that a fair contract is one in which both parties are treated fairly, regardless of their gender, race, religion, sexual orientation, disability, age, or any other factor that might affect the outcome of the contract. In other words, fair contracts are one-sided in favour of one party and disadvantageous to the other. This finding is consistent with the idea that fairness is an important aspect of social interaction.",0
"Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.",1
"Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we  ightly describe the algorithm, and then we describe how it can be used in real-world applications. We show that the new algorithm can outperform existing methods, such as LSTM, in terms of speed and accuracy.",0
"In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art.",1
"In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, the T5 model allows us to model a conversation as a series of interactions between two agents, each of which is trained to recognize the other's emotions and intentions, and to respond accordingly. This approach allows for the development of a robust model that can be applied to a wide range of real-world situations.",0
"Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users’ access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems’ effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing.",1
"Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users’. In this paper, we present a novel approach for simulating simultaneous speech translations, which is based on the use of a multilayer perceptron. We demonstrate that this approach can be applied to a wide range of speech-to-text translation tasks, such as translation of text from one language to another, translation from text to speech, and translation between speech and text. Our results show that simultaneous translation is possible even in the presence of non-linearities in speech processing.",0
"We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT.",1
"We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends in financial market behavior. In this paper, we present a novel approach to the analysis of sentiment in the context of the financial sector. We demonstrate that FinEs can be used to identify patterns in sentiment across a wide range of financial instruments, including stocks, bonds, commodities, currencies, futures, options, and options contracts. Our approach is based on the notion that sentiment is an emergent property of a financial instrument and that it is important to understand how sentiment changes over time in order to predict the future behavior of that instrument. In addition, our approach can also be applied to other types of information, such as stock price movements and stock market sentiment.",0
"Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs.",1
"Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we present a method for estimating translation quality based on the number of translation errors that occur in a given translation. We find that the accuracy of this method is significantly higher than previous estimates based solely on translation error rates. Our results also suggest that it may be possible to improve the reliability of the translation system by using a more robust method, such as the one described here.",0
"Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable.",1
"Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. The approach is based on the assumption that the interaction between the user and the system is the primary determinant of the quality of dialogues. We demonstrate that this approach can be applied to a wide range of dialogue systems, including text-to-speech (TTS), voice-over-IP (VOIP), and video-conferencing (VNC). We find that our approach outperforms existing approaches in several important ways. First, we show that we are able to learn the relationships between system-level interactions, such as the number of times a user interacts with a system, the amount of time it takes to complete a given task, or the frequency with which users interact with systems. Second, our method is robust to changes in user behavior, as well as to the nature of interactions between users and system components. Finally, it is possible to use this method to improve the usability of user interfaces.",0
"Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding – a critical component of NLP applications – by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement.",1
"Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding – a critical component of NLP applications – by evaluating models against four commonsense benchmarks. We find no evidence that models perform better on these benchmarks than non-model-based models. In contrast, models outperform models on a number of other benchmarks, including Bayesian inference, classification accuracy, and word embeddings. These results suggest that the performance of models depends on their understanding of the context in which they are used.",0
"Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.",1
"Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a vernacular model. In this paper, we propose a novel approach to fine tune the generative models of UDA, which is based on the use of a deep neural network (DNN) as a learning agent. The DNN is trained with a large number of training examples from a wide range of languages, including English, French, German, Italian, Spanish, Japanese, Korean, Chinese, Russian, Arabic, Portuguese, Turkish, Hindi, Urdu, Malayalam, Gujarati, Punjabi, Bengali, Tamil, Telugu, Kannada, Marathi, Pashto, Nepali and many others. In this way, the network is able to adapt to any language, even if the language is not widely spoken in the country where it was trained.",0
"We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources.",1
"We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a vernacular dictionary of English words. We find that there is a strong correlation between the number of words in each category and the probability that a candidate verb will be used in an English sentence. In addition, we show that this correlation is robust to the presence or absence of an infinitive. Our results suggest that the use of infinitives in English sentences is more common than previously thought.",0
"We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.",1
"We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature  of testimony. In particular, we find that testimony is more likely to be interpreted by people with IB than by those without it. This finding is consistent with the idea that people who have IB tend to believe more strongly in their own testimony than those who do not.",0
"This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover’s Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes.",1
"This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover’s Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method  can be applied to a wide range of data sets, including handwritten documents, handwritten images, and handwritten text. In addition, our approach can also be used to improve the performance of other supervised learning methods, such as Bayesian inference.",0
"Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks – POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks.",1
"Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of CS is illustrated in Figure 1. Figure 1 View largeDownload slide (A) A multilanguage model is trained on a large set of languages. (B) The model learns to switch from one language to another based on the context in which it is presented. In this case, the model switches from English to Spanish, from Spanish to French, or from French to German, depending on whether the language in question is spoken in the United States or in another country. For example, in English, Spanish and French are spoken at the same time, but in German they are different languages, whereas in French the first language is German and the second is English. Note that this model does not have to be trained for each language; it can be used to train a model for any language, for example for Spanish or French.",0
"Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.",1
"Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained models in combination with machine learning techniques to improve the performance of natural language processing tasks.",0
"Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diversity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribution, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improvements in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised methods on the FewRel 2.0 benchmark.",1
"Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited  experience. In this paper, we show that this assumption is wrong. We show, for the first time, that the optimal learning algorithm for a given task is not determined by the number of training trials, but rather by how much experience is available to the learner. Our results suggest that, in practice, it is possible to learn an optimal algorithm based on limited experience.",0
"Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on transfer learning to broaden its impact. Benchmarks are dominated by a small set of frequent phenomena, leaving a long tail of infrequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently addressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualitative meta-analysis of 100 representative papers on transfer learning for NLU. Our analysis asks three questions: (i) Which long tail dimensions do transfer learning studies target? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail performance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the performance of various adaptation methods on clinical narratives to show how systematically conducted metaexperiments can provide insights that enable us to make progress along these future avenues.",1
"Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on transfer learning to broaden its impact. Benchmarks are dominated by a small set of frequent phenomena, leaving a long tail of infrequent phenomena underrepresented. In this work, we reflect on the question: have transfer vernaculars made significant progress over the past decade, and if so, what are the implications for NLU? We find that the answer is yes, but only in the sense that there has been a significant increase in both the number and the frequency of NLUs. We also find evidence for a shift in how we think about the nature of language learning. Funding: This work was supported by grants from the National Science Foundation (NSF) to J.M.D. and NSF Graduate Research Fellowship (GRF). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors have declared that no competing interests exist.",0
"The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their negative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detection tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibility of leveraging domain-specific word embedding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets",1
"The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their negative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech online. In this article, we will look at some of the most popular NLP tools and how they can be used to identify and remove hate-speech from the internet. We will also discuss how to use these tools in real-world situations.",0
This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy.,1
This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported results.,0
"While recent work on automated fact-checking has focused mainly on verifying and explaining claims, for which the list of claims is readily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive task and challenging to conduct on a frequent basis across multiple domains. This paper explores methodology to identify check-worthy claim sentences from fake news articles, irrespective of domain, without explicit sentence-level annotations. We leverage two internal supervisory signals - headline and the abstractive summary - to rank the sentences based on semantic similarity. We hypothesize that this ranking directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on either the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the article’s veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough information for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system.",1
"While recent work on automated fact-checking has focused mainly on verifying and explaining claims, for which the list of claims is readily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive and time-consuming process. To address this challenge, we developed an automated system that automatically annotates sentences based on the content of the sentence and the context in which it appears. The system is trained on a large corpus of English-language texts, and it is able to annotate sentences in a wide variety of contexts, including news articles, reviews, blog posts, news stories, tweets, emails, videos, slideshows, etc. In addition, the annotated sentences can be used to verify the accuracy of statements made by other users. For example, if a user makes a claim about an event, such as an airplane crash, then the system can automatically check whether the claim is true or false. If the statement is false, it will display an error message, indicating that the user did not make a valid claim.",0
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.",1
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on this aspect. In this paper, we present a new approach to this problem, which is based on a combination of machine learning and natural language processing (NLP) techniques. We show that this approach can be applied to a wide range of speech-to-text (STT) and STT-based systems, and that it is possible to use it to understand the content of a conversation in real-time. The aim of this work was to develop a machine-learning approach for automatic speech understanding in a large-scale, multi-layered system of ATC systems. The system consists of two main components: (1) a speech recognition system, (2) an artificial neural network (ANN), and (3) two speech processing modules (SPMs). The SPMs are trained to recognize and process the spoken words, while the ANN is used to learn the meaning of each spoken word.",0
"In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems.",1
"In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in vernacular research. In this paper, we present a novel approach to evaluate the effectiveness of a chatbot system based on the use of an automated evaluation framework. We demonstrate that the system is able to perform well in a wide variety of domains, including research, teaching, and teaching-related tasks. Our results suggest that this approach can be applied to other domains as well.",0
"This paper presents a deep neural architecture for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious questions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset.",1
"This paper presents a deep neural architecture for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that BERT still does not capture the full complexity of text, so a deeper model is needed. We also show that the BERT model can be used as a model for natural language processing.",0
"User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultaneously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study.",1
"User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different opinions about the same topic. Themes are also often used to promote products and services. For example, a company might use a theme to advertise a new product or service. In this case, it is important to understand how the theme is used in the context of the company's business, and how it relates to the product and service being promoted.",0
"Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators’ labeling behavior and to successfully resolve corresponding individual misconceptions and wrongdoings regarding the annotation task. Our contribution is an explanation and improvement for precise neural end- to-end bias modeling and ground truth estimation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has potential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators.",1
"Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators’ labeling behavior. In this paper, we present a novel method for annotating the sentiment of a large corpus of tweets. The method is based on a combination of machine learning and sentiment analysis techniques, and it is able to classify sentiment as either positive or negative. In addition, the method can also be used to annotate tweets that have been deleted from the corpus. We show that the annotated tweets are highly correlated with each other and with other tweets in the same corpus, which is consistent with the notion of sentiment clustering. Our results suggest that sentiment annotation is a powerful tool for understanding the behavior of human users.",0
"This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be claimed that this corpus is currently the largest Persian corpus that has been prepared independently for the Persian language. This corpus is presented in both raw and preprocessed forms, and based on the preprocessed corpus some word embedding models are produced. By the provided models, the hmBlogs is compared with some of the most important corpora available in Persian, and the results show the superiority of the hmBlogs corpus over the others. These evaluations also present the importance and effects of corpora, evaluation datasets, model production methods, different hyperparameters and even the evaluation methods. In addition to evaluating the corpus and its produced language models, this research also presents a semantic analogy dataset.",1
"This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be found at http://www.hmblogs.com/. The corpus is divided into two parts. The first part consists of posts from the corpus that were published between January 1, 2000, and December 31, 2016. These posts are categorized according to the type of content they contain, such as news, reviews, comments, etc. In addition, there are also posts that have been published in the past, but are no longer relevant to this corpus. For example, if a blog post is about a new product or service, it will not be included in this section. Finally, the second part contains all posts published since 2000 that contain the word ""hmm"" in their title or body text. As a result, we are able to categorize these posts in a way that allows us to compare them to each other and to other corpus posts.",0
"Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations.",1
"Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with a high degree of generality and robustness.",0
"Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on large scale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality.",1
"Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on large scale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained data sets can be challenging. In this paper, we present a novel PLM approach that allows us to fully utilize prior language knowledge while still being able to tune the model to meet the specific needs of a given dialogue style. We show that this approach can outperform previous approaches in terms of accuracy and training time. In addition, our approach has the potential to significantly improve the accuracy of language-based dialogue models.",0
"Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models’ performance in terms of Accuracy in LAMBADA and Children’s Book Test, with and without the use of model-created coreference annotations.",1
"Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with the number of models. In this article, we will look at how to reduce the cost of Transformers by using a simple model-based approach. We will also discuss the advantages and disadvantages of this approach, and how it can be applied across a wide range of applications.",0
"In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients.",1
"In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated in recent years, but few have been able to provide answers to questions such as, ""What is the difference between aspirin and ibuprofen?"" or ""How do I get rid of my acne?"". The aim of the project was to develop a system that could answer these questions in a way that was easy to understand and easy for users to use. In addition, we wanted to make the system as accessible as possible, so that it could be used by anyone, regardless of their level of education or experience. We also wanted it to be flexible enough to adapt to the needs of different users, as well as to fit into existing medical curriculums and practice guidelines. The system we developed is based on a combination of machine learning and natural language processing (NLP) techniques, which allow us to answer questions that are difficult for humans to comprehend. Our goal is to create a tool that can be easily used.",0
"In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformer based models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks.",1
"In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using recurrent neural networks (RNNs) to train and test a novel model. We show that this model can be applied to a wide range of tasks, such as speech recognition, image classification, and machine translation. Our results demonstrate that the model is robust to training errors and that it is able to perform well in a variety of contexts. Funding: This work was supported by a grant from the National Science Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors declare that no competing interests exist.",0
"Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine translation systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to utilize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmentation method, our model achieves strong performances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results.",1
"Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine translation systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of grammatical errors. In this work, we show that GEC can be used to correct grammar errors by training a neural network on a large corpus of English sentences and comparing the accuracy of the trained model with that of a natural language model trained on the same corpus. Our results demonstrate that the training of neural networks on large corpora of sentences is feasible, and that it is possible to train such a network to perform the task in real-time.",0
"State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This rises the question whether different, possibly time-specific, representations of literal meanings may impact on the metaphor detection task. To the best of our knowledge, this is the first study which examines the metaphor detection task with a detailed explorative analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned to different state-of-the-art approaches. The results suggest that different word embeddings do impact on the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. However, results also suggest that temporal word embeddings may provide representations of words’ core meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented.",1
"State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural vernacular usage, linguistic evolution, and the evolution of the human brain. In this paper, we present a novel neural network approach for the detection of literal and contextual metaphors. We show that this approach can be applied to a wide range of linguistic contexts, including English, French, German, Italian, Spanish, Portuguese, Russian, Japanese, Korean, Chinese, Arabic, Hebrew, Hindi, Urdu, Persian, Greek, Latin, Sanskrit, Malayalam, Tamil, Telugu, Bengali, Punjabi, Gujarati, Kannada, Marathi, Pashto, Dari, Tagalog, Bhojpuri, Farsi, Hmong, Khmer, Burmese, Nepalese, Tibetan, Thai, Vietnamese, Cambodian, Guayana, Tigrinya, Xhosa, Yiddish, Swah.",0
"Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media.",1
"Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). We show that the LSTM-based classification algorithm can be used to categorise texts with a high degree of accuracy. We also demonstrate that this approach can also be applied to other types of text, including images, videos, and audio files.",0
"Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based model with a Fusion of Adapter layers which leverages knowledge from more simple sentiment analysis tasks to improve the emotion detection task on large scale dataset, such as CMU-MOSEI, using the textual modality only. Results show that our proposed method is competitive with other approaches. We obtained state-of-the-art results for emotion recognition on CMU-MOSEI even while using only the textual modality.",1
"Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based emotion classification algorithm that can be used to identify complex emotions such as anger, fear, disgust, and sadness. We demonstrate that the algorithm is able to discriminate between these emotions based on the content of the text. Our results indicate that this algorithm can also be applied to other types of emotions, including disgust and anger.",0
"Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is received by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo.",1
"Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with the same phonetic and phonological features as the speaker's native language. In addition, we show that this approach can be applied to other languages as well.",0
"Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for the NER tasks to minimize the annotation cost without sacrificing model performance. However, heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose AL sentence query evaluation functions which pay more attention to possible positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize too long or too short sentences. Our experiments on three datasets from different domains reveal that the proposed approaches reduce the number of annotated tokens while achieving better or comparable prediction performance with conventional methods. Keywords Active learning, Named entity recognition, Annotation cost, Semi-supervised clustering.",1
"Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), on the other hand, can be used to train a deep neural network on a large corpus of text. In this paper, we present a novel approach to classify text based on its semantic content. We show that we can train an active learning model to correctly classify the content of a text as follows: (1) we train the model with a set of words from the corpus, and (2) after training, the training set is used as the input for the classification task. The classification accuracy of the trained model is shown to be significantly higher than that of an AL model trained on an unlabeled corpus. Our results demonstrate that AL is a promising alternative to traditional text classification methods.",0
"In this work we consider the col- lection of deceptive April Fools’ Day (AFD) news articles as a useful addition in existing datasets for deception detection tasks. Such collections have an established ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current research thread, we also discuss the individualism/collectivism dimension in deception with respect to these two datasets. Lastly, we build classifiers by testing vari- ous monolingual and cross lingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observations of other deception detection works.",1
"In this work we consider the col- lection of deceptive April Fools’ Day (AFD) news articles as a useful addition in existing datasets for deception detection tasks. Such collections have an established ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that can be used to detect AFD news stories in a variety of languages, including English, French, German, Italian, Japanese, Korean, Russian, Spanish, and Chinese. We find that the corpus is robust and robustly discriminative in terms of the language in which it is used. Our results suggest that a language-based approach may be a promising tool for the detection of fake news.",0
"Recent methods in speech and language technology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent.",1
"Recent methods in speech and language technology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource vernaculars. This work is part of a larger project to build a large-scale speech-to-text (SLT) system for a wide range of low-resource languages. The SLT system is intended to be used in a variety of contexts, including education, business, government, and industry.",0
"Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efficient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classification datasets in four domains, TLM achieves results better than or similar to pretrained language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development.",1
"Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efficient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM  can be trained on a small subset of labeled data, and can be applied to any dataset. TLM is based on the idea that a language model should be able to predict the meaning of words in a given context. For example, if we want to learn how to pronounce a word, we need to know the context in which the word is used. In this context, it is important to understand how words are used in different contexts, so that we can predict how they will be used by the learner in the future. To do this, the model needs to have some knowledge about the language in question, such as its grammatical structure, phonology, syntax, semantics, etc.",0