text,#label
"Abstract An overarching goal of natural language pro- cessing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an in- teractive process known as repair: asking ques- tions and seeking clarification until their uncer- tainty is resolved. We propose a framework for building a visually grounded question- asking model capable of producing polar (yes- no) clarification questions to resolve misunder- standings in dialogue. Our model uses an ex- pected information gain objective to derive in- formative questions from an off-the-shelf im- age captioner without requiring any supervised question-answer data. We demonstrate our model’s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human an- swerers. Introduction Human-machine interaction relies on accurate transfer of knowledge from users. However, nat- ural language input can be ambiguous or unclear, giving rise to uncertainty. A fundamental aspect of human communication is collaborative grounding, or seeking and providing incremental evidence of mutual understanding through dialog. Specifically, humans can correct for uncertainty through cooper- ative repair (Clark, 1996; Purver et al., 2002; Arkel et al., 2020) which involves interactively asking questions and seeking clarification. Making and recovering from mistakes collaboratively through question-asking is a key ingredient in grounding meaning and therefore an important feature in di- alog systems (Benotti and Blackburn, 2021). In this work, we focus on the computational chal- lenge of generating clarification questions in visu- ally grounded human-machine interactions. One popular approach is to train an end-to-end model to map visual and linguistic inputs directly to questions (Yao et al., 2018; Das et al., 2017). This approach is heavily data-driven, requiring large annotated training sets of questions under different goals and contexts. Another approach has drawn from work on active learning and Optimal Experiment Design (OED) in cognitive science to search for questions that are likely to maximize ex- pected information gain from an imagined answerer (Wang and Lake, 2019; Lee et al., 2018; Misra et al., 2018; Rao and Daumé III, 2018; Rothe et al., 2017;Kovashka and Grauman, 2013). Much of this work has relied on large-scale question-answer datasets (Kumar and Black, 2020; de Vries et al., 2017) for training or retrieval to propose candidate ques- tions or evaluate their expected utility. Others, like (Yu et al., 2020), derive questions from attribute annotations for domain-specific systems. In this paper, we address an open-domain setting where one cannot rely on an immediate grounding of the meaning of questions in the target domain (in contrast to end-to-end approaches, which assume examples of questions to train on, or semantic pars- ing approaches, which assume a logical form for questions). Our key contribution is a lightweight method to ground question semantics in the open image domain without observing question exam- ples. Instead, our framework builds a visually grounded question-asking model from image cap- tioning data, deriving question selection and belief updating without existing semantics. Our model generates candidate polar questions, arguably the most common form of clarification in dialogue (Stivers, 2010), by applying rule-based linguistic transformations to the outputs of a pretrained image captioner. We then use self-supervision to train a re- sponse model that predicts the likelihood of differ- ent answers. Given these predictions, we estimate the expected information gain of each question and select the question with the highest utility. We demonstrate our method’s ability to pose questions that improve communicative success in a question- driven communication game with synthetic and human answerers. Conclusions We introduce a question generation framework ca- pable of producing open-domain clarification ques- tions. Instead of relying on specialized question- answer training data or pre-specified question meanings, our model uses a pretrained image cap- tioner in conjunction with expected information gain to produce informative questions for unseen images. We demonstrate the effectiveness of this method in a question-driven communication game with synthetic and human answerers. We found it important to generate questions varying in speci- ficity by decomposing captioner utterances into component noun phrases. Having generated this set of potential questions, selecting based on esti- mated information gain yielded useful questions. Without seeing question examples, our framework demonstrates a capacity for generating effective clarification questions. Future research should aim to generate more diverse question sets, allow for more expressive answers, and address abstract properties of objects within images. One approach, as demonstrated by our preliminary work with ‘what’-questions, would be to extend our framework to incorporate additional types of wh-questions. Integrating this clarification capacity more fully into collaborative, goal-directed dialog agents will allow them to en- gage in cooperative repair.",1
"Abstract An overarching goal of natural language pro- cessing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an in- teractive process known as repair: asking ques- tions and seeking clarification until their uncer- tainty is resolved. We propose a framework for building a visually grounded question- asking model capable of producing polar (yes- no) clarification questions to resolve misunder- standings in dialogue. Our model uses an ex- pected information gain objective to derive in- formative questions from an off-the-shelf im- age captioner without requiring any supervised question-answer data. We demonstrate our modelÃ¢ÂÂs ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human an- swerers. Introduction Human-machine interaction relies on accurate transfer of knowledge from users. However, nat- ural language input can be ambiguous or unclear, giving rise to uncertainty. A fundamental aspect of human communication is collaborative grounding, or seeking and providing incremental evidence of mutual understanding through dialog. Specifically, humans can correct for uncertainty through cooper- ative repair (Clark, 1996; Purver et al., 2002; Arkel et al., 2020) which involves interactively asking questions and seeking clarification. Making and recovering from mistakes collaboratively through question-asking is a key ingredient in grounding meaning and therefore an important feature in di- alog systems (Benotti and Blackburn, 2021). In this work, we focus on the computational chal- lenge of generating clarification questions in visu- ally grounded human-machine interactions. One popular approach is to train an end-to-end model to map visual and linguistic inputs directly to questions (Yao et al., 2018; Das et al., 2017). This approach is heavily data-driven, requiring large annotated training sets of questions under different goals and contexts. Another approach has drawn from work on active learning and Optimal Experiment Design (OED) in cognitive science to search for questions that are likely to maximize ex- pected information gain from an imagined answerer (Wang and Lake, 2019; Lee et al., 2018; Misra et al., 2018; Rao and DaumeÃÂ III, 2018; Rothe et al., 2017;Kovashka and Grauman, 2013). Much of this work has relied on large-scale question-answer datasets (Kumar and Black, 2020; de Vries et al., 2017) for training or retrieval to propose candidate ques- tions or evaluate their expected utility. Others, like (Yu et al., 2020), derive questions from attribute annotations for domain-specific systems. In this paper, we address an open-domain setting where one cannot rely on an immediate grounding of the meaning of questions in the target domain (in contrast to end-to-end approaches, which assume examples of questions to train on, or semantic pars- ing approaches, which assume a logical form for questions). Our key contribution is a lightweight method to ground question semantics in the open image domain without observing question exam- ples. Instead, our framework builds a visually grounded question-asking model from image cap- tioning data, deriving question selection and belief updating without existing semantics. Our model generates candidate polar questions, arguably the most common form of clarification in dialogue (Stivers, 2010), by applying rule-based linguistic transformations to the outputs of a pretrained image captioner. We then use self-supervision to train a re- sponse model that predicts the likelihood of differ- ent answers. Given these predictions, we estimate the expected information gain of each question and select the question with the highest utility. We demonstrate our methodÃ¢ÂÂs ability to pose questions that improve communicative success in a question- driven communication game with synthetic and human answerers. Conclusions We introduce a question generation framework ca- pable of producing open-domain clarification ques- tions. Instead of relying on specialized question- answer training data or pre-specified question meanings, our model uses a pretrained image cap- tioner in conjunction with expected information gain to produce informative questions for unseen images. We demonstrate the effectiveness of this method in a question-driven communication game with synthetic and human answerers. We found it important to generate questions varying in speci- ficity by decomposing captioner utterances into component noun phrases. Having generated this set of potential questions, selecting based on esti- mated information gain yielded useful questions. Without seeing question examples, our framework demonstrates a capacity for generating effective clarification questions. Future research should aim to generate more diversesets, allow for more expressive answers, and address abstract properties of objects within images. One approach, as demonstrated by our preliminary work with what questions, would be to extend our framework to incorporate additional types of wh-questions. Integrating this clarification capacity more fully into collaborative, goal-directed dialog agents will allow them to en- gage in cooperative, goal-directed dialog agents.",0
"Abstract While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest “I enjoyed the game this weekend”: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker’s broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive languagemodeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines. Introduction Language models are at the very heart of many modern NLP systems and applications (Young et al., 2018). Representations derived from largescale language models are used widely in many downstream NLP models (Peters et al., 2018; Devlin et al., 2019). However, an implicit assumption made in most modern NLP systems (including language models) is that language is independent of extra-linguistic context such as speaker/author identity and their social setting. While this simplifying assumption has undoubtedly encouraged remarkable progress in modeling language, there is overwhelming evidence in socio-linguistics that language understanding is influenced by the social context in which language is grounded (Nguyen et al., 2016; Hovy, 2018; Mishra et al., 2018; Garten et al., 2019; Flek, 2020; Bender and Koller, 2020). In fact, language use on social media where every utterance is grounded in a specific social context (like time, geography, social groups, communities) reinforces this often ignored aspect of language. When NLP applications ignore this social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021). Prior attempts to better leverage the social context surrounding language while learning language representations have mostly focused on learning social context dependent word embeddings and have been primarily used to characterize language variation across many dimensions (time, geography, and demographics). These methods learn word embeddings for each specific social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b). However, word embedding based approaches in general suffer from two fundamental limitations: (a) word embeddings are not linguistically contextualized as noted by Peters et al. (2018) (b) word embedding learning is transductive – they can only generate embeddings for words observed during training and usually assume a finite word vocabulary and a set of social contexts all of which need to be seen during training. Recent approaches have addressed the first limitation by learning word representations that are contextualized by their tokenspecific usage context (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b,a). The second limitation has been addressed by WordPiece tokenization methods (Schuster and Nakajima, 2012; Devlin et al., 2019; Liu et al., 2019). While these approaches have successfully captured linguistic context, they still do not capture social context in language representations.2 “How can we learn linguistically contextualized and socially contextualized language representations?” is the question we seek to answer in this paper. We propose LMSOC to (a) learn representations of tokens that are both linguistically contextualized and socially sensitive and (b) enable the language model to inductively generate representations for language grounded in social contexts it has never observed during the language model pre-training process. As an example, our model can enable NLP systems to associate the right entity being referred to based on the broader user/social context in which an utterance like “Our Prime Minister visited the UK last week.” is grounded.  Conclusion We proposed a method to learn socially sensitive contextualized representations from large-scale language models. Our method embeds social context in continuous space using graph representation algorithms and proposes a simple but effective socially sensitive pre-training approach. Our approach thus enables language models to leverage correlations between social contexts and thus generalize better to social contexts not observed in training. More broadly, our method sets the stage for future research on incorporating new types of social contexts and enabling NLP systems like personalized predictive typing systems and entity-linking systems to better accommodate language variation.",1
"Abstract While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a clozetest I enjoyed the game this weekend: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speakers broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive languagemodeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines. Introduction Language models are at the very heart of many modern NLP systems and applications (Young et al., 2018). Representations derived from largescale language models are used widely in many downstream NLP models (Peters et al., 2018; Devlin et al., 2019). However, an implicit assumption made in most modern NLP systems (including language models) is that language is independent of extra-linguistic context such as speaker/author identity and their social setting. While this simplifying assumption has undoubtedly encouraged remarkable progress in modeling language, there is overwhelming evidence in socio-linguistics that language understanding is influenced by the social context in which language is grounded (Nguyen et al., 2016; Hovy, 2018; Mishra et al., 2018; Garten et al., 2019; Flek, 2020; Bender and Koller, 2020). In fact, language use on social media where every utterance is grounded in a specific social context (like time, geography, social groups, communities) reinforces this often ignored aspect of language. When NLP applications ignore this social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021). Prior attempts to better leverage the social context surrounding language while learning language representations have mostly focused on learning social context dependent word embeddings and have been primarily used to characterize language variation across many dimensions (time, geography, and demographics). These methods learn word embeddings for each specific social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b). However, word embedding based approaches in general suffer from two fundamental limitations: (a) word embeddings are not linguistically contextualized as noted by Peters et al. (2018) (b) word embedding learning is transductive Ã¢ÂÂ they can only generate embeddings for words observed during training and usually assume a finite word vocabulary and a set of social contexts all of which need to be seen during training. Recent approaches have addressed the first limitation by learning word representations that are contextualized by their tokenspecific usage context (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b,a). The second limitation has been addressed by WordPiece tokenization methods (Schuster and Nakajima, 2012; Devlin et al., 2019; Liu et al., 2019). While these approaches have successfully captured linguistic context, they still do not capture social context in language representations.2 Ã¢ÂÂHow can we learn linguistically contextualized and socially contextualized language representations? is the question we seek to answer in this paper. We propose LMSOC to (a) learn representations of tokens that are both linguistically contextualized and socially sensitive and (b) enable the language model to inductively generate representations for language grounded in social contexts it has never observed during the language model pre-training process. As an example, our model can enable NLP systems to associate the right entity being referred to based on the broader user/social context in which an utterance likeÃÂ is grounded. Conclusion We proposed a method to learn socially sensitive contextualized representations from large-scale language models. Our method embeds social context in continuous space usingrepresentation algorithms and proposes a simple but effective socially sensitive pre-training approach. Our approach thus enables language models to leverage correlations between social contexts and thus generalize better to social contexts not observed in training. More broadly, our method sets the stage for future research on incorporating new types of representations and representations for many NLP tasks. More generally, our method Sets the Stage for Future Research.",0
"ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in this work that this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2020) trained with the SpanBERT objective for text (Joshi et al., 2020) and the w2v-BERT (Chung et al., 2021) objective for speech. We show that a model using only self-supervised objectives leads to good performance on both modalities, but is outperformed by mono-modal pre-trained models, suffering from significant transfer interference. To reduce the gap, we leverage supervised alignment losses, specifically a translation language model (Conneau & Lample, 2019; Zheng et al., 2021) and speech-text matching (Li et al., 2021) loss. We train our model in a multi-task fashion with the self-supervised and alignment losses. This leads to performance competitive with the state-of-the-art on SpeechStew and LibriSpeech ASR and on CoVoST 2 speech translation tasks. On speech translation, we demonstrate further quality improvements by continuing pre-training on speech-only, outperforming previous approaches by 1 BLEU on average. On text tasks, our joint model loses quality compared to equivalent mono-modal pre-trained models, but remains competitive with initial BERT results (Devlin et al., 2019), demonstrating the capacity limitations with modeling two high-resource modalities simultaneously. To the best of our knowledge, our work is the first to study and underline the benefits and limitations of speech-text unsupervised pre-training over mono-modal models, on various speech and text downstream tasks. Our initial results set a new challenge in multimodal self-supervised language understanding. DISCUSSION In this work, we demonstrate that a single encoder model can be pre-trained to learn strong contextualized representations of speech and text simultaneously. We combine self-supervised learning objectives for text (BERT) and self-supervised approaches for speech (w2v-BERT) to learn a joint Speech and LAnguage Model (SLAM). Downstream evaluations on speech and language understanding tasks, including LibriSpeech and SpeechStew ASR, CoVoST 2 speech translation, four GLUE tasks, and text-normalization uncover significant interference challenges when pre-training simultaneously on high-resource modalities. Using alignment losses such as translation language modeling and speech-text matching which leverage speech-text supervised aligned data, we show that we can improve the cross-modal representation alignment and improve over mono-modal models on the speech translation tasks, while maintaining state-of-the-art performance on speech recognition. We hope that this work would motivate further research on extending the universality of self-supervised learning of language representations to the multimodal speech-text setting.",1
"ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in thisthat this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2020b) trained with the SpanBERT objective for text (Joshi et al., 2020), and the w2v-BERT (Wang et al., 2021) objective for speech. We show that incorporating both objective points on speech and text during pre-training can significantly improve the quality of the learned representations, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020).",0
"Abstract This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language’s parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016; Johnson et al., 2017). Multilingual NMT is a compelling approach in production, as one only needs to train, deploy and maintain one model (instead of 2 × N ones, where N is the number of languages). It has also been shown to improve MT quality for low-resource languages (at the cost of a slight degradation for high-resource languages) and it can allow translation between languages that have no aligned data (“zero-shot translation”). However, such models can be costly to train, as they usually involve larger architectures and large datasets. Moreover, because they are trained jointly on all the languages, they require to know in advance the full set of languages. Adding a new language to an existing model usually means retraining the model on the full multilingual dataset. Naively fine-tuning the original model on the new language’s data is not an option because of vocabulary mismatch (the shared vocabulary needs to be modified to include the new language’s tokens) and catastrophic forgetting (the model will quickly forget how to translate in the other languages). In this paper, we study the problem of multilingual NMT incremental training or continual learning and propose a novel way to efficiently add a new source or target language. Some desirable properties of an incremental training method are: • No degradation on the existing language pairs; • Efficient training (e.g., no re-training on the existing language pairs); • Minimal amount of added parameters: the approach should scale to many languages and the model fit on a single GPU; • Minimal degradation in inference speed; • Good zero-shot performance: when training with X-EN (or EN-X) data, where X is a new language, we would like the model to be able to translate from X to any known language Y (resp. from Y to X). We propose a novel technique for incrementally adding a new source or target language, which consists in substituting the shared embedding matrix with a language-specific embedding matrix, which is fine-tuned on the new language’s data only while freezing the other parameters of the model. In some cases (e.g., when the new language is on the target size), a small number of additional parameters (e.g., adapter modules) have to be trained to match the performance of the re-training baseline. We perform two sets of experiments, with a 20-language Transformer Base trained on TED Talks, and a 20- language Transformer Big (with deep encoder and shallow decoder) trained on ParaCrawl; and show that this approach is fast and parameter-efficient and that it performs as well or better as the more costly alternatives. We propose a new technique for incrementally training multilingual NMT models on a new source or target language. It consists in creating a new monolingual BPE vocabulary for that language, substituting the shared embedding matrix with language-specific embeddings, and training those while freezing the other model parameters. At inference, translating in any of the initial languages is done by using the initial shared embeddings, and translating in the new language is done by using the newly trained embeddings. This approach does not change performance on the initial languages as the initial parameters are kept aside and not modified. For new source languages, it can achieve close performance to the more costly and less flexible bilingual and re-training baselines. For new target languages, this technique can be combined with language-specific parameters (finetuned Transformer layers or adapter modules) to match baseline performance at a small parameter cost. We validate this technique on two sets of experiments: small-scale on TED Talks and largescale on ParaCrawl; and show that it is compatible with two architectures: Transformer Base 6-6 and Big 12-2. We also show that incremental training on data aligned with English is enough to learn to translate between the new language and any of the initial languages. Translation between a new source and a new target language is also possible by combining their respective parameters at inference.",1
"Abstract This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new languages parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016; Johnson et al., 2017). Multilingual NMT is a compelling approach in production, as one only needs to train, deploy and maintain one model (instead of 2 N ones, where N is the number of languages). It has also been shown to improve MT quality for low-resource languages (at the cost of a slight degradation for high-resource languages) and it can allow translation between languages that have no aligned data (Ã¢ÂÂzero-shot translationÃ¢ÂÂ). However, such models can be costly to train, as they usually involve larger architectures and large datasets. Moreover, because they are trained jointly on all the languages, they require to know in advance the full set of languages. Adding a new language to an existing model usually means retraining the model on the full multilingual dataset. Naively fine-tuning the original model on the new languages data is not an option because of vocabulary mismatch (the shared vocabulary needs to be modified to include the new language) and catastrophic forgetting (the model will quickly forget how to translate in the other languages). In this paper, we study the problem of multilingual NMT incremental training or continual learning and propose a novel way to efficiently add a new source or target language. Some desirable properties of an incremental training method are: Ã¢ÂÂ¢ No degradation on the existing language pairs; Efficient training (e.g., no re-training on the existing language pairs); Minimal amount of added parameters: the approach should scale to many languages and the model fit on a single GPU; Minimal degradation in inference speed; Good zero-shot performance: when training with X-EN (or EN-X) data, where X is a new language, we would like the model to be able to translate from X to any known language Y (resp. from Y to X). We propose a novel technique for incrementally adding a new source or target language, which consists in substituting the shared embedding matrix with a language-specific embedding matrix, which is fine-tuned on the new languages data only while freezing the other parameters of the model. In some cases (e.g., when the new language is on the target size), a small number of additional parameters (e.g., adapter modules) have to be trained to match the performance of the re-training baseline. We perform two sets of experiments, with a 20-language Transformer Base trained on TED Talks, and a 20- language Transformer Big (with deep encoder and shallow decoder) trained on ParaCrawl; and show that this approach is fast and parameter-efficient and that it performs as well or better as the more costly alternatives. We propose a new technique for incrementally training multilingual NMT models on a new source or target language. It consists in creating a new monolingual BPE vocabulary for that language, substituting the shared embedding matrix with language-specific embeddings, and training those while freezing the other model parameters. At inference, translating in any of the initial languages is done by using the initial shared embeddings, and translating in the new language is done by using the newly trained embeddings. This approach does not change performance on the initial languages as the initial parameters are kept aside and not modified. For new source languages, it can achieve close performance to the more costly and less flexible bilingual and re-training baselines. For new target languages, this technique can be combined with language-specific parameters (finetuned Transformer layers or adapter modules) to match baseline performance at a smallcost. We validate this technique on two sets of experiments: small-scale on TED Talks and largescale on ParaCrawl; and show that it is compatible with two architectures: Transformer Base 6-6 and Big 12-2. We also show that incremental training on data aligned with English is enough to learn to translate between the new source and the new source. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016).",0
"Abstract This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, named S-APIR, where outlier detection models are investigated to properly handle various genres of news articles. Moreover, we propose a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 years’ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time. Keywords: business sentiment, sentiment analysis, deep learning, text analytics Introduction In Japan, there exist business sentiment indices, such as Economy Watchers Survey1 and Short-term Economic Survey of Principal Enterprise2 conducted by the Government and the Bank of Japan, respectively. These diffusion indices (DI) play a crucial role in decision-making for governmental/monetary policies, industrial production planning, institutional/private investment, and so on. However, these DIs rely on traditional surveys, which are costly and time-consuming to conduct. For example, Economy Watchers Survey is carried out in 12 regions of Japan, where 2,050 preselected respondents who can observe the regional business/economic conditions (e.g., store owners and taxi drivers) fill out a questionnaire and then an investigative organization in each region aggregates the surveys and calculates a DI. As the survey and subsequent processes take time, the DI is published only monthly. On the other hand, so-called alternative data, including merchandise sales, news, micro-blogs, query logs, credit card transactions, GPS location information, and satellite images, are constantly generated and accumulated. The availability of such data has accelerated the development of data-driven artificial intelligence (AI) models and techniques represented by deep learning. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys (Chen et al., 2019; Jain, 2019). For example, point of sales (POS) data have been used for estimating consumer price index (CPI) (Watanabe & Watan-abe, 2014); financial and economic reports for business sentiment (Yamamoto & Matsuo, 2016); newspaper for stock prices (Li et al., 2020; Picasso et al., 2019; Yoshihara et al., 2014, 2016), socio-economic indicators (Chakraborty et al., 2016), consumer sentiment (Shapiro et al., 2020); and social media for stock prices (Bollen et al., 2011; Derakhshan & Beigy, 2019; Levenberg et al., 2014). This work focuses on textual data and uses daily newspaper articles to develop a new business sentiment index, named the S-APIR index. In addition, using the computed index, we propose an approach to temporally analyzing the influence of an arbitrary event on business sentiment. The remainder of the paper is structured as follows: Section 2 introduces the related work on sentiment analysis in general and its applications to market sentiment and business sentiment prediction. Section 3 states the research objectives pursued in the present work. Section 4 details our proposed approach to forecasting business sentiment index and describes how to temporally analyze the contribution of a given event to business sentiment index based on predicted business sentiment scores. Section 5 conducts evaluative experiments using over 12 years’ worth of newspaper articles and discusses the properties of S-APIR, in addition to word-level temporal analysis. Section 6 discusses the implications and findings of this work. Section 7 concludes with a brief summary and possible future directions. Conclusions This paper reported our work to develop a new business sentiment index, called S-APIR. The main contribution of this work is threefold: Firstly, we proposed an approach to capturing business sentiment based on news texts and empirically validated it in comparison with an existing survey-based index. Secondly, we thoroughly studied the properties of the proposed index. Lastly, we illustrated how the predicted business sentiment can be used by policymakers and economists when it was broken down into individual events. The following describes, more specifically, the contribution from methodological, theoretical, and practical viewpoints. The methodological contribution is that we devised an effective framework composed of outlier detection and prediction models. The former used one-class SVM to identify news texts related to the economy and the latter was a BERT model fine-tuned on Economy Watchers Survey to predict the sentiment score of input news text. Another contribution is that we proposed an approach to analyzing the effect of an event represented by an individual/compound word on business sentiment. Next, the theoretical contribution is that business sentiment was shown to be accurately measured by news articles instead of a traditional, large-scale survey. Our evaluation using the Nikkei Newspaper demonstrated that S-APIR had a strong positive correlation with an existing business sentiment index, EWDI, up to 0.937. Also, the result suggested that the S-APIR index more accurately reflects business sentiment in industries. Then, the practical contribution is that S-APIR does not require a costly survey and can be computed much more frequently than monthly EWDI with almost no time lag. From the comparison with other business conditions indicators, it was revealed that S-APIR is useful as a leading index of actual consumption activity especially during major economic events such as the global financial crisis and the COVID-19 recession. Also, it could help us examine what factors cause economic fluctuations for specific periods. With several example events, such as “Tokyo Olympics”, it was demonstrated that S-APIR can be useful for economists or policymakers to measure the impact of any event of their interest on business sentiment over time to promptly respond to, if any, their negative effects when necessary. The findings of this study, however, should be considered in the light of the following limitations: • Both the outlier detection and sentiment analysis models are learned from the past Economy Watchers Survey responses and thus potentially suffered from new words unknown to the models. To keep up with the latest events, these models need to be regularly updated every time new survey data are available. • Similarly, while the usefulness of the S-APIR index was demonstrated, the evaluation was done retrospectively on the historical data. Ideally, it should be evaluated by prospective users on ongoing events with a real-time system where the S-APIR index is dynamically updated as breaking news comes in. • Predicted values of the S-APIR index depend on news texts we feed to the model. Currently, we use the Nikkei newspaper, which resulted in a strong correlation with EWDI, but feeding a different newspaper yields a different result as witnessed in Section 5.5. The difference may come from the different coverage of different newspapers but we do not know exactly if that is the case. For example, it might be caused by different political stances or tones of different newspapers. We plan to investigate it further in future work. We are currently working on automatically collecting online news and applying our models to nowcast daily business sentiment and are planning to provide a temporal analysis tool to be used by economists.",1
"Abstract This paper presents an approach to measuring business sentiment based on textual data. Business sentiment has been measured by traditional surveys, which are costly and time-consuming to conduct. To address the issues, we take advantage of daily newspaper articles and adopt a self-attention-based model to define a business sentiment index, named S-APIR, where outlier detection models are investigated to properly handle various genres of news articles. Moreover, we propose a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 yearsÃ¢ÂÂ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time. Keywords: business sentiment, sentiment analysis, deep learning, text analytics Introduction In Japan, there exist business sentiment indices, such as Economy Watchers Survey1 and Short-term Economic Survey of Principal Enterprise2 conducted by the Government and the Bank of Japan, respectively. These diffusion indices (DI) play a crucial role in decision-making for governmental/monetary policies, industrial production planning, institutional/private investment, and so on. However, these DIs rely on traditional surveys, which are costly and time-consuming to conduct. For example, Economy Watchers Survey is carried out in 12 regions of Japan, where 2,050 preselected respondents who can observe the regional business/economic conditions (e.g., store owners and taxi drivers) fill out a questionnaire and then an investigative organization in each region aggregates the surveys and calculates a DI. As the survey and subsequent processes take time, the DI is published only monthly. On the other hand, so-called alternative data, including merchandise sales, news, micro-blogs, query logs, credit card transactions, GPS location information, and satellite images, are constantly generated and accumulated. The availability of such data has accelerated the development of data-driven artificial intelligence (AI) models and techniques represented by deep learning. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys (Chen et al., 2019; Jain, 2019). For example, point of sales (POS) data have been used for estimating consumer price index (CPI) (Watanabe & Watan-abe, 2014); financial and economic reports for business sentiment (Yamamoto & Matsuo, 2016); newspaper for stock prices (Li et al., 2020; Picasso et al., 2019; Yoshihara et al., 2014, 2016), socio-economic indicators (Chakraborty et al., 2016), consumer sentiment (Shapiro et al., 2020); and social media for stock prices (Bollen et al., 2011; Derakhshan & Beigy, 2019; Levenberg et al., 2014). This work focuses on textual data and uses daily newspaper articles to develop a new business sentiment index, named the S-APIR index. In addition, using the computed index, we propose an approach to temporally analyzing the influence of an arbitrary event on business sentiment. The remainder of the paper is structured as follows: Section 2 introduces the related work on sentiment analysis in general and its applications to market sentiment and business sentiment prediction. Section 3 states the research objectives pursued in the present work. Section 4 details our proposed approach to forecasting business sentiment index and describes how to temporally analyze the contribution of a given event to business sentiment index based on predicted business sentiment scores. Section 5 conducts evaluative experiments using over 12 yearsÃ¢ÂÂ worth of newspaper articles and discusses the properties of S-APIR, in addition to word-level temporal analysis. Section 6 discusses the implications and findings of this work. Section 7 concludes with a brief summary and possible future directions. Conclusions This paper reported our work to develop a new business sentiment index, called S-APIR. The main contribution of this work is threefold: Firstly, we proposed an approach to capturing business sentiment based on news texts and empirically validated it in comparison with an existing survey-based index. Secondly, we thoroughly studied the properties of the proposed index.we illustrated how the predicted business sentiment can be used by policymakers and economists when it was broken down into individual events. The following describes, more specifically, the contribution from methodological, theoretical, and practical viewpoints. The methodological contribution is that we devised an effective framework composed of outlier detection and prediction models. The former used one-class model to define a business sentiment index, which is now a self-analysis model. The second contribution is that we proposed a simple approach to temporally analyzing how much any given event contributed to the predicted business sentiment index. To demonstrate the validity of the proposed approach, an extensive analysis is carried out on 12 yearsÃ¢ÂÂ worth of newspaper articles. The analysis shows that the S-APIR index is strongly and positively correlated with established survey-based index (up to correlation coefficient r = 0 .937) and that the outlier detection is effective especially for a general newspaper. Also, S-APIR is compared with a variety of economic indices, revealing the properties of S-APIR that it reflects the trend of the macroeconomy as well as the economic outlook and sentiment of economic agents. Moreover, to illustrate how S-APIR could benefit economists and policymakers, several events are analyzed with respect to their impacts on business sentiment over time. Keywords: business sentiment, sentiment analysis, deep learning, text analytics Introduction In Japan, there exist business sentiment indices, such as Economy Watchers Survey1 and Short-term Economic Survey of Principal Enterprise2 conducted by the Government and the Bank of Japan, respectively. These diffusion indices (DI) play a crucial role in decision-making for governmental/monetary policies, industrial production planning, institutional/private investment, and so on. However, these DIs rely on traditional surveys, which are costly and time-consuming to conduct. For example, Economy Watchers Survey is carried out in 12 regions of Japan, where 2,050 preselected respondents who can observe the regional business/economic conditions (e.g., store owners and taxi drivers) fill out a questionnaire and then an investigative organization in each region aggregates the surveys and calculates a DI. As the survey and subsequent processes take time, the DI is published only monthly. On the other hand, so-called alternative data, including merchandise sales, news, micro-blogs, query logs, credit card transactions, GPS location information, and satellite images, are constantly generated and accumulated. The availability of such data has accelerated the development of data-driven artificial intelligence (AI) models and techniques represented by deep learning. In econometrics, there is a growing interest in future/current forecasts of economic and financial indices by using such alternative, large-scale data instead of traditional surveys (Chen et al., 2019; Jain, 2019). For example, point of sales (POS) data have been used for estimating consumer price index (CPI) (Watanabe & Watan-abe, 2014); financial and economic reports for business sentiment (Yamamoto & Matsuo, 2016); newspaper for stock prices (Li et al., 2020; Picasso et al., 2019; Yoshihara et al., 2014, 2016), socio-economic indicators (Chakraborty et al., 2016), consumer sentiment (Shapiro et al., 2020); and social media for stock prices (Bollen et al., 2011; Derakhshan & Beigy, 2019; Levenberg et al., 2014). This work focuses on textual data and uses daily newspaper articles to develop a new business sentiment index, named the S-APIR index.",0
"Abstract Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online. Introduction Web sources, such as social networks, internet forums, and customer reviews from online shops, provide large amounts of unstructured text data. Along with the steady development of new platforms and the increasing number of internet users, the interest in methods that automatically extract the expressed opinions along with the corresponding topics and sentiments in text data has increased in recent years. Scholars and organizations from different fields can utilize such methods to identify patterns and generate new insights. Examples are opinion researchers investigating current opinions on political and societal issues, consumer researchers interested in consumers’ beliefs about the consumption and production of goods (Danner et al., 2020), and marketing managers curious about the public perception of their products and services (Berger et al., 2020; Murphy et al., 2014). (Kirchhoff, 2019) These domain-specific use cases are of interest for research disciplines which taken by itself are not directly related to natural language processing (NLP). Consequentially, there is a constant need to provide state-of-the-art NLP methods such that domain researchers from other fields can take advantage of them. The requirements therefore are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017). This is improved recently (Karamanolakis et al., 2019; Angelidis and Lapata, 2018; Luo et al., 2019), which underlines the recent impact and potential of related techniques. However, these have not been utilized for visualizations based on correlated topic modeling (Blei and Lafferty, 2006), where all pairs of topics ”are” analyzed to determine if two topics generally tend to occur in the same texts of a given dataset. Thus, the similarity between topics can be defined. This is successfully used to connect topics (nodes) among each other based on their correlations (edges) leading to more abstract and more meaningful meta topics (graph-clusters) which additionally improves topic coherence. Consequentially, these meta topics, e.g., company-related events or research subdisciplines (Liu et al., 2014; Maiya and Rolfe, 2014), can be successfully identified by graphbased visualization techniques. However, there is a lack of related prototypes on texts discussing consumption related issues in product reviews or social media. To the best of our knowledge, there is also no related integration of sentiment analysis into a system available for potential end users, i.e., domain experts. As according text data from customers is available on a large scale in social media, this can be considered as a shortcoming in the field. To address all denoted issues, we propose the SocialVisTUM toolkit, a new visualization and labeling tool to give users a comprehensible overview of the topics discussed in social media texts. It integrates a neural method for unsupervised sentence and comment clustering based on word vectors and attention. We denote the respective clusters as topics hereafter. In addition, we provide a graphbased visualization showing the topics as labeled nodes and the correlation between them as edges. A force-directed graph layout maintains readability even while many relevant topics and topic relations are displayed. (Kirchhoff, 2019) In our interactive graphical user interface, the number of topics displayed and the correlation threshold required to display a connection between two topics can be dynamically adjusted. Further, contextual topic information is provided, such as the number of respective topic occurrences in the social media texts as node diameter, the correlation between the topic occurrences as edge width, example sentences from the data for each topic, a list of representative words for each topic, and the regarding sentiment distribution of a topic. It is a common practice to represent topics merely by word lists (Blei et al., 2003; Chen et al., 2014), which tend to be insufficient to comprehensively express a topic on our given dataset. (Kirchhoff, 2019) To avoid manual labeling and to give users an immediate impression of each topic, topic labels are generated automatically based on a custom algorithm utilizing the most common WordNet hypernym in a topic’s top words. Furthermore, we find that topic hypernym statistics can serve as a metric for automatic hyperparameter optimization, which in our case gives practical advantages over widely used coherence scoring metrics. In addition to a more detailed description of our SocialVisTUM toolkit, we show the results of a case study based on social media texts from online commenters debating about organic food consumption. We demonstrate that the correlated topics give a meaningful graph representation of the social media discussions supporting the understanding of the concerns of consumers. In this regard, we also show how the combined illustration of different types of relevant topic and sentiment information and automatic labeling of clusters are a contribution.  Conclusion In this paper, a case of the proposed SocialVisTUM demonstrates the visualization of coherent topics on a given corpus of social media texts about organic food. The graph-based visualization with topics as nodes and topic correlations as edges reflects the topics and patterns found in a related qualitative content analysis (Danner and Menapace, 2020). The presentation of additional topic information, such as word lists, representative sentences, topic importance, and meaningful predefined labels, provide a basis for the understanding and interpretation of a topic for domain experts. The integrated hyperparameter optimization automatically yields interpretable topics and helps tailoring the model to the given data set. For future work, we plan to evaluate the correlated topics on other corpora and in other use cases. In addition to Pearson correlation, other correlations could improve the approach. We plan to integrate multi-lingual word features, such as BERT (Devlin et al., 2018), for cross-cultural comparisons.",1
"Abstract Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online. Introduction Web sources, such as social networks, internet forums, and customer reviews from online shops, provide large amounts of unstructured text data. Along with the steady development of new platforms and the increasing number of internet users, the interest in methods that automatically extract the expressed opinions along with the corresponding topics and sentiments in text data has increased in recent years. Scholars and organizations from different fields can utilize such methods to identify patterns and generate new insights. Examples are opinion researchers investigating current opinions on political and societal issues, consumer researchers interested in consumers beliefs about the consumption and production of goods (Danner et al., 2020), and marketing managers curious about the public perception of their products and services (Berger et al., 2020; Murphy et al., 2014). (Kirchhoff, 2019) These domain-specific use cases are of interest for research disciplines which taken by itself are not directly related to natural language processing (NLP). Consequentially, there is a constant need to provide state-of-the-art NLP methods such that domain researchers from other fields can take advantage of them. The requirements therefore are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017). This is improved recently (Karamanolakis et al., 2019; Angelidis and Lapata, 2018; Luo et al., 2019), which underlines the recent impact and potential of related techniques. However, these have not been utilized for visualizations based on correlated topic modeling (Blei and Lafferty, 2006), where all pairs of topics are analyzed to determine if two topics generally tend to occur in the same texts of a given dataset. Thus, the similarity between topics can be defined. This is successfully used to connect topics (nodes) among each other based on their correlations (edges) leading to more abstract and more meaningful meta topics (graph-clusters) which additionally improves topic coherence. Consequentially, these meta topics, e.g., company-related events or research subdisciplines (Liu et al., 2014; Maiya and Rolfe, 2014), can be successfully identified by graphbased visualization techniques. However, there is a lack of related prototypes on texts discussing consumption related issues in product reviews or social media. To the best of our knowledge, there is also no related integration of sentiment analysis into a system available for potential end users, i.e., domain experts. As according text data from customers is available on a large scale in social media, this can be considered as a shortcoming in the field. To address all denoted issues, we propose the SocialVisTUM toolkit, a new visualization and labeling tool to give users a comprehensible overview of the topics discussed in social media texts. It integrates a neural method for unsupervised sentence and comment clustering based on word vectors and attention. We denote the respective clusters as topics hereafter. In addition, we provide a graphbased visualization showing the topics as labeled nodes and the correlation between them as edges. A force-directed graph layout maintains readability even while many relevant topics and topic relations are displayed. (Kirchhoff, 2019) In our interactive graphical user interface, the number of topics displayed and the correlation threshold required to display a connection between two topics can be dynamically adjusted. Further, contextual topic informationprovided, such as the number of respective topic occurrences in the social media texts as node diameter, the correlation between the topic occurrences as edge width, example sentences from the data for each topic, a list of representative words for each topic, and the regarding sentiment distribution of a topic. The about-nodes clustering enables seamless integration between different types of data through the use of state-of-the-art NLP methods such that users of a given data study can take advantage of them. The benefits are simple usage, automatic hyperparameter optimization, minimal effort for manual labeling of text data, and built-in visualizations to give an abstract overview of the discussed topics and their relation with each other. While these practical requirements are important for domain experts, modern opinion mining approaches target specific machine learning objectives. Recently, there is a trend towards unsupervised neural methods for opinion target detection. Attention-based aspect extraction (ABAE) enables clustering of short review texts with significantly higher coherence as traditional LDA-based topic modeling, and it gives 70% F1 score for classification (He et al., 2017). This is improved recently (Karamanolakis et al., 2019; Angelidis and Lapata, 2018; Luo et al., 2019), which underlines the recent impact and potential of related techniques. However, these have not been utilized for visualizations based on correlated topic modeling (Blei and Lafferty, 2006), where all pairs of topics are analyzed to determine if two topics generally tend to occur in the same texts of a given dataset. Thus, the similarity between topics can be defined. This is successfully used to connect topics (nodes) among each other based on their correlations (edges) leading to more abstract and more meaningful meta topics (graph-clusters) which additionally improves topic coherence. Consequentially, these meta topics, e.g., company-related events or research subdisciplines (Liu et al., 2014; Maiya and Rolfe, 2014), can be successfully identified by graphbased visualization techniques. However, there is a lack of related prototypes on texts discussing consumption related issues in product reviews or social media. To the best of our knowledge, there is also no related integration of sentiment analysis into a system available for potential end users, i.e., domain experts. As according text data from customers is available on a large scale in social media, this can be considered as a shortcoming in the field.",0
"Abstract We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021). Given a potentially large collection of relatively short, opinionated texts focused on a topic of interest, the goal of KPA is to produce a succinct list of the most prominent key-points (KPs) in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle. Successful solutions to KPA can be used to gain better insights from public opinions as expressed in social media, surveys, and so forth, giving rise to a new form of a communication channel between decision makers and people that might be impacted by the decision. Various requirements govern the value of the KPA output. KPs are expected to be succinct, nonredundant, capturing points that are central to the topic of interest, and reflecting a clear stance towards that topic. Ideally, they should be at the right granularity for summarising the input data – not too specific and yet still informative and not overly general. In addition, accurate mapping of input texts to KPs is obviously essential. First, to ensure a reliable estimate of the prevalence of each key point. And second, to enable the user to drill-down, to gain a deeper understanding of the issues underlying each key point, as expressed by the input texts mapped to that key point. The goal of the KPA-2021 shared task was to further increase the attention of the NLP community to this emerging task, while enriching the space of existing KPA solutions. Since providing a complete KPA solution is challenging, we divided the task into two tracks, enabling teams to participate only in the first, relatively simpler track. Specifically, in the first track, referred to as Matching Track, KPs are given as part of the input, and the task is focused on mapping input text to these KPs. In contrast, in the second track, referred to as Generation Track, no KPs are provided and the task requires to further generate the KPs. The data being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. Discussion We presented KPA-2021, the first shared task focused on key point analysis. The shared task received 22 submissions from 17 teams, covering both the Matching Track and the Generation Track. We presented the submitted models, their performance on different evaluation measures, and further analyzed the results. As expected by its simpler nature, Matching Track received more submissions. However, evidently, success in this intermediate matching task, does not guarantee success in the overall KPA task, which also requires generating the KPs. Future work should determine whether future KPA shared tasks should focus on the Generation Track, or perhaps modify the evaluation measures of the intermediate Matching Track, such that they better reflect the model performance in the full KPA task. Given the inherent subjective nature of matching arguments to KPs, we opted for a ternary label, allowing argument-KP pairs to receive an ""undecided"" label, if the annotators votes were inconclusive. These undecided pairs are eventually considered either as positive or as negative matches, resulting with two gold standards, one potentially too strict, while the other perhaps too lenient. In future tasks, it may be valuable to consider a binary labeling scheme that will give rise to a single gold standard. Such labeling may be created conditionally, such that an undecided pair is marked as a positive match if and only if a minimum number of annotators marked it as positive and no other KP was labeled as matching to that argument. A further point for future consideration, is the choice of a pre-defined threshold of 50%, that guided our evaluation. Although, this has the advantage of not requiring submissions to tune a con- fidence threshold, it has the limitation that it complicates the evaluation, since the ground truth coverage depends on the arguments, topic and stance. A possible alternative would be to require each model to set its own minimum confidence threshold to determine if an input argument should not be matched to any KP. Finally, in this task, we used comparative evaluations to determine the quality of the sets of generated KPs. Obviously, this results with rankingbased evaluation. Establishing an absolute evaluation metric in this context merits further investigation. We thank all participating teams for taking the time to participate in this challenging new shared task. We believe that these contributions, along with the data collected and shared in this report, will be valuable to further advance the research around KPA and related tasks.",1
"Abstract We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021). Given a potentially large collection of relatively short, opinionated texts focused on a topic of interest, the goal of KPA is to produce a succinct list of the most prominent key-points (KPs) in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle. Successful solutions to KPA can be used to gain better insights from public opinions as expressed in social media, surveys, and so forth, giving rise to a new form of a communication channel between decision makers and people that might be impacted by the decision. Various requirements govern the value of the KPA output. KPs are expected to be succinct, nonredundant, capturing points that are central to the topic of interest, and reflecting a clear stance towards that topic. Ideally, they should be at the right granularity for summarising the input data not too specific and yet still informative and not overly general. In addition, accurate mapping of input texts to KPs is obviously essential. First, to ensure a reliable estimate of the prevalence of each key point. And second, to enable the user to drill-down, to gain a deeper understanding of the issues underlying each key point, as expressed by the input texts mapped to that key point. The goal of the KPA-2021 shared task was to further increase the attention of the NLP community to this emerging task, while enriching the space of existing KPA solutions. Since providing a complete KPA solution is challenging, we divided the task into two tracks, enabling teams to participate only in the first, relatively simpler track. Specifically, in the first track, referred to as Matching Track, KPs are given as part of the input, and the task is focused on mapping input text to these KPs. In contrast, in the second track, referred to as Generation Track, no KPs are provided and the task requires to further generate the KPs. The data being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. Discussion We presented KPA-2021, the first shared task focused on key point analysis. The shared task received 22 submissions from 17 teams, covering both the Matching Track and the Generation Track. We presented the submitted models, their performance on different evaluation measures, and further analyzed the results. As expected by its simpler nature, Matching Track received more submissions. However, evidently, success in this intermediate matching task, does not guarantee success in the overall KPA task, which also requires generating the KPs. Future work should determine whether future KPA shared tasks should focus on the Generation Track, or perhaps modify the evaluation measures of the intermediate Matching Track, such that they better reflect the model performance in the full KPA task. Given the inherent subjective nature of matching arguments to KPs, we opted for a ternary label, allowing argument-KP pairs to receive an ""undecided"" label, if the annotators votes were inconclusive. These undecided pairs are eventually considered either as positive or as negative matches, resulting with two gold standards, one potentially too strict, while the other perhaps too lenient. In future tasks, it may be valuable to consider a binary labeling scheme that will give rise to a single gold standard. Such labeling may be created conditionally, such that an undecided pair is marked as a positive match if and only if a minimum number of annotators marked it as positive and no other KP was labeled as matching to that argument. A further point for future consideration, is the choice of a pre-defined threshold ofthat guided our evaluation. Although, this has the advantage of not requiring submissions to tune a con- fidence threshold, it has the limitation that it complicates the evaluation, since the ground truth coverage depends on the arguments, topic and stance. A possible alternative would be to require each model to set its own minimum confidence threshold to determine if an argument should not be shown in the argument-K task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021).",0
"Abstract Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic selfattention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text. Representation from large language models can partially mitigate the data scarcity issue due to pretraining on a large amounts of unlabeled data. However, those models mostly consider token-level information and their pretraining tasks are not directly targeting long document representations. Another issue of directly applying transformer-based models is the limit of the input text length. Due to the quadratic complexity of self-attention, most of the pretrained transformers models can only handle a relatively short text. A wide spectrum of efficient, fast transformer models (collectively called “Xformers”) have been proposed to tackle this problem; e.g., Longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2020) use sparse attention to improve the computational and memory efficiency for long sequence text. Nevertheless, these models still focus on token-level interactions without considering high-level semantic structure of the document. Recently, there is a resurgence of interest in Contrastive Learning (CL) due to its success in selfsupervised representation learning in computer vision (Chen et al., 2020; He et al., 2020). Contrastive Learning offers a simple method to learn disentangled representation that encodes invariance to small and local changes in the input data without using any labeled data. In NLP domain, contrastive learning has been employed to learn sentence representation (Wu et al., 2020; Qu et al., 2020) under either self-supervised or supervised settings. In this work, we propose a Graph Attention Network (GAT) based model that explicitly utilizes the high-level semantic structure of the documents to learn document embeddings. We model the document as not just a sequence of text, but a collection of passages or sentences. Specifically, the proposed model introduces a graph on top of the document passages (Fig. 1) to utilize multi-granularity information. First, passages are encoded using RoBERTa (Liu et al., 2019) to collect word-level knowledge. Then passages are connected to leverage the higher-level structured information. At last, a graph attention network (Velickovi ˇ c et al. ´ , 2017) is applied to obtain the multi-granularity document representation. To better learn the document embedding, we propose a document-level contrastive learning strategy to pretrain our models. In our contrastive learning framework, we split the document into random sub-documents and train the model to maximize the agreement over the representations of the sub-documents that come from the same document. This simple strategy allows us to pretrain our models on a large unlabelled corpus without any additional priors. As we will see, this simple pretraining task indeed helps the model on the downstream tasks. The contributions of this paper can be summarized as follows. • We propose a graph document model with graph attention networks that can not only explicitly utilize the high-level structure of the document but also leverage pretrained Transformer encoders to obtain low-level contextual information. • We propose a simple document-level contrastive learning strategy, which does not require any handcrafted transformations and is suitable for large-scale pretraining. • We conduct empirical evaluations on our models and contrastive pretraining strategy. We show that our graph-roberta models achieve great performance on both document classi- fication and retrieval tasks. Specifically we demonstrate that our contrastive pretraining helps the model learn a meaningful document representation even without fine-tuning, and improve both the training convergence speed and final performance during end-to-end finetuning on downstream classification tasks. For document retrieval tasks, we demonstrate that our graph-roberta models have great semantic matching performance, compensating the typical lexical matching system. Conclusions In this work, we propose a simple graph attention network model to learn document embeddings. Our model not only can leverage the recent advancement of pretrained Transformer models as building blocks, but also explicitly utilize the high-level structure of the documents. In addition, we propose a simple document-level contrastive learning strategy that does not require handcraft transformations. With this strategy, we conduct large scale contrastive pretraining on a large corpus. Empirically we demonstrate our methods achieve great performance on both document classification and document retrieval tasks.",1
"Abstract Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic selfattention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text. Representation from large language models can partially mitigate the data scarcity issue due to pretraining on a large amounts of unlabeled data. However, those models mostly consider token-level information and their pretraining tasks are not directly targeting long document representations. Another issue of directly applying transformer-based models is the limit of the input text length. Due to the quadratic complexity of self-attention, most of the pretrained transformers models can only handle a relatively short text. A wide spectrum of efficient, fast transformer models (collectively called formers have been proposed to tackle this problem; e.g., Longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2020) use sparse attention to improve the computational and memory efficiency for long sequence text. Nevertheless, these models still focus on token-level interactions without considering high-level semantic structure of the document. Recently, there is a resurgence of interest in Contrastive Learning (CL) due to its success in selfsupervised representation learning in computer vision (Chen et al., 2020; He et al., 2020). Contrastive Learning offers a simple method to learn disentangled representation that encodes invariance to small and local changes in the input data without using any labeled data. In NLP domain, contrastive learning has been employed to learn sentence representation (Wu et al., 2020; Qu et al., 2020) under either self-supervised or supervised settings. In this work, we propose a Graph Attention Network (GAT) based model that explicitly utilizes the high-level semantic structure of the documents to learn document embeddings. We model the document as not just a sequence of text, but a collection of passages or sentences. Specifically, the proposed model introduces a graph on top of the document passages (Fig. 1) to utilize multi-granularity information. First, passages are encoded using RoBERTa (Liu et al., 2019) to collect word-level knowledge. Then passages are connected to leverage the higher-level structured information. At last, a graph attention network (Velickovic et al., 2017) is applied to obtain the multi-granularity document representation. To better learn the document embedding, we propose a document-level contrastive learning strategy to pretrain our models. In our contrastive learning framework, we split the document into random sub-documents and train the model to maximize the agreement over the representations of the sub-documents that come from the same document. This simple strategy allows us to pretrain our models on a large unlabelled corpus without any additional priors. As we will see, this simple pretraining task indeed helps the model on the downstream tasks. The contributions of this paper can be summarized as follows. We propose a graph document modelgraph attention networks that can not only explicitly utilize the high-level structure of the document but also leverage pretrained Transformer encoders to obtain low-level contextual information. We propose a simple document-level contrastive learning strategy, which does not require any handcrafted transformations and is suitable for large-scale pretraining. We conduct empirical evaluations on our models and document embeddings. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text.",0
"Abstract We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from two major U.S. journals to forecast the CBOE Volatility Index. Keywords: Aggregation, Penalized Regression, Prediction, R, sentometrics, Textual Senti- ment, Time Series. Introduction Individuals, companies, and governments continuously consume written material from various sources to improve their decisions. The corpus of texts is typically of a high-dimensional longitudinal nature requiring statistical tools to extract the relevant information. A key source of information is the sentiment transmitted through texts, called textual sentiment. Algaba, Ardia, Bluteau, Borms, and Boudt (2020) review the notion of sentiment and its applications, mainly in economics and finance. They define sentiment as “the disposition of an entity towards an entity, expressed via a certain medium.” The medium in this case is texts. The sentiment expressed through texts may provide valuable insights on the future dynamics of variables related to firms, the economy, political agendas, product satisfaction, and marketing campaigns, for instance. Still, textual sentiment does not live by the premise to be equally useful across all applications. Deciphering when, to what degree, and which layers of the sentiment add value is needed to consistently study the full information potential present within qualitative communications. The econometric approach of constructing time series of sentiment by means of optimized selection and weighting of textual sentiment is referred to as sentometrics by Algaba et al. (2020) and Ardia, Bluteau, and Boudt (2019). The term sentometrics is a composition of (textual) sentiment analysis and (time series) econometrics. The release of the R (R Core Team 2021) text mining infrastructure tm (Feinerer, Hornik, and Meyer 2008) over a decade ago can be considered the starting point of the development and popularization of textual analysis tools in R. A number of successful follow-up attempts at improving the speed and interface of the comprehensive natural language processing capabil- ities provided by tm have been delivered by the packages openNLP (Hornik 2019), cleanNLP (Arnold 2017), quanteda (Benoit, Watanabe, Wang, Nulty, Obeng, Müller, and Matsuo 2018), tidytext (Silge and Robinson 2016), and qdap (Rinker 2020). The notable tailor-made packages for sentiment analysis in R are meanr (Schmidt 2019), SentimentAnalysis (Feuerriegel and Proellochs 2021), sentimentr (Rinker 2019b), and syuzhet (Jockers 2020). Many of these packages rely on one of the larger above-mentioned textual analysis infrastructures. The meanr package computes net sentiment scores fastest, but offers no flexibility.1 The SentimentAnalysis package relies on a similar calculation as used in tm’s sentiment scoring function. The package can additionally be used to generate and evaluate sentiment dictionaries. The sentimentr package extends the polarity scoring function from the qdap package to handle more difficult linguistic edge cases, but is therefore slower than packages which do not attempt this. The SentimentAnalysis and syuzhet packages also become comparatively slower for large input corpora. The quanteda and tidytext packages have no explicit sentiment computation function but their toolsets can be used to construct one. Our R package sentometrics proposes a well-defined modeling workflow, specifically targeted at studying the evolution of textual sentiment and its impact on other quantities. It can be used (i) to compute textual sentiment, (ii) to aggregate fine-grained textual sentiment into various sentiment time series, and (iii) to predict other variables with these sentiment measures. The combination of these three facilities leads to a flexible and computationally efficient framework to exploit the information value of sentiment in texts. The package presented in this paper therefore addresses the present lack of analytical capability to extract time series intelligence about the sentiment transmitted through a large panel of texts. Furthermore, the sentometrics package positions itself as both integrative and supplementary to the powerful text mining and data science toolboxes in the R universe. It is integrative, as it combines the strengths of quanteda and stringi (Gagolewski 2021) for corpus construction and manipulation. It uses data.table (Dowle and Srinivasan 2021) for fast aggregation of textual sentiment into time series, and glmnet (Friedman, Hastie, and Tibshirani 2010) and caret (Kuhn 2021) for (sparse) model estimation. It is supplementary, given that it easily extends any text mining workflow to compute, aggregate and predict with textual sentiment.  The remainder of the paper is structured as follows. Section 2 introduces the methodology behind the R package sentometrics. Section 3 describes the main control functions and illustrates the package’s typical workflow. Section 4 applies the entire framework to forecast the Chicago Board Options Exchange (CBOE) Volatility Index. Section 5 concludes. Conclusion and future development The R package sentometrics provides a framework to calculate sentiment for texts, to aggregate textual sentiment scores into many time series at a desired frequency, and to use these in a flexible prediction modeling setup. It can be deployed to quantify a qualitative corpus of texts, relate it to a target variable, and retrieve which type of sentiment is most informative through visualization and attribution analysis. The main priorities for further development are integrating better prediction tools, enhancing the complexity of the sentiment engine, allowing user-defined weighting schemes, and adding intra-day aggregation.",1
"Abstract We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Textual sentiment analysis is increasingly used to unlock the potential information value of textual data. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from two major U.S. journals to forecast the CBOE Volatility Index. Keywords: Aggregation, Penalized Regression, Prediction, R, sentometrics, Textual Senti- ment, Time Series. Introduction Individuals, companies, and governments continuously consume written material from various sources to improve their decisions. The corpus of texts is typically of a high-dimensional longitudinal nature requiring statistical tools to extract the relevant information. A key source of information is the sentiment transmitted through texts, called textual sentiment. Algaba, Ardia, Bluteau, Borms, and Boudt (2020) review the notion of sentiment and its applications, mainly in economics and finance. They define sentiment as the disposition of an entity towards an entity, expressed via a certain medium. The medium in this case is texts. The sentiment expressed through texts may provide valuable insights on the future dynamics of variables related to firms, the economy, political agendas, product satisfaction, and marketing campaigns, for instance. Still, textual sentiment does not live by the premise to be equally useful across all applications. Deciphering when, to what degree, and which layers of the sentiment add value is needed to consistently study the full information potential present within qualitative communications. The econometric approach of constructing time series of sentiment by means of optimized selection and weighting of textual sentiment is referred to as sentometrics by Algaba et al. (2020) and Ardia, Bluteau, and Boudt (2019). The term sentometrics is a composition of (textual) sentiment analysis and (time series) econometrics. The release of the R (R Core Team 2021) text mining infrastructure tm (Feinerer, Hornik, and Meyer 2008) over a decade ago can be considered the starting point of the development and popularization of textual analysis tools in R. A number of successful follow-up attempts at improving the speed and interface of the comprehensive natural language processing capabil- ities provided by tm have been delivered by the packages openNLP (Hornik 2019), cleanNLP (Arnold 2017), quanteda (Benoit, Watanabe, Wang, Nulty, Obeng, Muller, and Matsuo 2018), tidytext (Silge and Robinson 2016), and qdap (Rinker 2020). The notable tailor-made packages for sentiment analysis in R are meanr (Schmidt 2019), SentimentAnalysis (Feuerriegel and Proellochs 2021), sentimentr (Rinker 2019b), and syuzhet (Jockers 2020). Many of these packages rely on one of the larger above-mentioned textual analysis infrastructures. The meanr package computes net sentiment scores fastest, but offers no flexibility.1 The SentimentAnalysis package relies on a similar calculation as used in tms sentiment scoring function. The package can additionally be used to generate and evaluate sentiment dictionaries. The sentimentr package extends the polarity scoring function from the qdap package to handle more difficult linguistic edge cases, but is therefore slower than packages which do not attempt this. The SentimentAnalysis and syuzhet packages also become comparatively slower for large input corpora. The quanteda and tidytext packages have no explicit sentiment computation function but their toolsets can be used to construct one. Our R package sentometrics proposes a well-defined modeling workflow, specifically targeted at studying the evolution of textual sentiment and its impact on other quantities. It can be used (i) to compute textual sentiment, (ii) to aggregate fine-grained textual sentiment into various sentiment time series, and (iii) to predict other variables with these sentiment measures. The combination of these three facilities leads to a flexible and computationally efficient framework to exploit the information value of sentiment in texts. The package presented in this paper therefore addresses the present lack of analytical capability to extract time series intelligence about the sentiment transmitted through a large panel of texts. Furthermore, the sentometrics package positions itself as both integrative and supplementary to the powerful textand data science toolboxes in the R universe. It is integrative, as it combines the strengths of quanteda and stringi (Gagolewski 2021) for corpus construction and manipulation. It uses data.table (Dowle and Srinivasan 2021) for fast aggregation of textual sentiment into time series, and glmnet (Friedman, Hastie, and Tibshirani 2010) and caret (Kuhn 2021) for (sparse) model estimation. It is cleanteed, as it uses data.table (Dowle and Srinivasan 2021) for fast aggregation of textual sentiment into time series, and glmnet (Friedman, Hastie, and Tibshirani 2010) and caret (Kuhn 2021) for (sparse) model estimation. It is computationally efficient as it extends the computational time series to compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables.",0
"Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST. Introduction Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregressive generation. However, low latency is required by an increasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simultaneously. Compared with autoregressive models, non-autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly finetune an off-the-shelf pre-trained encoder model for nonautoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a nontrivial problem. To fully exploit the power of pre-trained models, we propose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding methods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacrifice the inference speed. MIST is a simple and effective iterative training strategy that works during the training stage and has no effect on inference speed. During the training stage, the model predicts the entire target sequence first, then we treat the generated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data augmentation method, which needs to prepare data before training, MIST enables dynamical data augmentation in the training stage. The term “dynamic” in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model overfitting with static pseudo targets generated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo targets also enables the conditional dependence to help convergence. As our experiments show, we evaluate our method on three generation tasks including question generation, summarization and paraphrase generation. Our method achieves significant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further evaluate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: • We propose a new paradigm, adopting pre-trained encoder for non-autoregressive generation without modifying model architectures. • We propose a simple and novel training method to improve the performance. • We empirically verify the effectiveness of our method in different generation tasks and pre-trained models. In this paper, we proposed a new paradigm to adopt pretrained encoders to NAR tasks. Our method uses a transformer encoder as model architecture with a new iterative decoding method called MIST to improve the generation quality without additional latency in decoding. Our method achieves state of the art results among the NAR and semi-NAR models on three different NLG tasks. We also demonstrated that our method can successfully utilize the pre-trained models to achieve better results than large-scale NAR pre-trained models like BANG.",1
"Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST. Introduction Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregressive generation. However, low latency is required by an increasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simultaneously. Compared with autoregressive models, non-autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly finetune an off-the-shelf pre-trained encoder model for nonautoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a nontrivial problem. To fully exploit the power of pre-trained models, we propose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding methods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacrifice the inference speed. MIST is a simple and effective iterative training strategy that works during the training stage and has no effect on inference speed. During the training stage, the model predicts the entire target sequence first, then we treat the generated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data augmentation method, which needs to prepare data before training, MIST enables dynamical data augmentation in the training stage. The term dynamic in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model overfitting with static pseudo targets generated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo targets also enables the conditional dependence to help convergence. As our experiments show, we evaluate our method on three generation tasks including question generation, summarization and paraphrase generation. Our method achieves significant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further evaluate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: We propose a new paradigm, adopting pre-trained encoder for non-autoregressive generation without modifying architectures. We propose a simple and novel training method to improve the performance. We empirically verify the effectiveness of our method in different generation tasks and pre-trained models. In this paper, we proposed a new paradigm to adopt pretrained encoders to NAR tasks. Our method uses a transformer transformer as model and pre-trained transformer model as model model. Our method predicts the position and speed of the target transformer automatically before training. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST).",0
"ABSTRACT While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks. Introduction Deep neural networks are widely used in various applications ranging from computer vision to language processing. While deep learning has surpassed human-level performance in numerous tasks, neural networks are extremely vulnerable to overfitting to spurious correlations and therefore fail to generalize even under slight perturbations of the test distribution [Arjovsky et al., 2019]. This observation motivated the research community to tackle the problem of domain generalization (see [Ribeiro et al., 2020] for a detailed literature review). Recent benchmark datasets, such as Rotated MNIST [Arjovsky et al., 2019], Colored MNIST [Arjovsky et al., 2019], PACS [Li et al., 2017], VLCS [Fang et al., 2013], Office-Home [Venkateswara et al., 2017], Terra Incognita [Beery et al., 2018] and DomainNet [Peng et al., 2019], have shown difficulties for the generalization of deep neural network models under distribution shifts, and have sparked invention of many new algorithmic frameworks to address domain generalization. A standard approach for improving out-of-distribution performance is to guarantee that learned representations are invariant to certain transformations. For example, image representations and trained models for computer vision should generally be invariant to rotations, changes in color, or background. There are two main directions for promoting such invariance to transformations, namely data augmentation and geometric deep learning. Data augmentation promotes invariances in learned representations by curating synthetic examples that exhibit the desired invariances. Zhang et al. [2017] introduced mixup to train a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures. Volpi et al. [2018] proposed an adaptive data augmentation method where adversarial examples are generated at every iteration, offering performance gain over unseen domains. Kuznichov et al. [2019] showed data augmentation can be applied to leaf segmentation by proposing a method that preserves the geometric structure of the data objects and keep the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field. Tellez et al. [2019] showed stain color augmentation and stain color normalization could be used in computational pathology applications. Goel et al. [2020] proposed an approach to patch a model that fails due to spurious features on a real-world skin cancer dataset by data augmentation. Zhou et al. [2020] showed data augmentation with adversarial images could make the label classifier more robust to unknown domain shifts. Nam et al. [2021] improved domain generalization by reducing the intrinsic style bias of CNNs. This is achieved by training a separate network for randomizing the style of images and generating augmented data during training. Geometric deep learning bakes such invariances into the neural network architecture. For example, convolutional layers [Lecun et al., 1998] are fundamentally preserving translations. There are other specifically designed networks to maintain invariances: Zaheer et al. [2017] studied the problem of designing models for machine learning tasks defined on sets and characterized the permutation invariant functions. Bloem-Reddy and Teh [2020] obtained generative functional representations of probability distributions that are invariant under the action of a compact group. Finzi et al. [2021] provided an algorithm for solving for the equivariant layers of matrix groups. Besides two main directions mentioned above, researchers have proposed numerous algorithmic solutions to impose invariance and improve domain generalization: Ganin et al. [2016] introduced a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. [Arjovsky et al., 2019] proposed invariant risk minimization (IRM) — a novel learning paradigm that estimates nonlinear, invariant, causal predictors from multiple training environments, to enable out-of-distribution generalization. The idea is to learn representations that perform equally well across different environments. Sagawa et al. [2019] introduced distributionally robust optimization (DRO) framework to learn models by minimizing the worst-case training loss over a set of pre-defined groups/environments. Li et al. [2018a] proposed a novel meta-learning method for domain generalization (MLDG), which simulates domain shift during training by synthesizing virtual testing domains within each mini-batch. Correlation alignment for deep domain adaptation [Sun and Saenko, 2016] (Deep CORAL) learns a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Li et al. [2018b] extended adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to a prior distribution via adversarial feature learning. Li et al. [2018c] proposed a conditional invariant neural network that minimizes the discrepancy in conditional distribution of images given the class labels across different domains. The approaches listed above are more complex than simple training mechanisms such as empirical risk minimization (ERM) and hence they cannot be easily applied to involved tasks such as training generative models. In contrast, Gulrajani and Lopez-Paz [2020] demonstrated that the simple ERM method can achieve state of the art performance after fine-tuning in various datasets/applications. However, there are still problem instances that ERM performs very poorly. For example, in learning end-to-end dialogue models, Qian et al. [2021] showed 29% performance drop on MultiWOZ [Budzianowski et al., 2018] due to the memorization of named entities. In this paper, we propose a regularization technique, called data augmented invariant regularization (DAIR) that penalizes the inconsistency of loss on augmented samples with respect to the original ones. DAIR is applicable whenever data augmentation is used to promote invariances. DAIR only requires marginal additional cost on top of data augmentation, and is simple and applicable to a wide host of supervised and unsupervised learning tasks, including generative models. We introduce the DAIR formulation, motivate it, and theoretically prove some of its properties in Section 2. We empirically evaluate DAIR on a variety of problem setups ranging from defense against adversarial attacks to domain generalization in the presence of environment shift in Section 3. Our experimental results show that DAIR is competitive with or even outperforms state-of-the-art algorithms specifically designed for imposing invariance in these problems. Conclusion. In this paper, we proposed a simple yet effective regularizer that can be used wherever data augmentation is used to promote invariance. We rigorously showed that our proposed regularizer can recover the optimal solution in certain regression task where simple data augmentation can not. We also compare our DAIR-SQ regularizer with the existing off-the-shelf regularizers such as L1, L2 and KL divergence. We empirically showed that the DAIR-SQ regularizer is competitive with or outperforms state-of-the-art problem-specific baselines in a variety of problem setups. We evaluated DAIR in four different categories of machine learning tasks including regression, visual question answering, and training robust deep neural networks, and task-oriented dialog modeling. This is a major benefit of DAIR-SQ as some of other regularizers cannot be applied to regression tasks. Empirically the proposed algorithm outperforms well in all tasks. Better understanding of the tuning of the corresponding hyperparameter also remains as an open area for research. Finally, a more in-depth theoretical understanding of the properties of DAIR-SQ regularizer that lead to its superior empirical performance are also important questions for future work. While we showed that DAIR-SQ boosts existing performance metrics, such as average accuracy, the interplay of DAIR-SQ with other metrics, especially group fairness is also another important area for future research.",1
"ABSTRACT While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks. Introduction Deep neural networks are widely used in various applications ranging from computer vision to language processing. While deep learning has surpassed human-level performance in numerous tasks, neural networks are extremely vulnerable to overfitting to spurious correlations and therefore fail to generalize even under slight perturbations of the test distribution [Arjovsky et al., 2019]. This observation motivated the research community to tackle the problem of domain generalization (see [Ribeiro et al., 2020] for a detailed literature review). Recent benchmark datasets, such as Rotated MNIST [Arjovsky et al., 2019], Colored MNIST [Arjovsky et al., 2019], PACS [Li et al., 2017], VLCS [Fang et al., 2013], Office-Home [Venkateswara et al., 2017], Terra Incognita [Beery et al., 2018] and DomainNet [Peng et al., 2019], have shown difficulties for the generalization of deep neural network models under distribution shifts, and have sparked invention of many new algorithmic frameworks to address domain generalization. A standard approach for improving out-of-distribution performance is to guarantee that learned representations are invariant to certain transformations. For example, image representations and trained models for computer vision should generally be invariant to rotations, changes in color, or background. There are two main directions for promoting such invariance to transformations, namely data augmentation and geometric deep learning. Data augmentation promotes invariances in learned representations by curating synthetic examples that exhibit the desired invariances. Zhang et al. [2017] introduced mixup to train a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures. Volpi et al. [2018] proposed an adaptive data augmentation method where adversarial examples are generated at every iteration, offering performance gain over unseen domains. Kuznichov et al. [2019] showed data augmentation can be applied to leaf segmentation by proposing a method that preserves the geometric structure of the data objects and keep the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field. Tellez et al. [2019] showed stain color augmentation and stain color normalization could be used in computational pathology applications. Goel et al. [2020] proposed an approach to patch a model that fails due to spurious features on a real-world skin cancer dataset by data augmentation. Zhou et al. [2020] showed data augmentation with adversarial images could make the label classifier more robust to unknown domain shifts. Nam et al. [2021] improved domain generalization by reducing the intrinsic style bias of CNNs. This is achieved by training a separate network for randomizing the style of images and generating augmented data during training. Geometric deep learning bakes such invariances into the neural network architecture. For example, convolutional layers [Lecun et al., 1998] are fundamentally preserving translations. There are other specifically designed networks to maintain invariances: Zaheer et al. [2017] studied the problem of designing models for machine learning tasks defined on sets and characterized the permutation invariant functions. Bloem-Reddy and Teh [2020] obtained generative functional representations probability distributions that are invariant under the action of a compact group. Finzi et al. [2021] provided an algorithm for solving for the equivariant layers of matrix groups. Besides two main directions mentioned above, researchers have proposed numerous algorithmic solutions to impose invariance and improve domain generalization: Ganin et al. [2016] introduced a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. [Arjovsky et al., 2019] proposed invariant risk minimization (IRM) a novel learning paradigm that estimates nonlinear, invariant, causal predictors from multiple training environments, to enable out-of-distribution generalization. The idea is to learn representations that perform equally well across different environments. Sagawa et al. [2019] introduced distributionally robust optimization (DRO) framework to learn models by minimizing the worst-case training loss over a set of pre-defined groups/environments. Li et al. [2018a] proposed a novel meta-learning method for domain generalization (MLDG), which simulates domain shift during training by synthesizing virtual testing domains within each mini-batch. Correlation alignment for deep domain adaptation [Sun and Saenko, 2016] (Deep CORAL) learns a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Li et al. [2018b] extended adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to a prior distribution via adversarial feature learning. Li et al. [2018c] proposed a conditional invariant neural network that minimizes the discrepancy in conditional distribution of images given the class labels across different domains. The approaches listed above are more complex than simple training mechanisms such as empirical risk minimization (ERM) and hence they cannot be easily applied to involved tasks such as training generative models. In contrast, Gulrajani and Lopez-Paz [2020] demonstrated that the simple ERM method can achieve state of the art performance after fine-tuning in various datasets/applications. However, there are still problem instances that ERM performs very poorly. For example, in learning end-to-end dialogue models, Qian et al. [2021] showed 29% performance drop on MultiWOZ [Budzianowski et al., 2018] due to the memorization of named entities. In this paper, we propose a regularization technique, called data augmented invariant regularization (DAIR) that penalizes the inconsistency of loss on augmented samples with respect to the original ones. DAIR is applicable whenever data augmentation is used to promote invariances. DAIR only requires marginal additional cost on top of data augmentation, and is simple and applicable to a wide host of supervised and unsupervised learning tasks, including generative models. We introduce the DAIR formulation, motivate it, and theoretically prove some of its properties in Section 2. We empirically evaluate DAIR on a variety of problem setups ranging from defense against adversarial attacks to domain generalization in the presence of environment shift in Section 3. Our experimental results show that DAIR is competitive with or even outperforms state-of-the-art algorithms specifically designed for imposing invariance in these problems. Conclusion. In this paper, we proposed a simple yet effective regularizer that can be used wherever data augmentation is used to promote invariance. We rigorously showed that our proposed regularizer can recover the optimal solution in certain regression task where simple data augmentation can not. We also compare our DAIR-SQ regularizer with the existing off-the-shelf regularizers such as L1, L2 and KL divergence. We empirically showed that the DAIR-SQ regularizer is competitive with or outperforms state-of-the-art problem-specific baselines in a variety of problem setups. We evaluated DAIR in four different categories of machine learning tasks including regression, visual question answering, and training robust deep neural networks, and task-oriented dialog modeling. This is a major benefit of DAIR-SQ as some of other regularizers cannot be applied to regression tasks. Empirically the proposed algorithm outperforms well in all tasks. Better understanding of the tuning of the corresponding hyperparameter also remains as an open area for research. Finally, a more in-depth theoretical understanding of the properties of DAIR-SQ regularizer that lead to its superior empirical performance are also important questions for future work. While we showedDAIR-SQ boosts existing performance metrics, such as average accuracy, the interplay of DAIR-SQ with other metrics, especially group fairness is also another important area for future research.",0
"Abstract A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that “summarizes” texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words(Lin and Ng, 2019). Recently, with the development of representation learning for NLP (Vaswani et al., 2017; Devlin et al., 2018) and large-scale datasets (Fabbri et al., 2019), some studies have achieved promising results on abstractive MDS (Liu and Lapata, 2019; Jin et al., 2020). Nevertheless, we found there are two limitations that have not been addressed by previous studies. First, some works simply concatenate multiple documents into a flat sequence and then apply single-document summarization approaches (Liu et al., 2018; Fabbri et al., 2019). However, this paradigm fails to consider the hierarchical document structures, which plays a key role in MDS task (Jin et al., 2020). Also, the concatenation operation inevitably produces a lengthy sequence, and encoding long texts for summarization is a challenge (Cohan et al., 2018). Second, when dealing with multiple documents, a critical point is to learn the cross-document relations. Some studies address this problem by mining the co-occurrence words or entities (Wang et al., 2020a), which can hardly capture implicit associations due to the diverse language expressions. Some other studies (Jin et al., 2020; Liu and Lapata, 2019) first generate low-dimensional vectors in sentence- or paragraph-level and then build interaction based on these highly compressed representations. These methods inevitably result in the loss of large amounts of fine-grained interaction features and would damage the interpretability of models. Therefore, how to learn the relation across documents effectively remains an open question. To shed lights on these missing points, this paper proposes a novel abstractive MDS model that marries topic modeling into abstractive summary generation. The motivation is that both tasks aim to distil salient information from massive text and therefore could provide complementary features for each other. Concretely, we jointly optimize a neural topic model (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017) that learns topic distribution of source documents and corpus-level topic representations, and an abstractive summarizer that incorporates latent topics to summary generation process. In the encoding process, we represent multiple documents as a heterogeneous graph consisting of word, topic, and document nodes and encode it with a graph neural network to capture the interactions among different semantic units. In the decoding process, we devise a topic-aware decoder that leverages learned topics to guide the summary generation. We train the two modules with a multitask learning framework, where an inconsistency loss is applied to penalize the difference between the topic distribution of source documents and that of generated summaries. It encourages the summarizer to generate a summary that is thematically consistent with its source documents and also helps the two modules learn from each other. In this manner, our model is learned such that better topics can yield better summaries and vice versa. We conduct throughout experiments on the recently released Multi-News dataset (Fabbri et al., 2019). The results demonstrate the effectiveness and superiority of our model. To sum up, the contributions of this paper are threefolds: 1) To the best of our knowledge, we carry out the first systematic study on jointly modeling topic inference and abstractive MDS and demonstrate the positive mutual effect between the two tasks. 2) We propose a novel MDS model that joint optimizes a neural topic model and an abstractive summarizer. We propose an inconsistency loss to penalize the disagreement between the two modules and help them learn from each other. 3) Experimental results on the Multi-News dataset demonstrate that our model achieves the state-of-the-art performance on both Rouge scores and human evaluation, meanwhile learns highquality topics. Conclusion and Future Work This study proposes a novel abstractive MDS model that integrates a joint NTM to discover latent topics. Experimental results demonstrate that our model not only achieves the-state-of-the art results on summarization but also produce high-quality topics. Further discussions show that topic inference and summary generation can promote each other. In the future, we will explore how to apply latent topics in controllable summarization.",1
"Abstract A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that summarizes texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words(Lin and Ng, 2019). Recently, with the development of representation learning for NLP (Vaswani et al., 2017; Devlin et al., 2018) and large-scale datasets (Fabbri et al., 2019), some studies have achieved promising results on abstractive MDS (Liu and Lapata, 2019; Jin et al., 2020). Nevertheless, we found there are two limitations that have not been addressed by previous studies. First, some works simply concatenate multiple documents into a flat sequence and then apply single-document summarization approaches (Liu et al., 2018; Fabbri et al., 2019). However, this paradigm fails to consider the hierarchical document structures, which plays a key role in MDS task (Jin et al., 2020). Also, the concatenation operation inevitably produces a lengthy sequence, and encoding long texts for summarization is a challenge (Cohan et al., 2018). Second, when dealing with multiple documents, a critical point is to learn the cross-document relations. Some studies address this problem by mining the co-occurrence words or entities (Wang et al., 2020a), which can hardly capture implicit associations due to the diverse language expressions. Some other studies (Jin et al., 2020; Liu and Lapata, 2019) first generate low-dimensional vectors in sentence- or paragraph-level and then build interaction based on these highly compressed representations. These methods inevitably result in the loss of large amounts of fine-grained interaction features and would damage the interpretability of models. Therefore, how to learn the relation across documents effectively remains an open question. To shed lights on these missing points, this paper proposes a novel abstractive MDS model that marries topic modeling into abstractive summary generation. The motivation is that both tasks aim to distil salient information from massive text and therefore could provide complementary features for each other. Concretely, we jointly optimize a neural topic model (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017) that learns topic distribution of source documents and corpus-level topic representations, and an abstractive summarizer that incorporates latent topics to summary generation process. In the encoding process, we represent multiple documents as a heterogeneous graph consisting of word, topic, and document nodes and encode it with a graph neural network to capture the interactions among different semantic units. In the decoding process, we devise a topic-aware decoder that leverages learned topics to guide the summary generation. We train the two modules with a multitask learning framework, where an inconsistency loss is applied to penalize the difference between the topic distribution of source documents and that of generated summaries. It encourages theto generate a summary that is thematically consistent with its source documents and also helps the two modules learn from each other. In this manner, our model is learned such that better topics can yield better summaries and vice versa. We conduct throughout experiments on the recently released Multi-News dataset to develop a framework for future experiments. Introduction Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016; Zhang et al., 2018; Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words(Lin and Ng, 2019).",0
"Abstract We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent whitebox and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging — the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP models’ classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the model’s behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification model’s decision for financial texts. Related, textual adversaries also aim to change the model prediction (with modifications resembling natural text). The difference is that adversaries further aim to escape human detection (not changing a human’s classification), whereas counterfactuals do not have such requirement. Another line of related work is style transfer (Sudhakar et al., 2019; Wang et al., 2019; Hu et al., 2017), which aim to modify a given text according to a target style. It differs from adversary or counterfactual generation in that it seeks to fully change all style-related phrases, as opposed to minimally perturbing a text to change a classifier’s decision. White-box approaches have been widely used to generate adversaries or counterfactuals for vision tasks where the continuous inputs can be optimized to alter model predictions (Goodfellow et al., 2014; Carlini and Wagner, 2017; Neal et al., 2018). Such optimization based approaches are difficult to apply to language due to the discrete nature of text. We circumvent this difficulty by directly optimizing in the latent space of the input towards the desired classification. We then exploit the language generation capability of pre-trained language models, available for most state-of-the-art NLP models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), to generate semantically plausible substitutions from the optimized latent representations. We further introduce Shapley values to estimate the combinatoric effect of multiple simultaneous changes, which are then used to guide a beam search to generate the final counterfactual. Leveraging pre-trained language models to generate alternative texts has been a popular black-box approach in the recent literature on text adversaries (Li et al., 2020b; Garg and Ramakrishnan, 2020; Li et al., 2020a). Our work presents a first attempt to combine the strength of white-box optimization and the power of pre-trained language models. While Shapley values have been widely studied for the problem of feature importance (Lundberg and Lee, 2017; Sundararajan and Najmi, 2020) and data valuation (Jia et al., 2020), this is the first effort demonstrating their usefulness for text generation. We compare our method to several white-box and black-box baselines on two different text classification tasks. Automatic and human evaluation results show that our method significantly improves the success rate of counterfactual generation, while reducing the fraction of input tokens modified and enhancing the semantic plausibility of generated counterfactuals. We also show through ablation studies that both counterfactual optimization of the latent representations and Shapley value estimates contribute to our method’s strong performance. We are motivated by how humans use counterfactuals to explain the concept of a class and seek to automatically generate counterfactual text input as a means to understand a deep NLP model and its definition of class. We assume full white-box access to the given model and perform optimization in the latent space to maximize the probability of predicting a target class. We then map from the optimized latent representation to candidate token substitutions using a language model. A key novelty of CLOSS is using Shapley values to estimate the potential of a token substitution in changing the model’s prediction when used in combination with other substitutions. The Shapley value is then used to guide a breadth-first beam search to generate the final counterfactual. Through both automatic and human evaluations, we show that CLOSS achieves highly competitive performance both in terms of the success rate of generating counterfactuals as well as the quality of the generated counterfactuals. Our approach has several limitations. As a whitebox approach, we require full access to the model, which can be restrictive in practical applications. Our approach currently only considers substitutions, excluding deletions and insertions. Finally, our method is only applicable to models that are based on pre-trained language models. Future work will adapt CLOSS to adversarial and black box settings. We also hope to improve the efficiency of CLOSS via more efficient Shapley value estimation (Chen et al., 2018; Jia et al., 2020).",1
"Abstract We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the models prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent whitebox and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP models' classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the models behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification models decision for financial texts. Related, textual adversaries also aim to change the model prediction (with modifications resembling natural text). The difference is that adversaries further aim to escape human detection (not changing a humans classification), whereas counterfactuals do not have such requirement. Another line of related work is style transfer (Sudhakar et al., 2019; Wang et al., 2019; Hu et al., 2017), which aim to modify a given text according to a target style. It differs from adversary or counterfactual generation in that it seeks to fully change all style-related phrases, as opposed to minimally perturbing a text to change a classifier's decision. White-box approaches have been widely used to generate adversaries or counterfactuals for vision tasks where the continuous inputs can be optimized to alter model predictions (Goodfellow et al., 2014; Carlini and Wagner, 2017; Neal et al., 2018). Such optimization based approaches are difficult to apply to language due to the discrete nature of text. We circumvent this difficulty by directly optimizing in the latent space of the input towards the desired classification. We then exploit the language generation capability of pre-trained language models, available for most state-of-the-art NLP models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), to generate semantically plausible substitutions from the optimized latent representations. We further introduce Shapley values to estimate the combinatoric effect of multiple simultaneous changes, which are then used to guide a beam search to generate the final counterfactual. Leveraging pre-trained language models to generate alternative texts has been a popular black-box approach in the recent literature on text adversaries (Li et al., 2020b; Garg and Ramakrishnan, 2020; Li et al., 2020a). Our work presents a first attempt to combine the strength of white-box optimization and the power of pre-trained language models. While Shapley values have been widely studied for the problemfeature importance (Lundberg and Lee, 2017; Sundararajan and Najmi, 2020) and data valuation (Jia et al., 2020), this is the first effort demonstrating their usefulness for text generation. We compare our method to several white-box and black-box baselines on two different text classification tasks. Automatic and human evaluation results show that our method significantly improves success rate and the quality of the generated counterfactuals. Introduction Deep neural networks have achieved state-of-theart performances for many natural language processing (NLP) tasks (Otter et al., 2020; Ruder et al., 2019). When applying such models in real world applications, understanding their behavior can be challenging the ever increasing complexity of such models makes it difficult to understand and debug their predictions. A human can explain why an example belongs to a specific concept class by constructing a counterfactual of an example that is minimally altered but belongs to a different class. Contrasting the original example with its counterfactual highlights the critical aspects signifying the concept class. We study a similar approach to understand deep NLP models' classification criteria. Given a classifier and an input text, our goal is to generate a counterfactual by making a set of minimal modifications to the text that change the label assigned by the classifier. Additionally, our goal is to understand the models behavior when processing naturally occurring inputs, hence we wish to generate grammatically correct and semantically plausible counterfactuals. Automatic generation of text counterfactuals has been studied in different settings. Qin et al. (2019) considered counterfactual story rewriting which aims to minimally rewrite an original story to be compatible with a counterfactual event. Wu et al. (2021) used a fine-tuned GPT-2 model to generate general purpose counterfactuals that are not tied to a particular classification model. Yang et al. (2020) aim to generate plausible-sounding counterfactuals that flip a classification models decision for financial texts. Related, textual adversaries also aim to change the model prediction (with modifications resembling natural text).",0
"Abstract Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehen- sibility and maintainability of the law. With five intuitive law smells as running examples—namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsession—, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting. Keywords Refactoring · Software Engineering · Law · Natural Language Process- ing · Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host alone accounting for 250 million repositories between 2008 and 2020.1 Over the course of its growth, this codebase has become increasingly interconnected when viewed through the lens of network analy- sis (Lima et al. 2014). Naturally, software engineers have turned to technology to keep track of this development and manage code interdependencies. While the challenges for law and software engineering and the constraints within which these challenges must be addressed are not identical, both domains share three important characteris- tics: Materially, their subject matters, legal rules on the one hand and code fragments on the other hand, contain commands intended to control (human or machine) behav- ior in order to achieve specific outcomes. Procedurally, output creation in both law and software engineering is distributed in time and space, and thus, both domains are subject to the challenges of dynamic multi-agent systems. Methodologically, lawyers and software engineers alike use abstractions to accommodate problems that are not fully known in the present. The similarities between the law and software engineering domains suggest possi- bilities for knowledge transfer between them. In this paper, we explore one such pos- sibility for the software engineering subfield of refactoring. Introduced into academic discourse by Opdyke and Johnson (1990) and popularized by Becker et al. (1999) (Second Edition: Fowler 2018), refactoring “describes a disciplined technique for re- structuring an existing body of code, altering its internal structure without changing its external behavior.”2 In software engineering, refactoring is indispensable for en- suring software quality and maintainability, and it is also subject to vivid academic discourse, inspiring detailed analyses of large code bases and even dedicated confer- ences.3 This has resulted in an actionable understanding of how various variables— e.g., programming language, team size, project size, or commit size (Ray et al. 2017), repetition and duplicated code (Lopes et al. 2017), or component size and open source model (Stamelos et al. 2002)—impact code quality. In this paper, we demonstrate how concepts from refactoring can be used in law, focusing on the example of code smells. At a high level, a code smell is a characteris- tic of (a part of) the source code that may indicate a deeper problem in its design and implementation, highlighting a need for refactoring (Tufano et al. 2015). We port this idea to the legal domain, introducing the concept of law smells. Law smells constitute the first step towards both (semi-)automatically detecting problematic parts of exist- ing codifications and (semi-)automatically improving codification quality. They also pave the path towards reproducible, quantitative quality measures for collections of legal rules, which allow us to assess the overall law climate and facilitate automatic code review and quality assurance. The remainder of this paper is structured as follows. In Section 2, we present related literature from law and computer science, along with interdisciplinary work in the intersection of both fields. We develop the concept of law smells, guided by five illustrative examples, in Section 3, and describe methods to detect law smells in Section 4. In Section 5, we demonstrate the utility of our methods for detecting five example smells, deliberately showcasing a variety of ways to present law smell detection results to end users. We discuss the limitations of our approach along with opportunities for future work in Section 6, and conclude in Section 7. Conclusion We have initiated the systematic study of law smells, i.e., patterns in legal texts that might impair the comprehensibility and maintainability of the law and indicate the need for legal refactoring. Guided by five illustrative examples, we have developed a comprehensive law smell taxonomy, introduced a methodological toolkit for law smell detection, and confirmed the utility of this toolkit in practice by applying it to 22 years of codified statutory legislation in the United States Code (1998–2019). Our work demonstrates how ideas from software engineering can be ported to the legal domain, and it creates numerous opportunities for further research on defining, detecting, and deodorizing law smells in all types of legal texts. Thus, we hope to have contributed a building block for the road towards truly computational legal drafting.",1
"Abstract Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehen- sibility and maintainability of the law. With five intuitive law smells as running examples namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsession, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting. Keywords Refactoring· Software Engineering· Law · Natural Language Process- ing· Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host alone accounting for 250 million repositories between 2008 and 2020.1 Over the course of its growth, this codebase has become increasingly interconnected when viewed through the lens of network analy- sis (Lima et al. 2014). Naturally, software engineers have turned to technology to keep track of this development and manage code interdependencies. While the challenges for law and software engineering and the constraints within which these challenges must be addressed are not identical, both domains share three important characteris- tics: Materially, their subject matters, legal rules on the one hand and code fragments on the other hand, contain commands intended to control (human or machine) behav- ior in order to achieve specific outcomes. Procedurally, output creation in both law and software engineering is distributed in time and space, and thus, both domains are subject to the challenges of dynamic multi-agent systems. Methodologically, lawyers and software engineers alike use abstractions to accommodate problems that are not fully known in the present. The similarities between the law and software engineering domains suggest possi- bilities for knowledge transfer between them. In this paper, we explore one such pos- sibility for the software engineering subfield of refactoring. Introduced into academic discourse by Opdyke and Johnson (1990) and popularized by Becker et al. (1999) (Second Edition: Fowler 2018), refactoring describes a disciplined technique for re- structuring an existing body of code, altering its internal structure without changing its external behavior. In software engineering, refactoring is indispensable for en- suring software quality and maintainability, and it is also subject to vivid academic discourse, inspiring detailed analyses of large code bases and even dedicated confer- ences.3 This has resulted in an actionable understanding of how various variables e.g., programming language, team size, project size, or commit size (Ray et al. 2017), repetition and duplicated code (Lopes et al. 2017), or component size and open source model (Stamelos et al. 2002)impact code quality. In this paper, we demonstrate how concepts from refactoring can be used in law, focusing on the example of code smells. At a high level, a code smell is a characteris- tic of (a part of) the source code that may indicate a deeper problem in its design and implementation, highlighting a need for refactoring (Tufano et al. 2015). We port this idea to the legal domain, introducing the concept of law smells. Law smells constitute the first step towards both (semi-)automatically detecting problematic parts of exist- ing codifications and (semi-)automatically improving codification quality. They also pave the path towards reproducible, quantitative quality measures for collections of legal rules, which allow us to assess the overall law climate and facilitate automatic code review and qualityThe remainder of this paper is structured as follows. In Section 2, we present related literature from law and computer science, along with interdisciplinary work in the intersection of both fields. We develop the concept of law smells, guided by five illustrative examples, in Section 3, and describe methods to detect instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting. Keywords Refactoring ÃÂ· Software Engineering · Law· Natural Language Process- ing· Network Analysis. Introduction In modern societies, law is one of the main tools to regulate human activities. These activities are constantly changing, and law co-evolves with them. In the past decades, human activities have become increasingly differentiated and intertwined, e.g., in de- velopments described as globalization or digitization. Consequently, legal rules, too, have grown more complex, and statutes and regulations have increased in volume, interconnectivity, and hierarchical structure (Katz et al. 2020; Coupette et al. 2021a). A similar trend can be observed in software engineering, albeit on a much shorter time scale. The global codebase has grown exponentially in recent years (Yu et al. 2014), with GitHub as the largest source code host.",0
"Abstract. A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information. Almost everybody prefers to spend a short time reading a summary of a topic before deciding whether to engage in further reading. Presently, the majority of people still check documents by hand because the current technology is mainly based on users providing keywords to filter articles. The user would save much time if it were possible to perform automatic checks on the document. Consequently, we have an idea to implement an automated topic extraction method for research papers. Consequently, we have an idea to implement an automated topic extraction method for research papers. This will facilitate the reading process and give the readers a good sense of what the papers are about. This allows them to quickly and easily decide which documents to read. A proven technique for achieving this is topic extraction (also known as key phrase extraction). Research on this topic has been carried out by a large number of researchers. They have published some insightful articles about models and frameworks. For example, in Zhiyuan et al (2010)’s work, the authors propose two approaches for topic extraction: supervised and unsupervised approaches. Turney's work illustrates the supervised principle by providing a model for determining if a topic should be considered a key topic. An online digital library search engine offers this type of extraction. Human labeling is an inconvenience with this approach. It means that the users are required to come up with their own keywords, which are then compared to the whole database to see if any labels are found. However, if the purpose is to collect a large number of articles, the user will have to spend considerable time ensuring that the results are actually focused on his keywords instead of just briefly mentioning them several times. This requires a supervised approach because of the time-consuming nature of the unsupervised principle. The graph-based ranking algorithms, as suggested by Mihalcca and Tarau, have gained a lot of attention and have proven successful in the supervised setting. Based on recursively computing information from the entire graph, this approach can be used to determine the relative importance of vertexes within a graph. The most common uses of these algorithms are to analyze citations, social networks, and web links. Therefore, topic extraction cannot be used to summarize the text's concept, which is its major limitation. Here, we propose a method based on n-gram analysis to automate the topic extraction process and make it more efficient and reliable. To filter out irrelevant results, we create semi-customized blacklists and whitelists to make the process more efficient. after ngram analysis. We also take an adequate number of training samples to ensure the quality of our blacklist and whitelist. Our automated approach will also be evaluated with the aid of a manual process to determine its effectiveness and accuracy. The objective of our study is to help educators properly label selected documents so that they can decide if this is the right topic for them. The topic extraction process should also be fully automated. It means the reader should be able to identify the topic in the shortest amount of time, and the topic should be delivered automatically to the reader. 1.2 Research Objective As a result, we propose an integrated strategy to automate topic extraction automation by analyzing N-grams with the help of blacklists and whitelists, and subsequent labeling of each document based on the results. The aim of this is to develop a practical solution for automating article topic extraction from a large number of articles. The application works well in assisting scholars or any person interested in finding an article they desire from an abundance of articles. By analyzing the results of automatic and manual topic extraction methods, we will have a deeper understanding of the accuracy and efficiency of the N-gram based method. RQ1: “Is N-gram analysis useful in automating topic extraction from research papers?” RQ2: “How can we evaluate our automated topic extraction approach?”. Conclusion In light of the results we obtained, the text-based topic extraction approach is powerful and time-saving to use if the evaluation conditions are set up right. We have concluded that text-based topics can be very effective which provided the evaluation conditions are properly set up. We are able to efficiently perform topic extraction by automating our process while still maintaining an acceptable level of accuracy. Even though an automated approach does not always result in highly accurate labels, our approach can serve as a valuable starting point for topic extraction. As a result of continuous development over time, identifying more accurate labels can be achieved by regularly updating and carefully selecting the black- and whitelists. In order to accomplish the automated topic extraction, our approach excels at processing a variety of PDF documents and presenting the reader with the potential topics of interest. It saved the reader's time to read articles that they are unsure if the articles are relevant to their interests. With a more reasonable set of conditions, it will be more likely that our topic extraction will become more accurate for more topics.",1
"Abstract. A lot of manual work goes into identifying a topic for an article. With a large volume of articles, the manual process can be exhausting. Our approach aims to address this issue by automatically extracting topics from the text of large numbers of articles. This approach takes into account the efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information. Almost everybody prefers to spend a short time reading a summary of a topic before deciding whether to engage in further reading. Presently, the majority of people still check documents by hand because the current technology is mainly based on users providing keywords to filter articles. The user would save much time if it were possible to perform automatic checks on the document. Consequently, we have an idea to implement an automated topic extraction method for research papers. Consequently, we have an idea to implement an automated topic extraction method for research papers. This will facilitate the reading process and give the readers a good sense of what the papers are about. This allows them to quickly and easily decide which documents to read. A proven technique for achieving this is topic extraction (also known as key phrase extraction). Research on this topic has been carried out by a large number of researchers. They have published some insightful articles about models and frameworks. For example, in Zhiyuan et al (2010)'s work, the authors propose two approaches for topic extraction: supervised and unsupervised approaches. Turney's work illustrates the supervised principle by providing a model for determining if a topic should be considered a key topic. An online digital library search engine offers this type of extraction. Human labeling is an inconvenience with this approach. It means that the users are required to come up with their own keywords, which are then compared to the whole database to see if any labels are found. However, if the purpose is to collect a large number of articles, the user will have to spend considerable time ensuring that the results are actually focused on his keywords instead of just briefly mentioning them several times. This requires a supervised approach because of the time-consuming nature of the unsupervised principle. The graph-based ranking algorithms, as suggested by Mihalcca and Tarau, have gained a lot of attention and have proven successful in the supervised setting. Based on recursively computing information from the entire graph, this approach can be used to determine the relative importance of vertexes within a graph. The most common uses of these algorithms are to analyze citations, social networks, and web links. Therefore, topic extraction cannot be used to summarize the text's concept, which is its major limitation. Here, we propose a method based on n-gram analysis to automate the topic extraction process and make it more efficient and reliable. To filter out irrelevant results, we create semi-customized blacklists and whitelists to make the process more efficient. after ngram analysis. We also take an adequate number of training samples to ensure the quality of our blacklist and whitelist. Our automated approach will also be evaluated with the aid of a manual process to determine its effectiveness and accuracy. The objective of our study is to help educators properly label selected documents so that they can decide if this is the right topic for them. The topic extraction process should also be fully automated. It means the reader should be able to identify the topic in the shortest amount of time, and the topic should be delivered automatically to the reader. 1.2 Research Objective As a result, we propose an integrated strategy to automate topic extraction automation by analyzing N-grams with the help of blacklists and whitelists, and subsequent labeling of each document based on the results. The aimthis is to develop a practical solution for automating article topic extraction from a large number of articles. The application works well in assisting scholars or any person interested in finding an article they desire from an abundance of articles. By analyzing the results of automatic and manual topic extraction methods, we have a deeper understanding of the accuracy and efficiency of the process. Based on existing N-gram analysis, our research examines how often certain words appear in documents in order to support automatic topic extraction. In order to improve efficiency, we apply custom filtering standards to our research. Additionally, delete as many noncritical or irrelevant phrases as possible. In this way, we can ensure we are selecting unique keyphrases for each article, which capture its core idea. For our research, we chose to center on the autonomous vehicle domain, since the research is relevant to our daily lives. We have to convert the PDF versions of most of the research papers into editable types of files such as TXT. This is because most of the research papers are only in PDF format. To test our proposed idea of automating, numerous articles on robotics have been selected. Next, we evaluate our approach by comparing the result with that obtained manually. Keywords: Automatic Topic Extraction, Frequency Statistic, Keyphrase, Ngram 1 Introduction 1.1 Overview The rapid growth of information makes retrieving information vital today. Research papers can be retrieved most easily based on their topics. In today's world, it is more necessary than ever to filter out the irrelevant information and gather the interesting information.",0
"Abstract Over the recent years, large pretrained lan- guage models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general lan- guage has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include specific language that common LMs can not repre- sent accurately. We argue that this short- coming of today’s LMs limits the applica- bility of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., Grüning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018). Deep learning tech- niques that promise higher accuracy are grad- ually replacing these approaches (e.g., Kölbel et al., 2020; Luccioni et al., 2020; Bingler et al., 2021; Callaghan et al., 2021; Wang et al., 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP al- lows for impressive results, outperforming tradi- tional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretrain- ing and leads to the model learning representa- tions of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Represen- tations from Transformers) (Devlin et al., 2018) with its successors ROBERTA (Liu et al., 2019), Transformer-XL (Dai et al., 2019) and ELEC- TRA (Clark et al., 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and lan- guage patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of sam- ples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task- specific text exhibits better performance, com- pared to a model that has been pretrained solely on general text (Araci, 2019; Lee et al., 2020). Hence, a straightforward extension to the stan- dard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al., 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times — in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) + domain-adaptive pretraining (downstream domain) + training (downstream task). To date, regardless of the increase in using NLP for climate change related research, a model with climate domain-adaptive pretraining has not been publicly available, yet. Research so far rather re- lied on models pretrained on general language, and fine-tuned on the downstream task. To fill this gap, our contribution is threefold. First, we intro- duce CLIMATEBERT, a state-of-the-art language model that is specifically pretrained on climate- related text corpora of various sources, namely news, corporate disclosures, and scientific articles. This language model is designed to supports re- searchers of various disciplines in obtaining better performing NLP models for a manifold of down- stream tasks in the climate change domain. Sec- ond, to illustrate the strength of CLIMATEBERT, we highlight the performance improvements using CLIMATEBERT on three standard climate-related NLP downstream tasks. Third, to further promote research at the intersection of climate change and NLP, we make the weights of all trained language models publicly available at climatebert.ai. Conclusion We propose CLIMATEBERT, the first language model that was pretrained on a large scale dataset of over 1.6 million climate-related paragraphs. We study various selection strategies to find samples from our corpus which are most helpful for later tasks. Our experiments reveal that our domain- adaptive pretraining leads to considerably lower masked language modeling loss on our climate corpus. We further find that this improvement is also reflected in predictive performance across three essential downstream climate-related NLP tasks: text classification, the analysis of risk and opportunity statements by corporations, and fact- checking climate-related claims.",1
"Abstract Over the recent years, large pretrained lan- guage models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general lan- guage has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include specific language that common LMs can not repre- sent accurately. We argue that this short- coming of today's LMs limits the applica- bility of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., Gruning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018). Deep learning tech- niques that promise higher accuracy are grad- ually replacing these approaches (e.g., Kolbel et al., 2020; Luccioni et al., 2020; Bingler et al., 2021; Callaghan et al., 2021; Wang et al., 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP al- lows for impressive results, outperforming tradi- tional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretrain- ing and leads to the model learning representa- tions of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Represen- tations from Transformers) (Devlin et al., 2018) with its successors ROBERTA (Liu et al., 2019), Transformer-XL (Dai et al., 2019) and ELEC- TRA (Clark et al., 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and lan- guage patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of sam- ples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task- specific text exhibits better performance, com- pared to a model that has been pretrained solely on general text (Araci, 2019; Lee et al., 2020). Hence, a straightforward extension to the stan- dard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al., 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) + domain-adaptive pretraining (downstream domain) + training (downstream task). To date, regardless of the increase in using NLP for climate change related research, a model with climate domain-adaptive pretraining has not been publicly available, yet. Research so far rather re- lied on models pretrained on general language, and fine-tuned on the downstream task. To fill this gap, our contribution is threefold. First, we intro- duce CLIMATEBERT, a state-of-the-art language model that is specifically pretrained on climate- related text corporavarious sources, namely news, corporate disclosures, and scientific articles. This language model is designed to supports re- searchers of various disciplines in obtaining better performing NLP models for a manifold of down- stream tasks in the climate change domain. Sec- ond, to illustrate the strength of CLIMATEBERT, we highlight the performance improvements using CLIMATEBERT for a variety of down- stream tasks in the climate change domain. As a remedy, we propose CLIMATEBERT for: (i) National climate reporting, (ii) common news, and (iii) corporate disclosures. T 1:Toon: this domain is further trained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, corporate disclosures, and climate-related text. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like this. As a remedy, we propose CLIMATEBERT for: (i) National climate reporting, (ii) common news, and (iii) corporate disclosures.",0
"Abstract We present the task of Automated Punish- ment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Ad- dressing APE will enable the identification of sentencing patterns and constitute an im- portant stepping stone for many follow up le- gal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evalua- tion dataset, and implement rule-based and su- pervised models. We find that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models’ errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions. Much of the legal data, however, comes in unstructured formats that pose critical challenges for extracting and analyzing it in sys- tematic ways. In addition, different countries vary in their legal systems, norms and conventions, fur- ther compounding the challenges in developing multilingual approaches (Peruginelli, 2009). While legal NLP is gaining traction in recent years (Van Gog and Van Engers, 2001; Dale, 2019; Zhong et al., 2020), relatively little attention has been given to low-resource settings outside of the English language, where the availability of tools such as large pretrained language models, syntactic parsers, or named entity recognizers is limited. In this work, conducted as part of an on-going collaboration with The Association of Rape Crisis Centers in Israel (ARCCI), we focus specifically on the task of Automated Punishment Extraction (APE) in sexual assault cases in Hebrew within Israeli court sentencing decisions (see formal task definition in Section 2). Punishment decisions are of special importance as they constitute a prerequi- site for many other downstream tasks in legal NLP and digital humanities, such as legal prediction of judicial decisions (Aletras et al., 2016; Branting et al., 2021) and detecting biases in court deci- sions (Pinto et al., 2020). APE is difficult in the Israeli court system. This is due to the fact that sen- tencing decisions for criminal offences are reported, in natural language idiomatic to the legal field, in the written sentencing decision. We focus on sex- ual assault cases due to the legal and public debate around claims of lenient punishments (Phillips and Chagnon, 2020), that in the absence of system- atic rigorous data collection cannot be empirically examined and assessed. This worldwide debate requires legal NLP methods in multiple languages and legal systems. To address this challenge, we begin by curating a dataset of sexual assault sentencing decisions from the years 1990-2021 and manually annotate pun- ishment in a subset of 100 cases with the use of legal experts in our team and in collaboration with ARCCI (Section 3). Following, in Section 4, we use this data to build several models for the APE task, including rule-based and supervised methods, based on linguistically and semantically informed features, setting first benchmark results on the APE task in Hebrew. We thoroughly analyze our models’ performance in Section 5, finding that they are ca- pable of extracting the correct punishment in 68% of the cases,while the best model’s average error is roughly 5 months, attesting to the difficulty of the task. Based on our models, we find that in our data the median predicted punishment is 3 years, while more than a third of the punishments are below 15 months. Although these figures are obtained on a medium-size corpus, using automatic measures which do not account for the type of offense, we note that they are well below the maximum pun- ishments for sexual offenses as determined by the Israeli legislator, which range between 2-7 years for indecent acts and sodomy and up to 20 years for aggravated rape. We conclude by analyzing common error pat- terns in our models. For example, we find that models often tend to erroneously extract a proba- tion imprisonment punishment instead of the actual imprisonment punishment. Distinguishing between the two is left as an interesting avenue for future work. To the best of our knowledge, this is the first examination of automatic punishment extraction in the Hebrew language. It includes data collection, annotation, and benchmark models. We hope it will spur further research into this important task. Conclusions In this work we created the first annotated corpus of Hebrew language sentencing decisions, focus- ing on sexual assaults. We compared a rule-based approach with supervised learners using the unique attributes of the legal language for representing sentences. We found that the rule-based approach achieved best results with an average error rate of 5 months and accuracy of 68% in extracting the punishment sentence. Our analysis shows that such research could focus on fine tuning of the super- vised models. While supervised learning models help us narrow down a full legal document to 2 - 5 sentences that include the punishment, further research can contribute in reaching a single target sentence, which could also benefit from our error analysis, especially regarding the probation sen- tences, perhaps targeting them separately in a prior task and using them as features.",1
"Abstract We present the task of Automated Punish- ment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Ad- dressing APE will enable the identification of sentencing patterns and constitute an im- portant stepping stone for many follow up le- gal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evalua- tion dataset, and implement rule-based and su- pervised models. We find that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models' errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions. Much of the legal data, however, comes in unstructured formats that pose critical challenges for extracting and analyzing it in sys- tematic ways. In addition, different countries vary in their legal systems, norms and conventions, fur- ther compounding the challenges in developing multilingual approaches (Peruginelli, 2009). While legal NLP is gaining traction in recent years (Van Gog and Van Engers, 2001; Dale, 2019; Zhong et al., 2020), relatively little attention has been given to low-resource settings outside of the English language, where the availability of tools such as large pretrained language models, syntactic parsers, or named entity recognizers is limited. In this work, conducted as part of an on-going collaboration with The Association of Rape Crisis Centers in Israel (ARCCI), we focus specifically on the task of Automated Punishment Extraction (APE) in sexual assault cases in Hebrew within Israeli court sentencing decisions (see formal task definition in Section 2). Punishment decisions are of special importance as they constitute a prerequi- site for many other downstream tasks in legal NLP and digital humanities, such as legal prediction of judicial decisions (Aletras et al., 2016; Branting et al., 2021) and detecting biases in court deci- sions (Pinto et al., 2020). APE is difficult in the Israeli court system. This is due to the fact that sen- tencing decisions for criminal offences are reported, in natural language idiomatic to the legal field, in the written sentencing decision. We focus on sex- ual assault cases due to the legal and public debate around claims of lenient punishments (Phillips and Chagnon, 2020), that in the absence of system- atic rigorous data collection cannot be empirically examined and assessed. This worldwide debate requires legal NLP methods in multiple languages and legal systems. To address this challenge, we begin by curating a dataset of sexual assault sentencing decisions from the years 1990-2021 and manually annotate pun- ishment in a subset of 100 cases with the use of legal experts in our team and in collaboration with ARCCI (Section 3). Following, in Section 4, we use this data to build several models for the APE task, including rule-based and supervised methods, based on linguistically and semantically informed features, setting first benchmark results on the APE task in Hebrew. We thoroughly analyze our models' performance in Section 5, finding that they are ca- pable of extracting the correct punishment in 68% of the cases,while the best model's average error is roughly 5 months, attesting to the difficulty of the task. Based on our models, we find that in our data the median predicted punishment is 3 years, while more than a third of the punishments are below 15 months. Although these figures are obtained on a medium-size corpus, using automatic measures which do not account for the type of offense, we note that they are well below the maximum pun- ishments for sexual offenses as determined by the Israeli legislator, which range between 2-7 years for indecent acts and sodomy and up to 20 years for aggravated rape. We conclude by analyzing common error pat- terns in our models. For example, we find that models often tend to erroneously extract a proba- tion imprisonment punishment instead of the actual imprisonment punishment. Distinguishing between the two is left as an interesting avenue for future work. To the best of our knowledge, this is the first examination of automatic punishment extraction in the Hebrew language.includes data collection, annotation, and benchmark models. We hope it will spur further research into this important task. Conclusions In this work we created the first annotated corpus of Hebrew language sentencing decisions, focus- ing on sexual assaults. We compared a rule-based approach with supervised learners using the unique attributes of the legal language for representing sentences. We found that while super- vised models can identify the sentence contain- ing the punishment with good accuracy, rule- based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models' errors, indicat- ing avenues for future work, such as distin- guishing between probation and actual imprisonment punishment. We will make all our re- sources available upon request, including data, annotation, and first benchmark models. Introduction The legal world is rife with data, from constitu- tions and national legislation to legal cases and court decisions.",0
"Abstract We present Harmonic Memory Networks (HMem), a neural architecture for knowl- edge base completion that models entities as weighted sums of pairwise bindings between an entity’s neighbors and corresponding rela- tions. Since entities are modeled as aggregated neighborhoods, representations of unseen enti- ties can be generated on the fly. We demon- strate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methods—knowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a fact—which in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)—are com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces. Since representations must be learned from training-set instances of each component, this creates problems when such databases are to be scaled, and there- fore these methods have difficulty accommodating an open-world setting in which knowledge graphs evolve in time, since new facts inserted into the database after model training cannot be used for inference without model retraining. Furthermore, databases may be augmented in time not only with new facts about known entities, but also with new entities. In embedding-based models, new repre- sentation for such entities must be trained. We present Harmonic Memories (HMem), a neu- ral network which models entities by aggregating information about their neighborhoods using a su- perposition memory architecture, achieving gener- alization to new entities without retraining.1 The network combines two ideas. First, a represen- tation of entities as memory states consisting of superposed vector associations between learned en- tity and relation embeddings. Second, completion of memory states using a learned transformation based on Harmony-optimization methods (Smolen- sky and Legendre, 2006) (see §4). We refer to vector associations as bindings in the sense of the ”variable-binding problem” in the philosophy of cognitive science: in neural net models of cogni- tion, how are representations of the elements of a structure bound together into structures? In this work, we investigate two solutions prominent in the cognitive science literature—tensor product bind- ing (Smolensky, 1990) and circular convolution (Plate, 1994)—which have also both been effec- tively applied in KBC (Nickel et al., 2011, 2016). The approach is inspired by computational mod- eling of biological neural architectures for knowl- edge representation (Crawford et al., 2015), and is related to KBC methods based on convolution of graph neighborhoods (Schlichtkrull et al., 2017; Dettmers et al., 2018; Nguyen et al., 2018), in which inference is performed over representations of aggregated entity neighborhoods. Recent work has extended this idea using Graph Attention Net- works (GANs) (Nathani et al., 2018), which assign attention weights to entries in a graph neighbor- hood, these being later combined. For instance, Velicˇkovic ́ et al. (2018) use Graph Attention to gen- erate weights for triplet representations obtained by transforming concatenated entity and relation vectors, combining the results by averaging. This is similar to our approach, with the key difference that formulating the model—as we do—in terms of binding allows for clear formal analysis of certain scaling results (§7). We therefore gain in inter- pretability. HMem scales well in three respects. First, it allows a database with a fixed set of entities and relations to incorporate new facts into the model without parameter re-estimation. Empirically, per- formance improves in nearly every case when the neighborhoods are thus expanded. Second, it permits the addition of entities unseen in train- ing, whose representations are useless in a vec- tor embedding framework. For our model, infer- ences about these entities are possible when a sub- graph including them becomes available. Third, our model effectively handles nodes with high in-degree. We show that, whereas embedding- based approaches show decreased performance with highly connected nodes, our model exhibits improved performance on nodes with many neigh- bors. §2, §3 and §4 introduce the Harmonic Memory architecture, and §5 shows that our model achieves state-of-the-art results on benchmark KBC datasets. After evaluation on standard benchmarks, §6 introduces WNGen and FBGen, datasets based on Word- Net and FREEBASE that evaluate the network’s ability to abstract from node identity and make in- ferences exclusively on the basis of information about nodes in its neighborhood, and §7 examines in detail how the model scales with the size of en- tity neighborhoods and the addition of new input facts. §8 concludes. Conclusion This article presents a neural model for knowledge base completion that is powerful enough to achieve state of the art results on large databases, and flexi- ble enough to evolve with knowledge base content sans retraining. The approach complements ex- isting neighborhood-aggregation techniques (e.g. graph convolution), with the advantage of inter- pretable mechanisms: vector binding and memory completion. The results indicate that the model operates well at scale and in an open-world setting.",1
"Abstract We present Harmonic Memory Networks (HMem), a neural architecture for knowl- edge base completion that models entities as weighted sums of pairwise bindings between an entity's neighbors and corresponding rela- tions. Since entities are modeled as aggregated neighborhoods, representations of unseen enti- ties can be generated on the fly. We demon- strate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methods knowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a fact which in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge) are com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces. Since representations must be learned from training-set instances of each component, this creates problems when such databases are to be scaled, and there- fore these methods have difficulty accommodating an open-world setting in which knowledge graphs evolve in time, since new facts inserted into the database after model training cannot be used for inference without model retraining. Furthermore, databases may be augmented in time not only with new facts about known entities, but also with new entities. In embedding-based models, new repre- sentation for such entities must be trained. We present Harmonic Memories (HMem), a neu- ral network which models entities by aggregating information about their neighborhoods using a su- perposition memory architecture, achieving gener- alization to new entities without retraining.1 The network combines two ideas. First, a represen- tation of entities as memory states consisting of superposed vector associations between learned en- tity and relation embeddings. Second, completion of memory states using a learned transformation based on Harmony-optimization methods (Smolen- sky and Legendre, 2006) (see §4). We refer to vector associations as bindings in the sense of the variable-binding problem in the philosophy of cognitive science: in neural net models of cogni- tion, how are representations of the elements of a structure bound together into structures? In this work, we investigate two solutions prominent in the cognitive science literature tensor product bind- ing (Smolensky, 1990) and circular convolution (Plate, 1994) which have also both been effec- tively applied in KBC (Nickel et al., 2011, 2016). The approach is inspired by computational mod- eling of biological neural architectures for knowl- edge representation (Crawford et al., 2015), and is related to KBC methods based on convolution of graph neighborhoods (Schlichtkrull et al., 2017; Dettmers et al., 2018; Nguyen et al., 2018), in which inference is performed over representations of aggregated entity neighborhoods. Recent work has extended this idea using Graph Attention Net- works (GANs) (Nathani et al., 2018), which assign attention weights to entries in a graph neighbor- hood, these being later combined. For instance, Velickovic et al. (2018) use Graph Attention to gen- erate weights for triplet representations obtained by transforming concatenated entity and relation vectors, combining the results by averaging. This is similar to our approach, with the key difference that formulating the model as we do in terms of binding allows for clear formal analysis of certain scaling results (§7). We therefore gain in inter- pretability. HMem scales well in three respects. First, it allows a database with a fixed set of entities and relations to incorporate new facts into the model without parameter re-estimation. Empirically, per- formance improves in nearly every case when the neighborhoods are thus expanded. Second, it permits the addition of entities unseen in train- ing, whose representations are useless in a vec- tor embedding framework. For our model, infer- ences about these entities are possible when a sub- graph including them becomesThird, our model effectively handles nodes with high in-degree. We show that, whereas embedding- based approaches show decreased performance with highly connected nodes, our model exhibits improved performance on nodes with many neigh- bors. §2, §3 and §4 introduce the Harmonic Memory architecture, and §5 shows that our model achieves state-of-the-art results on benchmark servers, and ranks so high that it exceeds expectations. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methods knowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a fact which in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)are com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces.",0
"Abstract Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Dis- tant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstracti- fies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance. Introduction Relation extraction (RE) is a key facet of information extraction in large bodies of unstructured textual data. RE is particularly important in the biomedical domain where extracting relationships between pairs of biomedical entities, also known as “fact triples”, can produce new insights into complicated biological interactions. For instance, with the near-exponential growth of microbiome research [Sa’ed et al., 2019], advanced RE methods may help discover important links between gut microbiota and diseases. It is in this context that we motivate our work. RE within the biomedical domain comes with two inherent challenges: there are more than 30 million scientific articles, with hundreds of thousands of articles published every year, and there is a corresponding lack of labeled data. To resolve these challenges, many have leveraged distant supervision techniques which pair knowledge graphs with raw textual data to automatically generate labels to train deep-learning models [Gu et al., 2019, Su et al., 2019, Junge and Jensen, 2019]. We seek to improve distantly supervised biomedical RE methods in this work. We use the Unified Medical Language System (UMLS) Metathesaurus [Bodenreider, 2004] for our knowledge graph and pair it with raw textual data from PubMed [Canese and Weis, 2013]. To automatically generate labels, distantly supervised RE methods rely on a simple yet powerful assumption: any singular sentence that contains a pair of entities also expresses a relationship, as determined by the accompanying knowledge graph, between those entities [Mintz et al., 2009]. However, this assumption leads to a noisy training signal with many false positives as not all sentences express a relationship between an entity pair. To combat this, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping sentences into bags greatly reduces noise in the training signal since a bag of sentences is more likely to express a relationship than a single sentence. This enables the model to better classify relationships between unseen entity pairs. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences of evidence. After processing the PubMed corpus, we observe that a majority (∼ 52%) of extracted triples are supported by fewer than three sentences. Creating bags of sentences for such entity pairs requires heavy up-sampling. For example, if a pair of entities is only supported by one sentence and a bag size is equal to 16 sentences, the single sentence is duplicated 15 times to fill the bag. This erases the benefit of MIL. To counter this issue, we introduce abstractified multi-instance learning (AMIL) where, instead of grouping entity pairs by name, we group entities by the corresponding semantic type as determined by UMLS. UMLS categorizes each entity with a semantic type within the UMLS semantic network. The UMLS semantic network is curated by human experts for decades and provides a rich ontology of biomedical concepts which we leverage to group multiple different entity pairs within a single MIL bag, reducing the need to up-sample sentences. For example, consider two sentences: (1) a sentence containing the entity pair (fibula, tibia) and (2) a second sentence containing the entity pair (humerus, ulna). With distant supervision, we assume each sentence expresses the relationship linking both pairs, namely articulates with. Despite expressing the same relationship, without abstraction, these sentences are placed into separate MIL bags since bags are grouped by distinct entity pairs. By introducing abstractified multi-instance learning, the entities fibula, tibia, humerus, and ulna are grouped by their corresponding UMLS semantic type—“Body Part, Organ, or Organ Component.” This allows us to place the aforemen- tioned sentences into the same MIL bag based on their entity type, creating a heterogeneous bag of entity pairs that express the same relationship. With this reformulation, bags containing a single duplicated sentence are reduced by half. AMIL produces better overall performance for biomedical RE with significant performance gains for “rare” triples. Here, we define “rare” triples as triples that are supported by fewer than eight sentences. These triples make up roughly 80% of the long-tail distribution of triples. We also take inspiration from Soares et al.(2019) and conduct a suite of experiments with varia- tions of relationship embedding architectures. Such experiments are underexplored in the biomedical domain and many are novel to the general task of relationship classification. Soares et al. report the best RE performance using a relationship representation consisting of embedded entity start markers—special span tokens that denote the beginning of an entity. We test this RE architec- ture in the biomedical domain and also test the performance of entity end markers. Moreover, we introduce a novel relationship representation, namely the middle mention pool, which pools word pieces between head and tail entities. This embedding architecture is inspired by the observation that context between two biomedical entities in a sentence often contains the information-rich and relationship-relevant signal. Our best performing relationship embedding architecture results from the combination of both entity end markers and the middle mention pool. We observe that this architecture further increases the performance of our relation classification model. In this paper, we make the following contributions: • We introduce abstractified multiple-instance learning (AMIL), which achieves new state-of-the- art performance for biomedical relationship extraction. We also report significant performance gains for rare fact triples.  •  We propose an improved relationship representation for biomedical relation extraction. We show that concatenating embedding tokens from entity end markers with the middle mention pool produces the best performing model.  • We make all our code, saved models, and pre-processing scripts publicly available1 to facilitate future biomedical RE efforts. Pre-processing scripts can impact model performance and are important to prepare an up-to-date, ready-for-RE dataset from ever-growing PubMed and UMLS. Our results in Section 5 show that using updated pre-processing tools can improve model performance by ∼ 10%. Conclusion In this work, we propose abstractified multi-instance (AMIL), a novel denoising method that in- creases the efficacy of multi-instance learning in the biomedical domain. With it, we improve perfor- mance on biomedical relationship extraction and report significant performance gains on rare fact triples. We also propose a novel relationship embedding architecture which further increases model performance. For future work, we will explore combining AMIL with more advanced bag aggregation meth- ods. We will also explore applying our novel relationship embedding architectures to relationship extraction tasks using general-domain datasets.",1
"Abstract Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Dis- tant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstracti- fies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance. Introduction Relation extraction (RE) is a key facet of information extraction in large bodies of unstructured textual data. RE is particularly important in the biomedical domain where extracting relationships between pairs of biomedical entities, also known as fact triples, can produce new insights into complicated biological interactions. For instance, with the near-exponential growth of microbiome research [Saed et al., 2019], advanced RE methods may help discover important links between gut microbiota and diseases. It is in this context that we motivate our work. RE within the biomedical domain comes with two inherent challenges: there are more than 30 million scientific articles, with hundreds of thousands of articles published every year, and there is a corresponding lack of labeled data. To resolve these challenges, many have leveraged distant supervision techniques which pair knowledge graphs with raw textual data to automatically generate labels to train deep-learning models [Gu et al., 2019, Su et al., 2019, Junge and Jensen, 2019]. We seek to improve distantly supervised biomedical RE methods in this work. We use the Unified Medical Language System (UMLS) Metathesaurus [Bodenreider, 2004] for our knowledge graph and pair it with raw textual data from PubMed [Canese and Weis, 2013]. To automatically generate labels, distantly supervised RE methods rely on a simple yet powerful assumption: any singular sentence that contains a pair of entities also expresses a relationship, as determined by the accompanying knowledge graph, between those entities [Mintz et al., 2009]. However, this assumption leads to a noisy training signal with many false positives as not all sentences express a relationship between an entity pair. To combat this, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping sentences into bags greatly reduces noise in the training signal since a bag of sentences is more likely to express a relationship than a single sentence. This enables the model to better classify relationships between unseen entity pairs. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences of evidence. After processing the PubMed corpus, we observe that a majority (¼ 52%) of extracted triples are supported by fewer than three sentences. Creating bags of sentences for such entity pairs requires heavy up-sampling. For example, if a pair of entities is only supported by one sentence and a bag size is equal to 16 sentences, the single sentence is duplicated 15 times to fill the bag. This erases the benefit of MIL. To counter this issue, we introduce abstractified multi-instance learning (AMIL) where, instead of grouping entity pairs by name, we group entities by the corresponding semantic type as determined by UMLS. UMLS categorizes each entity with a semantic type within the UMLS semantic network. The UMLS semantic network is curated by human experts for decades and provides a rich ontology of biomedical concepts which we leverage to group multiple different entity pairs within a single MIL bag, reducing the need to up-sample sentences. For example, consider two sentences: (1) a sentence containing the entity pair (fibula, tibia) and (2) a second sentence containing the entity pair (humerus, ulna). With distant supervision, we assume each sentence expresses the relationship linking both pairs, namely articulatesDespite expressing the same relationship, without abstraction, these sentences are placed into separate MIL bags since bags are grouped by distinct entity pairs. By introducing abstractified multi-instance learning, the entities fibula, tibia, humerus, and ulna are grouped by their corresponding UMLS semantic type Body Part, Organ, or Organ Component. This allows the aforemen- tioned entity pairs to better classify their work. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences of evidence. After processing the PubMed corpus, we observe that a majority (¼ 52%) of extracted triples are supported by fewer than three sentences. Creating bags of entity pairs requires heavy up-sampling. For example, if a pair of entities is only supported by one sentence and a bag size is equal to 16 sentences, the single sentence is duplicated 15 times to fill the bag. This erases the benefit of MIL. To counter this issue, we introduce abstractified multi-instance learning (AMIL) where, instead of grouping entity pairs by name, we group entities by the corresponding semantic type as determined by UMLS. UMLS categorizes each entity with a semantic type within the UMLS semantic network. The UMLS semantic network is curated by human experts for decades and provides a rich ontology of biomedical concepts which we leverage to group multiple different entity pairs within a single MIL bag, reducing the need to up-sample sentences. For example, consider two sentences: (1) a sentence containing the entity pair (fibula, tibia) and (2) a second sentence containing the entity pair (humerus, ulna). With distant supervision, we assume each sentence expresses the relationship linking both pairs, namely articulates with. Despite expressing the same relationship, without abstraction, these sentences are placed into separate MIL bags since bags are grouped by distinct entity pairs. By introducing abstractified multi-instance learning, the entities fibula, tibia, humerus, and ulna are grouped by their corresponding UMLS semantic type Body Part, Organ, or Organ Component. This allows the entity pair to better classify their work. However, similar to many NLP tasks, biomedical RE suffers from a long-tail distribution of fact triples, where many entity pairs are only supported by a few sentences. After processing the PubMed corpus, we observe that a majority (¼ 52%) of extracted triples are supported by fewer than three sentences. BUR.. RE WORDS For many NLP tasks, referencing a single sentence is far more than is usually required. To ensure that the needs of my RE are met, many works have leveraged multi- instance learning (MIL) [Riedel et al., 2010, Hoffmann et al., 2011, Zeng et al., 2015] where, instead of assessing single sentences, MIL assesses positive and negative bags of sentences that contain the same entity pair. Grouping entities by the corresponding UMLS semantic type ensures that a bag of entity pairs is more than a single sentence. This enables the entity pair to better classify their work.",0
"Abstract This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such ap- proaches to handle productive UGC phenom- ena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of han- dling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this be- havior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. Introduction. Neural Machine Translation (NMT) models fall far short from being able to translate noisy User- Generated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addi- tion to ambiguous grammatical constructs and pro- fusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out- of-vocabulary tokens (OOVs) resulting from mis- spelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti et al., 2020). This is why, focusing more on the noise axis, char-based mod- els appear to offer a natural solution to this prob- lem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of- the-box character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Be- linkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BPE models even when translating noisy UGC. To understand this counter-intuitive result, we con- duct several experiments and analyses. In partic- ular, we manually annotated 400 sentences at the token level using a fine-grained typology, to per- form our analyses. These sentences correspond to the worst and the best translated utterances of two MT systems (a char-based and a transformer-based model). Our results highlight the extreme sensi- bility of character-based models to the vocabulary size, a parameter often overlooked in the literature. Using a simple set of experiments, we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robust- ness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set that enables in-depth evaluations of the impact of UGC idiosyncrasies; • we demonstrate that char-based neural machine translation models are extremely sensi- tive to unknown and rare characters on both synthetic data and noisy user-generated content; • we show how an overlooked hyper-parameter drastically improve char-based MT models ro- bustness to natural noise while maintaining the in-domain level of performance. Conclusion We showed that in zero-shot scenarios, char-based models are not robust to UGC idiosyncrasies. We presented several experiments that explained this counter-intuitive result by an over-sensibility of these models to the vocabulary size. We demonstrated that drastically lowering this param- eter increased the robustness of our char-based models when facing noisy UGC while maintaining almost the same level of performance for in-domain and out-of-domain canonical datasets. Interestingly, we noticed a lack in the literature regarding the importance of vocabulary considerations when training character-based NMTmodels. However, our results show that a path for improvement exists, leading to more generic and stable models and making us believe that further research on the matter can bring promising alternatives to domain-adaptation via target domain data addition.",1
"
Abstract This work explores the capacities of character- based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such ap- proaches to handle productive UGC phenom- ena, which almost by definition, cannot be seen at training time. Within a strict zero- shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of han- dling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this be- havior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. Introduction. Neural Machine Translation (NMT) models fall far short from being able to translate noisy User- Generated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Nunez et al., 2019). In addi- tion to ambiguous grammatical constructs and pro- fusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out- of-vocabulary tokens (OOVs) resulting from mis- spelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti et al., 2020). This is why, focusing more on the noise axis, char-based mod- els appear to offer a natural solution to this prob- lem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of- the-box character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Be- linkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BPE models even when translating noisy UGC. To understand this counter-intuitive result, we con- duct several experiments and analyses. In partic- ular, we manually annotated 400 sentences at the token level using a fine-grained typology, to per- form our analyses. These sentences correspond to the worst and the best translated utterances of two MT systems (a char-based and a transformer-based model). Our results highlight the extreme sensi- bility of character-based models to the vocabulary size, a parameter often overlooked in the literature. Using a simple set of experiments, we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robust- ness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: 
 we provide an annotated data set that enables in-depth evaluations of the impact of UGC idiosyncrasies; we demonstrate that char-based neural machine translation models are extremely sensi- tive to unknown and rare characters on both synthetic data and noisy user-generated content;  we show how an overlooked hyper-parameter drastically improve char-based MT models ro- bustness to natural noise while maintaining the in-domain level of performance. Conclusion We showed that in zero-shot scenarios, char-based models are not robust to UGC idiosyncrasies. We presented several experiments that explained this counter-intuitive result by an over-sensibility of these models to the vocabulary size. We demonstrated that drastically lowering this param- eterthe robustness of our char-based models when facing noisy UGC while maintaining almost the same level of performance for in-domain and out-of-domain canonical datasets. Interestingly, we noticed a lack in the literature regarding the importance of vocabulary considerations when training character-based NMTmodels. However, our results show that a path for improvement exists, leading to more generic models and making us believe that further research on the matter can bring promising alternatives to domain-adaptation via target-based NMTmodels.",0
"Abstract Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sen- tences one by one to compose a summary, which have two main drawbacks: (1) neglect- ing both the intra and cross-document relations between sentences; (2) neglecting the coher- ence and conciseness of the whole summary. In this paper, we propose a novel MDS frame- work (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly out- puts an integrate summary in the form of sub- graph which is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architec- ture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks. Introduction Currently, most extractive models treat summariza- tion as a sequence labeling task. They score and select sentences one by one (Zhong et al., 2020). These models (called sentence-level extractors) do not consider summary as a whole but a combina- tion of independent sentences. This may cause incoherent and redundant problem, and result in a poor summary even if the summary consists of high score sentences. Some works (Wan et al., 2015; Zhong et al., 2020) treat summary as a whole unit and try to solve the weakness of sentence- level extractors by using a summary-level extrac- tor. However, these models neglect the intra and cross-document relations between sentences which also have benefits for extracting salient sentences, detecting redundancy and generating overall coher- ent summaries. Relations become more necessary when input source documents are much longer and more complex such as multi-document input. In this paper, we propose a novel MDS frame- work called SgSum which formulates the MDS task as a sub-graph selection problem. In our frame- work, source documents are regarded as a relation graph of sentences (e.g., similarity graph or dis- course graph) and the candidate summaries are its sub-graphs. In this view, how to generate a good summary becomes how to select a proper sub- graph. In our framework, the whole graph structure is modeled to help extract salient information from source documents and the sub-graph structures are also modeled to help reflect the quality of candidate summaries. Moreover, the summary is considered as a whole unit, so SgSum directly outputs the final summary in the form of sub-graph. By capturing relations between sentences and evaluating sum- mary as a sub-graph, our framework can generate more informative and coherent summaries com- pared with traditional extractive MDS methods. We evaluate SgSum on two MDS datasets with several types of graphs which all significantly im- prove the MDS performance. Besides, the hu- man evaluation results demonstrate that SgSum can obtain more coherent and informative summaries compared with traditional MDS methods. Moreover, the experimental results also indicate that Sg- Sum has strong power on transfer ability when only trained on single-document data. It performs much better than several strong MDS baselines including supervised and unsupervised models. The contributions of our work are as follows: We propose a novel framework called SgSum which transforms MDS task into the problem of sub-graph selection. The framework lever- ages graph to capture relations between sen- tences, and generates more informative and coherent summaries by modeling sub-graph structures. Due to the graph-based multi-document en- coder, our framework unifies single and multi- document summarization and has strong trans- fer ability from SDS to MDS task without any parallel MDS training data. Thus, it can re- duce the resource bottleneck in MDS tasks. Our model is general to several well-known graph representations. We experiment with similarity graph, topic graph and discourse graph on two benchmark MDS datasets. Re- sults show that SgSum has achieved superior performance compared with strong baselines. Conclusion We propose a novel framework SgSum which trans- forms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by modelling both the graph structure of the whole document set and the candidate sub- graphs, then directly output an integrate summary in the form of sub-graph which is more informa- tive and coherent. Experimental results on two MDS datasets show that SgSum brings substantial improvements over several strong baselines. Moreover, the proposed architecture has strong transfer ability from single to multi-document, which can reduce the resource bottleneck in MDS tasks.",1
"
Abstract Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sen- tences one by one to compose a summary, which have two main drawbacks: (1) neglect- ing both the intra and cross-document relations between sentences; (2) neglecting the coher- ence and conciseness of the whole summary. In this paper, we propose a novel MDS frame- work (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly out- puts an integrate summary in the form of sub- graph which is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architec- ture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks. Introduction Currently, most extractive models treat summariza- tion as a sequence labeling task. They score and select sentences one by one (Zhong et al., 2020). These models (called sentence-level extractors) do not consider summary as a whole but a combina- tion of independent sentences. This may cause incoherent and redundant problem, and result in a poor summary even if the summary consists of high score sentences. Some works (Wan et al., 2015; Zhong et al., 2020) treat summary as a whole unit and try to solve the weakness of sentence- level extractors by using a summary-level extrac- tor. However, these models neglect the intra and cross-document relations between sentences which also have benefits for extracting salient sentences, detecting redundancy and generating overall coher- ent summaries. Relations become more necessary when input source documents are much longer and more complex such as multi-document input. In this paper, we propose a novel MDS frame- work called SgSum which formulates the MDS task as a sub-graph selection problem. In our frame- work, source documents are regarded as a relation graph of sentences (e.g., similarity graph or dis- course graph) and the candidate summaries are its sub-graphs. In this view, how to generate a good summary becomes how to select a proper sub- graph. In our framework, the whole graph structure is modeled to help extract salient information from source documents and the sub-graph structures are also modeled to help reflect the quality of candidate summaries. Moreover, the summary is considered as a whole unit, so SgSum directly outputs the final summary in the form of sub-graph. By capturing relations between sentences and evaluating sum- mary as a sub-graph, our framework can generate more informative and coherent summaries com- pared with traditional extractive MDS methods. We evaluate SgSum on two MDS datasets with several types of graphs which all significantly im- prove the MDS performance. Besides, the hu- man evaluation results demonstrate that SgSum can obtain more coherent and informative summaries compared with traditional MDS methods. Moreover, the experimental results also indicate that Sg- Sum has strong power on transfer ability when only trained on single-document data. It performs much better than several strong MDS baselines including supervised and unsupervised models. The contributions of our work are as follows: We propose a novel framework called SgSum which transforms MDS task into the problem of sub-graph selection. The framework lever- ages graph to capture relations between sen- tences, and generates more informative and coherent summaries by modeling sub-graph structures. Due to the graph-based multi-document en- coder, our framework unifies single and multi- document summarization and has strong trans- fer ability from SDS to MDS task without any parallel MDS training data. Thus, it can re- duce the resource bottleneck in MDS tasks. Our model is general to several well-known graph representations. We experiment with similarity graph, topic graph and discourse graphtwo benchmark MDS datasets. Re- sults show that SgSum has achieved superior performance compared with strong baselines. Conclusion We propose a novel framework SgSum which trans- forms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by modelling both the graph structure of the whole document set and the candidate summaries. By capturing relations between sentences individually and extracting salient sen- tences one can obtain a summary that is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that SgSum brings substantial improvements over strong baselines. Human evaluation results also demonstrate that SgSum can produce significantly more coherent and informative summaries compared with strong baselines.",0
"Abstract Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Cre- ating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (∼40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience’s laughter. The normalized duration (laughter duration divided by the clip duration) of laugh- ter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by compar- ing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that pro- vides a “funniness” score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our “Open Mic” dataset is re- leased for further research along with the code.  Introduction Humour is one of the most important lubricants of communication between people. Humour is subjec- tive and, at times, also requires cultural knowledge as humour is often dependent on stereotypes in a culture or a country. At times, even cultural ap- propriation is used to convey humour, which can be offensive to minority cultures (Rosenthal et al., 2015; Kuipers, 2017). The factors listed above, along with the underlying subjectivity in humour render the task of rating humour, difficult for ma- chines (Meaney, 2020). The task of humour classi- fication suffers due to this subjectivity and the lack of datasets that rate the “funniness” of content. In this paper, we propose rating humour on a scale of zero to four. We create the first multi- modal dataset2 using standup comedy clips and compute the humour quotient of each clip using the audience laughter. The validity of our scoring criteria is verified by finding the overall agreement between human annotation and automated scores. We use the audio and text-based signals to process this multi-modal data to generate ‘humour ratings’. Since humour annotation is subjective, even the data annotated by humans might not provide an objective measure. We reduce this subjectivity by taking laughter feedback from a larger audience. To the best of our knowledge, no previous literature has proposed an automatically humour-rated multi- modal dataset and used it in ML model-building to automatically obtain the humor score. Standup comedy is an art form where the deliv- ery of humour has a much larger context, and there are multiple jokes and multiple related punchlines in the same story. The resulting laughter from the audience depends on various factors, including the understanding of the context, delivery, and tonality of the comic. Standup comedy seems to be an ideal choice for a humour rating dataset as it inherently contains some feedback in terms of the audience laughter. We believe a smaller context window re- stricts computational models, but we know this is not the case for the human audience. Hence, our approach utilises live audience laughter as a mea- sure to rate the humour quotient in the data created. We also believe that such an approach can gener- ate insights into what aspects of stories and their delivery make them funny. Our humour rating model is partly inspired by the character “TARS” from the movie “Interstel- lar”, which generates funny responses based on adjustable humour setting (Nolan, 2014). An es- sential step in developing such a machine that can adjust its “funniness” is to create a model that can recognize and rate the “funniness” of a joke. With this work, we aim to release a dataset that can help researchers shed light on the humour quotient of a particular text. The key contributions of this paper are: (a) Creation and public release of an automatically rated multi-modal dataset based on English standup comedy clips and (b) Manual eval- uation of this dataset along with humour-rating quotient defined on a Likert-scale (Likert, 1932). Conclusion and Future Work We propose a novel scoring mechanism to show that humour rating can be automated using audi- ence laughter, which concurs well with the humour perception of humans. We create a multi-modal (audio & text) dataset for the task of humour rating. With the help of three human annotators, we man- ually evaluate our scoring mechanism and show a substantial agreement in terms of QWK. Our eval- uation shows that our scoring mechanism can be emulated with the help of pre-existing language models and traditional audio features. Our neu- ral network-based experiments show that the out- put obtained using various language models like RoBERTa show an agreement with our scoring mechanism. Despite the inherent subjectivity in hu- mour and its different perceptions among humans, we propose a method to rate humour and release this dataset under the CC-BY-SA-NC 4.0 license for further research. In the future, we would like to evaluate this scor- ing mechanism with the help of more human an- notators. We aim to extend the dataset with the help of more standup comedy clips. Further experi- ments can be conducted to compare the contribu- tion of audio, video and text features with a more detailed analysis. We would also like to perform experiments by including more audio features like Line Spectral Frequencies, Zero-Crossing rate, and Delta Coefficients. With the release of this dataset, we hope that research in computational humour can be taken further.",1
"
Abstract Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Cre- ating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour- annotated dataset (¼40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audiences laughter. The normalized duration (laughter duration divided by the clip duration) of laugh- ter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by compar- ing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that pro- vides a 
funniness score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our Open Mic dataset is re- leased for further research along with the code. Introduction Humour is one of the most important lubricants of communication between people. Humour is subjec- tive and, at times, also requires cultural knowledge as humour is often dependent on stereotypes in a culture or a country. At times, even cultural ap- propriation is used to convey humour, which can be offensive to minority cultures (Rosenthal et al., 2015; Kuipers, 2017). The factors listed above, along with the underlying subjectivity in humour render the task of rating humour, difficult for ma- chines (Meaney, 2020). The task of humour classi- fication suffers due to this subjectivity and the lack of datasets that rate the funniness of content. In this paper, we propose rating humour on a scale of zero to four. We create the first multi- modal dataset2 using standup comedy clips and compute the humour quotient of each clip using the audience laughter. The validity of our scoring criteria is verified by finding the overall agreement between human annotation and automated scores. We use the audio and text-based signals to process this multi-modal data to generate humour ratings. Since humour annotation is subjective, even the data annotated by humans might not provide an objective measure. We reduce this subjectivity by taking laughter feedback from a larger audience. To the best of our knowledge, no previous literature has proposed an automatically humour-rated multi- modal dataset and used it in ML model-building to automatically obtain the humor score. Standup comedy is an art form where the deliv- ery of humour has a much larger context, and there are multiple jokes and multiple related punchlines in the same story. The resulting laughter from the audience depends on various factors, including the understanding of the context, delivery, and tonality of the comic. Standup comedy seems to be an ideal choice for a humour rating dataset as it inherently contains some feedback in terms of the audience laughter. We believe a smaller context window re- stricts computational models, but we know this is not the case for the human audience. Hence, our approach utilises live audience laughter as a mea- sure to rate the humour quotient in the data created. We also believe that such an approach can gener- ate insights into what aspects of stories and their delivery make them funny. Our humour rating model is partly inspired by the character from the movie Interstel- lar, which generates funny responses based on adjustable humour setting (Nolan, 2014). An es- sential step in developing such a machine that can adjust its funniness is to create a model that can recognize and rate the funniness of a joke. With this work, we aim to release a dataset that can help researchers shed light on the humour quotient of a particular text. The key contributions of this paper are: (a) Creation and public release of an automatically rated multi-modal dataset based on English standup comedy clips and (b) Manual eval- uation of this dataset along with humour-rating quotient defined on a Likert-scale (Likert, 1932). Conclusion and Future Work We propose a novel scoring mechanism to show that humour rating can be automated using audi- encewhich concurs well with the humour perception of humans. We create a multi-modal (audio & text) dataset for the task of humour rating. With the help of three human annotators, we man- ually evaluate our scoring mechanism and show a substantial agreement in terms of QWK. Our eval- uation shows that despite the inherent grammatical contribu- tion, we are able to make straight compute- ing the humour quotient of a text. Our neu- ral network-based experiments show that the out- put obtained using various language models like RoBERTa show an agreement with our scoring mechanism. We extend our dataset with the help of three human an- notators: John, from the comedy communities The Daily Show, and Nick, from the standup comedy team Late Night, who devise a novel scoring mechanism. We use this dataset to train a model that pro- vides a funniness score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our Open Mic dataset is re- leased for further research along with the code. Introduction Humour is one of the most important lubricants of communication between people.",0
"Abstract Neural machine learning models can success- fully model language that is similar to their training distribution, but they are highly sus- ceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Dis- tillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We apply our method to a GRU architecture, demonstrating improve- ments on multiple language modeling (LM) datasets. Introduction Neural language models have become the main component of modern natural language processing systems, with larger and larger models being used as feature extractors for downstream tasks (Devlin et al., 2019), as probability estimators for ranking and ensembling (Gulcehre et al., 2015) or as lan- guage generators (Bahdanau et al., 2015; Vaswani et al., 2017; Brown et al., 2020). Despite their success, neural machine learning models can suffer large performance degradation when they are applied to out-of-domain data which is substantially different than their training data (La- puschkin et al., 2019; Hupkes et al., 2019; Recht et al., 2019). Autoregressive language models used for generation also suffer from the related expo- sure bias problem (Ranzato et al., 2015): as the model is fed its own samples, deviations from the training distribution are amplified, and eventually for sufficiently long sequences the model generates completely abnormal text. Unlike the older statistical language models, Re- current LMs (RNNLMs) (Mikolov et al., 2010) and their successors Transformers LMs (Vaswani et al., 2017) can consider the entire prefix of a sentence when predicting or generating the next token. By being able to relate a very high-dimensional input to the output, these models can learn many sub- tle correlations which are highly useful as long as the input is in-distribution, unfortunately these correlations tend to be brittle to distribution shift, causing a model that depends on them to go astray. This phenomenon is known as ""shortcut learning"" (Geirhos et al., 2020) and it has been found to also occur in humans and animals, but it is especially prevalent in artificial neural networks. Research on this problem has explored models invariant or equivariant w.r.t. certain transforma- tions by means of compositional representations (Sabour et al., 2017; Soulos et al., 2019; Liu et al., 2020), causal modeling (Schölkopf et al., 2021), or both (Arjovsky et al., 2019; Krueger et al., 2020), but these works focus on classification tasks often on synthetic datasets and can’t be straightforwardly applied to black-box language models. Approaches specific to LMs have focused on robustness where the data domains are known and represented in the training data (Oren et al., 2019; Gerstenberger et al., 2020). In this work we propose a method that uses Random Network Distillation (RND) (Burda et al., 2018) to dynamically adapt the amount of context that the model relies upon during inference based on an estimate of how much this context is out-of- distribution (OOD). This way the model can still make use of all available context when operating within a familiar context space, exploiting long- distance weak correlations, but it reduces to a less expressive and more robust model when operating OOD, relying only on the strongest correlations. As a proof of concept we implement our ap- proach on a GRU recurrent language model (Cho et al., 2014). While Transformer decoders out- perform RNNs when trained on large training sets, RNNs remain competitive on smaller datasets (< 107 tokens) where OOD phenomena are easier to measure, furthermore they are easier to opti- mize, simplifying architecture and hyperparameter search. We evaluate our method on language modeling tasks on English datasets, obtaining improvements when evaluating on eight OOD domains.We re- port additional preliminary sequence-to-sequence results on Transformer-RNN models (Zhang et al., 2018) in appendix A. We leave extensions of our method to full Transformers as future research. Conclusions and future work We proposed a method to improve the robustness of language models to distribution shift caused by train/test domain mismatch. Our model con- tracts the RNN state based on an unsupervised out-of-distribution estimator in order to reduce the model dependency on weak long-distance correla- tions, which are useful in-distribution but tend to be spurious in out-of-distribution conditions. We obtain perplexity improvements on multiple out-of- domain test sets without substantial degradation on in-domain test sets. While our approach is based on Recurrent de- coders, its general principles may be applicable to other neural architectures. For instance, the self-attention heads of a Transformer might modulated by an OOD detector in order to avoid attending to out-of-distribution parts of a sentence. We antic- ipate that extending our method to these kind of models will be a promising research direction.",1
"Abstract Neural machine learning models can success- fully model language that is similar to their training distribution, but they are highly sus- ceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Dis- tillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We apply our method to a GRU architecture, demonstrating improve- ments on multiple language modeling (LM) datasets. Introduction Neural language models have become the main component of modern natural language processing systems, with larger and larger models being used as feature extractors for downstream tasks (Devlin et al., 2019), as probability estimators for ranking and ensembling (Gulcehre et al., 2015) or as lan- guage generators (Bahdanau et al., 2015; Vaswani et al., 2017; Brown et al., 2020). Despite their success, neural machine learning models can suffer large performance degradation when they are applied to out-of-domain data which is substantially different than their training data (La- puschkin et al., 2019; Hupkes et al., 2019; Recht et al., 2019). Autoregressive language models used for generation also suffer from the related expo- sure bias problem (Ranzato et al., 2015): as the model is fed its own samples, deviations from the training distribution are amplified, and eventually for sufficiently long sequences the model generates completely abnormal text. Unlike the older statistical language models, Re- current LMs (RNNLMs) (Mikolov et al., 2010) and their successors Transformers LMs (Vaswani et al., 2017) can consider the entire prefix of a sentence when predicting or generating the next token. By being able to relate a very high-dimensional input to the output, these models can learn many sub- tle correlations which are highly useful as long as the input is in-distribution, unfortunately these correlations tend to be brittle to distribution shift, causing a model that depends on them to go astray. This phenomenon is known as ""shortcut learning"" (Geirhos et al., 2020) and it has been found to also occur in humans and animals, but it is especially prevalent in artificial neural networks. Research on this problem has explored models invariant or equivariant w.r.t. certain transforma- tions by means of compositional representations (Sabour et al., 2017; Soulos et al., 2019; Liu et al., 2020), causal modeling (Scholkopf et al., 2021), or both (Arjovsky et al., 2019; Krueger et al., 2020), but these works focus on classification tasks often on synthetic datasets and can't be straightforwardly applied to black-box language models. Approaches specific to LMs have focused on robustness where the data domains are known and represented in the training data (Oren et al., 2019; Gerstenberger et al., 2020). In this work we propose a method that uses Random Network Distillation (RND) (Burda et al., 2018) to dynamically adapt the amount of context that the model relies upon during inference based on an estimate of how much this context is out-of- distribution (OOD). This way the model can still make use of all available context when operating within a familiar context space, exploiting long- distance weak correlations, but it reduces to a less expressive and more robust model when operating OOD, relying only on the strongest correlations. As a proof of concept we implement our ap- proach on a GRU recurrent language model (Cho et al., 2014). While Transformer decoders out- perform RNNs when trained on large training sets, RNNs remain competitive on smaller datasets (< 107 tokens) where OOD phenomena are easier to measure, furthermore they are easier to opti- mize, simplifying architecture and hyperparameter search. We evaluate our method on language modeling tasks on English datasets, obtaining improvements when evaluating on eight OOD domains.We re- port additional preliminary sequence-to-sequence results on Transformer-RNN models (Zhang et al., 2018) in appendix A. We leave extensions of our method to fullas future research. Conclusions and future work We proposed a method to improve the robustness of language models to distribution shift caused by train/test domain mismatch. Our model con- tracts the RNN state based on an unsupervised out-of-distribution estimator in order to reduce the model dependency on weak long-distance correla- tions, which are useful in-domain but are brittle to distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. We propose a method based on OOD detection with an unsupervised out-of-distribution estimator in order to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning to- wards a less expressive but more robust model as the data becomes more OOD, while re- taining its full context capability when operat- ing in-distribution. We obtain- ments by means of probability estima- tions, which are based on < 50% accuracy.",0
"Abstract Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reason- ing process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decom- position. The Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (Velicˇkovic ́ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence, such as DFGN (Xiao et al., 2019). There is no evidence to show that such additional discrim- inators are strongly correlated with the reasoning results of the end-to-end model, which means not faithful. Secondly, although graph structure is help- ful to multi-hop reasoning in theory, but recent work (Shao et al., 2020) shows that the existing graph neural network is only a special attention mechanism (Bahdanau et al., 2014), and it’s not necessary for multi-hop QA, with the experiments that better results can be achieved by using only transformer network instead of graph neural net- work, as long as the same additional adjacency matrix information is provided. We observed that human reasoning about com- plex questions is not accomplished overnight and it’s usually divided into the steps of question de- composition, answering sub-questions, summariz- ing and comparing. For example, for the complex question, ""whose candidate will get more votes in the 2020 U.S. election, Democrats and Repub- licans?"" People will not think about the whole question, but firstly decompose the complex ques- tion. Realizing that the subject of the question is ""Democrats and Republicans"", and the question is about ""candidates"" and ""number of votes"", peo- ple can answer those sub-questions progressively– ""who is the Democratic candidate?"" and ""how many votes does ANS get?"" The same thinking process was performed for another question sub- ject, ""Republican Party"". Finally, the two votes were compared to obtain the answer to the entire complex question. Inspired by the way humans answer complex multi-hop questions, in this work we abandoned the end-to-end model structure, but imitated the human reasoning mechanism to propose a three- stage Relation Extractor-Reader and Comparator (RERC) model1. We first build a Relation Extrac- tor, which can automatically extract the subject and key relations of the question from the com- plex unstructured textual representation. For the Relation Extractor, we use two different structures, one is classification-type (CRERC), where the evi- dence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; the other is span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Next, we use the advanced ALBERT model (Lan et al., 2020) as the Reader, which reads the corre- sponding paragraphs and answer each sub-question composed of the subject and relations of the ques- tion in turn. Finally, for comparison type questions, our Comparator module compares the magnitude of each subject’s final answer, and then get the entire answer. Our contributions are summarized as follows: We propose a novel RERC model for multi- hop text-based QA and evidence path search tasks. We propose a Query-aware Entity Tree Para- graph Screening (QETPS) method to filter valid paragraphs from a large number of doc- uments before Reader module, which is more efficiently than previous paragraph selecting methods. We provide an experimental study on a pub- lic multi-hop dataset (2WikiMultiHopQA) to demonstrate that our proposed RERC model has the state-of-the-art performance in both answering multi-hop questions and extracting evidence at the same time. Conclusion and future work We propose a three-stage framework of Relation Extractor-Reader and Comparator (RERC), which solves the multi-hop QA task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2WikiMultiHopQA dataset, which is close to human performance. Our RERC framework can also provide faithful evi- dence with excellent interpretability. Multiple future research directions according to our proposed RERC model may be envisioned. First of all, benefiting to the three-stage structure, the RERC model has the potential to leverage the network structure of the Relation Extractor to gain generalization capabilities for more complex ques- tions. Moreover, we expect that collaborative error correction mechanism applied in Reader module will largely avoid accumulation of errors in multi- hop reasoning.",1
"Abstract Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine's reason- ing process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decom- position. The Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (Velickovic et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence, such as DFGN (Xiao et al., 2019). There is no evidence to show that such additional discrim- inators are strongly correlated with the reasoning results of the end-to-end model, which means not faithful. Secondly, although graph structure is help- ful to multi-hop reasoning in theory, but recent work (Shao et al., 2020) shows that the existing graph neural network is only a special attention mechanism (Bahdanau et al., 2014), and it's not necessary for multi-hop QA, with the experiments that better results can be achieved by using only transformer network instead of graph neural net- work, as long as the same additional adjacency matrix information is provided. We observed that human reasoning about com- plex questions is not accomplished overnight and it's usually divided into the steps of question de- composition, answering sub-questions, summariz- ing and comparing. For example, for the complex question, ""whose candidate will get more votes in the 2020 U.S. election, Democrats and Repub- licans?"" People will not think about the whole question, but firstly decompose the complex ques- tion. Realizing that the subject of the question is ""Democrats and Republicans"", and the question is about ""candidates"" and ""number of votes"", peo- ple can answer those sub-questions progressively ""who is the Democratic candidate?"" and ""how many votes does ANS get?"" The same thinking process was performed for another question sub- ject, ""Republican Party"". Finally, the two votes were compared to obtain the answer to the entire complex question. Inspired by the way humans answer complex multi-hop questions, in this work we abandoned the end-to-end model structure, but imitated the human reasoning mechanism to propose a three- stage Relation Extractor-Reader and Comparator (RERC) model1. We first build a Relation Extrac- tor, which can automatically extract the subject and key relations of the question from the com- plex unstructured textual representation. For the Relation Extractor,use two different structures, one is classification-type (CRERC), where the evi- dence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; the other is span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (Velickovic et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence.",0
"Abstract The fourth industrial revolution (4IR) is likely to have a substantial impact on the econ- omy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occu- pations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Re- searchers could validate our findings through further analyses with micro data, and our dataset can serve as a source for more complex labor market analyses. Further, we compared the 4IR ex- posure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. We show that a combination of 4IR exposure with other automation measures may provide additional in- sights. For example, near-term automation might be driven by non-4IR technologies. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations. These measures can provide valuable insights for future research in terms of overall automation potential. Our approach does not aspire to predict the share of automated jobs, but aims to reflect actual technology maturity (diffusion), which is not covered by the aforementioned indicators Arntz et al. (2020). For example, scores by Brynjolfsson & Mitchell (2017) are based on expert assessments of “what can machine learning do?” We use patent data as an indicator for technological progress; patents actually document what existing technology can currently do. The McKinsey Global Institute follows a similar objective and focuses on what actual automation might look like until 2030, acknowledging that there is a much higher automation potential in the long term (Manyika et al., 2017). They provide estimates of automation potentials per occupation, which they expect to be implemented until 2030. Linking patent data to occupation activities offers a direct indicator of the exposure of occu- pations to technology. There exist patent occupation mappings at an industry (Silverman, 2002) and occupation level (Kogan et al., 2020; Webb, 2019) which have been used for economic anal- yses (Mann & Pu ̈ttmann, 2017; Acemoglu et al., 2020). Webb (2019) found, for example, that exposure to previous automation technologies had a negative impact on employment at an occu- pation level, and Mann & Pu ̈ttmann (2017) identified an overall positive impact of automation patents on employment. We build on the approach of Kogan et al. (2020) and refine for improved accuracy and to account for task-level differences. Each occupation relates to several tasks, and technology ex- posure may vary among different tasks within an occupation (Brynjolfsson & Mitchell, 2017). The task level, as the “unit of work that produces output,” is a highly insightful level of detail for evaluating the impact of technologies on jobs (Acemoglu & Autor, 2011). Our approach has two main benefits. On the one hand, it allows accounting for a specific technology exposure for each task, which is ignored when looking at occupations as a whole. On the other hand, our task-level approach increases the accuracy of the mapping, as it identifies patents specific to each activity, rather than patents which have many words in common with the overall occupation description. For example, our approach might avoid associating a robot engineer mainly with robot patents in general (e.g., improved efficiency of assembly robots), but rather with patents which describe in- novations that help to “plan robot path”, “debug robot programs”, and “maintain robots.” Further, we introduce a measure of technology exposure; we therefore differentiate between technologies of the fourth industrial revolution (4IR patents) and other patents (non-4IR patents) for creating technology exposure scores. These scores indicate patent exposure at the task and occupation level. Our analysis includes patent data since 1970 and thus allows us to review developments over time, e.g., when 4IR technologies have been developed and how long it takes them to impact the labor market. Various researchers identify the lack of high-quality data on technological progress of key 4IR technologies as a key barrier to better understanding the impact of those technologies on the workforce (Frank et al., 2019b; Mitchell & Brynjolfsson, 2017). With this article, we address this issue by providing a mapping of patents to occupational tasks and introducing a 4IR technology exposure score per occupation. Conclusions and future work The aim of this paper is to better understand the exposure of occupations to technologies of the 4IR. Several existing indicators describe the theoretical automation potential or future exposure potential of occupations. We introduce an indicator reflecting actual technology diffusion, based on patent data. This paper presents a method for mapping patents to tasks and introduces an occupation and task-level indicator of exposure to patents of the 4IR (4IR exposure score). We refine existing approaches to better account for task-level differences in patent exposure and the context in which an activity is conducted (e.g., diagnose machine condition vs. diagnose patient condition). We therefore consider that this approach offers a highly valuable contribution towards mapping patents to tasks and occupations. Occupations with higher exposure scores may, for example, be more impacted by 4IR tech- nologies. The analysis shows that ratio of exposure to 4IR and non-4IR patents differs per oc- cupation. Occupations with many manual tasks, such as manufacturing and construction, have high non-4IR exposure and low 4IR exposure, whereas many non-manual occupations, such as finance and marketing occupations, have a higher ratio of 4IR exposure. The 4IR exposure score is also valuable as a complementary score to other technology or automation scores. For example, comparing theoretical and actual technology exposure can provide insights into which occupations might undergo changes through current technologies versus future diffusion. This direct measure of technological progress can provide highly valuable data for further ex- ploration of the impact of technological change on employment (Mitchell & Brynjolfsson, 2017) and may serve as a source for labor market analysis to explore impact patterns of technologies on jobs. We compared our 4IR exposure scores with labor market indicators and found that exposure to non-4IR patents is highest for medium-wage occupations, and that 4IR exposure is highest for high-wage occupations. Further, regression analysis showed a negative (concave) relation of 4IR exposure to job growth. Patent exposure 10 and 20 years ago showed higher coefficients on the impact on job growth than more recent patent exposure. The gap may reflect the time between invention and technology diffusion and is in line with findings of Kogan et al. (2020). Further analysis with micro data is required to confirm these findings. To estimate the overall impact on the labor market, more complex modeling is required, e.g., considering the effect of deepening of automation or capital accumulation. Acemoglu & Restrepo (2019) observed that different technologies may have different impact patterns on the labor markets. Therefore, differentiating between 4IR technologies may offer additional value for labor market analyses. Researchers can build on our mapping for technology- level analysis. On the one hand, we provide technology-specific exposure scores (e.g., CAD, augmented reality for surgery, and smart office technologies). On the other hand, our mapping of patents to tasks allows researchers to build any other exposure scores, such as robots, or breakthrough patents, as long as a patent technology mapping is available. Also, patent data is available at firm level and allows for time-varying measures. Our work provides an occupation (and task)-level indicator of 4IR patent exposure. Patents describe inventions, and not all inventions have an equal impact. Future work could thus fur- ther improve the indicator by accounting for a patent’s impact. The count of patent citations is frequently discussed as potential measure for novelty and social usefulness, but its validity is ambiguous (Strumsky & Lobo, 2015). Another approach is described by Kelly et al. (2020), who describe “breakthrough patents” which significantly differ in text content from previous patents and thus might have particularly high impact. Our approach builds on occupation and task description data provided by O*Net. We take advantage of its extensive and hierarchical descriptions of occupational activities and tasks. Fu- ture work could rely on additional information provided by O*Net. For example, at a task and occupation level, the dataset indicates which technologies and tools are used, such as word pro- cessing software or programmable logic controllers. Building on this information may provide information on inventions related specifically to labor augmentation. The O*Net database de- scribes occupations in the context of the US labor market. There exist concordance tables, which can help to use the patent occupation mapping in other contexts. These might provide additional accuracy to directly map patents to those regional occupation descriptions, if regional databases with similar hierarchical structures exist.",1
"Abstract The fourth industrial revolution (4IR) is likely to have a substantial impact on the econ- omy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occu- pations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Re- searchers could validate our findings through further analyses with micro data, and our dataset can serve as a source for more complex labor market analyses. Further, we compared the 4IR ex- posure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. We show that a combination of 4IR exposure with other automation measures may provide additional in- sights. For example, near-term automation might be driven by non-4IR technologies. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations. These measures can provide valuable insights for future research in terms of overall automation potential. Our approach does not aspire to predict the share of automated jobs, but aims to reflect actual technology maturity (diffusion), which is not covered by the aforementioned indicators Arntz et al. (2020). For example, scores by Brynjolfsson & Mitchell (2017) are based on expert assessments of what can machine learning do? We use patent data as an indicator for technological progress; patents actually document what existing technology can currently do. The McKinsey Global Institute follows a similar objective and focuses on what actual automation might look like until 2030, acknowledging that there is a much higher automation potential in the long term (Manyika et al., 2017). They provide estimates of automation potentials per occupation, which they expect to be implemented until 2030. Linkingdata to occupation activities offers a direct indicator of the exposure of occu- pations to technology. There exist patent occupation mappings at an industry (Silverman, 2002) and occupation level (Kogan et al., 2020; Webb, 2019) which have been used for economic anal- yses (Mann & Puttmann, 2017; Acemoglu et al., 2020). Webb (2019) found, for example, that exposure to previous automation technologies had a negative impact on employment at an occu- pation level, and Mann & Puttmann (2017) identified an overall positive impact of automation patents on employment. We build on the approach of Kogan et al. (2020) and refine for improved accuracy and to account for task-level differences. Each occupation relates to several tasks, and technology ex- posure may vary among different tasks within an occupation (Brynjolfsson & Mitchell, 2017). The task level, as the unit of work that produces output, is a highly insightful level of detail for evaluating the impact of technologies on jobs (Acemoglu & Autor, 2011). Our approach has two main benefits. On the one hand, it allows accounting for a specific technology exposure for each task, which is ignored when looking at occupations as a whole. On the other hand, our task-level approach increases the accuracy of the mapping, as it identifies patents specific to each activity, rather than patents which have many words in common with the overall occupation description. For example, our approach might avoid associating a robot engineer mainly with robot patents in general (e.g., improved efficiency of assembly robots), but rather with patents which describe in- novations that help to plan robot path, debug robot programs, and maintain robots. Further, we introduce a measure of technology exposure; we therefore differentiate between technologies of the fourth industrial revolution (4IR patents) and other patents (non-4IR patents) for creating technology exposure scores. These scores indicate patent exposure at the task and occupation level. Our analysis includes patent data since 1970 and thus allows us to review developments over time, e.g., when 4IR technologies have been developed and how long it takes them to impact the labor market. Various researchers identify the lack of high-quality data on technological progress of key 4IR technologies as a key barrier to better understanding the impact of those technologies on the workforce (Frank et al., 2019b; Mitchell & Brynjolfsson, 2017). With this article, we address this issue by providing a mapping of patents to occupational tasks and introducing a 4IR technology exposure score per occupation. Conclusions and future work The aim of this paper is to better understand the exposure of occupations to technologies of the 4IR. Several existing indicators describe the theoretical automation potential or future exposure potential of occupations. We introduce an indicator reflecting actual technology diffusion, based on patent data. This paper presents a method for mapping patents to tasks and introduces an occupation and task-level indicator of exposure to patents of the 4IR (4IR exposure score). We refine existing approaches to better account for task-level differences in patent exposure and the context in which an activity is conducted (e.g., diagnose machine condition vs. diagnose patient condition). We therefore consider that this approach offers a highly valuable contribution towards mapping patents to tasks and occupations. Occupations with higher exposure scores may, for example, be more impacted by 4IR tech- nologies. The analysis shows that ratio of exposure to 4IR and non-4IR patents differs per oc- cupation. Occupations with many manual tasks, such as manufacturing and construction, have high non-4IR exposure and low 4IR exposure, whereas many non-manual occupations, such as finance and marketing occupations, have a higher ratio of 4IR exposure. The 4IR exposure score is also valuable as a complementary score to other technology or automation scores. For example, comparing theoretical and actual technology exposure can provide insights into which occupations might undergo changes through current technologies versus future diffusion. This direct measure of technological progress can provide highly valuable data for further ex- ploration of the impact of technological change on employment (Mitchell & Brynjolfsson, 2017) and may serve as a source for labor market analysis to explore impact patterns of technologies on jobs. We compared our 4IR exposure scores with labor market indicators and found that exposure to non-4IR patents is highest for medium-wage occupations, and that 4IR exposure is highest for high-wage occupations. Further, regression analysis showed a negative (concave) relation of 4IR exposure to job growth. Patent exposure 10 and 20 years ago showed higher coefficientsthe impact on job growth than more recent patent exposure. The gap may reflect the time between invention and technology diffusion and is in line with findings of Kogan et al. (2020). Further analysis with micro data is required to confirm these findings. To estimate the overall impact on the labor market, more complex modeling is required, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), and future researchers may have to perform tasks other than scanning the labor market. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures. Keywords: technology exposure,fourth industrial revolution,occupational tasks,patent occupation mapping, natural language processing Introduction Technological progress continuously impacts the economic environment. The current wave of technological progress is driven by digitalization and the adoption of artificial intelligence (AI), and is often referred to as the fourth industrial revolution. AI might enable machines to become increasingly able to perform tasks that previously only humans could perform. Whereas previously machines were mainly able to perform clearly defined, repetitive, routine tasks (Ace- moglu & Autor, 2011), future automation might cover much more diverse tasks, for example, some requiring emotional intelligence (Brynjolfsson & Mitchell, 2017). These new automation patterns create fears of machines making workers obsolete and creating unemployment. How- ever, technological change has various effects on the labor market, and previous waves of au- tomation did not result in long-lasting technology-induced rising unemployment (Mokyr et al., 2015; Autor, 2015; Bessen, 2019). Acemoglu & Restrepo (2019) observe that automation not only decreases labor demand but also has a productivity effect, which increases labor demand. Further, they describe the reinstate- ment effect, where automation leads to newly-created tasks carried out by humans. The relative size of these effects and their interaction determine the overall effect of automation on the labor market. Therefore, automation causes changes in the task content of occupations due to lower demand for some tasks, higher demand for remaining tasks, and the creation of new tasks. To evaluate these effects and prepare for future shifts in the labor market, researchers, e.g., Frey & Osborne (2017) and Brynjolfsson & Mitchell (2017), construct measures of automation potential of occupations.",0
"Abstract Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages re- mains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Ara- bic written using an extension of the Latin script, called NArabizi, found mostly on so- cial media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similari- ties with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African di- alectal Arabic. Its Latin transcription (Arabizi) dis- plays a high level of linguistic variability1, on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual mod- els(Mulleretal.,2020).2 To Tackle The Resource scarcity issue regarding Arabic dialects, Antoun et al. (2020) use BERT architecture (Devlin et al., 2019) to train a model on Arabic text to compare this approach to standard multilingual models. In- deed, Martin et al. (2020) show that fine-tuning a monolingual model leads to better results than fine-tuning a multilingual one, meaning that when fine-tuning is used there is no significant perfor- mance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single lan- guage and was not trained to handle the presence of multiple languages in the same sentence (code- switching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being lim- ited by a subword tokenization-based vocabu- lary when facing out-of-domain training data lan- guage, especially in high-variability noisy scenar- ios (El Boukkouri et al., 2020; Clark et al., 2021), even though Muller et al. (2020) demonstrated a positive effect for NArabizi when using target lan- guage data to fine-tune a multilingual language model on its own objective function before pre- training. Following a different approach, we investigate the use of a recently issued character-based lan- guage model (El Boukkouri et al., 2020) that was shown to display a remarkable robustness to lexi- cal variation and noise when facing a new distant domain, namely biomedical. The pipeline we de- veloped is simple and consists in fine-tuning this character-based model for several tasks in a noisy low-resource language scenario. We show that a character-based model trained on only 99k sen- tences of NArabizi and fined-tuned on a small tree- bank of the language leads to performance close to that obtained with the same architecture pre- trained on large multilingual and monolingual mod- els (mBERT and CamemBERT). Interestingly, we generalize this observation by using the same architecture on a much larger French user-generated Content treebank that ex- hibits similar language variability issues than NAra- bizi. In fact, pre-training a character-based model on 1% of the large-scale French instance of the multilingual corpus OSCAR leads to similar per- formance as a subword based model trained on the full corpus, showing that such character-based lan- guage model can reach similar performance levels and that the resulting models exhibit the same tol- erance to noise as their much larger BERT counter- parts. This demonstrates the value of such models in very scarce resource scenario. Our code and models are freely available. Discussion In this work, we evaluate the benefits of using a character-based model in low-resource scenarios. Our results show that training such a model from scratch on much fewer data gives similar perfor- mance to a multilingual BERT adapted to the lan- guage using the same amount of data. Overall, our observations confirm the findings of El Boukkouri et al. (2020) regarding the robust- ness to noise and misspellings of the Character- BERT model. We showed that the model has com- petitive performance on noisy French UGC data when trained on only a fraction of the OSCAR corpus compared to CamemBERT trained on the full corpus and when trained on corpora containing about 1M words in the extremely noisy and low- resource case of NArabizi. This is consistent with the findings of Martin et al. (2020) and Micheli et al. (2020), who showed that MLM could already learn a lot from pre-training on smaller data set. Extending this investigation by training on a larger amount of data could help to explore the ability of the model to handle highly variable noisy data. However, one could question the usefulness of such Character-BERT based models if small Bert- based models were available on the same domain. To build an answer to that question, we conducted a quick set of experiments comparing our char- acterBert model trained on 1% of Oscar with the off-the-shelf Camembert version trained on 4gb of the Oscar corpus French instance (2.38% of the full corpus) and which was shown to perform al- most as well as the full model (Martin et al., 2020) on many downstream tasks. Both models were fined-tuned according to our MODEL+Task archi- tecture on either the FSMB or the Sequoia treebank, allowing us to evaluate their in-domain and out-of- domain performance. Results on Table 8 confirm the effectiveness of our characterBert model with overall better results than CamemBERT4gb in the in-domain scenario and similar, if not slightly bet- ter in the out-of-domain scenario, except for the labeled attachment score (75.83 vs 75.39). The fact that CamemBERT4gb was trained on more than twice as much data and with 200k pre-training steps while the characterBert pre-training stopped below 20k steps probably explains this small dis- crepancy but further investigations are needed with a fully parallel setting where both characterBert and CamemBERT are pretrained on the same amount of data and the same hyper-parameters. The take- home message from this in-domain experiments is that CharacterBert seems to be able to better cap- ture at least some of the UGC idiosyncracies that are prevalent in the FSMB (Seddah et al., 2012b) than its Bert-based counterparts. This was also shown by Rosales Núñez et al. (2021) in the con- text of character-based neural machine translation. Interestingly, their results showed that transformer- based models with subword tokenization also ex- hibit strong robustness to a certain type of lex- ical noise. This behavior has been very recently demonstrated by (Itzhak and Levy, 2021) and could explain why the BERT-based models we tested per- formed so well in our experiments. The key seems to be relying on the ability of the subword distri- bution to model some forms of lexical variations. Much more experiments are needed to clearly in- vestigate in what circumstances, besides noisy and resource-scarce scenarios, characterBERT models bring in a decisive advantage. Our results are based on the evaluation of two low-level tasks. Therefore, it would be interesting to see if they can be generalized to other – e.g. more semantic – tasks, as additional experiments on model layers configuration showed that most of the important information is captured early in the layers of the model (cf. Appendix A). Regarding the specific case of Arabic dialects written in Arabizi, a recent BERT-based model have been pretrained on 7 millions Egyptian tweets and displayed effective results on a sentiment anal- ysis task (Baert et al., 2020). Another very recent model, at the date of writing, was pre-trained on 4 millions Algerian tweets and also demonstrated interesting results on sentiment analysis (Abdaoui et al., 2021). Unfortunately, the authors did not perform any experiments on the Narabizi data set, making thus the comparison with our work not straightforward. It would be of course interesting to evaluate the interoperability between these new data sets and the NArabizi resources we used to produce our models. Head to head comparisons between these models and ours could be of value of course but we believe that given the shortcomings of finding enough data to pretrain large models for dialects, it would be probably better to first consol- idate a large enough common pre-training data set and then work on model performance. We leave this for future work. We showed that CharacterBert models trained on very little data could provide an interesting alterna- tive to large multilingual and monolingual models in resource-scarce and noisy scenarios. This is why we release all the code, data and models to repro- duce our experiments, hoping our work will favor the rise of efficient robust NLP models for under- resourced languages, domains and dialects.",1
"Abstract Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages re- mains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Ara- bic written using an extension of the Latin script, called NArabizi, found mostly on so- cial media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similari- ties with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African di- alectal Arabic. Its Latin transcription (Arabizi) dis- plays a high level of linguistic variability1, on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual mod- els(Mulleretal.,2020).2 To Tackle The Resource scarcity issue regarding Arabic dialects, Antoun et al. (2020) use BERT architecture (Devlin et al., 2019) to train a model on Arabic text to compare this approach to standard multilingual models. In- deed, Martin et al. (2020) show that fine-tuning a monolingual model leads to better results than fine-tuning a multilingual one, meaning that when fine-tuning is used there is no significant perfor- mance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single lan- guage and was not trained to handle the presence of multiple languages in the same sentence (code- switching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being lim- ited by a subword tokenization-based vocabu- lary when facing out-of-domain training data lan- guage, especially in high-variability noisy scenar- ios (El Boukkouri et al., 2020; Clark et al., 2021), even though Muller et al. (2020) demonstrated a positive effect for NArabizi when using target lan- guage data to fine-tune a multilingual language model on its own objective function before pre- training. Following a different approach, we investigate the use of a recently issued character-based lan- guage model (El Boukkouri et al., 2020) that was shown to display a remarkable robustness to lexi- cal variation and noise when facing a new distant domain, namely biomedical. The pipeline we de- veloped is simpleconsists in fine-tuning this character-based model for several tasks in a noisy low-resource language scenario. We show that a character-based model trained on only 99k sen- tences of NArabizi and fined-tuned on a small tree- bank of the language leads to performance close to that obtained with the same architecture pre- trained on large multilingual and monolingual mod- els (mBERT and CamemBERT). Interestingly, we generalize this observation by using the same architecture on a much larger French user-generated Content treebank that ex- hibits similar language variability issues than NAra- bizi. In fact, pre-training a character-based model on 1% of the large-scale French instance of the multilingual corpus OSCAR leads to similar per- formance as a subword based model trained on the full corpus, showing that such character-based lan- guage model can reach similar performance levels and that the resulting models exhibit the same tol- erance to noise as their much larger BERT counter- parts. This demonstrates the value of such models in very scarce resource scenario. Our code and models are freely available. Discussion In this work, we evaluate the benefits of using a character-based model in low-resource scenarios. Our results show that training such a model from scratch on much fewer data gives similar perfor- mance to a multilingual BERT adapted to the lan- guage using the same amount of data. Overall, our observations confirm the findings of El Boukkouri et al. (2020) regarding the robust- ness to noise and misspellings of the Character- BERT model. We showed that the model has com- petitive performance on noisy French UGC data when trained on only a fraction of the OSCAR corpus compared to CamemBERT trained on the full corpus and when trained on corpora containing about 1M words in the extremely noisy and low- resource case of NArabizi. This is consistent with the findings of Martin et al. (2020) and Micheli et al. (2020), who showed that MLM could already learn a lot from pre-training on smaller data set. Extending this investigation by training on a larger amount of data could help to explore the ability of the model to handle highly variable noisy data. However, one could question the usefulness of such Character-BERT based models if small Bert- based models were available on the same domain. To build an answer to that question, we conducted a quick set of experiments comparing our char- acterBert model trained on 1% of Oscar with the off-the-shelf Camembert version trained on 4gb of the Oscar corpus French instance (2.38% of the full corpus) and which was shown to perform al- most as well as the full model (Martin et al., 2020) on many downstream tasks. Both models were fined-tuned according to our MODEL+Task archi- tecture on either the FSMB or the Sequoia treebank, allowing us to evaluate their in-domain and out-of- domain performance. Results on Table 8 confirm the effectiveness of our characterBert model with overall better results than CamemBERT4gb in the in-domain scenario and similar, if not slightly bet- ter in the out-of-domain scenario, except for the labeled attachment score (75.83 vs 75.39). The fact that CamemBERT4gb was trained on more than twice as much data and with 200k pre-training steps while the characterBert pre-training stopped below 20k steps probably explains this small dis- crepancy but further investigations are needed with a fully parallel setting where both characterBert and CamemBERT are pretrained on the same amount of data and the same hyper-parameters. The take- home message from this in-domain experiments is that CharacterBert seems to be able to better cap- ture at least some of the UGC idiosyncracies that are prevalent in the FSMB (Seddah et al., 2012b) than its Bert-based counterparts. This was also shown by Rosales Nunez et al. (2021) in the con- text of character-based neural machine translation. Interestingly, their results showed that transformer- based models with subword tokenization also ex- hibit strong robustness to a certain type of lex- ical noise. This behavior has been very recently demonstrated by (Itzhak and Levy, 2021) and could explain why the BERT-based models weper- formed so well in our experiments. The key seems to be relying on the ability of the subword distri- bution to model some forms of lexical variations. Much more experiments are needed to clearly in- vestigate in what circumstances, besides noisy and resource-scarce scenarios, characterBERT models bring in a decisive advantage. We compare the downstream performance of a character-based model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character- based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. Introduction Current state-of-the-art monolingual and multi- lingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning archi- tectures: the language models need to be trained on large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, de- spite the emergence of large and successful multi- lingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similari- ties with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African di- alectal Arabic.",0
"Abstract Self-supervised learning (SSL) achieves great suc- cess in speech recognition, while limited explo- ration has been attempted for other speech pro- cessing tasks. As speech signal contains multi- faceted information including speaker identity, paralinguistics, spoken content, etc., learning uni- versal representations for all speech tasks is chal- lenging. In this paper, we propose a new pre- trained model, WavLM, to solve full-stack down- stream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker iden- tity preservation. We first equip the Transformer structure with gated relative position bias to im- prove its capability on recognition tasks. For bet- ter speaker discrimination, we propose an utter- ance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extrac- tion. WavLM Large achieves state-of-the-art per- formance on the SUPERB benchmark, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index Terms— Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks. In the past, it has been infeasible to build such a general model, as dif- ferent tasks focus on different aspects of speech signals. For instance, speaker verification requires the network to learn the speaker characteristic regardless of the spoken con- tent, while speech recognition demands the network discard speaker characteristics and focus only on content informa- tion. Meanwhile, unlike verification and recognition tasks, speaker diarization and speech separation involve multiple speakers, which creates additional obstacles for learning general speech representations. Recent advances fueled by large-scale pre-trained models have changed the situation. Yang et al. (2021) proves the potential of pre-trained models on full-stack speech tasks by using the weighted sum of em- beddings from different layers.1 They find different layers contain information useful for different tasks. For instance, the hidden states of the top layers are useful for ASR, while the bottom layers are more effective for speaker verification. While exciting as a proof of concept, there are still some drawbacks in existing pre-trained models: 1) Current pre- trained models are unsatisfactory for multi-speaker tasks, such as speaker diarization and speech separation. Our ex- periments show that speech separation models trained on top of HuBERT (Hsu et al., 2021a), a top performed speech pre-trained model, achieve only marginal improvement compared with the models trained from scratch. This is mainly because the pre-training methods do not sufficiently enforce the speaker discrimination, and the training data contain only single-speaker audios. 2) Speech pre-training crucially relies on high quality and large quantities of unlabeled au- dios. The existing system utilizes Libri-Light (Kahn et al., 2020) as the main source, but the massive audiobook data mismatches the data in a real scenario and using it exclu- sively hurts the model performance when the acoustic char- acteristics of the downstream tasks are different from those of the audiobook. Hsu et al. (2021b) train wav2vec 2.0 (Baevski et al., 2020b) on larger and more diverse datasets, but there are still over 90% audio data derived from au- diobook. To eliminate the audiobook data bias, we try to gather data from different sources as much as possible in our experiments. In this paper, we present WavLM which learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks. WavLM is built based on the pre-training strategy of HuBERT, with three extensions for better speech char- acteristic modeling. 1) We add gated relative position bias (grep) (Chi et al., 2021) to the Transformer structure as the backbone, which improves model performance for ASR and keeps almost the same parameter number and training speed. Compared with the convolutional relative position embedding used in wav2vec 2.0 and HuBERT, the gates allow the relative position bias to be adjusted adaptively by conditioning on the current speech content. 2) To handle multi-speaker tasks, such as speaker diarization and speech separation, we propose an utterance-mixing training strat- egy, where partially overlapped signals are constructed to augment the training data, by mixing individual training samples with randomly selected speech pieces. 3) To fur- ther improve the model robustness and alleviate the data mismatch, we scale up unlabeled pre-training data to 94k hours of public audios. The dataset consists of 60k hours of Libri-Light, 10k hours of GigaSpeech (Chen et al., 2021a), and 24k hours of VoxPopuli (Wang et al., 2021a). The new dataset consists of training instances from different scenar- ios, such as podcasts, YouTube and European Parliament (EP) event recordings. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets.  WavLM achieves state-of-the-art performance on SU- PERB (Yang et al., 2021). The leaderboard screenshot is shown in Appendix A.4. WavLM Large outperforms HuBERT Large on all subtasks, and achieves an abso- lute 2.4 point improvement in overall evaluation. Even WavLM Base+, a 3 times smaller model, is better than HuBERT Large owing to our three modifications. Speaker verification is a task to verify the speaker identity from the voice characteristics. We select this task to evaluate the model capability of extracting speaker-related features. WavLM Large exceeds the well-known state-of-the-art (SOTA) system, ECAPA- TDNN (Desplanques et al., 2020), by a large mar- gin and achieves 0.431%, 0.538% and 1.154% EER (Equal Error Rate) on the three official trial lists of VoxCeleb1 (Nagrani et al., 2017). Speech separation is a classic multi-speaker task, which is the key to solving the cocktail party prob- lem. The task can evaluate the model capability of extracting multiple speech signals from a mixture of sounds. WavLM achieves SOTA performance on the speech separation LibriCSS benchmark (Chen et al., 2020), and significantly outperforms the previous Con- former model (Chen et al., 2021b) by a 27.7% relative word error rate (WER) reduction. Speakerdiarizationisatasktorecognize“whospoke when” from an input audio stream (Park et al., 2021). WavLM achieves SOTA performance on the CALL- HOME speaker diarization benchmark. Compared to the EEND-EDA clustering method (Horiguchi et al., 2021b), our model achieves a 12.6% diarization error rate reduction. Speech recognition requires the model to learn con- tent information, which is the main focus of the pre- vious SSL work. We evaluate our model in the Lib- riSpeech 960h setting. WavLM shows comparable performance to the wav2vec 2.0 and HuBERT, which achieves 1.8% and 3.2% WER on the test-clean and test-other sets, respectively. The contribution of the paper can be summarized as follows: 1) WavLM sheds light on a general pre-trained model for full stack speech processing tasks, in contrast to the pre- vious SSL works focusing on a group of similar tasks. 2) We propose simple but effective modifications to the exist- ing pre-trained models, which show general and consistent improvements across downstream tasks. 3) We scale-up self-supervised speech pre-training with more unlabeled data and longer training steps. 4) We achieve state-of-the- art results on the SUPERB benchmark, and significantly boost the performance for various speech processing tasks on their representative benchmarks, including speech separation, speaker verification, and speaker diarization. The models and code are released to facilitate future research. Conclusion We present WavLM, a large-scale pre-trained model with 94k hour audio as inputs, to solve full stack speech pro- cessing tasks. WavLM is built based on the pre-training strategy of the HuBERT with utterance mixing strategy and the grep structure change for the Transformer. WavLM updates state-of-the-art results on the SUPERB, as well as the representative testsets of speaker verification, speech separation, and speaker diarization. In contrast to previous SSL models, WavLM is not only effective for the ASR task, but also has the potential to become the next-generation backbone network for speaker related tasks. In the future, we would like to scale up the model size to in- crease the model capability, as previous work has shown the benefits from more parameters (Zhang et al., 2021). Mean- while, the model compression technique is also worth trying due to the time constraint and limited test time resource in real scenarios. It is also a promising direction to jointly learn text and speech representation in a self-supervised pre- training framework (Ao et al., 2021), as the huge amount of text data might increase the capability on speech content modeling.",1
"Abstract Self-supervised learning (SSL) achieves great suc- cess in speech recognition, while limited explo- ration has been attempted for other speech pro- cessing tasks. As speech signal contains multi- faceted information including speaker identity, paralinguistics, spoken content, etc., learning uni- versal representations for all speech tasks is chal- lenging. In this paper, we propose a new pre- trained model, WavLM, to solve full-stack down- stream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker iden- tity preservation. We first equip the Transformer structure with gated relative position bias to im- prove its capability on recognition tasks. For bet- ter speaker discrimination, we propose an utter- ance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extrac- tion. WavLM Large achieves state-of-the-art per- formance on the SUPERB benchmark, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index Terms Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks. In the past, it has been infeasible to build such a general model, as dif- ferent tasks focus on different aspects of speech signals. For instance, speaker verification requires the network to learn the speaker characteristic regardless of the spoken con- tent, while speech recognition demands the network discard speaker characteristics and focus only on content informa- tion. Meanwhile, unlike verification and recognition tasks, speaker diarization and speech separation involve multiple speakers, which creates additional obstacles for learning general speech representations. Recent advances fueled by large-scale pre-trained models have changed the situation. Yang et al. (2021) proves the potential of pre-trained models on full-stack speech tasks by using the weighted sum of em- beddings from different layers.1 They find different layers contain information useful for different tasks. For instance, the hidden states of the top layers are useful for ASR, while the bottom layers are more effective for speaker verification. While exciting as a proof of concept, there are still some drawbacks in existing pre-trained models: 1) Current pre- trained models are unsatisfactory for multi-speaker tasks, such as speaker diarization and speech separation. Our ex- periments show that speech separation models trained on top of HuBERT (Hsu et al., 2021a), a top performed speech pre-trained model, achieve only marginal improvement compared with the models trained from scratch. This is mainly because the pre-training methods do not sufficiently enforce the speaker discrimination, and the training data contain only single-speaker audios. 2) Speech pre-training crucially relies on high quality and large quantities of unlabeled au- dios. The existing system utilizes Libri-Light (Kahn et al., 2020) as the main source, but the massive audiobook data mismatches the data in a real scenario and using it exclu- sively hurts the model performance when the acoustic char- acteristics of the downstream tasks are different from those of the audiobook. Hsu et al. (2021b) train wav2vec 2.0 (Baevski et al., 2020b) on larger and more diverse datasets, but there are still over 90%data derived from au- diobook. To eliminate the audiobook data bias, we try to gather data from different sources as much as possible in our experiments. In this paper, we present WavLM which learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks. WavLM is built based on the pre-training strategy of HuBERT, with three extensions for better speech char- acteristic modeling. 1) We add gated relative position bias (grep) (Chi et al., 2021) to the Transformer structure as the backbone, which improves model performance for ASR and keeps almost the same parameter number and training speed. Compared with the convolutional relative position embedding used in wav2vec 2.0 and HuBERT, the gates allow the relative position bias to be adjusted adaptively by conditioning on the current speech content. 2) To handle multi-speaker tasks, such as speaker diarization and speech separation, we propose an utterance-mixing training strat- egy, where partially overlapped signals are constructed to augment the training data, by mixing individual training samples with randomly selected speech pieces. 3) To fur- ther improve the model robustness and alleviate the data mismatch, we scale up unlabeled pre-training data to 94k hours of public audios. The dataset consists of 60k hours of Libri-Light, 10k hours of GigaSpeech (Chen et al., 2021a), and 24k hours of VoxPopuli (Wang et al., 2021a). The new dataset consists of training instances from different scenar- ios, such as podcasts, YouTube and European Parliament (EP) event recordings. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets. WavLM achieves state-of-the-art performance on SU- PERB (Yang et al., 2021). The leaderboard screenshot is shown in Appendix A.4. WavLM Large outperforms HuBERT Large on all subtasks, and achieves an abso- lute 2.4 point improvement in overall evaluation. Even WavLM Base+, a 3 times smaller model, is better than HuBERT Large owing to our three modifications. Speaker verification is a task to verify the speaker identity from the voice characteristics. We select this task to evaluate the model capability of extracting speaker-related features. WavLM Large exceeds the well-known state-of-the-art (SOTA) system, ECAPA- TDNN (Desplanques et al., 2020), by a large mar- gin and achieves 0.431%, 0.538% and 1.154% EER (Equal Error Rate) on the three official trial lists of VoxCeleb1 (Nagrani et al., 2017). Speech separation is a classic multi-speaker task, which is the key to solving the cocktail party prob- lem. The task can evaluate the model capability of extracting multiple speech signals from a mixture of sounds. WavLM achieves SOTA performance on the speech separation LibriCSS benchmark (Chen et al., 2020), and significantly outperforms the previous Con- former model (Chen et al., 2021b) by a 27.7% relative word error rate (WER) reduction. Speaker diarization is a task to recognize whos poke when from an input audio stream (Park et al., 2021). WavLM achieves SOTA performance on the CALL- HOME speaker diarization benchmark. Compared to the EEND-EDA clustering method (Horiguchi et al., 2021b), our model achieves a 12.6% diarization error rate reduction. Speech recognition requires the model to learn con- tent information, which is the main focus of the pre- vious SSL work. We evaluate our model in the Lib- riSpeech 960h setting. WavLM shows comparable performance to the wav2vec 2.0 and HuBERT, which achieves 1.8% and 3.2% WER on the test-clean and test-other sets, respectively. The contribution of the paper can be summarized as follows: 1) WavLM sheds light on a general pre-trained model for full stack speech processing tasks, in contrast to the pre- vious SSL works focusing on a group of similar tasks. 2) We propose simple but effective modifications to the exist- ing pre-trained models, which show general and consistentacross downstream tasks. 3) We scale-up self-supervised speech pre-training with more unlabeled data and longer training steps. 4) We achieve state-of-the- art results on the SUPERB benchmark, and significantly boost the performance for various speech processing tasks on their representative benchmarks, including speech separation, speaker verification, and speaker diarization. The SU- PERB benchmark ships in mid-January, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index Terms Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks.",0
"ABSTRACT Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi- automated fact-checking that can be used for either the general public but also useful for fact- checking organisations. FacTeR-Check enables retrieving fact-checked information, unchecked claims verification and tracking dangerous information over social media. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-checking tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI19-SP that is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering or context-aware sentences similarity evaluation. In this research, we leverage the most recent advances in Natural Language Processing to develop a semantic-aware multilingual Transformer-based architecture for semantic similarity evaluation, semi-automated fact-checking and tracking of information pieces in Online Social Networks. We present an architecture that, on the one hand, can help general public in checking the veracity of a claim (i.e. a tweet) through context-aware automated comparison against a databases of hoaxes. On the other hand, our proposal aims at providing useful tools for fact-checking organisations for tracking and monitoring hoaxes circulating in OSNs. To validate and to show the capabilities of the architecture proposed, we use the COVID-19 pandemic scenario in Spanish speaking countries. We manually selected 61 hoaxes related to Covid-19 and extracted related tweets using Twitter API. Our architecture allows to label the degree of entailment of these tweets with a hoax, providing a useful insight into the propagation of hoaxes in Spanish on Twitter throughout one year. In summary, this research presents the following contributions: A labelled dataset of Spanish tweets IDs with the degree of entailment against a list of 61 hoaxes. A context-aware multilingual semantic similarity method for searching hoaxes with high similarity to a given query. A Natural Language Inference model for semi-automated fact-checking. A deep insight of misinformation and disinformation circulating on Twitter related to Covid-19 in Spanish speaking countries. The remaining sections of this manuscript are organised as follows. Conclusion In this article we have proposed FacTr-Check to mitigate OSN misinformation. Our architecture proposes two pipelines, one for semi-automated verification of claims; another for tracking known hoaxes on social media. The pipelines share three modules: a semantic similarity module, a NLI module and a information retrieval module. Using semantic similarity to find related fact-checks and NLI to contrast the claim to reputable sources we are able to semi-automatically verify information. On the other hand, to track hoaxes, we retrieve tweets related to a hoax, filtering the most relevant tweets with semantic similarity and contrasting them with the original hoax, finding how this particular piece of misinformation has spread along a social media platform. While our validation has been limited to COVID-19 and Twitter we want to emphasise that our architecture is adaptable to other knowledge domains as well as other social networks. First we evaluate all three modules individually, then the modules are put together in both pipelines to test their joint performance. To begin with, the similarity module offers above average performance using multilingual models on the STS benchmark. The NLI module uses XLM-RoBERTa fine-tuned on XNLI and the SICK training dataset, which performs adequately on SICK test, offering similar results to state-of-the-art models, as well as offering multilingual capabilities. Finally, the information retrieval module is compared against KeyBERT and RAKE on a dataset of Spanish keywords from our gathered hoaxes. Using this architecture we build a dataset for misinformation detection using NLI in Spanish about COVID-19, as well as track a selection of hoaxes to analyse their spread. FacTr-Check proves to extract insightful information about the spread of many hoaxes, showing aggregate frequency peaks matching COVID-19 waves in Spain. Identified hoaxes have their own particular activity peaks, some have more longevity than others, others are used much more; they are extremely diverse in lifetime and popularity. FacTr-Check relies on external databases to operate, if a rumour reaches the verification pipeline, and there is no related fact-check retrievable on the topic, only similar articles will be retrieved. This means that the verification pipeline is as robust as the fact-check database. Alternatives may include composing a massive database of hoax embeddings, as well as a dynamic information retrieval process to detect new hoaxes and calculate their embeddings. The architecture has been tested on OSNs, meaning that it is blind to outside information such as news sites or other valuable sources of information. If a piece of disinformation is published outside of the OSN, it will be out of the scope of the tracking algorithm. Finally, information is varied, coming in many shapes and forms, including text but also audio, video or images; the verification and tracking pipeline can only work on textual data, meaning that there is room for building systems that support other formats.",1
"ABSTRACT Our society produces and shares overwhelming amounts of information through the Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern on every country. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is crucial to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi- automated fact-checking that can be used for either the general public but also useful for fact- checking organisations. FacTeR-Check enables retrieving fact-checked information, unchecked claims verification and tracking dangerous information over social media. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-checking tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI19-SP that is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering or context-aware sentences similarity evaluation. In this research, we leverage the most recent advances in Natural Language Processing to develop a semantic-aware multilingual Transformer-based architecture for semantic similarity evaluation, semi-automated fact-checking and tracking of information pieces in Online Social Networks. We present an architecture that, on the one hand, can help general public in checking the veracity of a claim (i.e. a tweet) through context-aware automated comparison against a databases of hoaxes. On the other hand, our proposal aims at providing useful tools for fact-checking organisations for tracking and monitoring hoaxes circulating in OSNs. To validate and to show the capabilities of the architecture proposed, we use the COVID-19 pandemic scenario in Spanish speaking countries. We manually selected 61 hoaxes related to Covid-19 and extracted related tweets using Twitter API. Our architecture allows to label the degree of entailment of these tweets with a hoax, providing a useful insight into the propagation of hoaxes in Spanish on Twitter throughout one year. In summary, this research presents the following contributions: A labelled dataset of Spanish tweets IDs with the degree of entailment against a list of 61 hoaxes. A context-aware multilingual semantic similarity method for searching hoaxes with high similarity to a given query. A Natural Language Inference model for semi-automated fact-checking. A deep insight of misinformation and disinformation circulating on Twitter related to Covid-19 in Spanish speaking countries. The remaining sections of this manuscriptorganised as follows. Conclusion In this article we have proposed FacTr-Check to mitigate OSN misinformation. Our architecture proposes two pipelines, one for semi-automated verification of claims; another for tracking known hoaxes on social media. The pipelines share three modules: a semantic similarity module, a NLI module and a information retrieval module. Using semantic similarity to find related fact-checks and a NLI to contrast the claim to reputable sources we are able to verify information. This architectures involves several modules developed to evaluate semantic similarity, to calculate natural language inference and to retrieve information from Online Social Networks. The union of all these modules builds a semi-automated fact-check tool able of verifying new claims, to extract related evidence, and to track the evolution of a hoax on a OSN. While individual modules are validated on related benchmarks (mainly MSTS and SICK), the complete architecture is validated using a new dataset called NLI, which is publicly released with COVID-19 related hoaxes and tweets from Spanish social media. Our results show state-of-the-art performance on the individual benchmarks, as well as producing useful analysis of the evolution over time of 61 different hoaxes. Keywords Misinformation, Transformers, COVID-19, Hoax, Natural Language Inference, Semantic Similarity Introduction Misinformation and disinformation are two terms that resound since a long time. Inaccurate information has been largely used for varied purposes for decades and centuries. However, the emergence of Internet, Online Social Networks and Instant Messaging Services has undoubtedly facilitated its rapid creation and diffusion. These two terms reflect a problem that continues to expand and which involves an increasing concern to society. Yet, there are important differences between both terms: while misinformation involves inaccurate information propagated without knowing it is false, disinformation involves disseminating deliberately false information in order to deceive people1. The COVID-19 pandemic has undoubtedly drawn attention to this problem, when misinformation and disinformation meet health and affect public safety. From the initiation of this pandemic, an incessant repetition of falsehoods has been generated and propagated, undermining the work of health authorities in the fight against COVID-19. False reports about its origin, its death rate, or about vaccines have been a constant threat to control this virus. Fact-checking organisations are on the forefront combating the propagation of false claims, where intensive work is done to deny hoaxes that circulate through different channels, such as Online Social Networks (OSNs), Instant Messaging Services or Mass Media. This verification process conducted by these companies is mostly carried out by hand, however, it is barely reflected in OSNs. Users of these platforms share fake information without even realising it is indeed a falsehood or deliberately posting false claims without further consequences. Recent advances in Natural Language Processing, such as the Transformer architecture [1], allow to deal with complex human language for a plethora of tasks, such as summarization, translation, sequence classification, question answering.",0
"Abstract Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One fac- tor that significantly affects the performance of NMT is the availability of high-quality paral- lel corpora. However, high-quality parallel cor- pora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently re- leased seven types of parallel corpora for Ko- rean. In this study, we conduct an in-depth ver- ification of the quality of corresponding par- allel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant ex- periments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictio- nary base. To the best of our knowledge, this study is the first to use LIWC to analyze par- allel corpora in the field of NMT. Our find- ings suggest the direction of further research toward obtaining the improved quality paral- lel corpora through our correlation analysis in LIWC and NMT performance. Introduction In recent years, the demand for machine translation (MT) systems has been continuously increasing and its importance is growing, especially for the industrial services. (Vieira et al., 2021; Zheng et al., 2019) Companies, such as Google, Facebook, Mi- crosoft, Amazon, and Unbabel continue to conduct research and formulate plans to commercialize ap- plications related to MT. From the late 1950s, numerous MT-related projects were proceeded by mainly focusing on rule-based and statistical-based approaches before the advent of deep learning technology. As deep learning based neural machine translation (NMT) was proposed and adopted to several researches, it has been gradually figured out that more supe- rior performance can be derived through NMT ap- proach (Bahdanau et al., 2014; Vaswani et al., 2017;Lample and Conneau, 2019; Song et al., 2019). Followed by the adoption of deep learning based technique, the improvements of computing power (e.g. GPU) and corresponding enhancement of par- allel processing accelerated the advancement of NMT. Recently, release of open source frameworks, such as Pytorch(Paszke et al., 2019), and lowered accessibility to the big data further facilitated vig- orous and diverse research. However, several issues considering the enhance- ment of the NMT system remain still. Represen- tatively, limitations in ensuring the quality of data is an unresolved issue. As have previously been studied, the quality of the training data is deeply related to the NMT performance (Park et al., 2020d, 2021b). The major problem is that the process of building a high-quality parallel corpus is time- consuming and expensive, and it is significantly difficult for low-resource languages, such as Ko- rean. Although data-augmentation techniques, such as back translation (Edunov et al., 2018) and copied translation (Currey et al., 2017) have been intro- duced, as the human supervision is generally mini- mized or excluded in the data generation process, the quality of such pseudo-generated parallel cor- pus cannot be guaranteed (Burlot and Yvon, 2019; Epaliyana et al., 2021). This restricted the usage of pseudo-generated parallel to complements of human-labeled gold parallel corpus, rather than its substitutes (Imankulova et al., 2017). For the alleviation of above limitations, numer- ous studies on the collection of high-quality train- ing data have been conducted, such as parallel cor- pus filtering (PCF) research and Data Dam project. PCF refers to a research field that aims to filter out low-quality noisy data (i.e. sentence pairs) residing in the parallel corpus, and improve the over- all quality of the corpus. PCF is currently being applied to various NMT studies and contributed to the advancement of the NMT systems (Koehn et al., 2019; Park et al., 2020c). While the amount of training data caused significant impact on the statistical-based MT approaches, the quality of data is treated as more important than the amount of data in general deep learning-based MT approaches (Khayrallah and Koehn, 2018; Koehn et al., 2020b). Moreover, Data Dam 1 projects for building high- quality parallel corpora nationally are in progress. In the Republic of Korea, a large number of paral- lel corpora is open to the public through AI-Hub 2, which is organized by the National Information Society Agency (NIA) (Park and Lim, 2020). Following these research trends, where the qual- ity is treated more importantly than the quantity in the data construction process, we analyzed the above Korean-English parallel corpus distributed by AI-Hub. Despite its sufficient amount of data, the quality of corresponding corpus has not been confirmed clearly. This may restrict the uncon- strained utilization of such corpus in adoption to the NMT model, as low quality data may degrade the overall performance. In this study, we conducted several quality verification experiments including Linguistic Inquiry and Word Count (LIWC) (Pen- nebaker et al., 2001; Tausczik and Pennebaker, 2010), and clarified the quality and characteris- tics of such corpus. By analyzing various factors that can affect NMT performance, we proposed a method that can be applied in future research using the analysis results. LIWC is a text-analysis tool that automatically analyzes the number of words in a sentence and classifies words with similar meanings and sen- timental characteristics. LIWC extracts various interpersonal variables related to clinical, social, physiological, cognitive, psychological, and de- velopmental contexts that cannot be detected us- ing previous text-analysis programs. Additionally, LIWC comprises a variety of features for analyzing text. LIWC generally used to recognize linguistic markers for mental health study in Psychopathol- ogy such as detecting Narcissism(Holtzman et al., 2019), schizophrenia(Bae et al., 2021), bipolar dis- order(Sekulic ́ et al., 2018). However, LIWC pro- vides various linguistic features, word count, gender bias and so on, so it can be used for various analyses. In this study, we use LIWC to analyze parallel corpora based on diverse properties. It is also first time to analyze corpus using LIWC. In addition, we conduct baseline translation ex- periments by training transformer-base model struc- ture (Vaswani et al., 2017) through all the parallel corpora given by AIhub. By analyzing MT per- formance of corresponding models, we propose further research directions on MT for the Korean language. The contributions of this study are as follows: For the first time, we conduct a deep data anal- ysis on AI-Hub data. To the best of our knowl- edge, this is the first time LIWC has been used to analyze corpora. This study acts as a mile- stone for further studies on NMT with respect to the Korean language. We conduct baseline translation experiments on all the data in the AI-Hub parallel corpus. Our experiments provide a foundation for fur- ther research on Korean-based NMT. We discovered that many factors might cause decreasing model performance, and we pro- vide the direction that those factors could be filtered through our correlation analysis be- tween LIWC and model performance. Conclusions In this work, we proceeded with a quality eval- uation of all the Korean-related parallel corpus, released by AI Hub. For the model-centric perfor- mance validation, we constructed a transformer based NMT model trained with each parallel cor- pus. Through quantitative and qualitative analysis of these NMT models, we point out some proba- ble limitations on constructing corpora. First, for learning NMT model well in specific field, the do- main corpora should contain various words and expressions in consideration of the excessive per- formance difference between domain and general corpora. Second, given the significant performance gap in terms of language direction, half of the paral- lel data to be built must be configured in the source language and the other half in the target language and then translated respectively. Away from the model-centric analysis, we en- couraged data-centric research through LIWC anal- ysis. We figured out the association between LIWC and model performance in terms of data filtering. Through this analysis, we suggested the direction of further work to improve model performance. The national level re-examination of the various standards and building processes should be made for the encouragement of AI data construction re- searches. In the future, we plan to investigate ef- ficient beam search strategies and new decoding methods by utilizing these AI Hub data. Also, to more accurately measure the model performance, we plan to build an official Korean-English test set.",1
"Abstract Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One fac- tor that significantly affects the performance of NMT is the availability of high-quality paral- lel corpora. However, high-quality parallel cor- pora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently re- leased seven types of parallel corpora for Ko- rean. In this study, we conduct an in-depth ver- ification of the quality of corresponding par- allel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant ex- periments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictio- nary base. To the best of our knowledge, this study is the first to use LIWC to analyze par- allel corpora in the field of NMT. Our find- ings suggest the direction of further research toward obtaining the improved quality paral- lel corpora through our correlation analysis in LIWC and NMT performance. Introduction In recent years, the demand for machine translation (MT) systems has been continuously increasing and its importance is growing, especially for the industrial services. (Vieira et al., 2021; Zheng et al., 2019) Companies, such as Google, Facebook, Mi- crosoft, Amazon, and Unbabel continue to conduct research and formulate plans to commercialize ap- plications related to MT. From the late 1950s, numerous MT-related projects were proceeded by mainly focusing on rule-based and statistical-based approaches before the advent of deep learning technology. As deep learning based neural machine translation (NMT) was proposed and adopted to several researches, it has been gradually figured out that more supe- rior performance can be derived through NMT ap- proach (Bahdanau et al., 2014; Vaswani et al., 2017;Lample and Conneau, 2019; Song et al., 2019). Followed by the adoption of deep learning based technique, the improvements of computing power (e.g. GPU) and corresponding enhancement of par- allel processing accelerated the advancement of NMT. Recently, release of open source frameworks, such as Pytorch(Paszke et al., 2019), and lowered accessibility to the big data further facilitated vig- orous and diverse research. However, several issues considering the enhance- ment of the NMT system remain still. Represen- tatively, limitations in ensuring the quality of data is an unresolved issue. As have previously been studied, the quality of the training data is deeply related to the NMT performance (Park et al., 2020d, 2021b). The major problem is that the process of building a high-quality parallel corpus is time- consuming and expensive, and it is significantly difficult for low-resource languages, such as Ko- rean. Although data-augmentation techniques, such as back translation (Edunov et al., 2018) and copied translation (Currey et al., 2017) have been intro- duced, as the human supervision is generally mini- mized or excluded in the data generation process, the quality of such pseudo-generated parallel cor- pus cannot be guaranteed (Burlot and Yvon, 2019; Epaliyana et al., 2021). This restricted the usage of pseudo-generated parallel to complements of human-labeled gold parallel corpus, rather than its substitutes (Imankulova et al., 2017). For the alleviation of above limitations, numer- ous studies on the collection of high-quality train- ing data have been conducted, such as parallel cor- pus filtering (PCF) research and Data Dam project. PCF refers to a research field that aims to filter out low-quality noisy data (i.e. sentence pairs) residing in the parallel corpus, and improve the over- all quality of the corpus. PCF is currently being applied to various NMT studies and contributed to the advancement of the NMT systems (Koehn et al., 2019; Park et al., 2020c). While the amount of training data caused significant impact on the statistical-based MT approaches, the quality of data is treated as more important than the amount of data in general deep learning-based MT approaches (Khayrallah and Koehn, 2018; Koehn et al., 2020b). Moreover, Data Dam 1 projectsbuilding high- quality parallel corpora nationally are in progress. In the Republic of Korea, a large number of paral- lel corpora is open to the public through AI-Hub 2, which is organized by the National Information Society Agency (NIA) (Park and Lim, 2020). Following these research trends, where the qual- ity is treated more importantly than the quantity in the data construction process, we analyzed the above Korean-English parallel corpus distributed by AI-Hub. Despite its sufficient amount of data, the quality of corresponding corpus has not been confirmed clearly. This may restrict the uncon- strained utilization of such corpus in adoption to the NMT model, as low quality data may degrade the overall performance. In this study, we conducted several quality verification experiments including Linguistic Inquiry and Word Count (LIWC) (Pen- nebaker et al., 2001; Tausczik and Pennebaker, 2010), and clarified the quality and characteris- tics of such corpus. By analyzing various factors that can affect NMT performance, we proposed a method that can be applied in future research using the analysis results. LIWC is a text-analysis tool that automatically analyzes the number of words in a sentence and classifies words with similar meanings and sen- timental characteristics. LIWC extracts various interpersonal variables related to clinical, social, physiological, cognitive, psychological, and de- velopmental contexts that cannot be detected us- ing previous text-analysis programs. Additionally, LIWC comprises a variety of features for analyzing text. LIWC generally used to recognize linguistic markers for mental health study in Psychopathol- ogy such as detecting Narcissism(Holtzman et al., 2019), schizophrenia(Bae et al., 2021), bipolar dis- order(Sekulic et al., 2018). However, LIWC pro- vides various linguistic features, word count, gender bias and so on, so it can be used for various analyses. In this study, we use LIWC to analyze parallel corpora based on diverse properties. It is also first time to analyze corpus using LIWC. In addition, we conduct baseline translation ex- periments by training transformer-base model struc- ture (Vaswani et al., 2017) through all the parallel corpora given by AIhub. By analyzing MT per- formance of corresponding models, we propose further research directions on MT for the Korean language. The contributions of this study are as follows: For the first time, we conduct a deep data anal- ysis on AI-Hub data. To the best of our knowl- edge, this is the first time LIWC has been used to analyze corpora. This study acts as a mile- stone for further studies on NMT with respect to the Korean language. We conduct baseline translation experiments on all the data in the AI-Hub parallel corpus. Our experiments provide a foundation for fur- ther research on Korean-based NMT. We discovered that many factors might cause decreasing model performance, and we pro- vide the direction that those factors could be filtered through our correlation analysis be- tween LIWC and model performance. Conclusions In this work, we proceeded with a quality eval- uation of all the Korean-related parallel corpus, released by AI Hub. For the model-centric perfor- mance validation, we constructed a transformer based NMT model trained with each parallel cor- pus. Through quantitative and qualitative analysis of these NMT models, we point out some proba- ble limitations on constructing corpora. First, for learning NMT model well in specific field, the do- main corpora should contain various words and expressions in consideration of the excessive per- formance difference between domain and general corpora. Second, given the significant performance gap in terms of language direction, half of the paral- lel data to be built must be configured in the source language and the other half in the target language and then translated respectively. Away from the model-centric analysis, we en- couraged data-centric research through LIWC anal- ysis. We figured out the association between LIWC and model performance in terms of data filtering. Through this analysis, we suggested the direction of further work to improve model performance. The national level re-examination of the various standards and building processes should be made for the encouragement of AI data construction re- searches. In the future, we plan to investigate ef- ficient beam search strategies and new decoding methods bythese AI Hub data. Also, to more accurately measure the model performance, we plan to build an official Korean-English test set.",0
"ABSTRACT The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification performance, and many use data that contain potential confounds, which makes it difficult to determine their validity. Two possible confounds are data-origin (i.e., the dataset is composed of more than one source) and product ownership (i.e., reviews written by individuals who own or do not own the reviewed product). In the present study, we investigate the effect of both confounds for fake review detection. Using an experimental design, we manipulate data-origin, product ownership, review polarity, and veracity. Supervised learning analysis suggests that review veracity (60.26 - 69.87%) is somewhat detectable but reviews additionally confounded with product-ownership (66.19 - 74.17%), or with data-origin (84.44 - 86.94%) are easier to classify. Review veracity is most easily classified if confounded with product-ownership and data-origin combined (87.78 - 88.12%), suggesting overestimations of the true performance in other work. These findings are moderated by review polarity. Introduction Online shopping is not new, but it is increasing in popularity as seen by the growth of companies such as Amazon and eBay [Palmer, 2020, Soper, 2021, Weise, 2020]. Previous work shows that consumers rely heavily on product reviews posted by other people to guide their purchasing decisions [Anderson and Magruder, 2012, Chevalier and Mayzlin, 2006, Watson, 2018]. While sensible, this has created the opportunity and market for deceptive reviews, which are currently among the most critical problems faced by online shopping platforms and those who use them [Dwoskin and Timberg, 2018, Nguyen, 2018]. Research suggests that for a range of deception detection tasks (e.g. identifying written or verbal lies about an individual’s experience, biographical facts, or any non-personal events), humans typically perform at the chance level [DePaulo et al., 2003, Kleinberg and Verschuere, 2021]. Furthermore, in the context of considering online reviews, the sheer volume of reviews [Woolf, 2014] makes the task of deception detection implausible for all but the most diligent consumers. With this in mind, the research effort has shifted towards the use and calibration of automated approaches. For written reviews, which are the focus of this article, such approaches typically rely on text mining and supervised machine learning algorithms [Newman et al., 2003, Pérez-Rosas et al., 2018, Ott et al., 2011, 2013]. However, while the general approach is consistent, classification performance varies greatly between studies, as do the approaches to constructing the datasets used. Higher rates of performance are usually found in studies for which data are constructed from several different sources such as a crowdsourcing platform and an online review platform [Ott et al., 2011, 2013], while lower rates of performance are typically found in studies for which data is extracted from a single source and for which greater experimental control is exercised (e.g., Kleinberg and Verschuere [2021]). Such findings suggest that confounds associated with the construction of datasets may explain some of the variation in classification performance between studies and highlights the need for the exploration of such issues. In the current study, we will explore two possible confounds and estimate their effects on automated classification performance. In what follows, we first identify and explain the two confounds. Next, we provide an outline of how we control for them through a highly controlled data collection procedure. Lastly, we will run six analyses on subsets of the data to demonstrate the pure and combined effects of the confounds in automated veracity classification tasks. Confounding factors In an experiment, confounding variables can lead to an omitted variable bias, in which the omitted variables affect the dependent variable, and the effects are falsely attributed to the independent variables(s). In the case of the detection of fake reviews, two potential confounds might explain why some studies report higher and possibly overestimated automated classification performances than others. The first concerns the provenance of the data used. For example, deceptive reviews are often collected from participants recruited through crowdsourcing platforms, while truthful reviews are scraped from online platforms Ott et al. [2011, 2013], such as TripAdvisor, Amazon, Trustpilot, or Yelp. Creating datasets in this way is efficient but introduces a potential confound. That is, not only do the reviews differ in veracity but also their origin. If origin and veracity were counterbalanced so that half of the fake (and genuine) reviews were generated using each source this would be unproblematic but unfortunately in some existing studies, the two are confounded. A second potential confound concerns ownership. In existing studies, participants who write fake reviews are asked to write about products (or services) that they do not own. In contrast, in the case of the scraped reviews – assuming that they are genuine (which is also a problematic assumption) – these will be written by those who own the products (or have used the services). As such, ownership and review veracity (fake or genuine) will also be confounded. Confounds in fake review detection Studies of possible confounding factors in deception detection tasks that involve reviews are scarce. In their study, Salvetti et al. [2016] investigated whether a machine learning classifier could disentangle the effects of two different types of deception – lies vs. fabrications. In the case of the former, participants recruited using Amazon’s Mechanical Turk (AMT) were asked to write a truthful and deceptive review about an electronic product or a hotel they knew. In the case of the latter, a second group of AMT participants was asked to write deceptive reviews about the same products or hotels. However, this time they were required to do this for products or hotels they had no knowledge of, resulting in entirely fabricated reviews. Salvetti et al. [2016] found that the classifier was able to differentiate between truthful reviews and fabricated ones but not particularly well. However, it could not differentiate between truthful reviews and lies – classification performance was around the chance level. These findings suggest that product ownership (measured here in terms of fabrications vs truthful reviews) is a potentially important factor in deceptive review detection. A different study examined the ability of a classifier to differentiate truthful and deceptive reviews from Amazon [Fornaciari et al., 2020] using the ""DeRev"" dataset [Fornaciari and Poesio, 2014]. The dataset contains fake Amazon book reviews that were identified through investigative journalism [Flood, 2012, Streitfeld, 2011]. Truthful reviews were selected from Amazon about other books, from famous authors such as Arthur Conan Doyle, Rudyard Kipling, Ken Follett, or Stephen King for which it was assumed that it would not make sense for someone to write fake reviews about them. A second corpus of fake reviews – written about the same books – was then generated by participants recruited through crowdsourcing to provide a comparison with the ""DeRev"" reviews. The authors then compared the performance of a machine learning classifier in distinguishing between different classes of reviews (e.g., crowdsourced-fake vs. Amazon-fake, crowdsourced-fake vs. Amazon-truthful). Most pertinent here was the finding that the authors found the crowdsourced-fake reviews differed from the Amazon-fake reviews. Both studies [Fornaciari et al., 2020, Salvetti et al., 2016] hint at the problems of confounding factors in deception detection tasks. However, a combination of both factors (ownership and data-origin) or how the two interact for true and deceptive reviews has not been tested yet. Aims of this paper Confounding variables have the potential to distort the findings of studies, leading researchers to conclude that a classifier can distinguish between truthful and fake reviews when, in reality, it is actually distinguishing between other characteristics of the data, such as the way in which it was generated. Such confounds would mean that the real-world value of the research is limited (at best). In the current study, we employ an experimental approach to systematically manipulate these possible confounders and to measure their effects for reviews of smartphones. Specifically, we estimate the effect of product-ownership by collecting truthful and deceptive reviews from participants who do and do not own the products they were asked to review. To examine the effect of data-origin we also use data (for the same products) scraped from an online shopping platform. We first examine how well reviews can be differentiated by veracity alone (i.e., without confounds), and if classification performance changes when this is confounded with product-ownership, data-origin, or both. If ownership or data-origin do influence review content (we hypothesize that they do), reviews should be easier to differentiate when either of the two confounds is present in veracity classification, but reviews should be most easily classifiable if both confounds (ownership, data-origin) are present at the same time. Conclusion Through careful experimental control, we found that product ownership and data-origin are confounding fake review detection resulting in overestimations of model performances. Especially data-origin seems to boost classification performance, and this could easily be misattributed to classifying veracity alone. Our findings suggest an overestimation of 24.89-46.23% when data is sourced from different platforms. More effort and experimental control are necessary to create datasets when investigating complex concepts such as deception.",1
"ABSTRACT The popularity of online shopping is steadily increasing. At the same time, fake product reviews are published widely and have the potential to affect consumer purchasing behavior. In response, previous work has developed automated methods for the detection of deceptive product reviews. However, studies vary considerably in terms of classification performance, and many use data that contain potential confounds, which makes it difficult to determine their validity. Two possible confounds are data-origin (i.e., the dataset is composed of more than one source) and product ownership (i.e., reviews written by individuals who own or do not own the reviewed product). In the present study, we investigate the effect of both confounds for fake review detection. Using an experimental design, we manipulate data-origin, product ownership, review polarity, and veracity. Supervised learning analysis suggests that review veracity (60.26 - 69.87%) is somewhat detectable but reviews additionally confounded with product-ownership (66.19 - 74.17%), or with data-origin (84.44 - 86.94%) are easier to classify. Review veracity is most easily classified if confounded with product-ownership and data-origin combined (87.78 - 88.12%), suggesting overestimations of the true performance in other work. These findings are moderated by review polarity. Introduction Online shopping is not new, but it is increasing in popularity as seen by the growth of companies such as Amazon and eBay [Palmer, 2020, Soper, 2021, Weise, 2020]. Previous work shows that consumers rely heavily on product reviews posted by other people to guide their purchasing decisions [Anderson and Magruder, 2012, Chevalier and Mayzlin, 2006, Watson, 2018]. While sensible, this has created the opportunity and market for deceptive reviews, which are currently among the most critical problems faced by online shopping platforms and those who use them [Dwoskin and Timberg, 2018, Nguyen, 2018]. Research suggests that for a range of deception detection tasks (e.g. identifying written or verbal lies about an individual's experience, biographical facts, or any non-personal events), humans typically perform at the chance level [DePaulo et al., 2003, Kleinberg and Verschuere, 2021]. Furthermore, in the context of considering online reviews, the sheer volume of reviews [Woolf, 2014] makes the task of deception detection implausible for all but the most diligent consumers. With this in mind, the research effort has shifted towards the use and calibration of automated approaches. For written reviews, which are the focus of this article, such approaches typically rely on text mining and supervised machine learning algorithms [Newman et al., 2003, Perez-Rosas et al., 2018, Ott et al., 2011, 2013]. However, while the general approach is consistent, classification performance varies greatly between studies, as do the approaches to constructing the datasets used. Higher rates of performance are usually found in studies for which data are constructed from several different sources such as a crowdsourcing platform and an online review platform [Ott et al., 2011, 2013], while lower rates of performance are typically found in studies for which data is extracted from a single source and for which greater experimental control is exercised (e.g., Kleinberg and Verschuere [2021]). Such findings suggest that confounds associated with the construction of datasets may explain some of the variation in classification performance between studies and highlights the need for the exploration of such issues. In the current study, we will explore two possible confounds and estimate their effects on automated classification performance. In what follows, we first identify and explain the two confounds. Next, we provide an outline of how we control for them through a highly controlled data collection procedure. Lastly, we will run six analyses on subsets of the data to demonstrate the pure and combined effects of the confounds in automated veracity classification tasks. Confounding factors In an experiment, confounding variables can lead to an omitted variable bias, in which the omitted variables affect the dependent variable, and the effects are falsely attributed to the independent variables(s). In the case of the detection of fake reviews, two potential confounds might explain why some studies report higher and possibly overestimated automated classification performances than others. The first concerns the provenance of the data used. For example, deceptive reviews are often collected from participants recruited through crowdsourcing platforms, while truthful reviews are scraped from online platforms Ott et al. [2011, 2013], such as TripAdvisor, Amazon, Trustpilot, or Yelp. Creating datasets in this way is efficient but introduces a potential confound. That is,only do the reviews differ in veracity but also their origin. If origin and veracity were counterbalanced so that half of the fake (and genuine) reviews were generated using each source this would be unproblematic but unfortunately in some existing studies, the two are confounded. A second potential confound concerns ownership. In existing studies, participants who write fake reviews are asked to write about products (or services) that they do not own. In contrast, in the case of the scraped reviews assuming that they are genuine (which is also a problematic assumption) these will be written by those who own the products (or have used the services). As such, ownership and review veracity (fake or genuine) will also be confounded. Confounds in fake review detection Studies of possible confounding factors in deception detection tasks that involve reviews are scarce. In their study, Salvetti et al. [2016] investigated whether a machine learning classifier could disentangle the effects of two different types of deception lies vs. fabrications. In the case of the former, participants recruited using Amazon's Mechanical Turk (AMT) were asked to write a truthful and deceptive review about an electronic product or a hotel they knew. In the case of the latter, a second group of AMT participants was asked to write deceptive reviews about the same products or hotels. However, this time they were required to do this for products or hotels they had no knowledge of, resulting in entirely fabricated reviews. Salvetti et al. [2016] found that the classifier was able to differentiate between truthful reviews and fabricated ones but not particularly well. However, it could not differentiate between truthful reviews and lies classification performance was around the chance level. These findings suggest that product ownership (measured here in terms of fabrications vs truthful reviews) is a potentially important factor in deceptive review detection. A different study examined the ability of a classifier to differentiate truthful and deceptive reviews from Amazon [Fornaciari et al., 2020] using the ""DeRev"" dataset [Fornaciari and Poesio, 2014]. The dataset contains fake Amazon book reviews that were identified through investigative journalism [Flood, 2012, Streitfeld, 2011]. Truthful reviews were selected from Amazon about other books, from famous authors such as Arthur Conan Doyle, Rudyard Kipling, Ken Follett, or Stephen King for which it was assumed that it would not make sense for someone to write fake reviews about them. A second corpus of fake reviews written about the same books was then generated by participants recruited through crowdsourcing to provide a comparison with the ""DeRev"" reviews. The authors then compared the performance of a machine learning classifier in distinguishing between different classes of reviews (e.g., crowdsourced-fake vs. Amazon-fake, crowdsourced-fake vs. Amazon-truthful). Most pertinent here was the finding that the authors found the crowdsourced-fake reviews differed from the Amazon-fake reviews. Both studies [Fornaciari et al., 2020, Salvetti et al., 2016] hint at the problems of confounding factors in deception detection tasks. However, a combination of both factors (ownership and data-origin) or how the two interact for true and deceptive reviews has not been tested yet. Aims of this paper Confounding variables have the potential to distort the findings of studies, leading researchers to conclude that a classifier can distinguish between truthful and fake reviews when, in reality, it is actually distinguishing between other characteristics of the data, such as the way in which it was generated. Such confounds would mean that the real-world value of the research is limited (at best). In the current study, we employ an experimental approach to systematically manipulate these possible confounders and to measure their effects for reviews of smartphones. Specifically, we estimate the effect of product-ownership by collecting truthful and deceptive reviews from participants who do and do not own the products they were asked to review. To examine the effect of data-origin we also use data (for the same products) scraped from an online shopping platform. We first examine how well reviews can be differentiated by veracity alone (i.e., without confounds), and if classification performance changes when this is confounded with product-ownership, data-origin, or both. If ownership or data-origin do influence review content (we hypothesize that they do), reviews should be easier to differentiate when either of the two confounds is present in veracity classification, but reviews should be most easily classifiable if both confounds (ownership, data-origin) are present at the same time. Conclusion Throughexperimental control, we found that product ownership and data-origin are confounding fake review detection resulting in overestimations of model performances. Especially data-origin seems to boost classification performance, and this could easily be misattributed to classifying veracity alone. Our findings suggest an overestimation of 24.89-46.23% when data is sourced from different sources such as a crowdsourcing platform and an online review platform [Ott et al., 2011, 2013].",0
"Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models have been fine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by performing transfer black-box adversarial attacks, where only the final decisions of victim models are accessi- ble, against three victim models on three bench- mark datasets. We use a local pre-trained language model to construct potential adversarial samples and then query the victim models for decisions. Ex- perimental results demonstrate the effectiveness of our framework and TPGD algorithm. Specifically, TPGD significantly outperforms all baseline meth- ods in terms of attack success rate, and produces more fluent and grammatical adversarial examples. To summarize, the main contributions of this paper are as follows: We propose a general gradient-based textual ad- versarial attack framework based on continuous perturbations, bridging the gap between CV and NLP on the study of adversarial attacks. Com- mon gradient-based attack methods in CV can be easily adapted to NLP within our framework. We propose a novel adversarial attack method called TPGD within our framework. We employ a local model to construct adversarial samples by iteratively perturbing its embedding layer through the gradient information, and accumu- lating these small perturbations to search for potential adversarial samples. We successfully handle the challenge of black- box attack where only the decisions of models are accessible, which is rarely investigated in NLP. Conclusion and Future Works In this paper, we propose a general framework to adapt gradient-based adversarial attack methods investigated in CV to NLP. In our framework, the problem of searching textual adversarial samples is transformed from the discrete text space to the embedding layer, where continuous gradient-based perturbations can be directly added to. The pertur- bations will be amplified in the forward propaga- tion process. Then a MLM head is employed to decode the final perturbed latent representations. With its extensive linguistic knowledge, the coher- ence and grammaticality of the adversary samples can be guaranteed. We instantiate our framework with TPGD, including the iterative perturbation process and the reconstruction process, to perform decision-based black-box attack. We conduct ex- haustive experiments to evaluate our framework and TPGD algorithm. Experimental results show the superiority of our method, especially in terms of attack success rate and adversarial samples quality. In the future, we will adapt other gradient-based methods in CV with our framework and explore to improve models’ robustness through adversarial training.",1
"Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models havefine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples.",0
"Abstract Predicting linearized Abstract Meaning Rep- resentation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Introduction The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the se- mantics of the sentence, with nodes representing concepts and edges representing their relations (Ba- narescu et al., 2013). Recent works utilizing pre- trained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These ap- proaches avoid explicit modeling of the graph struc- ture. Instead, they directly predict the linearized AMR graph treated as free text. While the use of pre-trained Transformer encoders is widely ex- tended in AMR parsing, the use of pre-trained Transformer decoders is recent and has shown to be very effective, maintaining current state-of-the-art results (Bevilacqua et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, pre- dicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapa- nipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, gen- erating the graph incrementally. They implicitly model graph structural constraints through transi- tions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequence- to-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to what degree they are complementary. To fully utilize the generation power of the pre-trained language models, we propose a transition system with a small set of basic actions – a generaliza- tion of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR gen- eration (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with linearized graphs, we modify the model structure to work with our transition system, and encode parser states in BART’s attention mechanism (Astudillo et al., 2020; Zhou et al., 2021). We also explore dif- ferent vocabulary strategies for action generation. These changes convert the pre-trained BART to a transition-based parser where graph constraints and alignments are internalized. We provide a detailed comparison with top- performing AMR parsers and perform ablation ex- periments showing that our proposed transition sys- tem and BART modifications are both necessary to achieve strong performance. Although BART has great language generation capacity, it still benefits from parser state encoding with hard attention, and can efficiently learn structural output. Our model establishes a new state of the art for AMR 2.0 while maintaining graph well-formedness guarantees and producing built-in alignments. Conclusion We explore the integration of pre-trained sequence- to-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the high performance of the former and structural advantages of the latter. We show that both approaches are complementary, establishing the new state of the art for AMR 2.0. Our results indicate that instead of simply converting the struc- tured data into unstructured sequences to fit the need of the pre-trained model, it is possible to ef- fectively re-purpose a generic pre-trained model to a structure-aware one achieving strong perfor- mance. Similar principles can be applied to adapt other powerful pre-trained models such as T5 (Raf- fel et al., 2019) and GPT-2 (Radford et al., 2019) for structured data predictions. It is worth exploring thoroughly the pros and cons of introducing struc- ture to the model compared to removing structure from the data (linearization) in various scenarios.",1
"Abstract Predicting linearized Abstract Meaning Rep- resentation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Introduction The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the se- mantics of the sentence, with nodes representing concepts and edges representing their relations (Ba- narescu et al., 2013). Recent works utilizing pre- trained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These ap- proaches avoid explicit modeling of the graph struc- ture. Instead, they directly predict the linearized AMR graph treated as free text. While the use of pre-trained Transformer encoders is widely ex- tended in AMR parsing, the use of pre-trained Transformer decoders is recent and has shown to be very effective, maintaining current state-of-the-art results (Bevilacqua et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, pre- dicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapa- nipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, gen- erating the graph incrementally. They implicitly model graph structural constraints through transi- tions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequence- to-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to what degree they are complementary. To fully utilize the generation power of the pre-trained language models, we propose a transition system with a small set of basic actions a generaliza- tion of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR gen- eration (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with linearized graphs, we modify the model structure to work with our transition system, and encode parser states in BART's attention mechanism (Astudillo et al., 2020; Zhou et al., 2021). We also explore dif- ferent vocabulary strategies for action generation. These changes convert the pre-trained BART to a transition-based parser where graph constraints and alignments are internalized. We provide a detailed comparison with top- performing AMR parsers and perform ablation ex- periments showing that our proposed transition sys- tem and BART modifications are both necessary to achieve strong performance. Although BART has great language generation capacity, it still benefits fromstate encoding with hard attention, and can efficiently learn structural output. Our model establishes a new state of the art for AMR 2.0 while maintaining graph well-formedness guarantees and producing built-in alignments. Conclusion We explore the integration of pre-trained sequence- to-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the de- sirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization. Our work establishes a new state of the art for AMR 2.0 while maintaining structure well-formedness guarantees and producing built-in alignments. Conclusion We explore the integration of pre-trained sequence- to-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the de- sirable properties of previous transition-based approaches.",0
"Abstract Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense prop- erties of everyday concepts to detailed factual knowledge about named entities. Among oth- ers, this makes it possible to distill high-quality word vectors from pre-trained language mod- els. However, it is currently unclear to what ex- tent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically gen- erated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and re- lation classification (supervised) benchmarks, even without any task-specific fine-tuning. Introduction One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word em- beddings is nonetheless important. For instance, understanding lexical relations is an important pre- requisite for understanding the meaning of com- pound nouns (Turney, 2012). Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema matching (Fernandez et al., 2018), completion and re- trieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Ar- guello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), le- gal reasoning (Ashley, 1988; Walton, 2010), on- tology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008). Given the recent success of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), we may wonder whether such mod- els are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relation- ship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above. Since it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that re- lation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as “The relation between Paris and France is <mask>"". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is repre- sented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judge- ments that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of a different nature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is en- coded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. Related Work Bommasani et al., 2020; Vulic et al., 2020), and var- ious forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in par- ticular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input “Dante was born in <mask>” and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. “the place where Dante is born” is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ̨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge. We address this con- cern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Unsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation be- tween two nouns as the dependency path connect- ing them. Their view is that two such depen- dency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which re- lations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embed- dings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts. The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on dis- tributional models have been explored that rely on similar intuitions but go beyond simple vec- tor operations of word embeddings.2 For instance, Jameel et al. (2018) introduced a variant of the GloVe word embedding model, in which relation vectors are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), rela- tion vectors are computed by averaging the embed- dings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facil- itate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Si- mon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation ex- traction. In contrast, our focus in this paper is on characterising the overall relationship between two words. Conclusion We have proposed a strategy for learning relation embeddings, i.e. vector representations of pairs of words which capture their relationship. The main idea is to fine-tune a pre-trained language model us- ing the relational similarity dataset from SemEval 2012 Task 2, which covers a broad range of seman- tic relations. In our experimental results, we found the resulting relation embeddings to be of high qual- ity, outperforming state-of-the-art methods on sev- eral analogy and relation classification benchmarks. Among the models tested, we obtained the best re- sults with RoBERTa, when using manually defined templates for encoding word pairs. Importantly, we found that high-quality relation embeddings can be obtained even for relations that are unlike those from the SemEval dataset, such as morphological and encyclopedic relations. This suggests that the knowledge captured by our relation embeddings is largely distilled from the pre-trained language model, rather than being acquired during training.",1
"Abstract Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense prop- erties of everyday concepts to detailed factual knowledge about named entities. Among oth- ers, this makes it possible to distill high-quality word vectors from pre-trained language mod- els. However, it is currently unclear to what ex- tent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically gen- erated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and re- lation classification (supervised) benchmarks, even without any task-specific fine-tuning. Introduction One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word em- beddings is nonetheless important. For instance, understanding lexical relations is an important pre- requisite for understanding the meaning of com- pound nouns (Turney, 2012). Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema matching (Fernandez et al., 2018), completion and re- trieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Ar- guello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), le- gal reasoning (Ashley, 1988; Walton, 2010), on- tology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008). Given the recent success of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), we may wonder whether such mod- els are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relation- ship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above. Since it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that re- lation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as The relation between Paris and France is <mask>"". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is repre- sented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judge- ments that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of anature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is en- coded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. Related Work Bommasani et al., 2020; Vulic et al., 2020), and var- ious forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in par- ticular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input Dante was born in <mask> and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. the place where Dante is born is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ¨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge. We address this con- cern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Unsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation be- tween two nouns as the dependency path connect- ing them. Their view is that two such depen- dency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which re- lations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embed- dings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts. The aforementioned worksrepresent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on dis- tributional models have been explored that rely on similar intuitions but go beyond simple vec- tor operations of word embeddings.2 For instance, Jameel et al. (2018) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input Dante was born in <mask> and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. the place where Dante is born is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ¨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge.",0
"Abstract Natural Language Processing (NLP) is a branch of artificial intelli- gence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level ac- curacy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words con- sideration. Introduction Artificial intelligence had shown a great progress in the recent years especially the deep learning branch where learning techniques have been improved very quickly. The combination of representation learning and deep learning have allowed the emerging of a new AI class called deep reinforcement learning. Deep Reinforcement learning tend to estimate value functions from exper- iments and simulations and using dynamic programming through Deep Re- inforcement learning is an efficient way to build reactive strategies acting on instantaneous control. An algorithm which approves its performance by experi- ence is an algorithm capable of avoiding his own mistakes through a combination of a strong memory fed by fresh helpful data and the ability to keep winning pre- dictions after a long-term performance (Barto, Bradtke, & Singh, 1995) (Mnih et al., 2015). Neural Network can be considered as a dynamic Reinforcement Learning scheme where the layers are putted in a parallel way to have a cascaded trans- mission of the treated signal (Fukushima & Miyake, 1982) (LeCun et al., 1989) and where a prior knowledge is important to predict the output state of new observations. Sequential modeling is a way to process data in natural language processing by maximizing awards after manipulating situation and producing resulting actions (Vithayathil Varghese & Mahmoud, 2020) (LeCun, Bengio, & Hinton, 2015). A sequential model representation is influenced by its data representation and how tensors are trained to produce an optimal control (Bengio, Courville, & Vincent, 2013) To improve the target learning task, transfer learning is used as a powerful technique to increase the value of the most probable cases inside a state matrix (Boutsioukis, Partalas, & Vlahavas, 2011). Transferring the knowledge helps us to reduce the amount of data consumed and rely on feature engineering to reduce the noise caused by annotation errors and other tag-set anomalies in a context of multi-agent system. Extracting ingredients automatically from a recipe text is an extremely use- ful activity especially when we want to analyze a massive data of text recipes. Rule-Based methods were implemented to extract information from unstruc- tured recipe data (Silva, Ribeiro, & Ferreira, 2019) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, Bagrow, & Barab ́asi, 2011) who introduced the notion of Flavor Network and tried to verify the Food Paring hypothesis introduced on the 90’s by Heston Blumenthal and Francois Benzi. Flavor network as described by Ahn is a graph where the nodes are the ingredients extracted from recipes and weights are shared flavors between nodes. Food paring hypothesis is an indicator calculated after forming the Flavor Network to show if in a country or in a geographical part of the word we have tasty recipes or the ingredients do not have similar molecules. Tiago Simas (Simas, Ficek, Diaz-Guilera, Obrador, & Rodriguez, 2017) introduced the notion of food bridging formed with semi-metric distances. A group of scientists in a recent publication (Van Erp et al., 2021) devel- oped a state of the art of the use of artificial intelligence and natural language processing in analyzing food recipes. In this article we can found collected ref- erences talking about the challenging part in collecting food and recipe data. For example, Ahnert (Ahnert, 2013) presented the emergence of computational gastronomy in food science and its effect on culinary practices. Aiello and al (Aiello, Schifanella, Quercia, & Del Prete, 2019) discovered what are the most important predictors in food responsible of three diseases in a population sit- uated in London. (Amato & Cozzolino, 2020) extracted ingredients from food text to alert readers from allergens presence in a recipe. I agree with (Van Erp et al., 2021) concerning how challenging to use IA in food domain and how it will resolve issues concerning the creation of a data driven analysis of nutrition. In our paper data extracted can be used in a phone application or a recommended system for people who want to take care of their health. All previously cited researches on cuisine recipes need information extrac- tion from text recipe to use it on graphical visualization and statistical analysis. Information extraction can be used manually by extracting ingredients indi- cated on recipes or automatically. The problem in automatic extraction is that information should be precise to have also precise analysis, for example some ingredients take only one word and others can take two or three words. Another problem on automatic information extraction is that some ingredients that take one word have in common some words with other ingredients that have more than one word which make automatic information extraction more difficult. I tried to develop a mathematical model dedicated to extract ingredients from text recipe written in Arabic language with precision higher than what tradi- tional methods could make. According to Cutting (Cutting, Kupiec, Pedersen, & Sibun, 1992), a Tagger must be robust that should deal with unknown words, efficient that can deal with large corpora, accurate that can tag with high accu- racy, tunable that can deal with different corpora and reusable that take small efforts to re-target a new corpus. There are three types of POS Tagger: Taggers based on stochastic models, Taggers based on rules and Taggers Based on neural networks. On this work we will use Taggers based on HMM models. The use of POS tags as external features to solve NER problems was experimented by Zhou (Zhou & Su, 2002) but it was discarded because it showed bad results but our methodology and experiments demonstrate that using POS tags as external features is not a bad idea. This could be explained by the difference between our tokens and Zhou’s tokens: tokens as defined by Zhou is a pair of word-feature and in our model, token is only a word from our corpus. Conclusion Our Ingredient Extractor algorithm showed great results. It is based on HMM methods. We realized it by training two layers: first we trained tokens by tagging POS tags and second, we trained tokens by extracting the ingredients. Our HMM model needed modifications in iteration step because we didn’t get a square transition probability matrix or a square lexical probability matrix after the training step. A detailed iterations of our method is illustrated in appendix A. We can ameliorate our model by calculating the probabilities in it differently not as simple as we deed. We can make our model more interesting by adding two layers, one for extracting quantities and the other for extracting unities.",1
"Abstract Natural Language Processing (NLP) is a branch of artificial intelli- gence that gives machines the ability to decode human languages. Part- of-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level ac- curacy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words con- sideration. Introduction Artificial intelligence had shown a great progress in the recent years especially the deep learning branch where learning techniques have been improved very quickly. The combination of representation learning and deep learning have allowed the emerging of a new AI class called deep reinforcement learning. Deep Reinforcement learning tend to estimate value functions from exper- iments and simulations and using dynamic programming through Deep Re- inforcement learning is an efficient way to build reactive strategies acting on instantaneous control. An algorithm which approves its performance by experi- ence is an algorithm capable of avoiding his own mistakes through a combination of a strong memory fed by fresh helpful data and the ability to keep winning pre- dictions after a long-term performance (Barto, Bradtke, & Singh, 1995) (Mnih et al., 2015). Neural Network can be considered as a dynamic Reinforcement Learning scheme where the layers are putted in a parallel way to have a cascaded trans- mission of the treated signal (Fukushima & Miyake, 1982) (LeCun et al., 1989) and where a prior knowledge is important to predict the output state of new observations. Sequential modeling is a way to process data in natural language processing by maximizing awards after manipulating situation and producing resulting actions (Vithayathil Varghese & Mahmoud, 2020) (LeCun, Bengio, & Hinton, 2015). A sequential model representation is influenced by its data representation and how tensors are trained to produce an optimal control (Bengio, Courville, & Vincent, 2013) To improve the target learning task, transfer learning is used as a powerful technique to increase the value of the most probable cases inside a state matrix (Boutsioukis, Partalas, & Vlahavas, 2011). Transferring the knowledge helps us to reduce the amount of data consumed and rely on feature engineering to reduce the noise caused by annotation errors and other tag-set anomalies in a context of multi-agent system. Extracting ingredients automatically from a recipe text is an extremely use- ful activity especially when we want to analyze a massive data of text recipes. Rule-Based methods were implemented to extract information from unstruc- tured recipe data (Silva, Ribeiro, & Ferreira, 2019) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, Bagrow, & Barabasi, 2011) who introduced the notion of Flavor Network and tried to verify the Food Paring hypothesis introduced on the 90% by Heston Blumenthal and Francois Benzi. Flavor network as described by Ahn is a graph where the nodes are the ingredients extracted from recipes and weights are shared flavors between nodes. Food paring hypothesis is an indicator calculated after forming the Flavor Network to show if in a country or in a geographical part of the word we have tasty recipes or the ingredients do not have similar molecules. Tiago Simas (Simas, Ficek, Diaz-Guilera, Obrador, & Rodriguez, 2017) introduced the notion of food bridging formed with semi-metric distances. A group of scientists in a recent publication (Van Erp et al., 2021)oped a state of the art of the use of artificial intelligence and natural language processing in analyzing food recipes. In this article we can found collected ref- erences talking about the challenging part in collecting food and recipe data. For example, Ahnert (Ahnert, 2013) presented the emergence of computational gastronomy in food science and its effect on culinary practices. Aiello and al (Aiello, Schifanella, Quercia, & Del Prete, 2019) discovered what are the most important predictors in food responsible of three diseases in a population sit- uated in London. (Amato & Cozzolino, 2020) extracted ingredients from food text to alert readers from allergens presence in a recipe. I agree with (Amato, Ribeiro, & Ferreira, 2021) concerning how challenging to use IA in food domain and how it will resolve issues concerning the creation of a data driven analysis of nutrition. An introduction to artificial intelligence and deep learning is an efficient way to build reactive strategies acting on instantaneous control. An algorithm which approves its performance by experi- ence is an algorithm capable of avoiding his own mistakes through a combination of a strong memory fed by fresh helpful data and the ability to keep winning pre- dictions after a long-term performance (Barto, Bradtke, & Singh, 1995) (Mnih et al., 2015). Neural Network can be considered as a dynamic Reinforcement Learning scheme where the layers are putted in a parallel way to have a cascaded trans- mission of the treated signal (Fukushima & Miyake, 1982) (LeCun et al., 1989) and where a prior knowledge is important to predict the output state of new observations. Sequential modeling is a way to process data in natural language processing by maximizing awards after manipulating situation and producing resulting actions (Vithayathil Varghese & Mahmoud, 2020) (LeCun, Bengio, & Hinton, 2015). A sequential model representation is influenced by its data representation and how tensors are trained to produce an optimal control (Bengio, Courville, & Vincent, 2013) To improve the target learning task, transfer learning is used as a powerful technique to increase the value of the most probable cases inside a state matrix (Boutsioukis, Partalas, & Vlahavas, 2011). Transferring the knowledge helps us to reduce the amount of data consumed and rely on feature engineering to reduce the noise caused by annotation errors and other tag-set anomalies in a context of multi-agent system. Extracting ingredients automatically from a recipe text is an extremely use- ful activity especially when we want to analyze a massive data of text recipes. Rule-Based methods were implemented to extract information from unstruc- tured recipe data (Silva, Ribeiro, & Ferreira, 2020) Ingredients is not the only useful information we want to extract; in this work we are going to use Hidden Markov Models especially Viterbi algorithm with some modification to make it receiving two unique features: POS-tags and tokens, to predict ingredient states. Previous works Many previous works were interested in analyzing cuisine recipes, for exam- ple Sina Sajadmanesh (Sajadmanesh et al., 2017) presented an analysis of the ingredients diversity around the word using an ingredient-based classifier to dif- ferentiate between recipes around the word based on its geographical identity. Sina Sajadmanesh (Sajadmanesh et al., 2017) studied the diversity of ingredi- ents in dishes with introduction of global diversity (the ability to have diversified ingredients between recipes) and local diversity (the ability to have diversified ingredients within a recipe). Other related work for culinary habits is Yong-Yeol Ahn paper (Ahn, Ahnert, Bagrow, & Barabasi, 2011) who introduced the notion of Flavor Network.",0
"ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to “universal” loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ́ 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates. A lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the “similarity” function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals. If this “similarity” function expresses the dot-product between embeddings, the InfoNCE loss is equivalent to the N-pair loss up to some constants. It was also shown that InfoNCE is equivalent to the Mutual Information Neural Estimator (MINE) up to a constant (Belghazi et al., 2018), whose minimization maximizes a lower bound on MI. Deep InfoMax (DIM) (Hjelm et al., 2019) improves MINE, and can be modified to incorporate some autoregression as InfoNCE. However, Tschannen et al. (2020) pointed out that the effectiveness of loss functions such as DIM and InfoNCE might be primarily connected not to deep metric learning but rather to MI. The idea gained a lot of popularity in Computer Vision with the advent of SimCLR (a Simple frame- work for Contrastive Learning of visual Representations), which introduced NT-Xent (normalized temperature-scaled cross-entropy loss) (Chen et al., 2020). It uses self-supervised learning, where augmentations of the same image are considered as positive examples and augmentations of dif- ferent images are used as negative examples. Thus, the task is as follows: for each example in a batch, find its paired positive augmentation. Here, the N-pairs loss is modified with a temperature parameter and with an L2 normalization of embeddings to the unit hypersphere. The loss was further extended for supervised learning as SupCon loss (Khosla et al., 2020), which aggregates all positive examples (from the same class) in the softmax numerator. Subsequently, these losses were introduced to the field of Natural Language Processing (NLP). Gunel et al. (2020) combined the SupCon loss with the cross-entropy loss and obtained state-of-the- art results for several downstream NLP tasks using RoBERTa. Giorgi et al. (2020) and Fang & Xie (2020) used NT-Xent to pre-train Transformers, considering spans sampled from the same document and sentences augmented with back-translation as positive examples. Luo et al. (2020) proposed to use NT-Xent in a self-supervised setting to learn noise-invariant sequence representations, where sentences augmented with masking were considered as positive examples. Finally, Gao et al. (2021) introduced the SimCLR loss to NLP under the name SimCSE (Simple Contrastive Learning of Sentence Embeddings), where sentences processed by a neural network with dropout served as augmentations of the original sentences. Here, we explore various ways to use a similar loss function for pairwise sentence scoring tasks. While the above-described loss functions have different names, they are all based on similar ideas. Below, we will use the name Batch-Softmax Contrastive (BSC) loss, which we believe reflects the main idea best. In our experiments below, we will use the “modern” variant of the loss: with tem- perature, normalization, and symmetrization components (described in more detail in Section 3.1). These components were not used for NLP in combination before. We further introduce a number of novel and important modifications in the definition of the loss and in the training procedure, which make it more efficient, and we show that using the resulting loss yields better task-specific sentence embeddings for pairwise sentence scoring tasks. CONCLUSION AND FUTURE WORK We explored the idea of using a batch-softmax contrastive loss for fine-tuning large-scale pre-trained transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduced and studied a number of variations in the calculation of the loss as well as in the overall training procedure. Our experimental results have shown sizable improvements on a number of datasets and pairwise sentence scoring tasks including ranking, classification, and regression. In future work, we want to explore new variations of the loss, and to gain better understanding of when to use which variation. We further plan experiments with a larger set of NLP tasks.",1
"ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to universal loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates.lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the similarity function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals. If this similarity function expresses the dot-product between embeddings of samples from one class and between samples from dif- ferent classes, the InfoNCE loss is equivalent to the N-pair loss up to some constants. It was also shown that InfoNCE is equivalent to the Mutual Information Neural Estimator (MINE) up to a constant (Belghazi et al., 2018), whose minimization maximizes a lower bound on MI. Deep InfoMax (DIM) (Hjelm et al., 2019) improves MINE, and can be modified to incorporate some autoregression as InfoNCE. However, Tschannen et al. (2020) pointed out that the effectiveness of loss functions such as DIM and InfoNCE might be primarily connected not to deep metric learning but rather to MI. The idea gained a lot of popularity in Computer Vision with the advent of SimCLR (a Simple frame- work for Contrastive Learning of visual Representations), which introduced NT-Xent (normalized temperature-scaled cross-entropy loss) (Chen et al., 2020). It uses self-supervised learning, where augmentations of the same image are considered as positive examples and augmentations of dif- ferent images are used as negative examples. Thus, the task is as follows: for each example in a batch, find its paired positive augmentation. Here, the N-pairs loss is modified with a temperature parameter and with an L2 normalization of embeddings to the unit hypersphere. The loss was further extended for supervised learning as SupCon loss (Khosla et al., 2020), which aggregates all positive examples (from the same class) in the softmax numerator. Subsequently, these losses were introduced to the field of Natural Language Processing (NLP). Gunel et al. (2020) combined the SupCon loss with the cross-entropy loss and obtained state-of-the- art results for several downstream NLP tasks using RoBERTa. Giorgi et al. (2020) and Fang & Xie (2020) used NT-Xent to pre-train Transformers, considering spans sampled from the same document and sentences augmented with back-translation as positive examples. Luo et al. (2020) proposed to use NT-Xent in a self-supervised setting to learn noise-invariant sequence representations, where sentences augmented with masking were considered as positive examples. Finally, Gao et al. (2021) introduced the SimCLR loss to NLP under the name SimCSE (Simple Contrastive Learning of Sentence Embeddings), where sentences processed by a neural network with dropout served as augmentations of the original sentences. Here, we explore various ways to use a similar loss function for pairwise sentence scoring tasks. While the above-described loss functions have different names, they are all based on similar ideas. Below, we will use the name Batch-Softmax Contrastive (BSC) loss, which we believe reflects the main idea best. In our experiments below, we will use the modern variant of the loss: with tem- perature, normalization, and symmetrization components (described in more detail in Section 3.1). These components were not used for NLP in combination before. We further introduce a number of novel and important modifications in the definition of the loss and in the training procedure, which make it more efficient, and we show that using the resulting loss yields better task-specific sentence embeddings for pairwise sentence scoring tasks. CONCLUSION AND FUTURE WORK We explored the idea of using a batch-softmax contrastive loss for fine-tuning large-scale pre-trained transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduced and studied a number of variations in the calculation of the loss as well as in the overall training procedure. Our experimental results have shown sizable improvements on a number of datasets and pairwise sentence scoring tasks including ranking, classification, and regression. In future work, we want to explore new variations of the loss, and to gainunderstanding of when to use which variation.",0
"Abstract For each goal-oriented dialog task of inter- est, large amounts of data need to be col- lected for end-to-end learning of a neural dia- log system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a related dialog task. Naively learning from related data fails to improve performance as the related data can be inconsistent with the target task. We de- scribe a meta-learning based method that selec- tively learns from the related dialog task data. Our approach leads to significant accuracy im- provements in an example dialog task. Introduction One key benefit of goal-oriented dialog systems that are trained end-to-end is that they only re- quire examples of dialog for training. Avoiding the modular structure of pipeline methods removes the human effort involved in creating intermediate annotations for data to train the modules. The end- to-end structure also enables automatic adaptation of the system, with different components of the model changing together. This flexibility is partic- ularly valuable when applying the system to a new domain. However, end-to-end systems currently require significantly more data, increasing the human effort in data collection. The most common method for training is Supervised Learning (SL) using a dataset of dialogs of human agents performing the task of interest (Bordes et al., 2017; Eric and Manning, 2017; Wen et al., 2017). To produce an effective model, the dataset needs to be large, high quality, and in the target domain. That means for each new dialog task of interest large amounts of new data has to be collected. The time and money involved in that collection process limits the potential appli- cation of these systems. We propose a way to reduce this cost by selec- tively learning from data from related dialog tasks: tasks that have parts/subtasks that are similar to the new task of interest. Specifically, we describe a method for learning which related task examples to learn from. Our approach uses meta-gradients to automatically meta-learn a scalar weight ∈ (0, 1) for each of the related task data points, such that learning from the weighted related task data points improves the performance of the dialog system on the new task of interest. These weights are dynami- cally adjusted over the course of training in order to learn most effectively. We still learn from data for the target task, but do not need as much to achieve the same results. To demonstrate this idea, we considered two ex- periments. First, we confirmed that the method can work in an ideal setting. We constructed a classifi- cation task where the related task data is actually from the same task, but with the incorrect label for 75% of examples, and there is an input feature that indicates whether the label is correct or not. Our approach is able to learn to ignore the misleading data, achieving close to the performance of a model trained only on the correct examples. Second, we evaluated the approach on a per- sonalized restaurant reservation task with limited training data. Here, the related task is also restau- rant reservation, but without personalization and with additional types of interactions. We compare our approach to several standard alternatives, in- cluding multi-task learning and using the related data for pre-training only. Our approach is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dia- log policy using Reinforcement Learning (RL) in a multi-domain setting which can then be trans- ferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learn- ing (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge to a new test task by learning how to do transfer on different training tasks. While each task only has a limited amount of dialog data, they need a lot of tasks during training. In contrast, we look at a setting where the task from which we want to transfer knowledge from and the task that we want to transfer knowledge to are the only tasks that we have access to at training time. Any learning about how to transfer knowledge has to happen from just these two tasks. None of the above methods are applicable to this setting. Learning a task, while simultaneously meta- learning certain aspects of the learning process has been done successfully in some SL and RL set- tings recently. Wu et al. (2018); Wichrowska et al. (2017) use meta-learning to adapt hyper parameters such as learning rate and and even learn entire op- timizers themselves during training for SL tasks such as image classification. Given a single task, Zheng et al. (2018) successfully meta-learn intrin- sic rewards that help the agent perform well on that task. Xu et al. (2018) use meta-gradients to learn RL training hyperparameters such as the discount factor and bootstrapping parameters. The meta- gradient technique used in our proposed method is closely related to Rajendran et al. (2020). They learn intrinsic rewards for an RL agent acting in given domain, such that learning with those intrin- sic rewards improves the performance of the agent in the task of interest in a different domain. While we use a meta-learning based method for learning the weights for the related task data points in this work, there are other techniques in the ma- chine learning literature, especially in the computer vision literature, that can potentially be used to learn the weights. A large section of these recent techniques are based on learning an adversarially trained discriminator for estimating the weights of related image classification task data points (Zhao et al., 2018; Cao et al., 2018; Sankaranarayanan et al., 2018; Wang et al., 2019). Jiang and Zhai (2007) use a combination of several domain adap- tation heuristics to assign weights and evaluate on NLP tasks. Moon and Carbonell (2017) cluster the related task data points and learn attention weights for the clusters. An interesting future direction would be to study which weighting methods are best suited for end-to-end learning of neural goal- oriented dialog systems using related tasks and under what conditions. Conclusion End-to-end learning of neural goal-oriented dialog systems requires large amounts of data for training. Collecting data is a costly and time consuming pro- cess. In this work we showed on an example dialog task we can utilise a related task’s data to improve the performance of a new task of interest with only a limited amount of data. Our proposed method uses meta-learning to automatically learn which of the related task data points to selectively learn from. An important future work is to evaluate/extend the proposed method to more challenging and complex dialog datasets/tasks. Data useful for a dialog task of interest (related data) could be present in different formats. The related data could include, for example, natural language instructions on how to perform the task of interest, or a description of how the new dialog task of interest is different from a related dialog task. An interesting future direction is to investi- gate methods to successfully utilise such related data.",1
"Abstract For each goal-oriented dialog task of inter- est, large amounts of data need to be col- lected for end-to-end learning of a neural dia- log system. Collecting that data is a costly and time-consuming process. Instead, we show that we can use only a small amount of data, supplemented with data from a related dialog task. Naively learning from related data fails to improve performance as the related data can be inconsistent with the target task. We de- scribe a meta-learning based method that selec- tively learns from the related dialog task data. Our approach leads to significant accuracy im- provements in an example dialog task. Introduction One key benefit of goal-oriented dialog systems that are trained end-to-end is that they only re- quire examples of dialog for training. Avoiding the modular structure of pipeline methods removes the human effort involved in creating intermediate annotations for data to train the modules. The end- to-end structure also enables automatic adaptation of the system, with different components of the model changing together. This flexibility is partic- ularly valuable when applying the system to a new domain. However, end-to-end systems currently require significantly more data, increasing the human effort in data collection. The most common method for training is Supervised Learning (SL) using a dataset of dialogs of human agents performing the task of interest (Bordes et al., 2017; Eric and Manning, 2017; Wen et al., 2017). To produce an effective model, the dataset needs to be large, high quality, and in the target domain. That means for each new dialog task of interest large amounts of new data has to be collected. The time and money involved in that collection process limits the potential appli- cation of these systems. We propose a way to reduce this cost by selec- tively learning from data from related dialog tasks: tasks that have parts/subtasks that are similar to the new task of interest. Specifically, we describe a method for learning which related task examples to learn from. Our approach uses meta-gradients to automatically meta-learn a scalar weight (0, 1) for each of the related task data points, such that learning from the weighted related task data points improves the performance of the dialog system on the new task of interest. These weights are dynami- cally adjusted over the course of training in order to learn most effectively. We still learn from data for the target task, but do not need as much to achieve the same results. To demonstrate this idea, we considered two ex- periments. First, we confirmed that the method can work in an ideal setting. We constructed a classifi- cation task where the related task data is actually from the same task, but with the incorrect label for 75% of examples, and there is an input feature that indicates whether the label is correct or not. Our approach is able to learn to ignore the misleading data, achieving close to the performance of a model trained only on the correct examples. Second, we evaluated the approach on a per- sonalized restaurant reservation task with limited training data. Here, the related task is also restau- rant reservation, but without personalization and with additional types of interactions. We compare our approach to several standard alternatives, in- cluding multi-task learning and using the related data for pre-training only. Our approach is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod-as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2016) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. Wen et al. (2020) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) use pre-trained language mod- els as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. The meta-learning method used in our proposed method is closely related to Rajendran et al. (2020). They learn from data automatically generated from different tasks and use that data for transfer purposes. They only learn from data that is actually useful for transfer purposes. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture. The proposed method uses meta-learning to automatically learn certain aspects of the transfer process using data from a different task. The method uses meta-learning to automatically learn a scalar weight (0, 1) for each of the weighted related task data points, such that learning from the weighted related task data points improves the performance of the dialog system on the new task of interest. An important future work is to compare and contrast our approach to several standard alternatives, in- cluding multi-task learning and using the related data for transfer purposes. The proposed method needs no additional research. The method is consis- tently the best, indicating its potential to effectively learn which parts of the related data to learn from and which to ignore. Successfully learning from available related task data can allow us to build end-to-end goal-oriented dialog systems for new tasks faster with reduced cost and human effort in data collection. Related Work The large cost of collecting data for every new dia- log task has been widely acknowledged, motivat- ing a range of efforts. One approach is to transfer knowledge from other data to cope with limited availability of training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded di- alogs, and also without any specific constraints on the structure of the end-to-end dialog system archi- tecture.",0
"Abstract We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner sim- ilar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environ- ment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments. Introduction In collaborative human-robot environments, embodied agents must be capable of integrating language commands into behavior. Natural language instruction can range from feedback on the subtlest of movements to abstract, high- level plans. While humans are generally capable of giving and receiving language feedback regarding all aspects of a task, methods for integrating language understanding into behavior differ based on the level of behavioral abstrac- tion the instruction refers to. Following Patel, Rodriguez- Sanchez, and Konidaris (2020), who argued that the struc- ture of language closely relates to the structure of an agent’s decision process, we focus on grounding adverbs—words used to describe the quality of a verb (i.e. a skill)—to di- rectly modify skill execution. We adopt the framework of hierarchical reinforcement learning (Barto and Mahadevan 2003), wherein an agent’s behavior is mainly generated by skills responsible for low- level motor control, and learning is primarily concerned with sequencing given skills to solve a task. Much existing re- search on integrating language understanding into hierarchi- cal agents attempts to map language to sequences of abstract skill executions (Andreas, Klein, and Levine 2017; Mei, Bansal, and Walter 2016; Oh et al. 2017). However, agents must also be able to use language to modify their underly- ing skill policies. Commands like “lift the pallet higher” and “crack the egg gently” clearly request adjustments to a spe- cific skill execution. Therefore, a key question is how natural language understanding can ground to changes in the lowest levels of behavior. The existence of adverbs that modify discrete verbs calls for agents with a discrete set of skills, with behavior that can be modified by a parameter vector describing how the skill can be executed (Da Silva, Konidaris, and Barto 2012; Masson, Ranchod, and Konidaris 2016). We propose a novel method for grounding adverbs to adjustments in skill parameters—called adverb-skill groundings—which, when integrated into policy search, lead to greater sample- efficiency than traditional policy search methods that typi- cally depend on explicit reward from the environment. We demonstrate the effectiveness of adverb-skill groundings for policy search in a toy ball-throwing domain and a domain involving a simulated 7-DoF robot arm. We compare the sample efficiency of our approach to PI2-CMA (Stulp and Sigaud 2012), a state-of-the-art local policy search method. Related Work Natural Language in Reinforcement Learning Most existing research that has used natural language in re- inforcement learning problems can be categorized as either language-conditional (in which agents must interact with language to solve problems) or language-assisted (in which language can be used to facilitate learning) (Luketina et al. 2019). Although our setting is language-conditional, since the agent is presumed only to have access to natural lan- guage feedback, there are related works in both categories. Some previous research has attempted to map language instructions to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modular sub-policies in the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2019) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed us- ing a separate model. Tellex et al. (2011) generate high-level plans from the semantic structure of language instructions. Gopalan et al. (2020) derive symbol sketches from demon- strated navigation trajectories which they ground language instructions to. While all of these works ground language to agent behavior, ours is the first to integrate language feed- back at the level of modifying the low-level behavior of motor skills. Semantic Representation While earlier semantic space representations were mainly concerned with encoding individual words or n-grams into vector space (Lund and Burgess 1996; Landauer and Du- mais 1997), there has been recent discussion regarding how to capture phrases and sentences with similar machinery. Mitchell and Lapata (2010) explore this problem, which hinges on linguistic structures as being compositional, i.e. that the meaning of a language fragment is a function of the meanings of its composite parts. Compositionality itself has been accounted for in older logic-based formalisms (Mon- tague 1974), but incorporating compositionality into modern semantic space representations is still an unsolved problem. Baroni and Zamparelli (2010) proposed a candidate so- lution that accounts for compositionality in semantic space models by representing nouns as vectors and adjectives as matrices, and the meaning of their combinations to be their tensor products. Krishnamurthy and Mitchell (2013) ex- pand on this idea by using Combinatory Categorial Gram- mar (CCG) (Steedman 1996) to prescribe tensors of various modes to syntactic categories, whose weights they learn via a training process that utilizes a corpus. While the primary focus of this research is not on semantic models, we firmly believe that core linguistic principles—such as the principle of compositionality— should be considered when designing systems for ground- ing language to behavior. Accordingly, we utilized the syn- tax/semantics formulation laid out by Steedman (1996) and the intuition behind more recent compositional distribu- tional semantics research (Baroni and Zamparelli 2010; Kr- ishnamurthy and Mitchell 2013) in our strategy for ground- ing adverbs. Adverbs by the CCG account are functions from verbs to verbs, and adverbs by our account are simi- larly functions from skills to skills. Conclusion We have presented a novel method for efficiently integrating granular natural language feedback into low-level behavior. The method relies on learning adverb-skill groundings— mappings of adverbs to adjustments in skill parameters— which can be learned once using few training examples and do not require the agent to interact directly with environ- ment reward. Using adverb-skill groundings, an agent can integrate adverb feedback into a policy search—in place of sample-based direct policy search methods—and achieve an order of magnitude increase in sample efficiency. This work can be extended in several directions. First, the ability of our model to ground adverbs is limited by the pa- rameterization of the skill. If no variation in skill parame- ters could result in the desired effect of an adverb, a new skill parameterization should be learned with more expres- sive power. Second, humans typically do not need to learn how to ground adverbs anew each time they learn a new skill, but our model does. Future work might consider the ability for agents to transfer adverb groundings to new skills. Another important question to address is how to learn lan- guage embeddings which can exclusively capture the mean- ings of language commands as they relate to specific tasks. Our embedding procedure was designed by a human expert with knowledge of the tasks and which adverbs most apply to it. Future work should look to relax this constraint, per- haps by defining a broad and exhaustive set of adverbs of motion which can be applied to any task.",1
"Abstract We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner sim- ilar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environ- ment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments. Introduction In collaborative human-robot environments, embodied agents must be capable of integrating language commands into behavior. Natural language instruction can range from feedback on the subtlest of movements to abstract, high- level plans. While humans are generally capable of giving and receiving language feedback regarding all aspects of a task, methods for integrating language understanding into behavior differ based on the level of behavioral abstrac- tion the instruction refers to. Following Patel, Rodriguez- Sanchez, and Konidaris (2020), who argued that the struc- ture of language closely relates to the structure of an agen's decision process, we focus on grounding adverbs words used to describe the quality of a verb (i.e. a skill) to di- rectly modify skill execution. We adopt the framework of hierarchical reinforcement learning (Barto and Mahadevan 2003), wherein an agent's behavior is mainly generated by skills responsible for low- level motor control, and learning is primarily concerned with sequencing given skills to solve a task. Much existing re- search on integrating language understanding into hierarchi- cal agents attempts to map language to sequences of abstract skill executions (Andreas, Klein, and Levine 2017; Mei, Bansal, and Walter 2016; Oh et al. 2017). However, agents must also be able to use language to modify their underly- ing skill policies. Commands like lift the pallet higher and crack the egg gently clearly request adjustments to a spe- cific skill execution. Therefore, a key question is how natural language understanding can ground to changes in the lowest levels of behavior. The existence of adverbs that modify discrete verbs calls for agents with a discrete set of skills, with behavior that can be modified by a parameter vector describing how the skill can be executed (Da Silva, Konidaris, and Barto 2012; Masson, Ranchod, and Konidaris 2016). We propose a novel method for grounding adverbs to adjustments in skill parameters called adverb-skill groundings which, when integrated into policy search, lead to greater sample- efficiency than traditional policy search methods that typi- cally depend on explicit reward from the environment. We demonstrate the effectiveness of adverb-skill groundings for policy search in a toy ball-throwing domain and a domain involving a simulated 7-DoF robot arm. We compare the sample efficiency of our approach to PI2-CMA (Stulp and Sigaud 2012), a state-of-the-art local policy search method. Related Work Natural Language in Reinforcement Learning Most existing research that has used natural language in re- inforcement learning problems can be categorized as either language-conditional (in which agents must interact with language to solve problems) or language-assisted (in which language can be used to facilitate learning) (Luketina et al. 2019). Although our setting is language-conditional, since the agent is presumed only to have access to natural lan- guage feedback, there are related works in both categories. Some previous research has attempted to map language instructions to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modularin the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2019) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed us- ing a separate model. Other research has mapped symbolic instructions directly to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modular sub-policies in the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2020) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed by a separate model. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to macro-actions. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to policy structure. Other research has mapped symbolic language instructions directly to structure? In contrast, other research has mapped language instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modular sub-policies in the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Shu, Xiong, and Socher (2017) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed by a separate model.",0
"ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67× and 1.15× faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowing these networks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these networks in resource-constrained environments like drones, mobile computers and IoT devices. As we introduce skip connections in the architecture to obtain a complex feature representation, the gradient flow during the backward pass improves. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network.Therefore, we expect BERMo architecture to be better candidate for compression and hence, we believe the proposed model could be ideal for resource-constrained settings. Contributions: The Contributions Of This Work Can Be Summarized As Follows: i  We propose BERMo, which generates complex feature maps using linear combination of features from different depths. We evaluate the proposed model on the probing task from SentEval dataset (Conneau et al. (2018)) and find our model performs 2.67% better than the baseline on the semantic tasks (Tense, Subjnum, Objnum, Somo, Coordinv) on average. ii  We observe our approach is stable when pruning with smaller datasets like SST-2 (Wang et al. (2018)), where BERT commonly diverges. iii  We show our model supports higher pruning rates when compressing and converges 1.67× and 1.15× faster than BERT on MNLI and QQP (Wang et al. (2018)), respectively. iv  Forlosspenaltybasedpruningmethodourapproachcanobtainbetterparameterefficiency, 1.35× for QQP, than BERT model for comparable performance. v  Our approach produces comparable results to the baseline for Knowledge Distillation with marginal improvements on SQuAD dataset. Outline: The rest of the paper is organised as follows: Section 2 de- scribes ELMo (Peters et al. (2018)), BERT (Devlin et al. (2019)) and Pruning methods. Section 3 elab- orates the proposed model. The experimental setup and the results are presented in Section 4. In Section 5 we summarize our work and discuss the future possibilities. Section 6 reports the related work and we conclude our paper with Broader Impact in Section 7. BROADER IMPACT Research presented in this work improves the baseline performance on semantic tasks. This work highlights the importance of adding skip connections to the network in improving the training con- vergence and stability. We believe this work would act as a stepping stone and motivate further research in this direction, reducing the training time for these models. These improved training speeds also make room for enlarging the dataset size generally correlated with improvements in generalization performance. Further, as this work deals with reducing the training time for prun- ing, a possible application would be online pruning on resource constrained setup. Moreover, from an environmental perspective reducing training time will reduce the carbon footprint of these large language models.",1
"ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67 and 1.15 faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowingnetworks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these networks in resource-constrained environments like drones, mobile computers and IoT devices. As we introduce skip connections in the network, we obtain a better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding.",0
"Abstract Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources. Introduction The cost of language model training increases exponentially. Among different models, Transformerbased language models [Vaswani et al., 2017, Devlin et al., 2019, Liu et al., 2019, Lewis et al., 2020] are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity (O(n) where n is the input sequence length). Consequently, Transformers cannot support long sequence processing and large batch size with limited resources. The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020, Kitaev et al., 2020] or low-rank approximation [Choromanski et al., 2020, Wang et al., 2020]. However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible. It is also unclear in theory how to set the hyper-parameters of those methods to attain a desired level of approximation accuracy. Another issue of Transformers is the training instability that small perturbations in parameter updates tend to be amplified, resulting in significant disturbances in the model output [Liu et al., 2020a]. Transformers on some NLP tasks have shown to be sensitive to hyper-parameters, learning schedulers, or even random seeds, which usually demands a time-costly grid search for the best configuration in real-world applications. It has also been observed in our experiments that a slight change in the learning rate may cause the failure of convergence for some models. We conjecture that the instability in Transformer training comes from the softmax structure, as the un-normalized attention score matrices before softmax tend to have extremely large condition numbers due to its fast singular value decay. To alleviate the instability issue, an extra factor of 1/ √p in the softmax kernel SM is suggested by Vaswani et al. [2017] to restrain the scale variation; Liu et al. [2020a] proposes a new scheme to control the magnitude of output change and stabilize the training in early stages. In practice, we also need to consider the lower numerical precision of GPU implementation in model training, which further deteriorates the stability. Kernel methods may be the answer to both challenges. As pointed out by Choromanski et al. [2020], the softmax structure is closely related to Gaussian kernels up to diagonal matrix multiplications, as the pairwise dot products naturally appear when expanding the squared `2 distance. We further notice some important connections between self-attention and Gaussian kernels. First, the un-normalized attention score matrix can be formed via basic matrix operations on an empirical Gaussian kernel matrix. Moreover, the form of Gaussian kernels has the natural interpretation of assigning “attention” to different tokens. Compared to the softmax function, Gaussian kernels automatically perform the normalization as softmax does (c.f. Section 4.1). These observations motivate us to replace the softmax structure with Gaussian kernels. As we demonstrated in this paper, the new attention model, Kernelized Attention, empirically stabilizes the model training while being comparable to self-attention in model accuracy. To further improve the efficiency, we propose Skyformer (Symmetrization of Kernelized attention for NYström method) to accelerate kernelized attention. Skyformer adapts the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005] to the non-PSD empirical Gaussian kernel matrix (as query matrices in general do not equal to key matrices), by instead lifting the kernelized attention score matrix into a large PSD matrix that contains the un-normalized attention score matrix as the off-diagonal block. We further conduct theoretical analysis by showing that Skyformer has a small matrix approximation error on kernelized attention in the spectral norm. Our experiments on the LRA benchmark show that Skyformer consistently uses less space and time while achieving better accuracy than other baseline methods. In summary, our main contributions are: (1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers. (2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the Nyström method to a non-PSD matrix. We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. (3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs.  Related Work Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads [Voita et al., 2019, Michel et al., 2019] and model size reduction with knowledge distillation [Jiao et al., 2020, Tang et al., 2019, Liu et al., 2020b], we focus on attention approximation models, which are closely related to kernel methods. To reduce the time and space complexity by avoiding exhaustive computation over the attention metric, recent studies propose to apply sparse attention patterns to limit the numbers of elements participating in matrix multiplications [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020]. Beyond limiting the attention to fixed patterns, some approaches learn the patterns by determining token assignments to relevant groups [Kitaev et al., 2020, Roy et al., 2021]. Those models utilize local and global information in the attention score matrix to perform approximation, which coincides with the attempt to accelerate the computation in Gaussian processes [Snelson and Ghahramani, 2007]. The attention score matrix is known to exhibit a very fast rate of singular value decay [Bhojanapalli et al., 2020, Dong et al., 2021], similar to that of an empirical kernel matrix [Yang et al., 2017]. This near singular property motivates many low-rank attention approximation methods to skillfully leverage the computation techniques in kernel methods. Among them, Linformer [Wang et al., 2020] compresses the size of the key and value matrix with random projections based on the Johnson–Lindenstrauss transform, a common randomized sketching method in Gaussian processes [Yang et al., 2017]; Reformer [Kitaev et al., 2020] applies locality-sensitive hashing (LSH) [Har-Peled et al., 2012] to simplify the computation of the attention score matrix, which is widely used in kernel density estimation [Charikar and Siminelakis, 2017, Backurs et al., 2019]; Performer [Choromanski et al., 2020] projects both query and key matrix through random Fourier features [Rahimi et al., 2007], heavily exploiting Bochner Theorem for stationary kernels. The most related papers to ours are linear attention [Katharopoulos et al., 2020], Synthesizer [Tay et al., 2020a], and Nyströmformer [Xiong et al., 2021]. Linear attention takes the softmax structure in self-attention as a measure of similarity and replaces it with the dot product of separately activated query and key matrices; Synthesizer aims to modify the original self-attention by replacing the dot product before softmax with Synthetic Attention, which generates the alignment matrix independent of token-token dependencies. Their attempts indicate that the softmax structure in self-attention is not the only feasible choice, and justify our usage of kernelized attention. Rather than remodeling self-attention, Nyströmformer applies the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005], a powerful and effective method for large-scale kernel machines acceleration, to approximate the attention score matrix. However, Nyströmformer applies the Nyström method to a non-PSD matrix, and thus fails to utilize the full potential of the Nyström method. This issue is resolved in our proposed Skyformer by instead lifting the kernelized attention score matrix into a large PSD matrix which contains the target non-PSD matrix as its off-diagonal block. For more details on attention approximation methods, we refer readers to a survey paper on efficient transformers [Tay et al., 2020c]. Conclusions and future work Motivated by the connection between kernel methods and self-attention, we introduce Kernelized Attention, which replaces the softmax structure in self-attention with a Gaussian kernel. We also propose Skyformer, which adapts the Nyström method to Kernelized Attention to improve its efficiency. We expect the new model can enjoy more stable training while inheriting the strong performance from self-attention. Extensive experiments verify our intuitions and show that both Kernelized Attention and its Nyström approximation variant have comparable accuracy to the original Transformer on the LRA benchmark. Direct development of this work is the incorporation of further computation tricks in kernel methods, such as the local and global approximation for gram matrix [Snelson and Ghahramani, 2007] and the importance sampling in Nyström methods [Musco and Musco, 2017, Chen and Yang, 2021b,a]. Other related questions include the choice of the kernel other than the Gaussian kernel in our kernelized attention model. It is expected that for different tasks there will be specific kernels more proper than the original self-attention. The results in this work also shed new light on the design of the attention mechanism, which may benefit board downstream NLP tasks.",1
"Abstract Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources. Introduction The cost of language model training increases exponentially. Among different models, Transformerbased language models [Vaswani et al., 2017, Devlin et al., 2019, Liu et al., 2019, Lewis et al., 2020] are shown to enjoy state-of-the-art (SOTA) performances on many Natural Language Processing (NLP) tasks despite their enormous training cost. One of the computation bottlenecks lies in the self-attention mechanism, which is known to be resource-intensive with quadratic time and space complexity (O(n) where n is the input sequence length). Consequently, Transformers cannot support long sequence processing and large batch size with limited resources. The challenge of improving computational efficiency of Transformers has motivated several recent studies on attention acceleration, using either sparse attention pattern [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020, Kitaev et al., 2020] or low-rank approximation [Choromanski et al., 2020, Wang et al., 2020]. However, there is usually a lack of theoretical analysis on the approximation error of these methods due to the complex softmax structure, which makes the theoretical comparison between the efficiency of each method infeasible. It is also unclear in theory how to set the hyper-parameters of those methods to attain a desired level of approximation accuracy. Another issue of Transformers is the training instability that small perturbations in parameter updates tend to be amplified, resulting in significant disturbances in the model output [Liu et al., 2020a]. Transformers on some NLP tasks have shown to be sensitive to hyper-parameters, learning schedulers, or even random seeds, which usually demands a time-costly grid search for the best configuration in real-world applications. It has also been observed in our experiments that a slight change in the learning rate may cause the failure of convergence for some models. We conjecture that the instability in Transformer training comes from the softmax structure, as the un-normalized attention score matrices before softmax tend to have extremely large condition numbers due to its fast singular value decay. To alleviate the instability issue, an extra factor of 1/p in the softmax kernel SM is suggested by Vaswani et al. [2017] to restrain the scale variation; Liu et al. [2020a] proposes a new scheme to control the magnitude of output change and stabilize the training in early stages. In practice, we also need to consider the lower numerical precision of GPU implementation in model training, which further deteriorates the stability. Kernel methods may be the answer to both challenges. As pointed out by Choromanski et al. [2020], the softmax structure is closely related to Gaussian kernels up to diagonal matrix multiplications, as the pairwise dot products naturally appear when expanding the squared `2 distance. We further notice some important connections between self-attention and Gaussian kernels. First, the un-normalized attention score matrix can be formed via basic matrix operations on an empirical Gaussian kernel matrix. Moreover, the form of Gaussian kernels has the natural interpretation of assigning attention to different tokens. Compared to the softmax function, Gaussian kernels automatically perform the normalization as softmax does (c.f. Section 4.1). These observations motivate us to replace the softmax structure with Gaussian kernels. As we demonstrated in this paper, the new attention model, Kernelized Attention, empirically stabilizes the model training while being comparable to self-attention in model accuracy. To further improve the efficiency, we propose Skyformer (Symmetrization ofattention for Nyström method) to accelerate kernelized attention. Skyformer adapts the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005] to the non-PSD empirical Gaussian kernel matrix (as query matrices in general do not equal to key matrices), by instead lifting the kernelized attention score matrix into a large PSD matrix that contains the un-normalized attention score matrix as the off-diagonal block. We further conduct theoretical analysis by showing that Skyformer has a small matrix approximation error on kernelized attention in the spectral norm. Our experiments on the LRA benchmark show that Skyformer consistently uses less space and time while achieving better accuracy than other baseline methods. In summary, our main contributions are: (1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers. (2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the Nyström method to a non-PSD matrix. We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. (3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs. Related Work Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads [Voita et al., 2019, Michel et al., 2019] and model size reduction with knowledge distillation [Jiao et al., 2020, Tang et al., 2019, Liu et al., 2020b], we focus on attention approximation models, which are closely related to kernel methods. To reduce the time and space complexity by avoiding exhaustive computation over the attention metric, recent studies propose to apply sparse attention patterns to limit the numbers of elements participating in matrix multiplications [Qiu et al., 2020, Child et al., 2019, Zaheer et al., 2020, Beltagy et al., 2020]. Beyond limiting the attention to fixed patterns, some approaches learn the patterns by determining token assignments to relevant groups [Kitaev et al., 2020, Roy et al., 2021]. Those models utilize local and global information in the attention score matrix to perform approximation, which coincides with the attempt to accelerate the computation in Gaussian processes [Snelson and Ghahramani, 2007]. The attention score matrix is known to exhibit a very fast rate of singular value decay [Bhojanapalli et al., 2020, Dong et al., 2021], similar to that of an empirical kernel matrix [Yang et al., 2017]. This near singular property motivates many low-rank attention approximation methods to skillfully leverage the computation techniques in kernel methods. Among them, Linformer [Wang et al., 2020] compresses the size of the key and value matrix with random projections based on the Johnson Lindenstrauss transform, a common randomized sketching method in Gaussian processes [Yang et al., 2017]; Reformer [Kitaev et al., 2020] applies locality-sensitive hashing (LSH) [Har-Peled et al., 2012] to simplify the computation of the attention score matrix, which is widely used in kernel density estimation [Charikar and Siminelakis, 2017, Backurs et al., 2019]; Performer [Choromanski et al., 2020] projects both query and key matrix through random Fourier features [Rahimi et al., 2007], heavily exploiting Bochner Theorem for stationary kernels. The most related papers to ours are linear attention [Katharopoulos et al., 2020], Synthesizer [Tay et al., 2020a], and Nyströmformer [Xiong et al., 2021]. Linear attention takes the softmax structure in self-attention as a measure of similarity and replaces it with the dot product of separately activated query and key matrices; Synthesizer aims to modify the original self-attention by replacing the dot product before softmax with Synthetic Attention, which generates the alignment matrix independent of token-token dependencies. Their attempts indicate that the softmax structure in self-attention is not the only feasible choice, and justify our usage of kernelized attention. Rather than remodeling self-attention, Nyströmformer applies the Nyström method [Williams and Seeger, 2001, Drineas et al., 2005], a powerful and effective method for large-scale kernel machines acceleration, to approximate the attention score matrix. However, Nyströmformer applies the Nyström method to a non-PSD matrix, and thus failsutilize the full potential of the Nyström method. This issue is resolved in our proposed Skyformer by instead lifting the kernelized attention score matrix into a large PSD matrix which contains the target non-PSD matrix as its off-diagonal block. For more details on attention approximation methods, we refer readers to a survey paper on attention approximation methods which demonstrates the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. Experiments on Long Range Arena benchmark show that the proposed Skyformer does not suffer from the known computation bottlenecks that weighting off the kernelized attention score matrix with random projections does. In summary, our main contributions are: (1) We revisit the intrinsic connection between self-attention and kernel methods, and explore a new kernel-based structure, kernelized attention, to stabilize the training of Transformers. (2) We propose Skyformer, which approximates the kernelized attention via low dimensional randomized sketches by adapting the Nyström method to a non-PSD matrix. We provide the theoretical guarantee that the matrix multiplication error is small in term of spectral norm. (3) Extensive experiments show that Skyformer achieves comparable performance to the original self-attention with fewer computational costs. Related Work Among all the transformer acceleration methods, including attention layer simplification by pruning redundant attention heads [Voita et al., 2019, Michel et al., 2019] and model size reduction with knowledge distillation [Jiao et al., 2020, Tang et al., 2019, Liu et al., 2020b], we focus on attention approximation models, which are closely related to kernel methods.",0
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Pérez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, accept it as given that certain legal clauses are fair and others unfair, for which they implement ML methods to draw a classification boundary. Testing their API (“CLAUDETTE” 2021) with the sentence ‘the boy will hit the girl’, for example, produces the result: ‘Claudette found no potentially unfair clause’. Which may be a reflection of out-of-domain knowledge limitations. Work done by (Schramowski et al. 2019) and (Jentzsch et al. 2019) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2019) hold implicit representations of moral values. They do so using vector comparisons based on a template of Do’s and Don’ts. Furthermore (Schramowski et al. 2019) replicates the moral choices found by (Jentzsch et al. 2019), then computes the variance explained by another LM, the Universal Sentence Encoder (USE) (Cer et al. 2018) with respect to Yes/No question templates on moral choices. Further work in (Izzidien 2021) replicates the finding that word embeddings contain implicit biases, and proposes to use them to make assessments of verbs. Building on these studies, we propose to harness these implicit social biases to act as a metric for an explainable assessment of sentences, specifically those related to fairness. However, in order to extract this bias, and instead of using a template of Do’s and Don’ts, we build on work in the social psychology literature on determining, which factors are able to explain acts of fairness made by humans, from which we extract a template that represents the principal perceptions, humans typically engage when making a fairness assessment. In doing so, we aim to approximate those perceptions, which we hypothesise will allow sentences to be classed according to which perception they are closer to, being fair or unfair. Although the paper does not set out to produce a fully validated and verified fairness measurement tool, it contributes to the development of one based on an approximation of the factors, humans engage when making such measurements. In effect, the metric may be considered a proxy for fairness perceptions. As such, we do not claim to be measuring a specific fairness type, e.g., distributional/outcome. However, as will be discussed, fairness evaluations engage a number of principal psychological factors. It is these factors that we attempt to approximate using a method of word embeddings. While the ML techniques used in this paper are well established, our approach to digitising the factors, and the theory behind their use in this domain is new. Not least as no such measure exists in the literature. The paper is organised as follows. The methods section is presented next, this incorporates a detailed study to determine the most explanatory psychological factors present in fairness assessments. The paper then details two approaches to digitise these psychological factors using word embeddings and ML. The results are then presented. A short discussion is followed by improvements, limitations, and the conclusion. This paper contributes originally with the following: 1) A new literature review to determine the principal factors that best explain pro-social acts, employing the dictator game (DG) to remove the confound of strategic intentions – i.e., where a pro-social act is engaged not out of fairness, but due to fear of punishment. 2) The use of the factors found in the above literature review to act as measures in multidimensional vector space. We implement an under-utilised method of vector linear algebra to define an ontological approximation of fairness perceptions. 3) The use of the above vectors as a measure of test sentences; are they closer to being: fair, or unfair, and to what degree?",1
"Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Pérez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, accept it as given that certain legal clauses are fair and others unfair,which they implement ML methods to draw a classification boundary. Testing their API (""CLAUDETTE"" 2021) with the sentence ""the boy will hit the girl"", for example, produces the result: ""Claudette found no potentially unfair clause"". Which may be a reflection of out-of-domain knowledge limitations. Work done by (Schramowski et al. 2021) and (Jentzsch et al. 2021) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2021) hold implicit representations of moral values. They do so using vector comparisons based on a template of Do's and Don'ts. Furthermore, they implement the projection of sentence embedding on to a subspace representation of fairness. These evaluations are made using the -PCA() method on sequence comparisons based on the observed sentence embedding, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said sentence projection, produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Pérez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair.",0
"Abstract Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.  Introduction Short texts such as tweets, news feeds and web search snippets appear daily in our life (Pang and Lee, 2005; Phan et al., 2008). To understand these short texts, short text classification (STC) is a fundamental task which can be found in many applications such as sentiment analysis (Chen et al., 2019), news classification (Yao et al., 2019) and query intent classification (Wang et al., 2017). STC is particularly hard in comparison to long text classification due to two key issues. The first key issue is that short texts only contain one or a few sentences whose overall length is small, which lack enough context information and strict syntactic structure to understand the meaning of texts (Tang et al., 2015; Wang et al., 2017). For example, it is hard to get the meaning of ""Birthday girl is an amusing ride"" without knowing ""Birthday girl"" is a 2001 movie. A harder case is to understand a web search snippet such as ""how much Tesla"", which usually does not contain word order nor function words (Phan et al., 2008). In addition, real STC tasks usually only have a limited number of labeled data compared to the abundant unlabeled short texts emerging everyday (Hu et al., 2019). Therefore, auxiliary knowledge is required to understand short texts, examples include concepts that can be found in common sense knowledge graphs (Wang et al., 2017; Chen et al., 2019), latent topics extracted from the short text dataset (Hu et al., 2019), and entities residing in knowledge graphs (Hu et al., 2019). However, simply enriching auxiliary knowledge cannot solve the shortage of labeled data, which is another key issue commonly faced by real STC tasks (Pang and Lee, 2005; Phan et al., 2008). Yet the popularly used deep models require large-scale labeled data to train well (Kim, 2014; Liu et al., 2016). Currently, graph neural networks (GNNs) designed for STC obtain the state-of-the-art performance (Hu et al., 2019; Ye et al., 2020). They both take the STC as the node classification problem on a graph with mixed nodes of different types: HGAT (Hu et al., 2019) builds a corpus-level graph modeling latent topics, entities and documents and STGCN (Ye et al., 2020) operates on a corpuslevel graph of latent topics, documents and words. In both works, each document is connected to its nodes of a different type such as entities and latent topics but not to other documents. However, they do not fully exploit interactions between nodes of the same type. They also fail to capture the similarities between short documents, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016). Besides, both works have large parameter sizes: HGAT (Hu et al., 2019) is a GNN with dual-level attention and STGCN (Ye et al., 2020) merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). To address the aforementioned problems, we propose a novel HIerarchical heterogeNEous graph representation learning method for STC called SHINE, which is able to fully exploit interactions between nodes of the same types and capture similarity between short texts. SHINE operates on a hierarchically organized heterogeneous corpuslevel graph, which consists of the following graphs at different levels: (i) word-level component graphs model interactions between words, partof-speech (POS) tags and entities which can be easily extracted and carry additional semantic and syntactic information to compensate for the lack of context information; and (ii) short document graph is dynamically learned and optimized to encode similarities between short documents which allows more effective label propagation among connected similar short documents. We conduct extensive experiments on a number of benchmark STC datasets including news, tweets, document titles and short reviews. Results show that the proposed SHINE consistently outperforms the state-of-theart with a much smaller parameter size. Related Works 2.1 Text Classification Text classification assigns predefined labels to documents of variable lengths which may consist of a single or multiple sentences (Li et al., 2020). Traditional methods adopt a two-step strategy: first extract human-designed features such as bagof-words (Blei et al., 2003) and term frequencyinverse document frequency (TF-IDF) (Aggarwal and Zhai, 2012) from documents, then learn classi- fiers such as support vector machine (SVM) (Cortes and Vapnik, 1995). Deep neural networks such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Liu et al., 2016) can directly obtain expressive representations from raw texts and conduct classification in an end-to-end manner. Recently, graph neural networks (GNNs) (Defferrard et al., 2016; Kipf and Welling, 2016) have obtained the state-of-the-art performance on text classification. They can be divided into two types. The first type of GNNs constructs document-level graphs where each document is modeled as a graph of word nodes, then formulates text classification as a whole graph classification problem (Defferrard et al., 2016). Examples are TLGNN (Huang et al., 2019), TextING (Zhang et al., 2020), HyperGAT (Ding et al., 2020), which establish word-word edges differently. In particular, some methods (Liu et al., 2019; Chen et al., 2020) propose to estimate the graph structure of the document-level graphs during learning. However, if only a few documents are labeled, these GNNs cannot work due to the lack of labeled graphs. As is known, GNNs such as graph convolutional network (GCN) (Kipf and Welling, 2016) can conduct semi-supervised learning to solve node classification task on a graph where only a small number of nodes are labeled (Kipf and Welling, 2016). Therefore, another type of GNNs instead operates on a heterogeneous corpus-level graph which takes both text and word as nodes, and classifies unlabeled texts by node classification. Examples include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021) and TG-Transformer (Zhang and Zhang, 2020) with different strategies to construct and handle heterogeneous nodes and edges. However, these methods cannot work well for short texts of limited length. 2.2 Short Text Classification (STC) Short text classification (STC) is particularly challenging (Aggarwal and Zhai, 2012; Li et al., 2020). Due to limited length, short texts lack context information and strict syntactic structure which are vital to text understanding (Wang et al., 2017). Therefore, methods tailored for STC strive to incorporate various auxiliary information to enrich short text representations. Popularly used examples are concepts existing in external knowledge bases such as Probase (Wang et al., 2017; Chen et al., 2019) and latent topics discovered in the corpus (Zeng et al., 2018). However, simply enriching semantic information cannot compensate for the shortage of labeled data, which is a common problem faced by real short texts such as queries and online reviews (Pang and Lee, 2005; Phan et al., 2008). Thus, GNN-based methods which perform node classification for semi-supervised STC are utilized. HGAT (Hu et al., 2019) applies a GNN with dual-level attention to forward messages on a corpus-level graph modeling topics, entities and documents jointly, where the entities are words linked to knowledge graphs. STGCN (Ye et al., 2020) operates on a corpus-level graph of topics, documents and words, and merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). Currently, the state-of-the-art method on STC is HGAT (Hu et al., 2019; Yang et al., 2021).  Conclusion In this paper, we propose SHINE, a novel hierarchical heterogeneous graph representation learning method for short text classification. It is particularly useful to compensate for the lack of context information and propagate the limited number of labels efficiently. Specially, SHINE can effectively learn from a hierarchical graph modeling different perspectives of the short text dataset: word-level component graphs are used to understand short texts from the semantic and syntactic perspectives, and the dynamically learned short document graph allows efficient and effective label propagation among similar short documents. Extensive experiments show that SHINE outperforms the others consistently. As for the future works, we plan to search graph structure (Zhao et al., 2021) and utilize automated machine learning (Yao et al., 2018) to improve learning performance.",1
"Abstract Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels. Introduction Short texts such as tweets, news feeds and web search snippets appear daily in our life (Pang and Lee, 2005; Phan et al., 2008). To understand these short texts, short text classification (STC) is a fundamental task which can be found in many applications such as sentiment analysis (Chen et al., 2019), news classification (Yao et al., 2019) and query intent classification (Wang et al., 2017). STC is particularly hard in comparison to long text classification due to two key issues. The first key issue is that short texts only contain one or a few sentences whose overall length is small, which lack enough context information and strict syntactic structure to understand the meaning of texts (Tang et al., 2015; Wang et al., 2017). For example, it is hard to get the meaning of ""Birthday girl is an amusing ride"" without knowing ""Birthday girl"" is a 2001 movie. A harder case is to understand a web search snippet such as ""how much Tesla"", which usually does not contain word order nor function words (Phan et al., 2008). In addition, real STC tasks usually only have a limited number of labeled data compared to the abundant unlabeled short texts emerging everyday (Hu et al., 2019). Therefore, auxiliary knowledge is required to understand short texts, examples include concepts that can be found in common sense knowledge graphs (Wang et al., 2017; Chen et al., 2019), latent topics extracted from the short text dataset (Hu et al., 2019), and entities residing in knowledge graphs (Hu et al., 2019). However, simply enriching auxiliary knowledge cannot solve the shortage of labeled data, which is another key issue commonly faced by real STC tasks (Pang and Lee, 2005; Phan et al., 2008). Yet the popularly used deep models require large-scale labeled data to train well (Kim, 2014; Liu et al., 2016). Currently, graph neural networks (GNNs) designed for STC obtain the state-of-the-art performance (Hu et al., 2019; Ye et al., 2020). They both take the STC as the node classification problem on a graph with mixed nodes of different types: HGAT (Hu et al., 2019) builds a corpus-level graph modeling latent topics, entities and documents and STGCN (Ye et al., 2020) operates on a corpuslevel graph of latent topics, documents and words. In both works, each document is connected to its nodes of a different type such as entities and latent topics but not to other documents. However, they do not fully exploit interactions between nodes of the same type. They also fail to capture the similarities between short documents, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016). Besides, both works have large parameter sizes: HGAT (Hu et al., 2019) is a GNN with dual-level attention and STGCN (Ye et al., 2020) merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). To address the aforementioned problems, we propose a novel HIerarchical heterogeNEous graph representation learning method for STC called SHINE, which is able to fully exploit interactions between nodes of the same types and capture similarity between short texts. SHINE operates on a hierarchically organized heterogeneous corpuslevel graph, which consists of the following graphs at different levels: (i) word-level component graphs model interactions between words, partof-speech (POS) tags and entities which can be easily extracted and carry additional semantic and syntactic information tofor the lack of context information; and (ii) short document graph is dynamically learned and optimized to encode similarities between short documents which allows more effective label propagation among connected similar short documents. We conduct extensive experiments on a number of benchmark STC datasets including news, tweets, document titles and short reviews. Results show that the proposed SHINE consistently outperforms the state-of-theart with a much smaller parameter size. Related Works 2.1 Text Classification Text classification assigns predefined labels to documents of variable lengths which may consist of a single or multiple sentences (Li et al., 2020). Traditional methods adopt a two-step strategy: first extract human-designed features such as bagof-words (Blei et al., 2003) and term frequencyinverse document frequency (TF-IDF) (Aggarwal and Zhai, 2012) from documents, then learn classi- fiers such as support vector machine (SVM) (Cortes and Vapnik, 1995). Deep neural networks such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Liu et al., 2016) can directly obtain expressive representations from raw texts and conduct classification in an end-to-end manner. Recently, graph neural networks (GNNs) (Defferrard et al., 2016; Kipf and Welling, 2016) have obtained the state-of-the-art performance on text classification. They can be divided into two types. The first type of GNNs constructs document-level graphs where each document is modeled as a graph of word nodes, then formulates text classification as a whole graph classification problem (Defferrard et al., 2016). Examples are TLGNN (Huang et al., 2019), TextING (Zhang et al., 2020), HyperGAT (Ding et al., 2020), which establish word-word edges differently. In particular, some methods (Liu et al., 2019; Chen et al., 2020) propose to estimate the graph structure of the document-level graphs during learning. However, if only a few documents are labeled, these GNNs cannot work due to the lack of labeled graphs. As is known, GNNs such as graph convolutional network (GCN) (Kipf and Welling, 2016) can conduct semi-supervised learning to solve node classification task on a graph where only a small number of nodes are labeled (Kipf and Welling, 2016). Therefore, another type of GNNs instead operates on a heterogeneous corpus-level graph which takes both text and word as nodes, and classifies unlabeled texts by node classification. Examples include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021) and TG-Transformer (Zhang and Zhang, 2020) with different strategies to construct and handle heterogeneous nodes and edges. However, these methods cannot work well for short texts of limited length. 2.2 Short Text Classification (STC) Short text classification (STC) is particularly challenging (Aggarwal and Zhai, 2012; Li et al., 2020). Due to limited length, short texts lack context information and strict syntactic structure which are vital to text understanding (Wang et al., 2017). Therefore, methods tailored for STC strive to incorporate various auxiliary information to enrich short text representations. Popularly used examples are concepts existing in external knowledge bases such as Probase (Wang et al., 2017; Chen et al., 2019) and latent topics discovered in the corpus (Zeng et al., 2018). However, simply enriching semantic information cannot compensate for the shortage of labeled data, which is a common problem faced by real short texts such as queries and online reviews (Pang and Lee, 2005; Phan et al., 2008). Thus, GNN-based methods which perform node classification for semi-supervised STC are utilized. HGAT (Hu et al., 2019) applies a GNN with dual-level attention to forward messages on a corpus-level graph modeling topics, entities and documents jointly, where the entities are words linked to knowledge graphs. STGCN (Ye et al., 2020) operates on a corpus-level graph of topics, documents and words, and merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). Currently, the state-of-the-art method on STC is HGATet al., 2019; Yang et al., 2021). Conclusion In this paper, we propose SHINE, a novel hierarchical heterogeneous graph representation learning method for short text classification. It is particularly useful to compensate for the lack of context information and propagate the limited number of labels efficiently. Specially, SHINE can effectively learn from a hierarchical graph modeling latent topics, documents and words. Extensive experiments show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels. As is known, SHINE consistently outperforms the state-of-theart with a much smaller parameter size. However, if we restrict ourselves to only a few documents, the effect of term frequencyinverse document frequency (TF-IDF) is hard to detect.",0
"Abstract In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy forcing. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy forcing auxiliary losses favor empathetic responses, as human evaluation results indicate, comparing with the current state-of-the-art. Introduction Since dialogue is regarded as a fundamental and complex element of human cognition (Jurafsky and Martin, 2000), the development of systems capable of understanding human language and communicating with humans can have a significant impact. However, human communication requires the acknowledgment and the exchange of conversational partner’s emotions, as emotions play an important role in developing a confidential relationship between the speaker and the listener. Open domain conversational agents have been widely studied in the past years and both retrievalbased and generation-based approaches (Wu et al., 2019; Cai et al., 2019; Weston et al., 2018) have been developed. However, prior research has shown that most of those conversational agents are unable to imitate dialogues between humans, as the produced responses are generic and short (Vinyals and Le, 2015; Li et al., 2016b). Several efforts have been made to make the conversationa more engaging by keeping track of the conversational context (Sordoni et al., 2015b,a; Serban et al., 2016, 2017) or by producing more diverse responses (Li et al., 2016a,c). Subsequently, a recent trend that was followed by various researchers (Li et al., 2016b; Zhang et al., 2018; Kulikov et al., 2019; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Mazaré et al., 2018; Dinan et al., 2020; Madotto et al., 2019; Hancock et al., 2019; Yavuz et al., 2019; Wolf et al., 2019) in order to make the responses more coherent and consistent through the dialogue, was to produce personalized responses by conditioning the generation on a persona profile. Apart from understanding what is being discussed, a conversational agent should also acknowledge the emotional state of the conversational partner, as it is a significant part of human communication. A lot of researchers have focused on detecting emotion (Fan et al., 2018b; Xu et al., 2018; Winata et al., 2017, 2019) and empathy in dialogue systems (Bertero et al., 2016; Chatterjee et al., 2019). Zhou et al., 2018 introduced a seq2seq (Sutskever et al., 2014) Emotional Chatting Machine in order to generate responses with high emotional context, using emotional embeddings and an internal and external memory mechanism. A GAN-based (Goodfellow et al., 2014) framework was also proposed by Wang and Wan, 2018 that controlled the sentiment of the generated response. Wu and Wu, 2019 also used a dual-decoder to similarly generate emotional responses, given the sentiment. Zhou and Wang, 2018 introduced a Twitter dataset which used the emojis of the Twitter posts as emotionlabels and they also proposed a seq2seq model to generate emotional responses. Lubis et al., 2018 introduced a new dataset and proposed a hierarchical seq2seq response generator for affect-sensitive dialogue generation. Rashkin et al., 2019 introduced the EmpatheticDialogues dataset and trained the baselines to generate empathetic responses and simultaneously predict the corresponding emotion of the dialogue context. Later, Lin et al., 2019 introduced the ""Mixture of Empathetic Listeners"" framework improving the initial baselines. Santhanam and Shaikh, 2019 finetuned the GPT2 (Radford et al., 2019) model to improve the results further, while Shin et al., 2019 used reinforcement learning for predicting the user’s sentiment look-ahead along side with response generation. Lin et al., 2019 improved the performance on EmpatheticDialogues by finetuning the GPT2 model with the use of multitask learning, while Majumder et al., 2020 followed a different approach introducing stochasticity into the emotion mixture and arguing that empathetic responses do not always mirror the emotion of the user. Significant improvements were also made by Roller et al., 2021 and Shuster et al., 2020 who used multi-task training on multiple dialog tasks, achieving state-of-the-art results. In this work, in order to enforce empathetic response generation we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we use three objectives: response language modeling, sentiment understanding and empathy forcing. The sentiment understanding objective is crucial for tracking and acknowledging the emotional state of the conversational partner, while the empathy forcing objective favors empathetic response generation by penalizing responses that have an opposite sentiment of that of the conversational partner. Our key contribution is the inclusion of the sentiment understanding and empathy forcing auxiliary losses to promote empathetic behavior. The proposed approach, EmpBot, 1 , is on par with state-of-the-art in terms of BLEU score. However, our model produces significantly more fluent and empathetic responses, as indicated by human evaluation results. Conclusions In this work we propose EmpBot, a T5-based chatbot, augmented with a novel finetuning procedure for generating empathetic dialogue responses. The proposed loss consists of three parts: an LM loss that produces valid textual responses, a sentiment classification loss that introduces emotional awareness to the model and an empathy forcing loss that ensures that the responses are emotionally relevant. We evaluate EmpBot using standard evaluation metrics, i.e. perplexity and BLEU score, achieving state-of-the-art results. Our human evaluation results indicate that EmpBot produces more fluent and empathetic responses, when compared with both the baseline and the state-of-the-art models. In the future we want to extend the proposed method for other architectures, and explore more empathy forcing losses using raw emotion values instead of sentiment polarities.",1
"Abstract In this paper, we introduce EmpBot: an endto-end empathetic chatbot. Empathetic conversational agents should not only understand what is being discussed, but also acknowledge the implied feelings of the conversation partner and respond appropriately. To this end, we propose a method based on a transformer pretrained language model (T5). Specifically, during finetuning we propose to use three objectives: response language modeling, sentiment understanding, and empathy in order to make the most of the implied feelings of the conversation partner. The first objective is crucial for generating relevant and coherent responses, while the next ones are significant for acknowledging the sentimental state of the conversational partner and for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. The inclusion of the sentiment understanding and empathy in order to enforce empathetic responses is crucial for generating relevant and coherent responses, while the inclusion of the sentiment understanding and empathy in order to promote empathetic behavior is significant for favoring empathetic responses. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metricshuman evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation. We evaluate our model on the EmpatheticDialogues dataset using both automated metrics and human evaluation.",0
"Abstract Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users’ access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems’ effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. Introduction Simultaneous speech translation (SimulST) is the task in which the translation of a source language speech has to be performed on partial, incremental input. This is a key feature to achieve low latency in scenarios like streaming conferences and lectures, where the text has to be displayed following as much as possible the pace of the speech SimulST is indeed a complex task in which the difficulties of performing speech recognition from partial inputs are exacerbated by the problem to project meaning across languages. Despite the increasing demand for such a system, the problem is still far from being solved. So far, research efforts mainly focused on the quality/latency trade-off, i.e. producing high quality outputs in the shortest possible time, balancing the need for a good translation with the necessity of a rapid text generation. Previous studies, however, disregard how the translation is displayed and, consequently, how it is actually perceived by the end users. After a concise survey of the state of the art in the field, in this paper we posit that, from the users’ experience standpoint, output visualization is at least as important as having a good translation in a short time. This raises the need for a broader, task-oriented and human-centered analysis of SimulST systems’ performance, also accounting for this third crucial factor. Background As in the case of offline speech translation, the adoption of cascade architectures (Stentiford and Steer, 1988; Waibel et al., 1991) was the first attempt made by the SimulST community to tackle the problem of generating text from partial, incremental input. Cascade systems (F ¨ugen, 2009; Fujita et al., 2013; Niehues et al., 2018; Xiong et al., 2019; Arivazhagan et al., 2020b) involve a pipeline of two components. First, a streaming automatic speech recognition (ASR) module transcribes the input speech into the corresponding text (Wang et al., 2020; Moritz et al., 2020). Then, a simultaneous text-to-text translation module translates the partial transcription into target language text (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). This approach suffers from error propagation, a well-known problem even in the offline scenario, where the transcription errors made by the ASR module are propagated to the MT module, which cannot recover from them as it does not have direct access to the audio. Another strong limitation of cascaded systems is the extra latency added by the two-step pipeline, since the MT module has to wait until the streaming ASR output is produced. To overcome these issues, the direct models initially proposed in B´erard et al. (2016; Weiss et al. (2017) represent a valid alternative that is gaining increasing traction (Bentivogli et al., 2021). Direct ST models are composed of an encoder, usually bidirectional, and a decoder. The encoder starts from the audio features extracted from the input signal and computes a hidden representation; the decoder transforms this representation into target language text. Direct modeling becomes crucial in the simultaneous scenario, as it reduces the overall system’s latency due to the absence of intermediate symbolic representation steps. Despite the data scarcity issue caused by the limited availability of speech-to-translation corpora, the adoption of direct architectures showed to be promising (Weiss et al., 2017; Ren et al., 2020; Zeng et al., 2021), driving recent efforts towards the development of increasingly powerful and efficient models.  Conclusions and Future directions SimulST systems have become increasingly popular in recent years and many efforts have been made to build robust and efficient models. Despite the difficulties introduced by the online framework, these models have rapidly improved, achieving comparable results to the offline systems. However, many research directions have not been explored enough (e.g., the adoption of dynamic or fixed segmentation, the offline or the online training). First among all, the visualization strategy that is adopted to display the output of the simultaneous systems is an important and largely under-analyzed aspect of the simultaneous experience. We posit that the presence of applicationoriented metrics (e.g., reading speed), which are strongly related to the visualization and, as an ultimate goal, to the user experience, is the factor that misses in the actual evaluation environment. Indeed, this paper points out that BLEU and Average Lagging are not the only metrics that matter to effectively evaluate a SimulST model, even if they are fundamental to judge a correct and real-timed translation. We hope that this will inspire the community to work on this critical aspect in the future.",1
"Abstract Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of multilingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users' access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems' effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. Introduction Simultaneous speech translation (SimulST) is the task in which the translation of a source language speech has to be performed on partial, incremental input. This is a key feature to achieve low latency in scenarios like streaming conferences and lectures, where the text has to be displayed following as much as possible the pace of the speech SimulST is indeed a complex task in which the difficulties of performing speech recognition from partial inputs are exacerbated by the problem to project meaning across languages. Despite the increasing demand for such a system, the problem is still far from being solved. So far, research efforts mainly focused on the quality/latency trade-off, i.e. producing high quality outputs in the shortest possible time, balancing the need for a good translation with the necessity of a rapid text generation. Previous studies, however, disregard how the translation is displayed and, consequently, how it is actually perceived by the end users. After a concise survey of the state of the art in the field, in this paper we posit that, from the users' experience standpoint, output visualization is at least as important as having a good translation in a short time. This raises the need for a broader, task-oriented and human-centered analysis of SimulST systems' performance, also accounting for this third crucial factor. Background As in the case of offline speech translation, the adoption of cascade architectures (Stentiford and Steer, 1988; Waibel et al., 1991) was the first attempt made by the SimulST community to tackle the problem of generating text from partial, incremental input. Cascade systems (F¨ugen, 2009; Fujita et al., 2013; Niehues et al., 2018; Xiong et al., 2019; Arivazhagan et al., 2020b) involve a pipeline of two components. First, a streaming automatic speech recognition (ASR) module transcribes the input speech into the corresponding text (Wang et al., 2020; Moritz et al., 2020). Then, a simultaneous text-to-text translation module translates the partial transcription into target language text (Gu et al., 2017; Dalvi et al., 2018; Ma et al., 2019; Arivazhagan et al., 2019). This approach suffers from error propagation, a well-known problem even in the offline scenario, where the transcription errors made by the ASR module are propagated to the MT module, which cannot recover from them as it does not have direct access to the audio. Another strong limitation of cascaded systems is the extra latency added by the two-step pipeline, since the MT module has to wait until the streaming ASR output is produced. To overcome these issues, the direct models initially proposed in B´erard et al. (2016; Weiss et al. (2017) represent a valid alternative that is gaining increasing traction (Bentivogli et al., 2021). Direct ST models are composed of an encoder, usually bidirectional, and a decoder. The encoder starts from the audio features extracted from the input signal and computes a hidden representation; the decoder transforms this representation into target language text. Direct modeling becomes crucial in the simultaneous scenario, as it reduces the overall system's latency due to the absence of intermediate symbolic representation steps. Despite the data scarcity issue caused by the limited availability of speech-to-translation corpora, the adoption of direct architectures showed to be promising (Weiss et al., 2017; Ren et al., 2020; Zeng et al., 2021), driving recent efforts towards the development of increasingly powerful and efficient models. Conclusions and Future directions SimulST systems have become increasingly popular in recent years andefforts have been made to build robust and efficient models. Despite the difficulties introduced by the online framework, these models have rapidly improved, achieving comparable results to the offline systems. However, many research directions have not been explored enough (e.g., the adoption of dynamic or fixed segmentation, the offline or the online strategy). To overcome these difficulties, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems' effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. We argue that SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing.",0
"ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformerbased language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong “out of the box” sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term Frequency–Inverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight Araci (2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: • Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. • Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci, 2019b) was fine-tuned on the Financial Phrasebank (Malo et al., 2014) and FiQA Task 1 sentiment scoring dataset,3 thereby achieving state-of-the-art results. Sentence-BERT In Reimers and Gurevych (2019), authors noted that the sentence embeddings obtained from vanilla BERT (the ones pre-trained with the NSP task) lack in quality. In fact, considerably simpler baselines are competitive with BERT in this regard (e.g., averaging word embeddings). Liu et al. (2019) emphasized that the NSP task was not as useful as thought, and authors suggested removing it from the BERT pre-training scheme. Consequently, Reimers and Gurevych (2019) propose the Sentence-BERT model. Starting from a pre-trained BERT checkpoint, they fine-tune it with supervision with a Siamese BERT network (meaning that they encode pairs of sentences with the same encoder), and predict the sentence entailment from the two sentence embeddings (Natural Language Inference (NLI) task). This approach results in more meaningful sentence representations. It is now well-known that pre-trained Transformers achieve state-of-the-art performance in NLP tasks (Araci, 2019a). In this article, unlike Araci (2019a), rather than starting from vanilla BERT, which is state-of-the-art for token-level embeddings but not for sentence-level tasks, we base our work on a model that has been fine-tuned for producing high-quality sentence embeddings. We believe this is a more sensible approach in the case of financial sentiment analysis. In contrast to Araci (2019a), we model financial sentiment as a continuous variable (from -1 to 1), instead of using discrete values.  Conclusion and Future Work We have demonstrated that FinEAS, a model based on BERT pre-trained on the general domain but fine-tuned for sentence-level tasks, is a sensible approach for financial sentiment classification. In conclusion, our model is simple to implement and outperforms several common baselines, including vanilla BERT and task-specific approaches. We make our code and model weights publicly available. In future work, we think it will be interesting to further explore Transformers in the financial domain, with an emphasis on models fine-tuned for sentence and/or document-level tasks.",1
"ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformer based language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong 'out of the box' sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term Frequency Inverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight(2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci et al., 2019) is a sentence-level model that has been shown to be as strong as the token-level embeddings obtained from vanilla BERT (Araci et al., 2019). In summary, BERT is a sentence-level model that is well-known for its use in the financial domain, but has not been able to produce high-quality sentence embeddings. Since financial models are useful for sentence-level analysis, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term Frequency Inverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020).",0
"ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation · Machine Translation ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) system’s translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE “is concerned about predicting the quality of a system’s output for a given input, without any information about the expected output” [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as ‘good for nothing’ [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in O’Connell [2001] which assert that round-trip translation relies heavily on a model accurately translating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentences back to their original language so that errors in one translation system are reduced by the collection of models used, however, in Zaanen and Zwarts [2006], they still concluded that round-trip translation was not a good way to measure machine translation quality. One common theme we saw throughout these research papers was that prior research mainly focused on the round-trip translation and did not focus too heavily on how they were evaluating the similarity between these sentences. Evaluation metrics like BLEU are limited as they only measure the lexical similarity and do not take the semantics of the sentences into consideration. Maintaining the semantic information of a sentence is something we should want to ensure that a machine translation system does when evaluating its translations. While Somers [2005] does conclude that the results given by his RTT experiment do not look promising, he also recognises in his conclusion that there is a big reliance on BLEU and F-Score throughout his paper and he says that he would like to replicate his experiments using human rating of intelligibility. During the course of our work, Moon et al. [2020] was published which very closely resembles our approach. They report a Pearson R correlation of 0.95 on the WMT19 metrics task evaluation set for English-German sentence pairs. We were unable to compare our approach on this data, as the ground truth direct assessment scores were not made publicly available. Conclusion In this paper, we have investigated the efficacy of round-trip translation as an estimate of translation quality and proposed a novel use of sentence embeddings to measure the similarity between source and round-trip translated sentences. The aim of our approach was to use the semantic information retained through round-trip translation as a proxy for translation quality. Experimentation shows increased performance of sentence embedding based similarity measures over traditional lexical metrics. We also highlight current pitfalls of the round-trip translation approach and suggest directions for future research on this topic.",1
"ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation· Machine Translation· Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) system's translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE is concerned about predicting the quality of a system's output for a given input, without any information about the expected output [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as good for nothing [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in O'Connell [2001] which assert that round-trip translation relies heavily on a modeltranslating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentence back to its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Transitions made during the first translation are measured in terms of the speed of a machine translation system's translation by translating the sentence back into the original language, without using a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE is concerned about predicting the quality of a system's output for a given input, without any information about the expected output [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word.",0
"Abstract Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the system’s language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a system’s shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives. System misunderstandings and low user engagement are factors of low quality that are relatively easy to identify, but more subtle factors such as boring responses, awkward topic switches, and individual preferences can also have a substantial effect. Furthermore, the practical value of any approach to estimate the quality of individual dialogue turns is highly sensitive to the cost of collecting relevant data. Chatbots, and the settings they are placed in, can differ drastically in both their topics of conversation and interaction styles. And while conversation-level quality labels can be obtained relatively quickly by asking users to provide a rating at the end of a conversation, collecting data with turn-level labels that adequately characterizes a new chatbot or chat setting is an expensive process. In this paper we present our dialogue analysis approach, which addresses these challenges by producing quality scores for each utterance in a given conversation dataset using only conversation-level quality ratings. Unlike other work that focuses on utterance-level quality prediction using labeled data, our approach involves training a neural model to learn explicit relationships between utterancelevel features and conversation quality without the need for costly utterance-level annotations. We evaluate this approach on two conversation datasets and show high agreement between our model and experts for identifying problematic interactions. By developing an empirical technique that models the relationship between specific interactions and overall conversation quality, our work has the potential to remove much of the human effort and guesswork involved in dialogue system development. Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation breakdowns at the turn level. Ghazarian et al. (2020)’s work is similar, predicting utterance-level user engagement. Ghazarian et al. (2020) and Choi et al. (2019)’s work is similar to ours, as they build models targeted towards utterance-level quality outcomes. However, unlike our approach, these works are reliant on costly turn-level annotations: given conversations annotated for quality on the utterance level, their approach is to train a model that can predict utterance quality on unseen conversations within a similar conversation setting. This strategy incurs a substantial cost whenever the training data needs to be updated to fit a novel conversational setting or chatbot. To avoid the cost of collecting turn-level labels, our approach is more in line with techniques such as multiple regression analysis, where fitting a model to a dataset is used to explain the relationship between features and some outcome, rather than to predict an outcome for unseen examples. In our case, our model can be fit to any dataset of conversations with conversation-level quality labels in order to estimate the quality impact of each utterance on the overall conversation quality. This approach has a couple advantages over existing work. First, collecting utterance-level annotations in a supervised setting is not necessary for our approach as it was for Choi et al. (2019) and Ghazarian et al. (2020). Second, our model learns empirically-derived relationships between the utterance level and conversation quality, rather than learning to mimic human judgements of utterance level quality irrespective of conversation-level impact. Given the unreliability of human judgements on conversation quality noted in prior work (Liang et al., 2020), it is possible that removing human estimations of the relationship between turn-level and conversation-level quality eliminates a source of bias in the model’s training objective. To our knowledge, no previous work has presented an approach for utterance-level quality estimation that does not require data annotated on the turn level. Conclusion Our approach and evaluation results demonstrate the feasibility of an automatic approach for dialogue quality analysis. By training a model to learn the relationship between utterance-level features and conversation-level outcomes, it is possible to obtain an empirically-derived ranking of interactions based on whether they relate to positive or negative conversation quality. This work has obvious applications in dialogue system development, mitigating the high developer effort involved in manual analysis and human bias in determining the relative importance of system issues.",1
"Abstract Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the system's language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives. System misunderstandings and low user engagement are factors of low quality that are relatively easy to identify, but more subtle factors such as boring responses, awkward topic switches, and individual preferences can also have a substantial effect. Furthermore, the practical value of any approach to estimate the quality of individual dialogue turns is highly sensitive to the cost of collecting relevant data. Chatbots, and the settings they are placed in, can differ drastically in both their topics of conversation and interaction styles. And while conversation-level quality labels can be obtained relatively quickly by asking users to provide a rating at the end of a conversation, collecting data with turn-level labels that adequately characterizes a new chatbot or chat setting is an expensive process. In this paper we present our dialogue analysis approach, which addresses these challenges by producing quality scores for each utterance in a given conversation dataset using only conversation-level quality ratings. Unlike other work that focuses on utterance-level quality prediction using labeled data, our approach involves training a neural model to learn explicit relationships between utterancelevel features and conversation quality without the need for costly utterance-level annotations. We evaluate this approach on two conversation datasets and show high agreement between our model and experts for identifying problematic interactions. By developing an empirical technique that models the relationship between specific interactions and overall conversation quality, our work has the potential to remove much of the human effort and guesswork involved in dialogue system development. Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation breakdowns at the turn level. Ghazarian et al. (2020)Ã¢ÂÂs work is similar, predicting utterance-level user engagement. Ghazarian et al. (2020) and Choi et al. (2019)Ã¢ÂÂs work is similar to ours, as they build models targeted towards utterance-level quality outcomes. However, unlike our approach, these works are reliant on costly turn-level annotations: given conversations annotated for quality on the utterance level, their approach is to train a model that can predict utterance quality on unseen conversations within a similar conversation setting. This strategy incurs a substantial cost whenever the training data needs to be updated to fit a novel conversational setting or chatbot. To avoid the cost of collecting turn-level labels, our approach is more in line with techniques such as multiple regression analysis, where fitting a model to a dataset is used to explain the relationship between features and some outcome, rather than to predict an outcome for unseen examples. In our case, our model can be fit to any dataset of conversations with conversation-level quality labels in order to estimate the quality impact of eachon the overall conversation quality. This approach has a couple advantages over existing work. First, collecting utterance-level annotations in a supervised setting is not necessary for our approach as it was for Choi et al. (2019) and Ghazarian et al. (2020). Second, our model learns empirically-derived relationships between the utterance-level quality prediction and the overall dialogue quality prediction, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the systemÃ¢ÂÂs language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives.",0
"Abstract Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.  Introduction Unsupervised domain adaption (UDA) is an essential task in the realm of deep learning since it mitigates the expensive burden of manual annotation by focusing on cheap unlabeled data from target domains [Ramponi and Plank, 2020]. Among all existing approaches for UDA, pre-trained language model (PrLM) based approaches become the de-facto standard [Gururangan et al., 2020, Ben-David et al., 2020, Yu et al., 2021, Karouzos et al., 2021] since these PrLMs are equipped with generic knowledge learned from large corpora [Howard and Ruder, 2018] and lead to promising results. The primary focuses of UDA methods are to capture the transferable features for the target domain while reserving the knowledge learned from the source domain [Blitzer et al., 2006, Pan et al., 2010]. However, most existing pre-training-based UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . Our contributions can be summarized as: 1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters are introduced in a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. 2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. Related Work Unsupervised Domain Adaption: Existing UDA approaches can be generally classified into two categories: 1) The model-based methods target at augmenting the feature spaces [Glorot et al., 2011, Chen et al., 2012, Ziser and Reichart, 2019, Ben-David et al., 2020], designing new losses [Ganin and Lempitsky, 2015, Ganin et al., 2016] or refining model structures [Bousmalis et al., 2016]; 2) The data-based methods aim to utilize pseudo-labels [Ruder and Plank, 2018, Lim et al., 2020] and develop better data selection schemes [Han and Eisenstein, 2019, Ma et al., 2019]. Some works also try to tackle UDA tasks utilizing large PrLM [Li et al., 2019, Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are becoming the de-facto standard for various NLP tasks. Although promising results are reported, fine-tuning the whole model on a small amount of domain-specific data may distort underlying PrLM and lead to sub-optimal performances. Adapters: In NLP studies, adapter modules are primarily used for parameter-efficient fine-tuning of large PrLMs [Lauscher et al., 2020, Wang et al., 2020, Lin et al., 2021, Poth et al., 2021, Han et al., 2021, Mahabadi et al., 2021]. The most similar works comparing to our study are the models for zero-shot cross-lingual transfer tasks [Pfeiffer et al., 2020, Vidoni et al., 2020]. However, these models aim to separate language-specific knowledge using adapters, while our UDA task tries to capture common and transferable features across different domains. Conclusion This paper presents an adapter-based fine-tuning approach for unsupervised domain adaptation. Trainable adapter modules are inserted in a pre-trained LM, and a two-step training process is introduced to learn the parameters of these adapters. We demonstrate our method on two datasets with different sizes and domain similarities. The results show that the introduced adapter modules facilitate the adaption process, and our two-step training approach helps to further improve the UDA performance. As future works, we will explore more effective architectures of the adapter module.",1
"Abstract Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities. Introduction Unsupervised domain adaption (UDA) is an essential task in the realm of deep learning since it mitigates the expensive burden of manual annotation by focusing on cheap unlabeled data from target domains [Ramponi and Plank, 2020]. Among all existing approaches for UDA, pre-trained language model (PrLM) based approaches become the de-facto standard [Gururangan et al., 2020, Ben-David et al., 2020, Yu et al., 2021, Karouzos et al., 2021] since these PrLMs are equipped with generic knowledge learned from large corpora [Howard and Ruder, 2018] and lead to promising results. The primary focuses of UDA methods are to capture the transferable features for the target domain while reserving the knowledge learned from the source domain [Blitzer et al., 2006, Pan et al., 2010]. However, most existing pre-training-based UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . Our contributions can be summarized as: 1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters arein a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. 2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. Related Work Unsupervised Domain Adaption: Existing UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes.",0
"Abstract Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding – a critical component of NLP applications – by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement. Introduction Large language models (with more than 1 billion parameters) perform well on a range of natural language processing (NLP) tasks in zero- and few-shot settings, without requiring task-specific supervision (e.g., Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). This observation suggests that given enough capacity, language models may extract the knowledge required to perform well on these NLP tasks from raw text, simply using the transformer architecture (Vaswani et al., 2017) and auto-regressive language modeling objective. Consequently, various recent efforts have focused on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark – questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. We also observe a correlation between zero-shot performance and the similarity of a benchmark to the pre-training data (measured as the perplexity of the benchmark under a pre-trained model). Our results show that the zero-shot performance of pretrained language models on commonsense benchmarks is mostly attributed to the dataset bias in the benchmarks; it also highlights the need of reporting strong baselines in our evaluations which is missing from some of the recent work (Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). Next we investigate to what extent hyperparameters (such as the prompt format) and increasing model size impact the zero-shot performance. In all commonsense benchmarks, we see a gap between the zero-shot performance of the worst and best hyper-parameter settings (ranging between 2 to 19 accuracy points). This result shows that for commonsense understanding tasks, it is best not to take pre-trained language models as off-the-shelf tools. When increasing model size, across benchmarks, we observe improvements on both zero-shot performance and the Answer-only baseline, suggesting that larger models are better in exploiting the surface cues. Finally, we examine if pre-trained language models benefit from adding more examples in a fewshot setting or leveraging knowledge extracted from existing commonsense knowledge bases. We do not observe a substantial gain from the few-shot evaluation compared to the zero-shot one. Adding commonsense knowledge also does not yield notable improvements, showing that our language models cannot leverage the relevant knowledge when it is simply added to the prompt. It is an exciting time for language research in both academia and industry, with various efforts working on pre-training stronger language models. However, to better understand the goodness of these models, we need to compare them with strong baselines. Moreover, we need to compare different models under similar hyper-parameter settings given that the choice of parameters can substantially impact model performance even in a zeroshot evaluation setting. We hope our work encourages the community to consider stronger evaluation protocols for pre-trained language models. Conclusion We examine the extent to which large language models exhibit commonsense understanding by evaluating them against a range of commonsense benchmarks in a zero-shot way. At first sight, these models show impressive zero-shot performance suggesting that they capture commonsense knowledge. However, a closer inspection reveals that the good performance of these models is due to existing dataset bias in our benchmarks: the zero-shot performance is closer to that of a strong baseline that does not use questions at all than to the state-ofthe-art performance. We also observe that as model size increases, the improvements in the zero-shot performance plateaus, indicating building larger models is not enough for achieving human-level commonsense understanding. In addition, the fewshot evaluation does not show notable improvements over the zero-shot setting except for Social IQa. Our results suggest that the language modeling objective and larger model capacity are not enough to extract commonsense knowledge from text. Future work needs to explore alternative modeling paradigms to better capture commonsense knowledge from text. Moreover, given the implicit nature of commonsense knowledge, the text modality alone might not be enough for improving the commonsense understanding capacity of NLP systems; an interesting future direction is leveraging other modalities such as images or videos to improve this capacity.",1
"Abstract Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding a critical component of NLP applications  by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement. Introduction Large language models (with more than 1 billion parameters) perform well on a range of natural language processing (NLP) tasks in zero- and few-shot settings, without requiring task-specific supervision (e.g., Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). This observation suggests that given enough capacity, language models may extract the knowledge required to perform well on these NLP tasks from raw text, simply using the transformer architecture (Vaswani et al., 2017) and auto-regressive language modeling objective. Consequently, various recent efforts have focused on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. We also observe a correlation between zero-shot performance and the similarity of a benchmark to the pre-training data (measured as the perplexity of the benchmark under a pre-trained model). Our results show that the zero-shot performance of pretrained language models on commonsense benchmarks is mostly attributed to the dataset bias in the benchmarks; it also highlights the need of reporting strong baselines in our evaluations which is missing from some of the recent work (Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). Next we investigate to what extent hyperparameters (such as the prompt format) and increasing model size impact the zero-shot performance. In all commonsense benchmarks, we see a gap between the zero-shot performance of the worst and best hyper-parameter settings (ranging between 2 to 19 accuracy points). This result shows that for commonsense understanding tasks, it is best not to take pre-trained language models as off-the-shelf tools. When increasing model size, across benchmarks, we observe improvements on both zero-shot performance and the Answer-only baseline, suggesting that larger models are better in exploiting the surface cues. Finally, we examine if pre-trained language models benefit from adding more examples in a fewshot setting or leveraging knowledge extracted from existing commonsense knowledge bases. We do not observe a substantialfrom the few-shot evaluation compared to the zero-shot one. Adding commonsense knowledge also does not yield notable improvements, showing that our language models cannot leverage the relevant knowledge when it is simply added to the prompt. It is an exciting time for language research in both academia and industry, with various recent efforts working on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline.",0
"Abstract We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative-inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. These differences are due to the different semantic roles of the subject of unergative verbs, which shares its agentivity with subjects in transitive frames, and that of unaccusative verbs, more similar to the stereotypically patient- or theme-like objects in transitive frames. Moreover, these categories relate to the causative-inchoative alternation (Haspelmath, 1993) in which unaccusative verbs can express a same event with either agent and patient (1-a) or with a patient only (1-b), corresponding respectively to a causative and inchoative interpretation of the event: (1) a. Hannah popped the balloon. b. The balloon popped. The main property of the causative-inchoative alternation is the ability of the patient to be promoted from object to subject, and most theoretical accounts focus on this transitive to intransitive frame change. Once the event is in its intransitive form, however, it is syntactically indistinguishable from any other intransitive verb construction, and in order to categorize the verb one must know the specific semantic roles of its arguments. This phenomenon is particularly hard to mine because the different realisations are not disambiguated by the context but by the arguments of the verb, and the frequency of the constructions themselves is not an indication of their acceptability. Furthermore, the meaning of the sentence remains virtually the same. 1 However, picking up this categorisation is important not only for reasons involving the appropriateness of different types of subjects for each verb, but also because it has been shown to influence coreference patterns (Loáiciga et al., 2018). Nevertheless, efforts to discover verbs participating in the alternation using automatic methods are very limited. Here we focus on the unaccusative vs unergative distinction as it allows us to disambiguate the verbs based on their intransitive frames. While the subjects of unaccusatives are patients (1-b), subjects of unergatives are agents (2-a). The assumption is that alternating verbs belong to the unaccusative category and we can discover them by separating them from the unergative category. 2 (2) a. Hannah slept The key in using these categories is that, without knowing the semantic roles of the verb, we can measure how well a noun fits the subjective position using a large corpus. Our method relies on language modeling to do just this. We investigate the effect of different language models on the task of identifying unaccusatives vs unergatives by testing the intransitive frames of a large quantity of verbs. Related Work Levin’s (1993) seminal work on verb alternations remains the most comprehensive collection of alternating verbs for English. Other collections, for instance Framenet (Baker et al., 1998) also specify if a verb allows the alternation based on Levin. Typological work for other languages exists in the linguistics literature (Haspelmath, 1993; Schäfer, 2009), but large collections are practically nonexistent.3 Building on Haspelmath’s theory, Samardžic´ (2014) estimates a Spontaneity score based on the ratio of a verb’s transitive to intransitive occurrence. In this scale, verbs are ranked according to the degree to which they are non-agentive. In other words, verbs without an explicit agent causing the event are more likely to participate in the causative alternation. Samardžic and Merlo (2018) ´ report between 61% and 85% agreement between their model and theoretical classifications. Kann et al. (2019) rely on Levin’s work to create synthetic data and build classifiers able to discriminate between several alternations. Their data sets are built using proper names as subjects and common nouns as objects, and they focus on the transitive to intransitive construction case. In this paper we rely on intransitive constructions exclusively, as will be explained below in Section 3. In addition, we use their FAVA data set for evaluation in Section 4. Our method is most similar to the RNN-based method reported by Seyffarth (2019). Contrary to our work which queries language models with in- flected sentences, Seyffarth uses an RNN to score artificially created transitive and intransitive argument sequences of the type invite-Pat-Kim vs invite-Pat. Using Framenet as gold standard, they report an accuracy of 66% on all verbs. Conclusions We proposed a method to detect unaccusative vs unergative verbs based on the generation of intransitive sentence frames of candidate verbs. The results with a large language model show moderate success, highlighting that the causative-inchoative alternation is a challenging meaning distinction to detect automatically. Since the method relies primarily on parsed data and language models, it has the potential to be extended to languages where verbal annotated resources are scarce.",1
"Abstract We present an unsupervised method to detect English unergative and unaccusative verbs. These categories allow us to identify verbs participating in the causative-inchoative alternation without knowing the semantic roles of the verb. The method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. These differences are due to the different semantic roles of the subject of unergative verbs, which shares its agentivity with subjects in transitive frames, and that of unaccusative verbs, more similar to the stereotypically patient- or theme-like objects in transitive frames. Moreover, these categories relate to the causative-inchoative alternation (Haspelmath, 1993) in which unaccusative verbs can express a same event with either agent and patient (1-a) or with a patient only (1-b), corresponding respectively to a causative and inchoative interpretation of the event: (1) a. Hannah popped the balloon. b. The balloon popped. The main property of the causative-inchoative alternation is the ability of the patient to be promoted from object to subject, and most theoretical accounts focus on this transitive to intransitive frame change. Once the event is in its intransitive form, however, it is syntactically indistinguishable from any other intransitive verb construction, and in order to categorize the verb one must know the specific semantic roles of its arguments. This phenomenon is particularly hard to mine because the different realisations are not disambiguated by the context but by the arguments of the verb, and the frequency of the constructions themselves is not an indication of their acceptability. Furthermore, the meaning of the sentence remains virtually the same. 1 However, picking up this categorisation is important not only for reasons involving the appropriateness of different types of subjects for each verb, but also because it has been shown to influence coreference patterns (Lo¡iciga et al., 2018). Nevertheless, efforts to discover verbs participating in the alternation using automatic methods are very limited. Here we focus on the unaccusative vs unergative distinction as it allows us to disambiguate the verbs based on their intransitive frames. While the subjects of unaccusatives are patients (1-b), subjects of unergatives are agents (2-a). The assumption is that alternating verbs belong to the unaccusative category and we can discover them by separating them from the unergative category. 2 (2) a. Hannah slept The key in using these categories is that, without knowing the semantic roles of the verb, we can measure how well a noun fits the subjective position using a large corpus. Our method relies on language modeling to do just this. We investigate the effect of different language models on the task of identifying unaccusatives vs unergatives by testing the intransitive frames of a large quantity of verbs. Related Work Levin's (1993) seminal work on verb alternations remains the most comprehensive collection of alternating verbs for English. Other collections, for instance Framenet (Baker et al., 1998) also specify if a verb allows the alternation based on Levin. Typological work for other languages exists in the linguistics literature (Haspelmath, 1993; Sch¤fer, 2009), but large collections are practically nonexistent.3 Building on Haspelmath's theory, Samard ¾ic´ (2014) estimates a Spontaneity score based on the ratio of a verb's transitive to intransitive occurrence. In this scale, verbs are ranked according to the degree to which they are non-agentive. In other words, verbs without an explicit agent causing the event are more likely to participate in the causative alternation. Samard ¾ic and Merlo (2018) ´ report between 61% and 85% agreement between their model and theoretical classifications. Kann et al. (2019) rely on Levin's work to create synthetic data and build classifiers able to discriminate between several alternations.data sets are built using proper names as subjects and common nouns as objects, and they focus on the transitive to intransitive construction case. In this paper we rely on intransitive constructions exclusively, as will be explained below in Section 3. In addition, we use their FAVA data set for evaluation in Section 4. Our method is based on the generation of intransitive sentence variants of candidate verbs and probing a language model. We obtained results on par with similar approaches, with the added benefit of not relying on annotated resources. Introduction As NLP systems push towards Natural Language Understanding, their ability to grasp verb meaning is central. In this paper we present an unsupervised method to detect English unergative and unaccusative verbs. Within the wider category of intransitive verbs, these subgroups show differences in their behaviour, such as (in different languages) auxiliary selection, passivisation, cliticisation and causative-inchoative alternation. These differences are due to the different semantic roles of the subject of unergative verbs, which shares its agentivity with subjects in transitive frames, and that of unaccusative verbs.",0
"Abstract We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices. INTRODUCTION In this paper, we describe and analyze an as far as we know theoretically un-examined kind of bias, which we call interpretive blindness(IB). IB is exemplified by humans (and perhaps soon by sophisticated machine learning algorithms) whose beliefs are guided and shaped by testimony. When learning through testimony—perhaps the primary way that most people acquire information nowadays—an agent acquires beliefs through conversations with other agents, or from books, newspapers or social networks, and so on. Typically, such people lack direct access to the phenomena described via that testimony. Typically too, humans only pay attention to a restricted set of bodies of testimony from a limited number of sources for their information—which makes sense in terms of an agent’s limited resources and attention span. Our paper is about the strategic consequences of opinion diffusion through testimony and the distortions on learning and information that can result. IB results from this restriction to few sources of testimony and a natural co-dependence between beliefs and interpretation (Asher and Paul, 2018). Relying on testimony T from a restricted set of sources to update one’s beliefs leads to the mutual reinforcement of our confidence in the source and our belief in T; this creates a bias that can preclude learning when an agent tries to exploit new data that are incompatible with or simply distinct from T. Agents who are interpretively blind will discount any evidence that challenges their beliefs. We use Wolpert’s 2018 extended Bayesian framework to prove our results. While IB is problematic for a standard Bayesian framework, it also poses problems for hierarchical Bayesian approaches (Gelman et al., 2013), because testimony from sources on social media like Facebook, 24/7 media outlets and web interest groups is often argumentatively complete, a notion we analyze precisely in Section 4; in an argumentatively complete body of testimony T, the authors of that testimony can respond to and argue with any doubts raised by other data or arguments in a body T ′ that might threaten T’s credibility. A skillful climate denier, for example, will always find a way to undercut the most scientifically careful argument. Argumentatively complete testimony thus can undermine higher order constraints and good epistemic practices that should guide first order learning. Our paper starts in Section ?? by discussing testimony. We then introduce the codependence of belief and interpretation and apply it to the situation of testimony and the sources that support it. In Section 3 we formally show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings.  Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and Capp´e (2011). It is also related to work on generalization in machine learning. Epistemic biases affect generalization and learning capacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016) show that standard techniques in machine learning for promoting good epistemic biases and generalization—training error minimization, regularization techniques like weight decay or dropout, or complexity measures used to minimize generalization error (the difference between training error and test error)—do not necessarily lead to good generalization and test performance. Argumentatively complete testimony T incorporates an adversarial attack mechanism against any good epistemic practices that might discount T. It’s this mechanism that guarantees IB. The argumentation literature (Amgoud and Demolombe, 2014; Dung, 1995) is also relevant to IB. If testimony T is argumentatively complete, then T always provides a counterargument to an attack against T–much like an acceptable argument in Dung (1995). In addition, however, an argumentatively complete T also supports higher order evaluation hypotheses that support hypotheses that support T. There are also important connections to the literature on trust (Castelfranchi and Falcone, 2010); in our set up learning agents trust certain sources over others, and our higher order setting invokes a hierarchy of reasons. Nevertheless, the argumentation and trust-based work of which we are aware is complementary to our approach. An argumentation framework takes a possibly inconsistent belief base and imposes a static constraint on inference in such a setting. Similarly, trust is typically modeled in some sort of static modal framework. By contrast, ME learning games and the whole Bayesian framework are dynamic, with beliefs evolving under evidence and game strategies evolving under agent interaction. It is this dynamic evolution that is crucial to our approach and, we think, to modeling agents and learning. In sum, we are not looking at the problem of consistency, but rather the problems of entrenchment and bias. Conclusions Interpretive blindness results from a dynamic, iterative process whereby a learner’s background beliefs and biases lead her to update her beliefs based on a body of testimony T, and then biases inherent in T come back to reinforce her beliefs and her trust in T’s source(s), further biasing her towards these sources for future updates. We have introduced and formally characterized IB. We have shown that IB can prevent learning even in higher order Bayesian frameworks for learning from argumentatively complete testimony, despite the presence of constraints designed to promote good epistemic practices. We also shown that IB is computationally complex as a co-r.e. set via a game theoretic analysis, and that an agent may rationally remain in IB in the face of epistemic arguments. Our game theoretic analysis can also be extended to cases where the agent falls out of IB but then is a recidivist and becomse a prisoner once more. We leave that for future work. Investigating IB alas is not just an academic enterprise. IB really does happen, with sometimes tragic or dangerous results. We think a careful formal analysis is urgent for society. Finally, we note that while we have focused on IB as a problem for learning from testimony, the problem it raises for learning extends to any case in which we do not have unmediated access to ground truth and our data is “theory laden” Hanson (1958).",1
"Abstract We model here an epistemic bias we call interpretive blindness (IB). IB is a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a co-dependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices. INTRODUCTION In this paper, we describe and analyze an as far as we know theoretically un-examined kind of bias, which we call interpretive blindness(IB). IB is exemplified by humans (and perhaps soon by sophisticated machine learning algorithms) whose beliefs are guided and shaped by testimony. When learning through testimony perhaps the primary way that most people acquire information nowadays an agent acquires beliefs through conversations with other agents, or from books, newspapers or social networks, and so on. Typically, such people lack direct access to the phenomena described via that testimony. Typically too, humans only pay attention to a restricted set of bodies of testimony from a limited number of sources for their information which makes sense in terms of an agent's limited resources and attention span. Our paper is about the strategic consequences of opinion diffusion through testimony and the distortions on learning and information that can result. IB results from this restriction to few sources of testimony and a natural co-dependence between beliefs and interpretation (Asher and Paul, 2018). Relying on testimony T from a restricted set of sources to update one's beliefs leads to the mutual reinforcement of our confidence in the source and our belief in T; this creates a bias that can preclude learning when an agent tries to exploit new data that are incompatible with or simply distinct from T. Agents who are interpretively blind will discount any evidence that challenges their beliefs. We use Wolpert's 2018 extended Bayesian framework to prove our results. While IB is problematic for a standard Bayesian framework, it also poses problems for hierarchical Bayesian approaches (Gelman et al., 2013), because testimony from sources on social media like Facebook, 24/7 media outlets and web interest groups is often argumentatively complete, a notion we analyze precisely in Section 4; in an argumentatively complete body of testimony T, the authors of that testimony can respond to and argue with any doubts raised by other data or arguments in a body T² that might threaten T's credibility. A skillful climate denier, for example, will always find a way to undercut the most scientifically careful argument. Argumentatively complete testimony thus can undermine higher order constraints and good epistemic practices that should guide first order learning. Our paper starts in Section ?? by discussing testimony. We then introduce the codependence of belief and interpretation and apply it to the situation of testimony and the sources that support it. In Section 3 we formally show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings. Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and Capp´e (2011). It is also related to work on generalization in machine learning. Epistemic biases affect generalization andcapacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016) show that standard techniques in machine learning for promoting good epistemic biases and generalization training error minimization, regularization techniques like weight decay or dropout, or skillful climate denier disguising her beliefs as science or politics or from testimony. We show how IB can result in ordinary Bayesian learning. Section 4 shows how IB is reinforced in a hierarchical Bayesian learning setting. Section 6 develops a game theoretic setting to investigate the complexity of IB. We provide results as to whether it is possible to free agents from interpretive bias in several epistemic settings. Comparisons to Prior Work IB is an epistemological bias that is clearly related to confirmation bias (Lord et al., 1979; Nickerson, 1998; Oswald and Grosjean, 2004), in which agents interpret new evidence in a way that confirms their beliefs, and to the framing biases of Tversky and Kahneman (1975, 1985). People tend to see in the evidence what they believe. These forms of bias, however, concern how beliefs and bias influence interpretation, painting only part of the picture of IB (see also Asher and Paul (2018)). Further, unlike much of the psychological literature which finds epistemologically exogenous justifications for this bias (Dardenne and Leyens, 1995), we show how IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. IB is a concrete application of the work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Banks and Sundaram (1994); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and Capp´e (2011). It is also related to work on generalization in machine learning. Epistemic biases affect generalization and learning capacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016) show that IB is a natural outcome of Bayesian updating, rational resource management and the belief interpretation co-dependence. Zhang et al. (2016). Epistemic biases affect generalization and learning capacity in ways that are still not fully understood (Lampinen and Vehtari, 2001; Zhang et al., 2016; Kawaguchi et al., 2017; Neyshabur et al., 2017). Zhang et al. (2016). Epistemic biases affect learning capacity and the capacity to interpret. Paul and Vehtari (2018); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Garivier and Capp´e (2011). It is also related to work on bandits in, determining optimal allocation of resources to the exploration and exploitation of sources Whittle (1980); Lai and Robbins (1985); Burnetas and Katehakis (1997); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006). Cesa-Bianchi and Lugosi (2007). Epistemic biases affect learning capacity and the capacity to interpret. Katehakis and Wittgenstein (1958); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006). Epistemic biases affect learning capacity and the capacity to interpret. Paul and Vehtari (2018); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006). Epistemic biases affect learning capacity and the capacity to interpret. Katehakis and Wittgenstein (1958); Auer et al. (2002); Cesa-Bianchi and Lugosi (2006).",0
"Abstract This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover’s Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking - a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes. Introduction Modern computational methods for Natural Language Processing (NLP) rely on embeddings into metric spaces such as the Euclidean, and more recently non-linear spaces such as the Wasserstein space, to achieve state-of-art performance for various tasks. In these embeddings, the semantic differences and similarities between words and documents, correspond to the distances in the represented space. For embedding into Euclidean spaces, a large body of work is based on Word2vec (Mikolov et al., 2013), where each word is represented as a vector in the Euclidean space. From these word embeddings one can further compute document and sentence embeddings using various models Ramos et al. (2003), Arora et al. (2017), Wang and Kuo (2020), Le and Mikolov (2014), Kiros et al. (2015), Logeswaran and Lee (2018) for higher level NLP tasks. Instead of embedding and comparing documents in the Euclidean space, Word Mover’s Distance (WMD) Kusner et al. (2015) was proposed to measure the similarity between documents in the Wasserstein space Peyré and Cuturi (2018), representing the documents with (empirical) probability distributions. In Huang et al. (2016) WMD is used for supervised learning and more recently in (Yurochkin et al., 2019), for multi-scale representation. To understand how these models work, a lot of effort has been put into aiding interpretability of these embeddings. In Arora et al. (2018) the authors proposed a linear algebraic structure to explain the polysemy of words. Recent works attempted to explain the meaning of each dimension, such as the sparse word embedding Faruqui et al. (2015); Panigrahi et al. (2019) and the POLAR Framework Mathew et al. (2020). To make WMD embeddings interpretable, Xu et al. (2018) proposed an unsupervised topic model in the representation space. In this work our focus is on enabling interpretable supervised WMD embeddings of the documents. Below we summarize the main contributions in this direction. Summary of main contributions - A new approach for contrastive representation learning is proposed via enforcing a clustering promoting mechanism using a set of anchors that in turn are also learned from the data. This, in contrast to previous approaches Huang et al. (2016); Kusner et al. (2015), allows for interpretability, i.e. allows one to determine which words are important for a particular class. Furthermore, compared to the K Nearest Neighbour (KNN), our classification using the learned anchors is faster (O(n) for usual KNN vs O(1) for our NN using anchors), and our method can be generalized to any other supervised contrastive learning. Results on public data sets as well as a on a novel data set evaluating written scientific work by students show the superiority and utility of our method. Discussion of lab report results: The discriminatory words identified by our approach, suggest a good fit with the qualitative differences, namely, claim complexity, scope of evidence, and consistency and closure, used by human coders to make classifications. The words also suggest themes not directly coded for. For example, differences in adjectives reflect differences in claim structure. The importance of adjectives such as positive, negative and relative, reflect the more complex claim structure in high scoring reports. While low scoring reports stated simple claims, high scoring reports compared the relative influence of competing effects (i.e. positive and negative mutations). Another hallmark of high scoring report is qualified or conditional claims that indicate context specificity or uncertainty. The importance of adverbs such as predominantly, largely, and disproportionately, in high-scoring reports, reflects uncertainty, expressed as of probabilistic claims, that were common in these reports. While these properties were not observed from the top words generated by TF-IDF. The predominance of nouns and verbs that describe laboratory procedures (e.g. method, procedure, standardize) in low-scoring reports is an interesting difference not directly coded for by human coders. It is nevertheless consistent with the shift in the laboratory curriculum from an emphasis on reporting on procedures to interpreting and arguing about findings that underlies the shift from low to high scores. Overall these findings suggest that our method captures meaningful qualitative differences originally identified by qualitative researchers",1
"Abstract This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover's Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking - a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes. Introduction Modern computational methods for Natural Language Processing (NLP) rely on embeddings into metric spaces such as the Euclidean, and more recently non-linear spaces such as the Wasserstein space, to achieve state-of-art performance for various tasks. In these embeddings, the semantic differences and similarities between words and documents, correspond to the distances in the represented space. For embedding into Euclidean spaces, a large body of work is based on Word2vec (Mikolov et al., 2013), where each word is represented as a vector in the Euclidean space. From these word embeddings one can further compute document and sentence embeddings using various models Ramos et al. (2003), Arora et al. (2017), Wang and Kuo (2020), Le and Mikolov (2014), Kiros et al. (2015), Logeswaran and Lee (2018) for higher level NLP tasks. Instead of embedding and comparing documents in the Euclidean space, Word Mover's Distance (WMD) Kusner et al. (2015) was proposed to measure the similarity between documents in the Wasserstein space Peyr© and Cuturi (2018), representing the documents with (empirical) probability distributions. In Huang et al. (2016) WMD is used for supervised learning and more recently in (Yurochkin et al., 2019), for multi-scale representation. To understand how these models work, a lot of effort has been put into aiding interpretability of these embeddings. In Arora et al. (2018) the authors proposed a linear algebraic structure to explain the polysemy of words. Recent works attempted to explain the meaning of each dimension, such as the sparse word embedding Faruqui et al. (2015); Panigrahi et al. (2019) and the POLAR Framework Mathew et al. (2020). To make WMD embeddings interpretable, Xu et al. (2018) proposed an unsupervised topic model in the representation space. In this work our focus is on enabling interpretable supervised WMD embeddings of the documents. Below we summarize the main contributions in this direction. Summary of main contributions - A new approach for contrastive representation learning is proposed via enforcing a clustering promoting mechanism using a set of anchors that in turn are also learned from the data. This, in contrast to previous approaches Huang et al. (2016); Kusner et al. (2015), allows for interpretability, i.e. allows one to determine which words are important for a particular class. Furthermore, compared to the K Nearest Neighbour (KNN), our classification using the learned anchors is faster (O(n) for usual KNN vs O(1) for our NN using anchors), and our method can be generalized to any other supervised contrastive learning. Results on public data sets as well as a on a novel data set evaluating written scientific work by students show the superiority and utility of our method. Discussion of lab report results: The discriminatory words identified by our approach, suggest a good fit with the qualitative differences, namely, claim complexity, scope of evidence, and consistency and closure, used by human coders to make classifications. The words also suggest themes not directly coded for. For example, differences in adjectives reflect differences in claim structure. The importance of adjectives such as positive, negative and relative, reflect the more complex claim structure in high scoring reports. While low scoring reports stated simple claims, high scoring reports compared the relative influence of competing effects (i.e. positive and negative mutations). Another hallmark of high scoring reportqualified or conditional claims that indicate context specificity or uncertainty. The importance of adverbs such as predominantly, largely, and disproportionately, in high-scoring reports, reflects uncertainty, expressed as of probabilistic claims, that were common in these reports. While these properties were not observed from the top words generated by TF-IDF. The predominant feature of our sample is the contribution of a large number of qualified and conditional keywords that are used in our Word Mover's Embedding. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the novel problem of assessing student work for scientific writing and thinking - a problem that is central to the area of (educational) Learning Sciences (LS).",0
"Abstract Code-switching (CS), a ubiquitous phe- nomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks – POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Introduction Code-switching (CS) is a phenomenon of switching back and forth between multiple languages and is very common in multilingual communities such as India, Singapore, etc. Understanding mixed language texts has several applications in an in- creasingly online world like hateful content de- tection, maintaining engagement with virtual as- sistants. Despite this pervasive prevalence, CS is often overlooked in language processing research and current models still cannot effectively handle CS. We believe that the reasons behind this are (1) the lack of efforts in leveraging existing large scale multilingual resources or pretrained models and (2) dearth of annotated resources in switching scenar- ios. In this paper, we present solutions to address these two problems specifically. The advent of pretraining techniques marshalled the celebrated successes of several language un- derstanding and generation tasks in English (Dong et al., 2019) and multilingual tasks (Chaudhary et al., 2020). However, the same level of com- mendatory results are not translated to CS scenar- ios; as studied by Aguilar et al. (2020); Khanuja et al. (2020) presenting a preliminary evaluation of multi-lingual pretrained models for CS scenarios. It is still largely unclear if the inadequacies are re- sulting due to dearth of data or ineptitude of quick adoption of multilingual models. We study pre- cisely this problem of identifying the artifacts that hinder the competent performance of pretrained models on CS with a case study on sequence label- ing tasks including Part-Of-Speech (POS) tagging and Named Entity Recognition (NER). Our contributions from this work are as fol- lows: (1) We first conduct a comprehensive bench- marking of different pretrained models for two se- quence labeling tasks across 4 different language pairs. Specifically we evaluate datasets in Hinglish, Tenglish, Benglish and Spanglish CS for the tasks NER and POS. (2) To broaden understanding to- wards the usefulness of different fine-tuning strate- gies, we investigate multitasking, character model- ing uncovering the problematic switch point cases in §4. (3) We propose a novel switch-point bias based self training approach built upon on obser- vations from the benchmarks and demonstrate im- proved results on both tasks. Related Work CS benchmarks: From one of the recent surveys (Sitaram et al., 2019), linguistic CS has been stud- ied in the context of many NLP tasks including language identification (Solorio et al., 2014) (Bali et al., 2014), POS tagging (Soto and Hirschberg, 2018) (Molina et al., 2019) (Das, 2016), NER (Aguilar et al., 2019), parsing (Partanen et al., 2018), sentiment analysis(Vilares et al., 2015), and question answering (Chandu et al., 2019) (Raghavi et al., 2015). Many CS datasets have been made available through the shared-task series FIRE (Choudhury et al., 2014); (Roy et al., 2013) and CALCS (Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other re- searchers have provided datasets such as humor detection (Khandelwal et al., 2018), sub-word CS detection (Mager et al., 2019) among others. More recently new CS benchmarks (Aguilar et al., 2020) (Khanuja et al., 2020) have been developed to com- pare models across language pairs, domains and general language processing in CS. Pretrained Models for CS: Before the advent of pretrained multingual models, pretrained mono- lingual models were combined in different ways to derive word embeddings (AlGhamdi and Diab, 2019; Pratapa et al., 2018), POS tagging (Bhattu et al., 2020), sentiment analysis (Singh and Lefever, 2020) etc., Similarly, pretrained multilingual mod- els have been explored on various CS tasks like language identification, POS tagging, NER, ques- tion answering and Natural language inference (Khanuja et al., 2020). However, (Winata et al., 2021) show that these pretrained models do not assure high quality representations on CS. We ex- amine prospective reasons for this and present a data augmentation technique to mitigate this. Motivation for our work - Gaps in CS adapta- tion: Building off the prior work, we will briefly discuss primarily three techniques that demon- strated usefulness in adapting models to CS. First, non-standardization of cross-scripting (i.e, translit- eration of words to another language) is identified as one of the major reasons behind the noisiness of CS datasets (Chandu et al., 2019). Prior literature on noisy texts proved the superiority of charac- ter level modeling to combat this problem (Cherry et al., 2018); (Adouane et al., 2018). Secondly, the domains of most of these noisy datasets are still vastly scattered. In order to improve general- ization in CS patterns, prior studies have shown the potency of multitasking with an auxiliary task of language tag prediction (Winata et al., 2018). Thirdly, the dearth of annotated CS data has been a dramatic problem across tasks. (Bhattu et al.,2020) compare pretrained models with fined-tuned models augmented with unlabeled Twitter text to exemplify the improved performance with the lat- ter model. Despite these takeaways, the usefulness of the three points above is not thoroughly inves- tigated in the context of pretrained models for CS. To this end, we adapt these techniques in conjunc- tion with the pretraining strategies and propose a novel bias-based data iterative augmentation tech- nique to get more bang for the buck in terms of the performance to augmented dataset size ratio. Conclusions CS, despite being a natural and prevalent form of communication is still vastly understudied in em- pirical research. This mainly stems from the (1) lack of efforts in re-purposing the celebrated pre-trained models to CS scenarios and (2) lack of annotated resources. We tackle precisely these 2 problems with the main focus on evaluating and improving how these models fare at switch points between languages. First, we benchmark a suite of monolingual and multilingual pretrained models on CS and identify that particular switch points fare poorly. We propose a novel switch point bias based self training method to strategically use unla- beled data to enhance performance at switch points. While improving or retaining the overall perfor- mance compared to finetuning char-BERT and mul- titasking, we show that our approach improves the performance of underperforming switch points as well. We believe that this bias based augmentation technique particularly helps in scenarios with less annotated data. Broader Impact We believe that this work is a step towards effac- ing the hesitation of utilizing large scale pretrained mono and multilingual models for code-switched scenarios. We were able to successfully demon- strate the utility of a switch point based annota- tor model to perform biased data augmentation. We do not foresee any immediate ethical concerns branching directly from our work. However, we cautiously advise anyone using or extending our work for their application or research to bear in mind that we inherit any kinds of biases and tox- icity and privacy concerns that the pretrained lan- guage models bear. Although our end tasks are not directly affected forthwith due to these, we still rec- ommend caution when our self training approach is used for other tasks especially with user interaction such as dialog response generation etc., to ensure the model does not predict toxic content. Overall, we expect the users to benefit from our research to prospectively apply this to scenarios where there is a dearth of annotated resources, thereby economiz- ing on annotations cost and efforts and enabling scaling up to a wealth of crawled data, if available in those language-pairs.",1
"Abstract Code-switching (CS), a ubiquitous phe- nomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Introduction Code-switching (CS) is a phenomenon of switching back and forth between multiple languages and is very common in multilingual communities such as India, Singapore, etc. Understanding mixed language texts has several applications in an in- creasingly online world like hateful content de- tection, maintaining engagement with virtual as- sistants. Despite this pervasive prevalence, CS is often overlooked in language processing research and current models still cannot effectively handle CS. We believe that the reasons behind this are (1) the lack of efforts in leveraging existing large scale multilingual resources or pretrained models and (2) dearth of annotated resources in switching scenar- ios. In this paper, we present solutions to address these two problems specifically. The advent of pretraining techniques marshalled the celebrated successes of several language un- derstanding and generation tasks in English (Dong et al., 2019) and multilingual tasks (Chaudhary et al., 2020). However, the same level of com- mendatory results are not translated to CS scenar- ios; as studied by Aguilar et al. (2020); Khanuja et al. (2020) presenting a preliminary evaluation of multi-lingual pretrained models for CS scenarios. It is still largely unclear if the inadequacies are re- sulting due to dearth of data or ineptitude of quick adoption of multilingual models. We study pre- cisely this problem of identifying the artifacts that hinder the competent performance of pretrained models on CS with a case study on sequence label- ing tasks including Part-Of-Speech (POS) tagging and Named Entity Recognition (NER). Our contributions from this work are as fol- lows: (1) We first conduct a comprehensive bench- marking of different pretrained models for two se- quence labeling tasks across 4 different language pairs. Specifically we evaluate datasets in Hinglish, Tenglish, Benglish and Spanglish CS for the tasks NER and POS. (2) To broaden understanding to- wards the usefulness of different fine-tuning strate- gies, we investigate multitasking, character model- ing uncovering the problematic switch point cases in 4. (3) We propose a novel switch-point bias based self training approach built upon on obser- vations from the benchmarks and demonstrate im- proved results on both tasks. Related Work CS benchmarks: From one of the recent surveys (Sitaram et al., 2019), linguistic CS has been stud- ied in the context of many NLP tasks including language identification (Solorio et al., 2014) (Bali et al., 2014), POS tagging (Soto and Hirschberg, 2018) (Molina et al., 2019) (Das, 2016), NER (Aguilar et al., 2019), parsing (Partanen et al., 2018), sentiment analysis(Vilares et al., 2015), and question answering (Chandu et al., 2019) (Raghavi et al., 2015). Many CS datasets have been made available through the shared-task series FIRE (Choudhury et al., 2014); (Roy et al., 2013) and CALCS (Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other re- searchers have provided datasets such as humor detection (Khandelwal et al., 2018), sub-word CS detection (Mager et al., 2019) among others. More recently new CS benchmarks (Aguilar et2020) (Khanuja et al., 2020) have been developed to com- pare models across language pairs, domains and general language processing in CS. Pretrained Models for CS: Before the advent of pretrained multingual models, pretrained mono- lingual models were combined in different ways to derive word embeddings (AlGhamdi and Diab, 2019; Pratapa et al., 2018), POS tagging (Bhattu et al., 2020), sentiment analysis (Singh and Lefever, 2020) etc., Similarly, pretrained multilingual mod- els have been explored on various CS tasks like language identification, POS tagging, NER, ques- tion answering and Natural language inference (Khanuja et al., 2020). However, (Winata et al., 2021) show that these pretrained models do not assure high quality representations on CS. We ex- amine prospective reasons for this and present a data augmentation technique to mitigate this. Motivation for our work - Gaps in CS adapta- tion: Building off the prior work, we will briefly discuss primarily three techniques that demon- strated usefulness in adapting models to CS. First, non-standardization of cross-scripting (i.e, translit- eration of words to another language) is identified as one of the major reasons behind the noisiness of CS datasets (Chandu et al., 2019). Prior literature on noisy texts proved the superiority of charac- ter level modeling to combat this problem (Cherry et al., 2018); (Adouane et al., 2018). Secondly, the domains of most of these noisy datasets are still vastly scattered. In order to improve general- ization in CS patterns, prior studies have shown the potency of multitasking with an auxiliary task of language tag prediction (Winata et al., 2018). Thirdly, the dearth of annotated CS data has been a dramatic problem across tasks. (Bhattu et al.,2020) compare pretrained models with fined-tuned models augmented with unlabeled Twitter text to exemplify the improved performance with the lat- ter model. Despite these takeaways, the usefulness of the three points above is not thoroughly inves- tigated in the context of pretrained models for CS. To this end, we adapt these techniques in conjunc- tion with the pretraining strategies and propose a novel bias-based data iterative augmentation tech- nique to get more bang for the buck in terms of the performance to augmented dataset size ratio. Conclusions CS, despite being a natural and prevalent form of communication is still vastly understudied in em- pirical research. This mainly stems from the (1) lack of efforts in re-purposing the celebrated pre-trained models to CS scenarios and (2) lack of annotated resources. We tackle precisely these 2 problems with the main focus on evaluating and improving how these models fare at switch points between languages. First, we benchmark a suite of monolingual and multilingual pretrained models on CS and identify that particular switch points fare poorly. We propose a novel switch point bias based self training method to strategically use unla- beled data to enhance performance at switch points. While improving or retaining the overall perfor- mance compared to finetuning char-BERT and mul- titasking, we show that our approach improves the performance of underperforming switch points as well. We believe that this bias based augmentation technique particularly helps in scenarios with less annotated data. Broader Impact We believe that this work is a step towards effac- ing the hesitation of utilizing large scale pretrained mono and multilingual models for code-switched scenarios. We were able to successfully demon- strate the utility of a switch point based annota- tor model to perform biased data augmentation. We do not foresee any immediate ethical concerns branching directly from our work. However, we cautiously advise anyone using or extending our work for their application or research to bear in mind that we inherit any kinds of biases and tox- icity and privacy concerns that the pretrained lan- guage models bear. Although our end tasks are not directly affected forthwith due to these, we still rec- ommend caution when our self training approach is used for other tasks especially with user interaction such as dialog response generation etc., to ensure the model does not predict toxic content. Overall, we expect the users to benefit from our research to prospectively apply this to scenarios where there is a dearth of annotated resources, thereby economiz- ing on annotations cost and efforts and enabling scaling up to a wealth of crawled data, whichoften not translated into Titan- ing benchmarks.",0
"Abstract Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of re- cent work that uses these large language mod- els to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation ap- proaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function. Note that the latent representation needs to be learned afresh for each new NLP task, and that, in many cases, the size of the training data limits the quality of the latent feature representation. Given that the nu- ances of language are common to all NLP tasks, one could posit that we could learn a generic la- tent feature representations from some generic task once, and then share it across all NLP tasks. Lan- guage modeling, where the model needs to learn how to predict the next word given previous words, is such a generic task with abundant naturally oc- curring text to pre-train such a model (hence the name pre-trained language models). In fact, the lat- est, ongoing paradigm shift begins when PLMs are introduced: for numerous NLP tasks, researchers now leverage existing PLMs via fine-tuning for the task of interest, prompting the PLMs to perform the desired task, or reformulating the task as a text gen- eration problem with application of PLMs to solve it accordingly. Advances in these three PLM-based paradigms have continuously established new state- of-the-art performances. This paper surveys recent works that leverage PLMs for NLP. We organize these works into the following three paradigms: Pre-train then fine-tune (§ 2): perform general- purpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. Prompt-based learning (§ 3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLM’s pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompt- ing can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. NLP as text generation (§ 4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). Generative PLMs can be also used for text gen- eration tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic pars- ing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), senti- ment analysis, and so on. In addition to the three paradigms, there is an- other, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: Data generation (§ 5): run PLMs to automat- ically generate data for NLP tasks. The gen- erated data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as coun- terexamples, clarifications, contexts, or other. In the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the target task in some way. The paper is organized as follows: Section 2 provides background on the PLMs and describes the first paradigm, pre-train then fine-tune. Sec- tion 3 discusses the second paradigm, prompt- based learning. Section 4 summarizes works in the third paradigm, NLP as text generation. In Sec- tion 5, we describe approaches that generate data via PLMs for a broad range of NLP tasks. We dis- cuss limitations and provide directions for future research in Section 6 and conclude in Section 7. Conclusion In this paper, we present a survey of the three trend- ing paradigms that use pre-trained language models for NLP. We describe each of them in depth, and summarize prior works whose applications have shown promise. In addition, we describe the use of pre-trained language models to automatically generate data that is used to improve performance in NLP tasks. We hope this survey will provide readers with key fundamental concepts and a com- prehensive view of the paradigm shift.",1
"Abstract Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of re- cent work that uses these large language mod- els to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation ap- proaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function. Note that the latent representation needs to be learned afresh for each new NLP task, and that, in many cases, the size of the training data limits the quality of the latent feature representation. Given that the nu- ances of language are common to all NLP tasks, one could posit that we could learn a generic la- tent feature representations from some generic task once, and then share it across all NLP tasks. Lan- guage modeling, where the model needs to learn how to predict the next word given previous words, is such a generic task with abundant naturally oc- curring text to pre-train such a model (hence the name pre-trained language models). In fact, the lat- est, ongoing paradigm shift begins when PLMs are introduced: for numerous NLP tasks, researchers now leverage existing PLMs via fine-tuning for the task of interest, prompting the PLMs to perform the desired task, or reformulating the task as a text gen- eration problem with application of PLMs to solve it accordingly. Advances in these three PLM-based paradigms have continuously established new state- of-the-art performances. This paper surveys recent works that leverage PLMs for NLP. We organize these works into the following three paradigms: Pre-train then fine-tune (2): perform general- purpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. Prompt-based learning (3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLMs pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompt- ing can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. NLP as text generation (4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). Generative PLMs can be also used for text gen- eration tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic pars- ing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), senti- ment analysis, and so on. In addition to the three paradigms, there is an- other, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: Data generation (5): run PLMs to automat- ically generate data for NLP tasks. The gen-data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as coun- terexamples, clarifications, contexts, or other. In the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the task in some way. The latter case is usually more efficient. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function.",0
"Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently – utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracy over unsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised distri- butions and how they relate to each other. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of NLP tasks including sentiment classification, entity typing, text classi- fication, sentence-pair classification and relation classification. Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al., 2019). Unsupervised meta-learning has been explored in computer vi- sion (Hsu et al., 2019; Khodadadeh et al., 2019) and reinforcement learning (Gupta et al., 2018). Hsu et al. (2019) cluster images using pre-trained embeddings to create tasks. Metz et al. (2019) meta-learn an unsupervised update rule in a semi- supervised framework. Bansal et al. (2020b) de- veloped the SMLMT approach to unsupervised meta-learning in NLP. Contemporary work (Murty et al., 2021) explored the use of clustering, though focused only on natural language inference tasks. Curriculum learning (Bengio et al., 2009) in the context of meta-learning has been unexplored in NLP, prior to this work. Jabri et al. (2019) found unsupervised curriculum to be beneficial for meta- reinforcement learning. We refer to Hospedales et al. (2020) for a comprehensive review of meta- learning. Self-supervised learning has emerged as an effi- cient approach to representation learning in NLP (Howard and Ruder, 2018; Peters et al., 2018; De- vlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Multi-task learning of pre-trained models has shown improved results on many tasks (Phang et al., 2018; Liu et al., 2019a), including few-shot setting. Yin et al. (2020) leveraged entailment tasks for few-shot learning. Du et al. (2020) developed self-training methods for semi-supervised few-shot learning. Recently, extremely large language mod- els have been shown to have few-shot capacities (Brown et al., 2020), while Schick and Schütze (2020) demonstrated few-shot capacities for small models in the semi-supervised setting. Meanwhile, Bansal et al. (2020a,b) showed meta-learning to be effective at improving few-shot performance in multi-task and unsupervised settings, as well as improving performance for small models. Conclusion We explored several approaches to self-supervised task distribution for meta-learning. Our results demonstrate improvements in few-shot perfor- mance over a wide-range of classification tasks. This demonstrates the utility of meta-learning from unlabeled data, opening up the possibility of large- scale meta-learning for pertinent applications in NLP such as continual learning, architecture search, learning for low-resource languages, and more.",1
"Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracyunsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised methods on a wide range of new tasks. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of new tasks. We find that (4.3, 4.4) is often used to describe meta-learning ap- proach in SMLMT. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average.",0
"Abstract Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on trans- fer learning to broaden its impact. Bench- marks are dominated by a small set of fre- quent phenomena, leaving a long tail of in- frequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently ad- dressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualita- tive meta-analysis of 100 representative pa- pers on transfer learning for NLU. Our anal- ysis asks three questions: (i) Which long tail dimensions do transfer learning studies tar- get? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction “There is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora.” (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to Zipf’s law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented. For example, since most popu- lar coreference benchmarks focus on English nar- ratives, they do not contain many instances of zero anaphora, a phenomenon quite common in other languages (e.g., Japanese, Chinese). In such sit- uations, model performance on benchmark cor- pora may not be truly reflective of expected perfor- mance on micro-level long tail phenomena, raising questions about the ability of state-of-the-art mod- els to generalize to the long tail. Most benchmarks do not explicitly catalogue the list of micro-level language phenomena that are included or excluded in the sample, which makes it non-trivial to construct a list of long tail micro-level language phenomena. Hence, we formalize an alternate conceptualization of the long tail: undersampled portions of the macro- level space that can be treated as proxies for long tail micro-level phenomena. These undersampled long tail macro-level dimensions highlight gaps and present potential new challenging directions for the field. Therefore, periodically taking stock of research to identify long tail macro-level di- mensions can help in highlighting opportunities for progress that have not yet been tackled. This idea has been gaining prominence recently; for ex- ample, Joshi et al. (2020) survey languages stud- ied by NLP papers, providing statistical support for the existence of a macro-level long tail of low- resource languages. In this work, our goal is to attempt to charac- terize the macro-level long tail in NLU and ef- forts that have tried to address it from research on transfer learning. Large benchmarks have driven much of the recent methodological progress on NLU (Bowman et al., 2015; Rajpurkar et al., 2016; McCann et al., 2018; Talmor et al., 2019; Wang et al., 2019c,b), but the generalization abilities of benchmark-trained models to the long tail have been unclear. In tandem, the NLP community has been successfully developing transfer learn- ing methods to improve generalization of models trained on NLU benchmarks (Ruder et al., 2019). The goal of transfer learning research is to tackle the macro-level long tail in NLU, leading to the question: how far has transfer learning addressed performance of benchmark models on the NLU long tail, and where do we still fall behind? Probing further, we perform a qualitative meta- analysis of a representative sample of 100 pa- pers on domain adaptation and transfer learning in NLU. We sample these papers based on citation counts and publication venues (§2.1), and docu- ment 7 facets for each paper such as tasks and do- mains studied, adaptation settings evaluated, etc. (§2.2). Adaptation methods proposed (or applied) are documented using a hierarchical categoriza- tion described in §2.3, which we develop by ex- tending the hierarchy from Ramponi and Plank (2020). With this information, our analysis fo- cuses on three questions: Q1: What long tail macro-level dimensions do transfer learning studies target? Here di- mensions include tasks, domains, languages and adaptation settings covered in transfer learning research. Q2: Which properties help adaptation meth- ods improve performance on long tail dimen- sions? Q3: Which methodological gaps have great- est negative impact on long tail performance? The rest of the paper presents thorough answers to these questions, laying out avenues for future research on transfer learning that more effectively address the macro-level long tail in NLU. We also present a case study to demonstrate that our meta- analysis framework can be use to systematically design and conduct experiments that provide in- sights that enable us to make progress along these avenues. Conclusion This work presents a qualitative meta-analysis of 100 representative papers on domain adaptation and transfer learning in NLU, with the aim of understanding performance of adaptation methods on the long tail. Through this analysis, we assess current trends and highlight methodological gaps that we consider to be major avenues for future re- search in transfer learning for the long tail. We ob- serve that current research has a tendency to side- line certain types of tasks, languages, domains, and adaptation settings, indicating that long tail coverage is far from comprehensive. We also iden- tify two properties that help long tail performance: (i) incorporating source-target domain distance, and (ii) incorporating a nuanced view of domain variation. Additionally, we identify three major gaps that must be addressed to improve long tail performance: (i) combining adaptation methods, (ii) incorporating extra-linguistic knowledge and (iii) application to data-scarce adaptation settings. Finally, we demonstrate the utility of our meta- analysis framework and observations in guiding the design of systematic meta-experiments to ad- dress prevailing open questions by conducting a systematic evaluation of popular adaptation meth- ods for high-expertise domains in a data-scarce setting. This case study reveals interesting insights about the adaptation methods evaluated and shows that significant progress can be made towards de- veloping a better understanding of adaptation for the long tail by conducting such experiments.",1
"Abstract Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on trans- fer learning to broaden its impact. Bench- marks are dominated by a small set of fre- quent phenomena, leaving a long tail of in- frequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently ad- dressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualita- tive meta-analysis of 100 representative pa- pers on transfer learning for NLU. Our anal- ysis asks three questions: (i) Which long tail dimensions do transfer learning studies tar- get? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction There is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora. (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to Zipfs law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented. For example, since most popu- lar coreference benchmarks focus on English nar- ratives, they do not contain many instances of zero anaphora, a phenomenon quite common in other languages (e.g., Japanese, Chinese). In such sit- uations, model performance on benchmark cor- pora may not be truly reflective of expected perfor- mance on micro-level long tail phenomena, raising questions about the ability of state-of-the-art mod- els to generalize to the long tail. Most benchmarks do not explicitly catalogue the list of micro-level language phenomena that are included or excluded in the sample, which makes it non-trivial to construct a list of long tail micro-level language phenomena. Hence, we formalize an alternate conceptualization of the long tail: undersampled portions of the macro- level space that can be treated as proxies for long tail micro-level phenomena. These undersampled long tail macro-level dimensions highlight gaps and present potential new challenging directions for the field. Therefore, periodically taking stock of research to identify long tail macro-level di- mensions can help in highlighting opportunities for progress that have not yet been tackled. This idea has been gaining prominence recently; for ex- ample, Joshi et al. (2020) survey languages stud- ied by NLP papers, providing statistical support for the existence of a macro-level long tail of low- resource languages. In this work, our goal is to attempt to charac- terize the macro-level long tail in NLU and ef- forts that have tried to address it from research on transfer learning. Large benchmarks have driven much of the recent methodological progress on NLU (Bowman et al., 2015; Rajpurkar et al., 2016; McCann et al., 2018; Talmor et al., 2019; Wang et al., 2019c,b), but the generalization abilities of benchmark-trained modelsthe long tail have been unclear. In tandem, the NLP community has been successfully developing transfer learn- ing methods to improve generalization of models trained on NLU benchmarks (Ruder et al., 2019). The goal of transfer learning research is to tackle the macro-level long tail in NLU, leading to the question: how far has transfer learning addressed performance of benchmark-trained models on the long tail, and where do we still fall behind? Probing further, we perform a qualitative meta- analysis of a representative sample of 100 pa- pers on domain adaptation and transfer learning in NLU. We sample these papers based on citation counts and publication venues (§2.1), and docu- ment 7 facets for each paper such as tasks and do- mains studied, adaptation settings evaluated, etc. (§2.2). Adaptation methods proposed (or applied) are documented using a hierarchical categoriza- tion described in §2.3, which we develop by ex- tending the hierarchy from Ramponi and Plank (2020). With this information, our analysis fo- cuses on three questions: Q1: What long tail dimensions do transfer learning studies target? Here di- mensions include tasks, domains, languages and adaptation settings covered in transfer learning research. Q2: Which properties help adaptation meth- ods improve performance on long tail dimen- sions? Q3: Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction There is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora. (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to Zipfs law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented.",0
"Abstract The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their neg-ative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detec-tion tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibil-ity of leveraging domain-specific word embed-ding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirec-tional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets. Introduction Social media has been used extensively for various purposes, such as advertising, business, news, etc. The idea of allowing users to post anything at any time on social media contributed to the existence of inappropriate content on social media. As a result, these platforms become a fertile environment for this type of content. Hate speech is the most com- mon form of destructive content on social media, and it can come in the form of text, photographs, or video. It is defined as an insult directed at a per-son or group based on characteristics such as color, gender, race, sexual orientation, origin, nationality, religion, or other characteristics (Weber, 2009). Hate speech poses a significant threat to commu-nities, either by instilling hatred in young people against others or by instigating criminal activity or violence against others. Hate speech on the internet is on the rise around the world, with approximately 60% of the global population (4:54 billion) using social media to com- municate (Ltd, 2020). According to studies, ap- proximately 53 percent of Americans have encoun- tered online harassment and hatred (League, 2019). This score is 12 points higher than the findings of a similar survey performed in 2017 (Duggan, 2017). According to Clement (2019), 21% of students frequently encounter hate speech on social media. Thus, the detection of hate content on social media is an essential and necessary requirement for so- cial media platforms. Social media providers work hard to get rid of this content for a safer social en- vironment. Detecting hateful content is considered one of the challenging NLP tasks as the content might target/attack individuals or groups based on various characteristics using different hate terms and phrases (Badjatiya et al., 2017). Social media users often employ abbreviations and ordinary words(not hateful) to express their hate intent implicitly that known as code words to evade from being detected (e.g., using Google to refer to dark skin people), which adds extra diffi- culties to detect hate speech. Many studies have proposed machine learning models to handle this problem by utilizing a wide range of features set and machine learning algorithms for classification (Magu et al., 2017; Agarwal and Sureka, 2015; Jaki and De Smedt, 2019; Hartung et al., 2017). These methods often utilize features that require considerable effort and time to be extracted, such as text-based, profile-based, and community-based features. Other studies have worked on linguistic-based features (e.g., word frequency) and deep learning for classification (de Gibert et al., 2018), or distributional based features (e.g., word embed-ding) and machine learning classifier (Gupta and Waseem, 2017; Badjatiya et al., 2017; Djuric et al., 2015; Nobata et al., 2016). Studies show that distributional features provide a promising result in NLP tasks such as sentiment analysis (Gupta and Waseem, 2017). Recently, deep learning methods also show that it performs well on various NLP problems (Socher et al., 2012). Thus, this study investigates the performance of em- ploying these two methods. Accordingly, this study uses the distributional-based learning method to extract meaningful domain-specific embedding as features and deep learning based on Bidirectional Long Short Term Memory (BiLSTM) classifier to detect hate speech. The word embedding/ distribu- tional representation in this research is built upon a hate speech corpus of 1; 048; 563 sentences to reach the closest meaningful representation vec-tor of hate words. Then, compare it with the domain- agnostic embedding model such as Google Word2Vec and GloVe under the same classifier. We also assess the performance of detecting hate speech using Google’s pre-trained BERT model, which has generally achieved a state-of-the-art for many NLP tasks. The contributions of this research are highlighted as follow: An unsupervised domain-specific word em-bedding model was developed to extract the meaning of commonly used terminology, acronyms, and purposefully misspelled hate words. A comparison between the domain- specific and domain agnostic embedding was provided. The findings show that domain agnostic em-bedding performs slightly better (about 1%), despite the huge difference in the trained cor-pus size. The evaluation of a BiLSTM-based deep model with domain-specific embeddings shows an improvement ranging from 5 to 6 points on available datasets over the state-of-the-art techniques. The evaluation of the BERT language model on the hate speech binary classification task shows an improvement of about 2 points com-pared to the domain-specific word embedding model. The remaining of this paper is constructed as follows: the background section provides back- ground information about hate detection and the used methodologies; the review of literature sec-tion includes most recent studies in the field; the methodology section describes the methods used in this study and its specification; the experiment and result section presents the used datasets, em- bedding models, and results of the experiments; the discussion section includes analysis and obser- vation from the results, and finally the conclusion section summarizes all the findings. Background This section gives an overview of hate speech detec-tion in the field and it provides information about the used methodologies for both of the features and classifiers. 2.1 Hate Speech Detection Several research have attempted to solve the prob- lem of detecting hate speech in general by differ- entiating hate and non-hate speech. (Ribeiro et al., 2017; Djuric et al., 2015). Others have tackled the issue of recognizing certain types of hate speech, such as anti-religious hate speech. (Albadi et al., 2018; Zhang et al., 2018), jihadist (De Smedt et al., 2018; Ferrara et al., 2016; Wei et al., 2016; Gialam- poukidis et al., 2017), sexist and racist (Badjatiya et al., 2017; Pitsilis et al., 2018; Gamback ̈ and Sik- dar, 2017). Several platforms have been used to collect datasets from various online resources such as websites or online forums (e.g., 4Chan, Dai- lyStorm), or recent social media platforms (e.g., Twitter, Facebook). Hate speech has been applied also on different languages (e.g., English, Arabic, German). 2.2 Word Embedding Word embedding (Bengio et al., 2003) is a promi-nent natural language processing (NLP) technique that seeks to convey the semantic meaning of a word. It provides a useful numerical description of the term based on its context. The words are repre-sented by a dense vector that can be used in estimat-ing the similarities between the words (Liu, 2018). The word is represented by an N- dimensional vec-tor appropriate to represent the word meaning in a specific language (Mikolov et al., 2013). The word embedding has been widely used in many recent NLP tasks due to its efficiency such as text classification (Gamback ̈ and Sikdar, 2017; Lilleberg et al., 2015), document clustering (Ailem et al., 2017), part of speech tagging (Wang et al., 2015), named entity recognition (Siencnikˇ, 2015), sentiment anal-ysis (Tang et al., 2014; Wang et al., 2016; Al-Azani and El-Alfy, 2017), and many other problems. The most common pretrained word embedding models are Google Word2Vec, Stanford GloVe and they are described as follow: 2.2.1 Word2Vec Word2Vec is one of the most recently used word embedding models. It is provided by the Google research team (Mikolov et al., 2013). Word2Vec associates each word with a vector-based on its surrounding context from a large corpus. The train- ing process for extracting the word vector has two types, the continuous bag of words model (CBOW), which predicts the target word from its context, and the Skip-Gram model (SG), which predicts the tar- get context from a given word. The feature vector of the word is manipulated and updated accord-ing to each context the word appears in the cor-pus. If the word embedding is trained well, similar words appear close to each other in the dimensional space. The word similarities between the words are measured by the cosine distance between their vec- tors. Google released a vector model called Google Word2Vec that has been trained on a massive cor- pus of over 100 billion words. 2.2.2 GloVe Pennington et al. (2014) provides another popular word embedding model named GloVe (Global Vec- tors for Word Representation). GloVe learns em- beddings using an unsupervised learning algorithm that is trained on a corpus to create the distribu- tional feature vectors. During the learning process, a statistics-based matrix is built to represent the words to words co-occurrence of the corpus. This matrix represents the word vectors. The learning process requires time and space for the matrix con- struction, which is a highly costly process. The difference between GloVe and Word2Vec is in the learning process, Word2Vec is a prediction based model, and GloVe is a count-based model. The GloVe is learned from Wikipedia, web data, Twit-ter, and each model is available with multiple vector dimensions. 2.3 Bidirectional Long Short-Term Memory (BiLSTM) LSTM (Hochreiter and Schmidhuber, 1997) is an enhanced version of the recurrent neural network, which is one of the deep learning models that is designed to capture information from a sequence of information. It differs from the feed-forward neural network in that it has a backward connection. RNN suffers from a vanishing gradient problem that happens when the weights are not updated anymore due to the small value of the received from error function in respect to the current weights in the iteration. The value is vanishing in very long sequences and becomes close to zero. This problem stops RNN from training. LSTM solves this problem by adding an extra interaction cell to preserve long sequence dependencies. Thus, LSTM saves data for long sequences, but it saves the data only from left to right. However, to save sequence data from both directions, a Bidirectional LSTM (BiLSTM) is used. BiLSTM consist of two LSTM, one process the data from left to right and the other in opposite direction then concatenates and flattens both forward and backward LSTM to improve the knowledge of the surrounding context. 2.4 BERT Pre-trained Language Model Bidirectional Encoder Representations from Trans- formers (BERT) (Devlin et al., 2018) is a language model trained on very huge data based on con- textual representations. BERT consists of feature extraction layers, which consist of word embed-ding and layer for the model (e.g., Classification, Question Answering, Named Entity Recognition). BERT is the most recent language model and pro- vides state of the art results in comparison to other language models for various NLP tasks. BERT training procedure of word embedding differs from other word embedding models. It creates a bidirec- tional representation of words that may be learned from both left and right directions. Word embed-ding approaches like Word2Vec and GloVe only examine one direction (either left to right or right to left), resulting in static word representations that do not alter with context. If the word’s meaning varies depending on the context, GloVe and Word2Vec map the word to only one embedding vector. As a result, Word2Vec and GloVe are referred to as context-free models. BERT is also different from previous language models (e.g., ELMo stands for Embeddings from Language Models (Peters et al., 2018)) in that it manipulates the context in all layers in both directions (left and right). Instead of shal-low combining processes such as concatenating, it use cooperatively conditioning to combine both left and right context. BERT is trained on Books Cor-pus (800M words) and English Wikipedia (2,500M words) (Devlin et al., 2018). Review of Literature It is worth noting that word embedding is an effec- tive approach to a variety of NLP issues. To extract bio-events from the scientific literature, Li et al. (2015) used word embedding. They used multiple sets of features such as, word embedding, BOW + n-gram joint model, and word embedding BOW joint model with SVM classifier and the overall per- formance of word embedding BOW is better than other models on different events, which reached to 77:37% f1-score. The pure word embedding model has lower performance because the dataset size is small. Wu et al. (2015) also used word embedding to distinguish clinical abbreviations as a special case of word sense disambiguation (WSD). The performance of SVM increased when employing word embedding features with an average accuracy of 93%. Hate speech identification is a prevalent issue that has gotten a lot of attention from researchers. Liu (2018) employed domain-specific word em- bedding model trained on the articles from hate speech websites and high centrality users’ tweets to reach to the semantics of code words used in hate speech. They experimented on CNN, and LSTM models and concluded that CNN performed better than LSTM on tweets due to the length of tweets. The achieved f1-score is 78% given that they ex- perimented on the previous tweet-length 180 char- acters. Gupta and Waseem (2017) evaluate the performance of using hate Word2Vec (i.e., domain- specific) model with Logistic Regression (LR) clas- sifier on three datasets. They achieved up to 91% f1-score. The results showed that domain-specific word embedding has a desirable performance and is suitable for unbalanced classes datasets. Nobata et al. (2016) aimed to detect abusive language using pretrained word embeddings on two domains (finance and news) and regression model for classification, they achieved 60:2% and 64:9% f1-score respectively. The results showed that Google Word2Vec provides better performance with 5% on both domains. Badjatiya et al. (2017) ​​employed deep learning techniques to extract em- bedding features from hate speech text and then used a decision tree model for classification. They reached 93% f1-score using random embeddings initialization that is fed to LSTM to construct fea- tures. The results proved that domain-specific em- bedding can provide better representation of hate words such as “racist” or “sexist” words, because it can extract the meaning of frequently used terms by the hate community, domain-specific-based de- tection is a promising method for the detection of hate speech, according to all of the research above. The authors of (Devlin et al., 2018) looked into the BERT model’s performance on a variety of NLP tasks. On eleven of these tasks, the model produced state-of-the-art results. It improved by 7:7 points in the General Language Understanding Evaluation (GLUE) benchmark, 4:6 points in Multi- Genre Nat-ural Language Inference (MultiNLI), and 1:5 to 5:1 points in the SQuAD various versions question an-swering tests. BERT language model was recently employed in a shared task to detect offensive language (Zhu et al., 2019; Pelicon et al., 2019; Wu et al., 2019). Zhu et al. (2019) fine tuned BERT model for this task and came in third place among competitors. They used 13; 240 in tweets to train the algorithm, with each message categorized as offensive or not offensive. They achieved 83:88% f1-score. Moza- fari et al. (2020) studied the performance of the BERT language model on hate speech detection as a multi-class problem in a recently released work. They employed a BERT basis and a variety of clas- sifiers, including CNN, which provided the highest score that reached to 92% f1-score. Conclusion and Future Work To conclude, BERT design provides an appropriate feature extraction and classification procedure for hate speech detection. BERT combines the benefits of domain agnostic and domain-specific word embedding by train the model on vast data then add an extra layer to trained on domain-specific data (fine-tuning). BERT also saves effort and time for building an embedding model from scratch. However, domain-specific word embedding overcomes BERT model in that it can detect hate terms and abbreviations and intentionally misspellings meaning. This study can be extended to detect multi-class hate speech for future work.",1
"Abstract The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their neg-ative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detec-tion tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibil-ity of leveraging domain-specific word embed-ding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirec-tional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets. Introduction Social media has been used extensively for various purposes, such as advertising, business, news, etc. The idea of allowing users to post anything at any time on social media contributed to the existence of inappropriate content on social media. As a result, these platforms become a fertile environment for this type of content. Hate speech is the most com- mon form of destructive content on social media, and it can come in the form of text, photographs, or video. It is defined as an insult directed at a per-son or group based on characteristics such as color, gender, race, sexual orientation, origin, nationality, religion, or other characteristics (Weber, 2009). Hate speech poses a significant threat to commu-nities, either by instilling hatred in young people against others or by instigating criminal activity or violence against others. Hate speech on the internet is on the rise around the world, with approximately 60% of the global population (4:54 billion) using social media to com- municate (Ltd, 2020). According to studies, ap- proximately 53 percent of Americans have encoun- tered online harassment and hatred (League, 2019). This score is 12 points higher than the findings of a similar survey performed in 2017 (Duggan, 2017). According to Clement (2019), 21% of students frequently encounter hate speech on social media. Thus, the detection of hate content on social media is an essential and necessary requirement for so- cial media platforms. Social media providers work hard to get rid of this content for a safer social en- vironment. Detecting hateful content is considered one of the challenging NLP tasks as the content might target/attack individuals or groups based on various characteristics using different hate terms and phrases (Badjatiya et al., 2017). Social media users often employ abbreviations and ordinary words(not hateful) to express their hate intent implicitly that known as code words to evade from being detected (e.g., using Google to refer to dark skin people), which adds extra diffi- culties to detect hate speech. Many studies have proposed machine learning models to handle this problem by utilizing a wide range of features set and machine learning algorithms for classification (Magu et al., 2017; Agarwal and Sureka, 2015; Jaki and De Smedt, 2019; Hartung et al., 2017). These methods often utilize features that require considerable effort and time to be extracted, such as text-based, profile-based, and community-based features. Other studies have worked on linguistic-based features (e.g., word frequency) and deep learning for classification (de Gibert et al., 2018), or distributional based features (e.g., word embed-ding) and machine learning classifier (Gupta and Waseem, 2017; Badjatiya et al., 2017; Djuric et al., 2015; Nobata et al., 2016). Studies show that distributional features provide a promising result in NLP tasks such as sentiment analysis (Gupta and Waseem, 2017). Recently, deep learning methods also show that it performs well on various NLP problems (Socher et al., 2012). Thus, this study investigates the performance of em- ploying these two methods. Accordingly, this study uses the distributional-based learning method to extract meaningful domain-specific embedding as features and deep learning based on BidirectionalShort Term Memory (BiLSTM) classifier to detect hate speech. The word embedding/ distribu- tional representation in this research is built upon a hate speech corpus of 1; 048; 563 sentences to reach the closest meaningful representation vec-tor of hate words. Then, compare it with the domain- agnostic embedding model such as Google Word2Vec and GloVe under the same classifier. We also assess the performance of detecting hate speech using Google's pre-trained BERT model, which has generally achieved a state-of-the-art for many NLP tasks. The contributions of this research are highlighted as follow: An unsupervised domain-specific word em-bedding model was developed to extract the meaning of commonly used terminology, acronyms, and purposefully misspelled hate words. A comparison between the domain- specific and domain agnostic embedding was provided. The findings show that domain agnostic em-bedding performs slightly better (about 1%), despite the huge difference in the trained cor-pus size. The evaluation of a BiLSTM-based deep model with domain-specific embeddings shows an improvement ranging from 5 to 6 points on available datasets over the state-of-the-art techniques. The evaluation of the BERT language model on the hate speech binary classification task shows an improvement of about 2 points com-pared to the domain-specific word embedding model. The remaining of this paper is constructed as follows: the background section provides back- ground information about hate detection and the used methodologies; the review of literature sec-tion includes most recent studies in the field; the methodology section describes the methods used in this study and its specification; the experiment and result section presents the used datasets, em- bedding models, and results of the experiments; the discussion section includes analysis and obser- vation from the results, and finally the conclusion section summarizes all the findings. Background This section gives an overview of hate speech detec-tion in the field and it provides information about the used methodologies for both of the features and classifiers. 2.1 Hate Speech Detection Several research have attempted to solve the prob- lem of detecting hate speech in general by differ- entiating hate and non-hate speech. (Ribeiro et al., 2017; Djuric et al., 2015). Others have tackled the issue of recognizing certain types of hate speech, such as anti-religious hate speech. (Albadi et al., 2018; Zhang et al., 2018), jihadist (De Smedt et al., 2018; Ferrara et al., 2016; Wei et al., 2016; Gialam- poukidis et al., 2017), sexist and racist (Badjatiya et al., 2017; Pitsilis et al., 2018; Gamback ̈ and Sik- dar, 2017). Several platforms have been used to collect datasets from various online resources such as websites or online forums (e.g., 4Chan, Dai- lyStorm), or recent social media platforms (e.g., Twitter, Facebook). Hate speech has been applied also on different languages (e.g., English, Arabic, German). 2.2 Word Embedding Word embedding (Bengio et al., 2003) is a promi-nent natural language processing (NLP) technique that seeks to convey the semantic meaning of a word. It provides a useful numerical description of the term based on its context. The words are repre-sented by a dense vector that can be used in estimat-ing the similarities between the words (Liu, 2018). The word is represented by an N- dimensional vec-tor appropriate to represent the word meaning in a specific language (Mikolov et al., 2013). The word embedding has been widely used in many recent NLP tasks due to its efficiency such as text classification (Gamback ̈ and Sikdar, 2017; Lilleberg et al., 2015), document clustering (Ailem et al., 2017), part of speech tagging (Wang et al., 2015), named entity recognition (Siencnik, 2015), sentiment anal-ysis (Tang et al., 2014; Wang et al., 2016; Al-Azani and El-Alfy, 2017), and many other problems. The most common pretrained word embedding models are Google Word2Vec, Stanford GloVe and they are described as follow: 2.2.1 Word2Vec Word2Vec is one of the most recently used word embedding models. It is provided by theresearch team (Mikolov et al., 2013). Word2Vec associates each word with a vector-based on its surrounding context from a large corpus. The train- ing process for extracting the word vector has two types, the continuous bag of words model (CBOW), which predicts the target word from its context, and the Skip-Gram model (SG), which predicts the tar- get context from a given word. The feature vector of the word is manipulated and updated accord-ing to each context the word appears in the cor-pus. If the word embedding is trained well, similar words appear close to each other in the dimensional space. The word similarities between the words are measured by the cosine distance between their vec- tors. Google released a vector model called Google Word2Vec that has been trained on a massive cor- pus of over 100 billion words. 2.2.2 GloVe Pennington et al. (2014) provides another popular word embedding model named GloVe (Global Vec- tors for Word Representation). GloVe learns em- beddings using an unsupervised learning algorithm that is trained on a corpus to create the distribu- tional feature vectors. During the learning process, a statistics-based matrix is built to represent the words to words co-occurrence of the corpus. This matrix represents the word vectors. The learning process requires time and space for the matrix con- struction, which is a highly costly process. The difference between GloVe and Word2Vec is in the learning process, Word2Vec is a prediction based model, and GloVe is a count-based model. The GloVe is learned from Wikipedia, web data, Twit-ter, and each model is available with multiple vector dimensions. 2.3 Bidirectional Long Short-Term Memory (BiLSTM) LSTM (Hochreiter and Schmidhuber, 1997) is an enhanced version of the recurrent neural network, which is one of the deep learning models that is designed to capture information from a sequence of information. It differs from the feed-forward neural network in that it has a backward connection. RNN suffers from a vanishing gradient problem that happens when the weights are not updated anymore due to the small value of the received from error function in respect to the current weights in the iteration. The value is vanishing in very long sequences and becomes close to zero. This problem stops RNN from training. LSTM solves this problem by adding an extra interaction cell to preserve long sequence dependencies. Thus, LSTM saves data for long sequences, but it saves the data only from left to right. However, to save sequence data from both directions, a Bidirectional LSTM (BiLSTM) is used. BiLSTM consist of two LSTM, one process the data from left to right and the other in opposite direction then concatenates and flattens both forward and backward LSTM to improve the knowledge of the surrounding context. 2.4 BERT Pre-trained Language Model Bidirectional Encoder Representations from Trans- formers (BERT) (Devlin et al., 2018) is a language model trained on very huge data based on con- textual representations. BERT consists of feature extraction layers, which consist of word embed-ding and layer for the model (e.g., Classification, Question Answering, Named Entity Recognition). BERT is the most recent language model and pro- vides state of the art results in comparison to other language models for various NLP tasks. BERT training procedure of word embedding differs from other word embedding models. It creates a bidirec- tional representation of words that may be learned from both left and right directions. Word embed-ding approaches like Word2Vec and GloVe only examine one direction (either left to right or right to left), resulting in static word representations that do not alter with context. If the words meaning varies depending on the context, GloVe and Word2Vec map the word to only one embedding vector. As a result, Word2Vec and GloVe are referred to as context-free models. BERT is also different from previous language models (e.g., ELMo stands for Embeddings from Language Models (Peters et al., 2018)) in that it manipulates the context in all layers in both directions (left and right). Instead of shal-low combining processes such as concatenating, it use cooperatively conditioningcombine both left and right context. BERT is trained on Books Cor-pus (800M words) and English Wikipedia (2,500M words) (Devlin et al., 2018). Review of Literature It is worth noting that word embedding is an effec- tive approach to a variety of NLP issues. To extract bio-events from the scientific literature, Li et al. (2015) used word embedding. They used multiple sets of features such as, word embedding, BOW + n-gram joint model, and word embedding BOW joint model with SVM classifier and the overall per- formance of word embedding BOW is better than other models on different events, which reached to 77:37% f1-score. The pure word embedding model has lower performance because the dataset size is small. Wu et al. (2015) also used word embedding to distinguish clinical abbreviations as a special case of word sense disambiguation (WSD). The performance of SVM increased when employing word embedding features with an average accuracy of 93%. Hate speech identification is a prevalent issue that has gotten a lot of attention from researchers. Liu (2018) employed domain-specific word em- bedding model trained on the articles from hate speech websites and high centrality users tweets to reach to the semantics of code words used in hate speech. They experimented on CNN, and LSTM models and concluded that CNN performed better than LSTM on tweets due to the length of tweets. The achieved f1-score is 78% given that they ex- perimented on the previous tweet-length 180 char- acters. Gupta and Waseem (2017) evaluate the performance of using hate Word2Vec (i.e., domain- specific) model with Logistic Regression (LR) clas- sifier on three datasets. They achieved up to 91% f1-score. The results showed that domain-specific word embedding has a desirable performance and is suitable for unbalanced classes datasets. Nobata et al. (2016) aimed to detect abusive language using pretrained word embeddings on two domains (finance and news) and regression model for classification, they achieved 60:2% and 64:9% f1-score respectively. The results showed that Google Word2Vec provides better performance with 5% on both domains. Badjatiya et al. (2017) employed deep learning techniques to extract em- bedding features from hate speech text and then used a decision tree model for classification. They reached 93% f1-score using random embeddings initialization that is fed to LSTM to construct fea- tures. The results proved that domain-specific em- bedding can provide better representation of hate words such as ""racist"" or ""sexist"" words, because it can extract the meaning of frequently used terms by the hate community, domain-specific-based de- tection is a promising method for the detection of hate speech, according to all of the research above. The authors of (Devlin et al., 2018) looked into the BERT models performance on a variety of NLP tasks. On eleven of these tasks, the model produced state-of-the-art results. It improved by 7:7 points in the General Language Understanding Evaluation (GLUE) benchmark, 4:6 points in Multi- Genre Nat-ural Language Inference (MultiNLI), and 1:5 to 5:1 points in the SQuAD various versions question an-swering tests. BERT language model was recently employed in a shared task to detect offensive language (Zhu et al., 2019; Pelicon et al., 2019; Wu et al., 2019). Zhu et al. (2019) fine tuned BERT model for this task and came in third place among competitors. They used 13; 240 in tweets to train the algorithm, with each message categorized as offensive or not offensive. They achieved 83:88% f1-score. Moza- fari et al. (2020) studied the performance of the BERT language model on hate speech detection as a multi-class problem in a recently released work. They employed a BERT basis and a variety of clas- sifiers, including CNN, which provided the highest score that reached to 92% f1-score. Conclusion and Future Work To conclude, BERT design provides an appropriate feature extraction and classification procedure for hate speech detection. BERT combines the benefits of domain agnostic and domain-specific word embedding by train the modelvast data then add an extra layer to trained on domain-specific data (fine-tuning). BERT also saves effort and time for building an embedding model from scratch. However, domain-specific word embedding overcomes BERT model in that it can detect hate terms and abbreviations and intentionally misspellings meaning. This study can be extended to detect multi-class hate speech using Google's pre-trained BERT model, which has generally achieved a state-of-the-art results.",0
"ABSTRACT This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a user’s account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012. The total loss from these attacks as of August of 2012 was around $690 million, and over $1.5 billion (Kessem 2013) by the end of the year. This is a 59% increase from 2011 in the number of phishing attacks and 22% higher in the losses. It clear that the phishing attacks are on the rise. The results of the phishing attack can be devastating, as a recent example of the attack on South Korea showed: an attack that shut down several prominent South Korean banks and broadcasters in March 2013 had an origin as a spear phishing campaign (Donohue 2013). The recommended defense against phishing attacks is to educate a user not to fall for them. Unfortunately, the awareness campaigns are not the most effective solution: it only takes one employee to fall for the bait for a company and for its clients to become vulnerable. Because phishing emails rely on human emotions, it is essential to provide an emotion-free solution to a phishing detection game. Such a solution should work before a user can click on the fraudulent link and, thus, before any damage can be made. At the same time, if a user is well educated and understands that an email is a phishing attempt, it should be possible for the user to indicate it as such, so that the solution software has a chance to validate its results or correct its evaluation based on real-time data. To make it more complicated, phishing emails can target a particular individual or a small group of individuals based on their known behaviors. Thus, it is important for the software to adapt to individual characteristics of an individual or a group of individuals and predict their vulnerabilities for phishing. There is a need for a phishing-detection application to understand “the buttons” that are being pushed by fraudulent emails. When such buttons are detected, it is possible to react to them in a desired way. To date, there have been a number of papers that report detection of phishing emails (Hong 2012, Basnet et al 2008, Cleber et al 2011, Mutton 2011, Aaron 2010, Xiang 2011, Shahriar & Zulkernine 2012) based on non-text features claiming an accuracy over 90%. Yet, the problem still exists and growing. One possible explanation of this is these methods don’t address the content of the messages that people fall for. BACKGROUND Phishing is a malicious use of internet resources done to trick internet users to reveal personal information to the attacker. An attack is typically performed by sending an email to an unsuspecting user that contains a link to a domain that is seemingly legitimate in the hopes that the user will input their private information for the attacker to steal (DigiCert 2009). Phishing is a criminal act which uses a combination of “social engineering and technical subterfuge” to steal user information (APWG 2012). The most common type of phishing attack leverages email. The emphasis of this paper is to detect phishing attempts within emails. The threat phishing poses to Internet users at large calls for action within the information security industry to create ways of detecting and preventing such attacks. Research into the area of phishing detection has yielded several types of email analysis to determine if an email should be classified as phishing such as link analysis, header analysis, and text analysis. Link analysis refers to the using information about the links included within an email to determine whether the email is legitimate or a phishing attempt. This usually involves checking to see if the displayed link in the email matches the actual website URL that the user is taken to if the link is clicked. Header analysis refers to examining the header contents of an email to decide if the email is a phishing email or not. This analysis typically includes checking that the 'From' field of the email matches the actual sender and checking the IP address from which the email was sent against phishing blacklists. Blacklist is a set of well-known phishing Web sites and addresses reported by trusted entities such as Google's and Microsoft's black list (Gaurav et al. 2012). For black listing, both a client and a server side are necessary. The client component's implementation can be done through an email or browser plug-in that communicate with a server component. The server component is a public Web site containing a list of phishing sites (Tout & Hafner 2009). Text analysis refers to the examination of text included in the body of the email to find out if an email has a suspicious content. CONCLUSION The paper expands a version of a text-based phishing detection algorithm. It is shown that though accounting for all POS rather than selective ones increases the phishing detection results. At the same time, false positive rate can slightly increase when more emphasis is put on the importance of detecting as many phishing emails as possible, as expected. It is expected that adding semantic component in addition to considering all POS will reduce the false positive rate while preserving the detection accuracy.",1
"ABSTRACT This paper reports on an experiment into text-based phishing detection using readily available resources and without the use of semantics. The developed algorithm is a modified version of previously published work that works with the same tools. The results obtained in recognizing phishing emails are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a user's account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012. The total loss from these attacks as of August of 2012 was around $690 million, and over $1.5 billion (Kessem 2013) by the end of the year. This is a 59% increase from 2011 in the number of phishing attacks and 22% higher in the losses. It clear that the phishing attacks are on the rise. The results of the phishing attack can be devastating, as a recent example of the attack on South Korea showed: an attack that shut down several prominent South Korean banks and broadcasters in March 2013 had an origin as a spear phishing campaign (Donohue 2013). The recommended defense against phishing attacks is to educate a user not to fall for them. Unfortunately, the awareness campaigns are not the most effective solution: it only takes one employee to fall for the bait for a company and for its clients to become vulnerable. Because phishing emails rely on human emotions, it is essential to provide an emotion-free solution to a phishing detection game. Such a solution should work before a user can click on the fraudulent link and, thus, before any damage can be made. At the same time, if a user is well educated and understands that an email is a phishing attempt, it should be possible for the user to indicate it as such, so that the solution software has a chance to validate its results or correct its evaluation based on real-time data. To make it more complicated, phishing emails can target a particular individual or a small group of individuals based on their known behaviors. Thus, it is important for the software to adapt to individual characteristics of an individual or a group of individuals and predict their vulnerabilities for phishing. There is a need for a phishing-detection application to understand the buttons that are being pushed by fraudulent emails. When such buttons are detected, it is possible to react to them in a desired way. To date, there have been a number of papers that report detection of phishing emails (Hong 2012, Basnet et al 2008, Cleber et al 2011, Mutton 2011, Aaron 2010, Xiang 2011, Shahriar & Zulkernine 2012) based on non-text features claiming an accuracy over 90%. Yet, the problem still exists and growing. One possible explanation of this is these methods don't address the content of the messages that people fall for. BACKGROUND Phishing is a malicious use of internet resources done to trick internet users to reveal personal information to the attacker. An attack is typically performed by sending an email to an unsuspecting user that contains a link to a domain that is seemingly legitimate in the hopes that the user will input their private information for the attacker to steal (DigiCert 2009). Phishing is a criminal act which uses a combination of social engineering and technical subterfuge to steal user information (APWG 2012). The most common type of phishing attack leverages email. The emphasis of this paper is to detect phishing attempts within emails. The threat phishing poses to Internet users at large calls for action within the information security industry to create ways of detecting and preventing such attacks. Research into the area of phishing detection has yielded several types of email analysis to determine if an email should be classified as phishing such as link analysis, header analysis, and text analysis. Link analysis refers to the using information about the links included within an email to determine whether the email is legitimate or a phishing attempt. This usually involves checking to see if the displayed link in the email matches the actual website URLthe user is taken to if the link is clicked. Header analysis refers to examining the header contents of an email to decide if the email is a phishing email or not. This analysis typically includes checking that the 'From' field of the email matches the actual sender and checking the IP address from which the email was sent against phishing blacklists. NOTE. The recommended defense against phishing attacks is to educate a user not to fall for them. The results of the analysis are considerably better than the previously reported work; but the rate of text falsely identified as phishing is slightly worse. It is expected that adding semantic component will reduce the false positive rate while preserving the detection accuracy. INTRODUCTION Phishing is one of the most potentially disruptive actions that can be performed on the internet. Stealing a user's account information within a business network through a phishing scam can be an easy way to gain access to that business network. Intellectual property and other pertinent business information could potentially be at risk if a user behind the keyboard falls for a phishing attack. The most common way of carrying out a phishing attack is through email. While such attacks may be easily identifiable for those well-versed in technology, it may be difficult for the typical internet user to spot a fraudulent email. According to RSA (Brook 2012), there have been nearly 33 thousand phishing attacks each month in 2012.",0
"Abstract While recent work on automated fact-checking has focused mainly on verifying and explain- ing claims, for which the list of claims is read- ily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive task and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the article’s veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021). End-to-end automated fact-checking systems in- volve three core objectives - (1) identifying check- worthy claims, (2) verifying claims against authori- tative sources, and (3) delivering corrections/ expla- nations on the claims (Graves, 2018). The majority of the recent work focuses on verification and ex- planation objectives for which a list of claims is readily available (Thorne et al., 2018; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020; Kazemi et al., 2021). Identifying check-worthy claims, which is a critical first step for fact-checking, remains a challenging task. ClaimBuster is the first work to target check- worthiness (Hassan et al., 2017). It is trained on transcripts of 30 US presidential elections de- bates. Each sentence of the transcripts is annotated for three categories - non-factual sentence, unimportant factual sentence, and check-worthy factual sentence. They then build classifiers to classify sentences into these three labels. Another classification-based approach is to predict whether the content of a given statement makes ”an asser- tion about the world that is checkable” (Konstanti- novskiy et al., 2021). This approach utilizes anno- tations for sentences extracted from subtitles of UK political shows. The models are trained to classify statements into binary labels - claim or non-claim. Finally, a system called ClaimRank (Jaradat et al., 2018) aims to prioritize the sentences that fact- checkers should consider first for fact-checking. ClaimRank is trained on pre-existing annotations on political debates from 9 fact-checking organiza- tions. This approach first classifies the statement as check-worthy or not. The statements are then ranked based on the probabilities that the model as- signs to a statement to belong to the positive class. While these works are fundamental towards ap- proaching the problem of check-worthy claim iden- tification, the focus is only on a single domain (pol- itics). Additionally, the models rely on sentence- level human annotations, which is an expensive task, challenging to conduct regularly for multiple domains, and subject to personal bias. In this work, we try to overcome these limitations by exploring the effectiveness of using internal signals from un- labeled data. We focus on fake news articles and experiment with two types of internal signals for overall supervision - headline and abstractive sum- mary. We make two hypotheses regarding these sig- nals - first, these two elements of an article contain the gist of the content. To support this hypothesis, we evaluate the headline and the abstractive sum- mary against the manually written Snopes claims for news articles. Claims that Snopes write con- tain the salient factual idea of the source article. Second, sentences that are semantically close to the headline or the summary are check-worthy. To assess this hypothesis, we experiment with end-to- end fact-checking pipelines. The pipelines lever- age the top-ranked sentences relevant to the head- line/summary for the downstream fact-checking tasks of evidence retrieval and veracity prediction. The dataset used for these experiments contains articles from multiple domains, such as medical, crime, politics, technology. Through comparative experiments, we find that the top-3 ranked sen- tences contain enough information for evidence- based fact-checking of a fake news article. We also observe that the summary-based pipeline is the most promising for an end-to-end automated fact-checking system. Conclusion We explore identification of check-worthy claim sentences from a news article without any sentence- level annotation. We show experiments leveraging two internal signals - headline and abstractive summary of the article. We test two hypotheses - (1) headline/abstractive summary contains the gist of the article, and (2) sentences of the content seman- tically relevant to the headline/summary are check- worthy claim sentences. We build fact-checking pipelines for this purpose and show that the check- worthy claim sentences identified based on the sum- mary of the article are adequate for downstream tasks of evidence gathering and veracity predic- tion of the article. Our experiments use articles ranging on a variety of topics and associated with four veracity labels. For future work, we aim to use the abstractive-summary-based methodology for fact-checking of other types of textual data - online discourse, debate transcripts, etc. We be- lieve that leveraging topic detection along with the summary-based check-worthiness identification can help overcome the issues and biases introduced due to sentence-level manual annotations.",1
"Abstract While recent work on automated fact-checking has focused mainly on verifying and explain- ing claims, for which the list of claims is read- ily available, identifying check-worthy claim sentences from a text remains challenging. Current claim identification models rely on manual annotations for each sentence in the text, which is an expensive task and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the article's veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021). End-to-end automated fact-checking systems in- volve three core objectives - (1) identifying check- worthy claims, (2) verifying claims against authori- tative sources, and (3) delivering corrections/ expla- nations on the claims (Graves, 2018). The majority of the recent work focuses on verification and ex- planation objectives for which a list of claims is readily available (Thorne et al., 2018; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020; Kazemi et al., 2021). Identifying check-worthy claims, which is a critical first step for fact-checking, remains a challenging task. ClaimBuster is the first work to target check- worthiness (Hassan et al., 2017). It is trained on transcripts of 30 US presidential elections de- bates. Each sentence of the transcripts is annotated for three categories - non-factual sentence, unimportant factual sentence, and check-worthy factual sentence. They then build classifiers to classify sentences into these three labels. Another classification-based approach is to predict whether the content of a given statement makes an asser- tion about the world that is checkable (Konstanti- novskiy et al., 2021). This approach utilizes anno- tations for sentences extracted from subtitles of UK political shows. The models are trained to classify statements into binary labels - claim or non-claim. Finally, a system called ClaimRank (Jaradat et al., 2018) aims to prioritize the sentences that fact- checkers should consider first for fact-checking. ClaimRank is trained on pre-existing annotations on political debates from 9 fact-checking organiza- tions. This approach first classifies the statement as check-worthy or not. The statements are then ranked based on the probabilities that the model as- signs to a statement toto the positive class. While these works are fundamental towards ap- proaching the problem of check-worthy claim iden- tification, the focus is only on a single domain (pol- itics). Additionally, the models rely on sentence- level human annotations, which is an expensive task, challenging to conduct regularly for multiple domains, and challeng- ing to conduct on a frequent basis across mul- tiple domains. This paper explores methodol- ogy to identify check-worthy claim sentences from fake news articles, irrespective of do- main, without explicit sentence-level annota- tions. We leverage two internal supervisory signals - headline and the abstractive sum- mary - to rank the sentences based on seman- tic similarity. We hypothesize that this rank- ing directly correlates to the check-worthiness of the sentences. To assess the effectiveness of this hypothesis, we build pipelines that leverage the ranking of sentences based on ei- ther the headline or the abstractive summary. The top-ranked sentences are used for the downstream fact-checking tasks of evidence retrieval and the article's veracity prediction by the pipeline. Our findings suggest that the top 3 ranked sentences contain enough infor- mation for evidence-based fact-checking of a fake news article. We also show that while the headline has more gisting similarity with how a fact-checking website writes a claim, the summary-based pipeline is the most promising for an end-to-end fact-checking system. Introduction With the rise of social media in recent years, it has become possible to disseminate fake news to mil- lions of people easily and quickly. An MIT media lab study (Vosoughi et al., 2018) from two years ago showed that false information goes six times farther and spreads much faster than real informa- tion. Additionally, personalization techniques have enabled targetting people with specific types of fake news based on their interests and confirma- tion biases. In response, there has been an increase in the number of fact-checking organizations that manually identify check-worthy claims and correct them based on evidence (Graves and Cherubini, 2016). However, a study shows that 50% of the lifetime spread of some very viral fake news hap- pens in the first 10 minutes, which limits the ability of manual fact-checking - a process that takes a day or two, sometimes a week1. Automating any part of the fact-checking process can help scale up the fact-checking efforts. Additionally, end-to-end automation can also enable human fact-checkers to devote more time to complex cases that require careful human judgment (Konstantinovskiy et al., 2021). End-to-end automated fact-checking systems in- volve three core objectives - (1) identifying check- worthy claims, (2) verifying claims against authori- tative sources, and (3) delivering corrections.",0
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively. CCS Concepts: • Computing methodologies → Information extraction. Additional Key Words and Phrases: speaker role identification, air traffic control, text classification, speech classification, spoken instruction understanding, multi-modal learning INTRODUCTION Speech communication between air traffic controllers (ATCOs) and pilots is one of the most important interaction ways in air traffic control (ATC) procedures. Recently, there is increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Lin 2021; Pardo et al. 2011]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [Smídl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. Subsequently, the TIU module converts the natural texts into predefined structured instructions that are further processed by the computer. Finally, the computer-readable instructions and the speaker role (ATCO or pilot which was output by the SRI module) jointly provide a conversation context for other downstream applications. As can be seen from mentioned illustrations, the SRI module is a critical component of the SIU system in the field of ATC. However, most of the existing research of the ATC SIU systems focuses on the ASR and TIU techniques [Lin et al. 2021a,c; Oualil et al. 2017; Zuluaga-Gomez et al. 2020], and no detailed description of the SRI task was presented. A instruction understanding model and ATC communication rule-based methods for the SRI tasks were studied in [Lin et al. 2019], without providing SRI performance. To the best of our knowledge, none of the published works have reported complete approaches and results for the SRI tasks in the ATC domain. Since the ATCO communicates with several pilots by radio in a single frequency, the role of the speaker cannot be distinguished from the communication data link. However, the speaker role is a kind of indispensable and important information in many ATC-related applications, such as safety detection systems, ATCO workload analysis systems. Therefore, the inability to identify the speaker role directly from communication brings a certain challenge to the ATC-related SIU tasks. Fortunately, there are two kinds of data that can be served as the potential entities for the SRI tasks. • Text: On the one hand, according to the communication rules recommended by the interna- tional civil aviation organization (ICAO), the ATCOs should declare the call sign of the target aircraft before issuing the instructions, while the pilots read back the instructions firstly and then reporting their call sign. In general, most of the controller-pilot speech communication follow these rules, allowing the text classification to be a promising technology for SRI tasks. • Speech: On the other hand, the speech can be considered as a representation of the speaker role from two aspects of signal and text. a) the controller-pilot speech communication presents distinctive features depending on the equipment and environment, such as a microphone, push-to-talk (PTT), background noise, radio. b) It implies the representation of its transcripts, which further provides more discriminative knowledge for the SRI task. In this paper, we define the SRI task as a binary classification problem, i.e., all the instructions are classified into two classes: ATCO or pilot. Meanwhile, the SRI task is addressed by the data- driven approaches from three different inputs, i.e., text, speech, speech-text. To this end, the text classification, audio classification, and multi-modal classification approaches are proposed to achieve the SRI task. In this procedure, several popular network architectures are introduced to serve as backbone networks for each approach to eliminate the impact of the difference between network architectures. The BiLSTM [Zhou et al. 2016], TextCNN [Kim 2014], and Transformer [Vaswani et al. 2017] architecture are developed as the backbone network in the text-based methods, while x-vector [Snyder et al. 2018], SincNet [Ravanelli and Bengio 2018a], and CRNN [Choi et al. 2017] architecture are built for the speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to learn the distinctive representations from both the speech and textual modalities for the speech-text based methods. Specifically, a modal attention mechanism is proposed to fuse the different representations to a joint feature vector. In addition, the self-attention pooling layer is applied to produce the joint vector by the weighted sum operations, which further be regarded as the multi-modal embedding. Finally, the multi-modal embedding is further fed into classification layers to generate the final probabilities of the speaker role. All the proposed methods were validated on the ATCSpeech corpus [Yang et al. 2020] that was collected from a real-world ATC environment. In addition, in order to analyze and compare the performance and robustness of the model, we evaluate the trained model in two ways: 1) The model is validated on the test set of the ATCSpeech to evaluate the performance on the seen samples. 2) A supplement test set called test-s is used to verify the robustness of the model on the unseen samples of in controller-pilot communication. In summary, our contributions are listed as follows: • A thorough comparison between the aforementioned deep learning based SRI techniques is investigated. To the best of our knowledge, this is the first work that investigates the SRI task in the ATC domain. • The robustness and performance of the comparative models are comprehensively analyzed and discussed on the seen and unseen samples. • A multi-modal model, called MMSRINet, is proposed to achieve the ATC-related SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5. Finally, this paper is concluded in Section 6.  RELATED WORK 2.1 Text Classification Text classification is a classical task in the field of natural language processing (NLP), which aims to classify a given text sequences into a certain class. In general, the approach can be grouped into two categories: rule-based methods and data-driven based methods. The rule-based approach usually requires a large number of predefined rules and is strongly dependent on domain knowledge, which can only be applied to limited scenarios due to poor flexibility. Thanks to the development of deep learning techniques, the performance of data-driven meth- ods has generally outperformed that of rule-based methods in recent years and has become the standard paradigm of text classification tasks [Minaee et al. 2021]. Zeng et al. [Kim 2014] utilized a convolutional neural network (CNN) [Lecun et al. 1998] to achieve the sentence classification tasks which makes representative progress in the NLP domain. To capture the long-term depen- dencies, the Att-BiLSTM model [Zhou et al. 2016] was built on a recurrent neural network (RNN) [Mikolov et al. 2010]. Currently, various improved methods based on the CNN or RNN block were proposed to achieve the text classification task, such as Character-level CNNs [Zhang et al. 2015], tree-based CNN [Mou et al. 2016], Tree-LSTM [Tai et al. 2015], Multi-Timescale LSTM [Liu et al. 2015]. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in text classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.2 Audio Classification Audio classification is widely applied in audio pattern recognition tasks, such as speaker identifica- tion [Ravanelli and Bengio 2018a; Snyder et al. 2018], acoustic event detection [Kumar and Raj 2016], accent classification [Hansen and Liu 2016; Lopez-Moreno et al. 2014], audio emotion recognition [Jermsittiparsert et al. 2020]. Recently, deep learning methods showed promising performance compared to traditional approaches for this task [Hershey et al. 2017]. Enormous works have been investigated to explore different model architectures and applications for audio classification. Shawn Hershey et al. [Hershey et al. 2017] demonstrated that the CNNs used in the image clas- sification task, such as AlexNet [Krizhevsky et al. 2012], VGG [Simonyan and Zisserman 2014], and ResNet [He et al. 2016], achieved desired performance for the large-scale audio classification task. Meanwhile, the convolutional recurrent neural network (CRNN) was proposed and to achieve music classification [Choi et al. 2017], audio event detection [Cakır et al. 2017], audio tagging [Xu et al. 2018], etc. In addition, in recent years, there are increasing interest in learning features from raw waveforms directly instead of handcraft features. Mirco Ravanelli et al. proposed the SincNet [Ravanelli and Bengio 2018a] to achieve speaker recognition which employs band-pass filters (based on the parametrized Sinc functions) in the first convolutional layer. Jee-weon Jung et al. proposed the RawNet [Jung et al. 2019] to improve the performance of the speaker verification from raw waveforms. In short, deep learning-based audio classification is still an interesting task in many applications. 2.3 Multi-modal Classification With the explosive growth of multi-modal data in the digit world, multi-modal learning is attracting increasing research interest and shows powerful performance than that of unimodal modal methods [Ngiam et al. 2011]. Various modalities can be used to achieve classification tasks, including audio- video [Nagrani et al. 2018], audio-text [Mittal et al. 2020], image-text [Gallo et al. 2018; Kiela et al. 2018], etc. In general, the fusion strategy of the classification task can be implemented in the following two ways: early fusion and later fusion [Baltrušaitis et al. 2019]. Early fusion methods fuse the multi-modal feature vectors to a joint representation that is further fed into the classifier, while the later fusion makes a second decision on the output of two classifiers by an extra strategy. Due to the advantages of early fusion in exploring the correlations and interactions between different modalities, in this paper, we introduce the early fusion methods to the SRI task. In an early work [Kiela and Bottou 2014], direct concatenation was employed to produce multi-modal joint representations. In order to identify the correlations of learned multi-modal features, a structural regularization was proposed in [Wu et al. 2014] to empower the deep neural network (DNN) based fusion layer, which also preserves the diversity of the different modality features. In addition to the classification task, more powerful fusion methods were successfully integrated into the ASR and NLP architectures. Modality attention was proposed to fuse the audio-visual features for ASR tasks in [Zhou et al. 2019]. The works of [Fukui et al. 2016] and [Ovalle et al. 2017] used compact bilinear pooling and complex gating mechanisms to obtain multi-modal representations. CONCLUSIONS In this paper, we presented a comprehensively comparative study for the SRI tasks using deep learning in the ATC domain. Three kinds of methods with different inputs were investigated to solve the problems of the SRI tasks, i.e., text-based methods, speech-based methods, and multi- modal methods. Firstly, we formulated the SRI task as the binary classification problem, and further refine the above methods as text classification, speech classification, and multi-modal classification task. Secondly, the efficacy of the above methods is confirmed by theoretical and experimental demonstrations. Finally, the experiments demonstrated that the proposed MMSRINet is a competitive approach that achieves the best performance and robustness in the seen and unseen ATC environments. In the future, we plan to explore more efficient and effective approaches for the SRI tasks. In addition, the fusion and application of the multi-modal data in the ATC environment would be also an interesting research topic.",1
"Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively. CCS Concepts: Computing methodologies Information extraction. Additional Key Words and Phrases: speaker role identification, air traffic control, text classification, speech classification, spoken instruction understanding, multi-modal learning INTRODUCTION Speech communication between air traffic controllers (ATCOs) and pilots is one of the most important interaction ways in air traffic control (ATC) procedures. Recently, there is increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Lin 2021; Pardo et al. 2011]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [Smidl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. Subsequently, the TIU module converts the natural texts into predefined structured instructions that are further processed by the computer. Finally, the computer-readable instructions and the speaker role (ATCO or pilot which was output by the SRI module) jointly provide a conversation context for other downstream applications. As can be seen from mentioned illustrations, the SRI module is a critical component of the SIU system in the field of ATC. However, most of the existing research of the ATC SIU systems focuses on the ASR and TIU techniques [Lin et al. 2021a,c; Oualil et al. 2017; Zuluaga-Gomez et al. 2020], and no detailed description of the SRI task was presented. A instruction understanding model and ATC communication rule-based methods for the SRI tasks were studied in [Lin et al. 2019], without providing SRI performance. To the best of our knowledge, none of the published works have reported complete approaches and results for the SRI tasks in the ATC domain. Since the ATCO communicates with several pilots by radio in a single frequency, the role of the speaker cannot be distinguished from the communication data link. However, the speaker role is a kind of indispensable and important information in many ATC-related applications, such as safety detection systems, ATCO workload analysis systems. Therefore, the inability to identify the speaker role directly from communication brings a certain challenge to the ATC-related SIU tasks. Fortunately, there are two kinds of data that can be served as the potential entities for the SRI tasks. Text: On the one hand, according to the communication rules recommended by the interna- tional civil aviation organization (ICAO),ATCOs should declare the call sign of the target aircraft before issuing the instructions, while the pilots read back the instructions firstly and then reporting their call sign. In general, most of the controller-pilot speech communication follow these rules, allowing the text classification to be a promising technology for SRI tasks. Speech: On the other hand, the speech can be considered as a representation of the speaker role from two aspects of signal and text. a) the controller-pilot speech communication presents distinctive features depending on the equipment and environment, such as a microphone, push-to-talk (PTT), background noise, radio. b) It implies the representation of its transcripts, which further provides more discriminative knowledge for the SRI task. In this paper, we define the SRI task as a binary classification problem, i.e., all the instructions are classified into two classes: ATCO or pilot. Meanwhile, the SRI task is addressed by the data- driven approaches from three different inputs, i.e., text, speech, speech-text. To this end, the text classification, audio classification, and multi-modal classification approaches are proposed to achieve the SRI task. In this procedure, several popular network architectures are introduced to serve as backbone networks for each approach to eliminate the impact of the difference between network architectures. The BiLSTM [Zhou et al. 2016], TextCNN [Kim 2014], and Transformer [Vaswani et al. 2017] architecture are developed as the backbone network in the text-based methods, while x-vector [Snyder et al. 2018], SincNet [Ravanelli and Bengio 2018a], and CRNN [Choi et al. 2017] architecture are built for the speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to learn the distinctive representations from both the speech and textual modalities for the speech-text based methods. Specifically, a modal attention mechanism is proposed to fuse the different representations to a joint feature vector. In addition, the self-attention pooling layer is applied to produce the joint vector by the weighted sum operations, which further be regarded as the multi-modal embedding. Finally, the multi-modal embedding is further fed into classification layers to generate the final probabilities of the speaker role. All the proposed methods were validated on the ATCSpeech corpus [Yang et al. 2020] that was collected from a real-world ATC environment. In addition, in order to analyze and compare the performance and robustness of the model, we evaluate the trained model in two ways: 1) The model is validated on the test set of the ATCSpeech to evaluate the performance on the seen samples. 2) A supplement test set called test-s is used to verify the robustness of the model on the unseen samples of in controller-pilot communication. In summary, our contributions are listed as follows: A thorough comparison between the aforementioned deep learning based SRI techniques is investigated. To the best of our knowledge, this is the first work that investigates the SRI task in the ATC domain. The robustness and performance of the comparative models are comprehensively analyzed and discussed on the seen and unseen samples. A multi-modal model, called MMSRINet, is proposed to achieve the ATC-related SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5. Finally, this paper is concluded in Section 6. RELATED WORK 2.1 Text Classification Text classification is a classical task in the field of natural language processing (NLP), which aims to classify a given text sequences into a certain class. In general, the approach can be grouped into two categories: rule-based methods and data-driven based methods. The rule-based approach usually requires a large number of predefined rules and is strongly dependent on domain knowledge, which can only be applied to limited scenarios due to poor flexibility. Thanks to the development of deep learning techniques, the performance of data-driven meth- ods has generally outperformed that of rule-based methods in recent years and has become the standard paradigm of text classification tasks [Minaee et2021]. Zeng et al. [Kim 2014] utilized a convolutional neural network (CNN) [Lecun et al. 1998] to achieve the sentence classification tasks which makes representative progress in the NLP domain. To capture the long-term depen- dencies, the Att-BiLSTM model [Zhou et al. 2016] was built on a recurrent neural network (RNN) [Mikolov et al. 2010]. Currently, various improved methods based on the CNN or RNN block were proposed to achieve the text classification task, such as Character-level CNNs [Zhang et al. 2015], tree-based CNN [Mou et al. 2016], Tree-LSTM [Tai et al. 2015], Multi-Timescale LSTM [Liu et al. 2015]. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in text classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.2 Audio Classification Audio classification is widely applied in audio pattern recognition tasks, such as speaker identifica- tion [Ravanelli and Bengio 2018a; Snyder et al. 2018], acoustic event detection [Kumar and Raj 2016], accent classification [Hansen and Liu 2016; Lopez-Moreno et al. 2014], audio emotion recognition [Jermsittiparsert et al. 2020]. Recently, deep learning methods showed promising performance compared to traditional approaches for this task [Hershey et al. 2017]. Enormous works have been investigated to explore different model architectures and applications for audio classification. Shawn Hershey et al. [Hershey et al. 2017] demonstrated that the CNNs used in the image clas- sification task, such as AlexNet [Krizhevsky et al. 2012], VGG [Simonyan and Zisserman 2014], and ResNet [He et al. 2016], achieved desired performance for the large-scale audio classification task. Meanwhile, the convolutional recurrent neural network (CRNN) was proposed and to achieve music classification [Choi et al. 2017] which employs band-pass filters or other parametrized methods. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in audio classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.3 Multi-modal Classification With the explosive growth of multi-modal data in the digit world, multi-modal learning is attracting increasing research interest and shows powerful performance than that of unimodal modal methods [Ngiam et al. 2011]. Various modalities can be used to achieve classification tasks, including audio- video [Nagrani et al. 2018], audio-text [Mittal et al. 2020], image-text [Gallo et al. 2018; Kiela et al. 2018], etc. In general, the fusion strategy of the classification task can be implemented in the following two ways: early fusion and later fusion [Baltrus aitis et al. 2019]. Early fusion methods fuse the multi-modal feature vectors to a joint representation that is further fed into the classifier, while the later fusion makes a second decision on the output of two classifiers by an extra strategy. Due to the advantages of early fusion in exploring the correlations and interactions between different modalities, in this paper, we introduce the early fusion methods to the SRI task. In an early work [Kiela and Bottou 2014], direct concatenation was employed to produce multi-modal joint representations. In order to identify the correlations of learned multi-modal features, a structural regularization was proposed in [Wu et al. 2014] to empower the deep neural network (DNN) based fusion layer, which also preserves the diversity of the different modality features. In addition to the classification task, more powerful fusion methods were successfully integrated into the ASR and NLP architectures. Modality attention was proposed to fuse the audio-visual features for ASR tasks in [Zhou et al.The works of [Fukui et al. 2016] and [Ovalle et al. 2017] used compact bilinear pooling and complex gating mechanisms to obtain multi-modal representations. CONCLUSIONS In this paper, we presented a comprehensively comparative study for the SRI tasks using deep learning in the ATC domain. Three kinds of methods with different inputs were investigated to solve the problems of the SRI tasks, i.e., text-based methods, speech-based methods, and speech and text based multi-modal methods. Firstly, we formulated the SRI task as the binary classification problem, and further refine the above methods as text classification, speech classification, and multi-modal classification tasks. Secondly, the efficacy of the above methods is confirmed by theoretical and experimental demonstrations. Finally, the experiments demonstrated that all the proposed methods achieved the SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5.",0
"Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): ● Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. ● Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1​ ​: ●  DSTC6 human evaluation data​ (Hori et al., 2017) ●  DSTC7 human evaluation data​ (Galley et al., 2019) ●  Persona-Chatlog dataset​ (See et al., 2019) ●  USR dataset​ (Mehri & Eskenazi, 2020) ●  FED dataset​ (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without even realizing of that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of generative approaches to repeat words from the user utterances or dialogue history. In this task, our goal is to evaluate the capability of generative dialogue systems to generate appropriate answers that can go beyond detecting toxicity and moderate the conversation by producing appropriate and correct answers that allow the system to continue with the dialogue. For this task a dataset of pairs of 100K messages (training and validation set) will be automatically collected with the following characteristics: ●  A toxic user sends a Tweet message using one or several of the most common swear words found on the Internet. The Tweet message must be directed to one of the customer service channels. ●  A toxic user writes a Tweet message using one or several swear words and the message is replied by another user. ●  A toxic user posts a message in Reddit using one or several swear words and the message is replied by another user. In all cases, for a pair to be considered as part of the dataset, the replied message must not contain any swear word or being classified as toxic or inappropriate. In addition, since many platforms forbid the usage of swear words we will be considering different variants of the swear words by using obfuscation techniques (Rojas-Galeano, 2017). Scripts will be provided to the participants to collect desired tweet training/validation splits based on the tweet IDs. A hidden test set will be reserved to evaluate the performance of the submitted systems. The desired system should generate responses that are polite, specific and semantically appropriate. Participants will be evaluated based on the objective similarity between the generated response and the original response (e.g. sentence embedding similarity, Deep AM-FM (Zhang et al., 2020), BLEU, ROUGE, etc). For the top-3 submission systems, a set of 100 responses will be manually evaluated allowing a subjective analysis of the best system and analysis of correlations between the human evaluations and the automatic scores.",1
"Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1: DSTC6 human evaluation data(Hori et al., 2017) DSTC7 human evaluation data (Galley et al., 2019) Persona-Chatlog dataset (See et al., 2019) USR dataset (Mehri & Eskenazi, 2020)  FED dataset (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without evenof that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of the chatbot to repeat words from the user utterances or dialogue history. In this task, our goal is to seek effective automatic dialogue evaluation metrics, which can perform robustly across a range of dialogue evaluation tasks. The metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the performance of the submitted evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: BLEU (PAL) or deep AM (Zhilin). For each evaluation item, a leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Explainable - the metrics should provide constructive and explicit feedback to the chatbot in terms of the quality of its delivered responses. For instance, if a chatbot is contradicting itself, the evaluation metrics should signal such behavior to the chatbot. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1: DSTC6 human evaluation data (Hori et al., 2017) DSTC7 human evaluation data(Galley et al., 2019) Persona-Chatlog dataset(See et al., 2019) USR dataset(Mehri & Eskenazi, 2020).",0
"Abstract This paper presents a deep neural architec- ture, for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our anal- ysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious ques- tions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset. Introduction Natural Language Sentence Matching (NLSM) is a fundamental task in Natural Language Pro- cessing(NLP) designed to identify similarities in terms of semantic and conceptual content between two input sentence. A wide range NLP tasks, such as Natural Language Inference(NLI), Para- phrase Identification, Question Answering, and Machine Comprehension (MC), are implemented with NLSM. The NLI, also known as Recog- nizing Textual Entailment (RTE), seeks to deter- mine whether a hypothesis can be deduced from a premise. This requires examining the semantic similarity between the hypothesis and the premise. In Paraphrase Identification, the goal is to deter- mine if two texts are paraphrases or not (Lan and Xu). In Machine Comprehension, the model must match the sentence structure of the passage to the question, pointing out where the answer is located (Lan and Xu). Question answering systems rely on NLSM in two ways: (1) question retrieval, (2) the answer selection. In question retrieval, the matching between the query and existing ques- tions, and in answer selection, the matching be- tween the query and existing answers are deter- mined. In fact, answer selection is used to discover the relationship between the query and answer and ranks all possible answers. These NLSM- based question answering systems can be applied in question-answering forums. Many websites use these question-answering forums and they can use these systems to answer their user’s questions. There are so many repetitive questions in these fo- rums that NLSM-based question answering sys- tems can use FAQs to answer users questions. Users will see the answers similar questions, when a new question is asked in the forum. Using this NLSM-based question answering systems user questions will be answered faster (Singh Tomar et al.). Multiple datasets can be accessed in NLSM, including Stanford Natural Language In- ference (SNLI) (Bowman et al., 2015), Multi- Genre Natural Language Inference (MultiNLI) (Williams et al., 2018), FarsTail (Amirkhani et al., 2021), SciTail (Khot et al., 2018), and more. One of the challenges when using these datasets is that in many existing samples the two input texts have the same and many similar words. In fact, both texts are very similar in appearance but express different meanings, or conversely, two texts have many different words, but the nature and meaning of both questions are very similar. The objective of NLSM models is to predict a category or scale value for a pair of input texts, which indicates their similarity or relationship. To achieve this goal, NLSM models generally use two main steps: (1) designing a model to obtain a proper representation of whatever text will be ana- lyzed so that it can extract semantic features from it. (2) By using the representation obtained from texts, a matching mechanism can be developed to extract complex interactions. In the current NLSM field, deep neural net- works are the preferred approach. To determine semantic features and relationships among sen- tences, researchers use convolutional and recur- rent neural networks. The structure of convo- lutional neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNN’s that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract text features, deep learn- ing networks require a lot of labeled data, which is one of the challenges of using deep learning, es- pecially in natural language processing. To extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data. The advantage of these pre-trained language models is that they do not need labeled data to be trained, so they can be trained on a large amount of unlabeled data and then fine-tuned them in downstream tasks. A tech- nique known as transfer learning is one of the best ways to resolve the challenges of natural language processing in low-resource languages today. In this paper, we used the following steps to de- sign a model based on NLSM principles identify- ing the semantic similarity of the input text pair: 1. Collecting Persian Religious question match- ing dataset for training and evaluating the proposed NLSM model. 2. Investigating related models to the field of NLSM. 3. Implementing ERT with Deep Recursive En- coder (BERT-DRE) model by adding recur- sive encoder module to BERT (Devlin et al., 2019). 4. Evaluating BERT-DRE with related models using the introduced religious and benchmark datasets. Dataset annotated for this study includes 18,000 samples, contains two questions, appropriate an- swers, and a match or not-match label for each question pair, which is used for designing a chat- bot. This dataset is collected by crawling reli- gious questions from religious question-answering websites, and then by annotating two similar ques- tions are annotated for each question, and dissim- ilar questions are generated automatically. It was noted our BERT-DRE model, which used the an- notated religious dataset to train and evaluate, was able to achieve an F1-score of 90.27% on the test data, making it the strongest model among the ones studied. Furthermore, in order to better eval- uate the BERT-DRE model, the model was trained using SNLI, MultiNLI, FarsTail, and SciTail and achieved appropriate F1-scores. Related Works Natural language processing studies have focused on NLSM, an essential part of many natural lan- guage tasks. In this regard, various datasets and models have been developed. There have been dif- ferent models developed for NLI, semantic match- ing, and paraphrase identification. Since all of these tasks are similar, the same model can be used today. In most cases, deep neural networks are used for this purpose. To this end, in the following, we shall examine the relevant works. As stated in (Wang et al., 2017), in contrast to previous work performed in text-similarity that adapted the sentence from one direction or used a word-to-word or sentence-to-sentence corre- spondence only, this study used bilateral multi- perspective matching from several perspectives. As a result, one of the main architectural pil- lars is based on Bidirectional LSTM. Furthermore, they examined the mechanism of different meth- ods, such as attention. This interaction between aggregate words is then examined from multiple perspectives. In (Wang et al., 2017) (BiMPM), sentences are matched bidirectionally with a new function calculating similar vectors. This method can result in similarity in multiple ways, which is called multi-perspective cosine similarity. Language objects internal structures and inter- actions of language objects need to be properly modeled to achieve a successful matching algo- rithm. A component of this goal is achieved in (Hu et al., 2014) through combining ideas from neu- ral networks’ implementation in image and audio processing. Besides demonstrating the hierarchi- cal structures of sentences in layers, this model also maintains rich matching patterns on differ- ent levels. The (He et al., 2015) (MPCNN) pre- sented work based on CNN, analyzing multiple perspectives of sentences. CNN first extracts fea- tures for each sentence, then compares representa- tions of the sentences at various levels of granular- ity, and then uses various pooling techniques. Fol- lowing that, multiple similarity metrics are used to compare sentences at multiple levels of granular- ity. The MPCNN model processes each sentence independently, and there is no interactivity until the final Fully-Connected layer, which results in the loss of a great deal of useful information. For this reason, (Cao et al., 2018) and (He et al., 2016) make changes to the MPCNN architecture. In (Cao et al., 2018) has also improved MPCNN, one of which is the use of pre-trained embedding instead of random embedding. In the next step, characters are used to extract features. Finally, an input layer based on attention is added between the embedding and the multi-perspective sentence modeling layers. Attention-based neural network models have been successfully used in an- swer selection, which is an important sub-task of question answering. These models often represent the question using a vector and match it by refer- ring to the candidate answers. However, questions and answers may be interrelated in complex ways that cannot be represented by single-vector repre- sentations. In (Tran and Niedereée, 2018), the idea of using multipurpose attention networks (MANs) is proposed, which aims to discover this complex relationship for ranking question and answer pairs. Unlike previous models, this paper does not turn the question into a single vector, but focuses on different parts of the question from several vectors to show its general meaning, and applies different stages of attention to learning the representations of the candidate answers. The study (Zhao et al., 2019) proposes an inter- active attention model for semantic text matching that uses the global matching matrix to learn repre- sentations for source and target texts during inter- active attention. This model can take a rich repre- sentation of source and target texts and derive an entirely related encoding. In (Chen et al., 2017), a model with a good performance called ESIM has a non-complex structure of LSTM layers and is cited in many later works as a base model. It has been claimed that recursive architecture can achieve good performance in local inference mod- eling and inference composition. In (Tay et al., 2018), a deep learning model named CAFE is presented in which alignment pairs are compared, compressed, and then spread to the upper layers to increase representation learning. Then, the alignment vectors are ex- pressed in scalar features through factorization layers. One of the other models that was able to achieve good accuracy in SNLI and MultiNLI datasets (Yin et al., 2016) is another deep learning model that is intended for modeling sentence pairs with CNNs named ABCNN. In this study, three different models of the combination of convolu- tional layers along with the attention mechanism have been used and the results of these three mod- els have been examined. Based on BERT representation of sentences and using convolution layers and the semantic role la- beler as features added to the model, (Zhang et al., 2020) has achieved good accuracy on MultiNLI and QNLI (Wang et al., 2018) data. Using a com- bination of CNN, attention mechanism, and resid- ual connection structures for inference, a deep learning method was proposed in (Yang et al., 2019) named RE2. The authors regard attention to pointwise features, previous alignments, and con- textual features as three success principles. This model has the advantage of being fast in calculat- ing model inferences. In comparison with other existing models, this model increases speed by six times and uses fewer parameters. In the (Kim et al., 2018) a deep learning model is proposed using the LSTM structure named DRCN. In this paper, the alignment module is used to extract the relationship between the words of the first and second text sequences. Residual connections are also used to transfer the extracted features to each layer. The embedding layer in this model includes trainable word embedding, non-trainable word embedding, character embedding, and ex- act match feature. In this model, due to the use of residual connections, the dimensions of the ex- tracted feature along with the network increase. In order to reduce the feature dimensions in this model, an autoencoder is used. Conclusion In this paper, we proposed a novel architecture by adding recursive encoder module with BERT. The proposed model uses BERT as an embedding layer and On top of the embedding layer, three bidi- rectional LSTM were used densely to represent the input text semantically. In order to pay more or less emphasis on different words, an attention module is applied to the outputs of LSTMs. This makes the semantic representations more informa- tive. These semantic representations are passed to a pooling layer consisting average and max pooling To make the resulting feature maps more robust to features’ positional changes. To show the potential of this architecture, we evaluated it in four benchmark datasets and got competitive results. The evaluation results in our annotated data show that the accuracy of proposed model is very acceptable. Our empirical results suggest that our proposed model improves the performance on some NLP benchmarks (e.g., FarsTail) with the state-of-the-art pre-trained models (e.g., BERT). This has been developed for the Persian/English language but it could be easily extended to other languages.",1
"Abstract This paper presents a deep neural architec- ture, for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our anal- ysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious ques- tions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset. Introduction Natural Language Sentence Matching (NLSM) is a fundamental task in Natural Language Pro- cessing(NLP) designed to identify similarities in terms of semantic and conceptual content between two input sentence. A wide range NLP tasks, such as Natural Language Inference(NLI), Para- phrase Identification, Question Answering, and Machine Comprehension (MC), are implemented with NLSM. The NLI, also known as Recog- nizing Textual Entailment (RTE), seeks to deter- mine whether a hypothesis can be deduced from a premise. This requires examining the semantic similarity between the hypothesis and the premise. In Paraphrase Identification, the goal is to deter- mine if two texts are paraphrases or not (Lan and Xu). In Machine Comprehension, the model must match the sentence structure of the passage to the question, pointing out where the answer is located (Lan and Xu). Question answering systems rely on NLSM in two ways: (1) question retrieval, (2) the answer selection. In question retrieval, the matching between the query and existing ques- tions, and in answer selection, the matching be- tween the query and existing answers are deter- mined. In fact, answer selection is used to discover the relationship between the query and answer and ranks all possible answers. These NLSM- based question answering systems can be applied in question-answering forums. Many websites use these question-answering forums and they can use these systems to answer their user's questions. There are so many repetitive questions in these fo- rums that NLSM-based question answering sys- tems can use FAQs to answer users questions. Users will see the answers similar questions, when a new question is asked in the forum. Using this NLSM-based question answering systems user questions will be answered faster (Singh Tomar et al.). Multiple datasets can be accessed in NLSM, including Stanford Natural Language In- ference (SNLI) (Bowman et al., 2015), Multi- Genre Natural Language Inference (MultiNLI) (Williams et al., 2018), FarsTail (Amirkhani et al., 2021), SciTail (Khot et al., 2018), and more. One of the challenges when using these datasets is that in many existing samples the two input texts have the same and many similar words. In fact, both texts are very similar in appearance but express different meanings, or conversely, two texts have many different words, but the nature and meaning of both questions are very similar. The objective of NLSM models is to predict a category or scale value for a pair of input texts, which indicates their similarity or relationship. To achieve this goal, NLSM models generally use two main steps: (1) designing a model to obtain a proper representation of whatever text will be ana- lyzed so that it can extract semantic features from it. (2) By using the representation obtained from texts, a matching mechanism can be developed to extract complex interactions. In the current NLSM field, deep neural net- works are the preferred approach. To determine semanticand relationships among sen- tences, researchers use convolutional and recur- rent neural networks. The structure of convo- lutional neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNN's that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data. The advantage of these pre-trained language models is that they do not need labeled data to be trained, so they can be trained on a large amount of unlabeled data and then fine-tuned them in downstream tasks. A tech- nique known as transfer learning is one of the best ways to resolve the challenges of natural language processing in low-resource languages today. In this paper, we used the following steps to de- sign a model based on NLSM principles identify- ing the semantic similarity of the input text pair: 1. Collecting Persian Religious question match- ing dataset for training and evaluating the proposed NLSM model. 2. Investigating related models to the field of NLSM. 3. Implementing ERT with Deep Recursive En- coder (BERT-DRE) model by adding recur- sive encoder module to BERT (Devlin et al., 2019). 4. Evaluating BERT-DRE with related models using the introduced religious and benchmark datasets. Dataset annotated for this study includes 18,000 samples, contains two questions, appropriate an- swers, and a match or not-match label for each question pair, which is used for designing a chat- bot. This dataset is collected by crawling reli- gious questions from religious question-answering websites, and then by annotating two similar ques- tions are annotated for each question, and dissim- ilar questions are generated automatically. It was noted our BERT-DRE model, which used the an- notated religious dataset to train and evaluate, was able to achieve an F1-score of 90.27% on the test data, making it the strongest model among the ones studied. Furthermore, in order to better eval- uate the BERT-DRE model, the model was trained using SNLI, MultiNLI, FarsTail, and SciTail and achieved appropriate F1-scores. Related Works Natural language processing studies have focused on NLSM, an essential part of many natural lan- guage tasks. In this regard, various datasets and models have been developed. There have been dif- ferent models developed for NLI, semantic match- ing, and paraphrase identification. Since all of these tasks are similar, the same model can be used today. In most cases, deep neural networks are used for this purpose. To this end, in the following, we shall examine the relevant works. As stated in (Wang et al., 2017), in contrast to previous work performed in text-similarity that adapted the sentence from one direction or used a word-to-word or sentence-to-sentence corre- spondence only, this study used bilateral multi- perspective matching from several perspectives. As a result, one of the main architectural pil- lars is based on Bidirectional LSTM. Furthermore, they examined the mechanism of different meth- ods, such as attention. This interaction between aggregate words is then examined from multiple perspectives. In (Wang et al., 2017) (BiMPM), sentences are matched bidirectionally with a new function calculating similar vectors. This method can result in similarity in multiple ways, which is called multi-perspective cosine similarity. Language objects internal structures and inter- actions of language objects need to be properly modeled to achieve a successful matchingrithm. A component of this goal is achieved in (Hu et al., 2014) through combining ideas from neu- ral networks' implementation in image and audio processing. Besides demonstrating the hierarchi- cal structures of sentences in layers, this model also maintains rich matching patterns on differ- ent levels. The (He et al., 2015) (MPCNN) pre- sented work based on CNN, analyzing multiple perspectives of sentences. CNN first extracts fea- tures for each sentence, then compares representa- tions of the sentences at various levels of granular- ity, and then uses various pooling techniques. Fol- lowing that, multiple similarity metrics are used to compare sentences at multiple levels of granular- ity. The MPCNN model processes each sentence independently, and there is no interactivity until the final Fully-Connected layer, which results in the loss of a great deal of useful information. For this reason, (Cao et al., 2018) and (He et al., 2016) make changes to the MPCNN architecture. In (Cao et al., 2018) has also improved MPCNN, one of which is the use of pre-trained embedding instead of random embedding. In the next step, characters are used to extract features. Finally, an input layer based on attention is added between the embedding and the multi-perspective sentence modeling layers. Attention-based neural network models have been successfully used in an- swer selection, which is an important sub-task of question answering. These models often represent the question using a vector and match it by refer- ring to the candidate answers. However, questions and answers may be interrelated in complex ways that cannot be represented by single-vector repre- sentations. In (Tran and Niedereee, 2018), the idea of using multipurpose attention networks (MANs) is proposed, which aims to discover this complex relationship for ranking question and answer pairs. Unlike previous models, this paper does not turn the question into a single vector, but focuses on different parts of the question from several vectors to show its general meaning, and applies different stages of attention to learning the representations of the candidate answers. The study (Zhao et al., 2019) proposes an inter- active attention model for semantic text matching that uses the global matching matrix to learn repre- sentations for source and target texts during inter- active attention. This model can take a rich repre- sentation of source and target texts and derive an entirely related encoding. In (Chen et al., 2017), a model with a good performance called ESIM has a non-complex structure of LSTM layers and is cited in many later works as a base model. It has been claimed that recursive architecture can achieve good performance in local inference mod- eling and inference composition. In (Tay et al., 2018), a deep learning model named CAFE is presented in which alignment pairs are compared, compressed, and then spread to the upper layers to increase representation learning. Then, the alignment vectors are ex- pressed in scalar features through factorization layers. One of the other models that was able to achieve good accuracy in SNLI and MultiNLI datasets (Yin et al., 2016) is another deep learning model that is intended for modeling sentence pairs with CNNs named ABCNN. In this study, three different models of the combination of convolu- tional layers along with the attention mechanism have been used and the results of these three mod- els have been examined. Based on BERT representation of sentences and using convolution layers and the semantic role la- beler as features added to the model, (Zhang et al., 2020) has achieved good accuracy on MultiNLI and QNLI (Wang et al., 2018) data. Using a com- bination of CNN, attention mechanism, and resid- ual connection structures for inference, a deep learning method was proposed in (Yang et al., 2019) named RE2. The authors regard attention to pointwise features, previous alignments, and con- textual features as three success principles. This model has the advantage of being fast in calculat- ing model inferences. In comparison with other existing models, this model increases speed by six times and uses fewer parameters. In the (Kim et al., 2018) a deep learning model is proposed using the LSTM structure named DRCN. In this paper, the alignment module is used to extract the relationship between the words of the first and second text sequences. Residual connections are also used to transfer the extractedto each layer. The embedding layer in this model includes trainable word embedding, non-trainable word embedding, character embedding, and ex- act match feature. In this model, due to the use of residual connections, the dimensions of the ex- tracted feature along with the network increase. In order to reduce the feature dimensions, the network is trained using recurrent neural networks. The structure of recurrent neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNN's that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract text features, deep learn- ing networks require a lot of labeled data, which is one of the challenges of using deep learning, es- pecially in natural language processing. To extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data.",0
"Keywords: Opinion Mining, Topic Modeling, Sentiment Analysis, Cross-Lingual, Multi-Lingual, Market Research. Abstract: User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics. on English and German newspaper and social media texts in Section 4. We aim at inspiring pragmatic ideas to explore the potential for comparative, inter- cultural market research and agenda setting studies. Unsolved problems and future potential are given in Section 5. RELATED WORK Topic modeling is meant to learn thematic structure from text corpora. With probabilistic topic model- ing methods, such as latent semantic indexing (LSI) (Deerwester et al., 1990) or latent Dirichlet allocation (LDA) (Blei et al., 2003), researchers try to extend the capabilities of topic modeling for application from a single language to multiple languages. Using multi- lingual dictionaries and translated corpora is an intu- itive way to tackle cross-lingual topic modeling prob- lems (Zhang et al., 2010; Vulic ́ et al., 2013). Further examples exist for topic modeling with either dictio- naries or translation text collections (Gutie ́rrez et al.,2016; Boyd-Graber and Blei, 2012; Jagarlamudi and Daume ́, 2010). However, this puts dependence on the availability of dictionary or good quality of transla- tions. Significant manual labor and verification are required to prevent deteriorating noise. Recently, methods converting words to vectors according to their semantics are widely adopted (Mikolov et al., 2013). Several studies showed text embeddings improve topic coherence (Bianchi et al., 2020; Srivastava and Sutton, 2017). Regarding multi- linguality, embeddings in word level and sentence level enable text in different languages to be projected to the same vector space (Cer et al., 2018) such that semantically similar texts are clustered together in- dependently of their languages. This favors studies on multi-lingual topic modeling without relying on dictionaries and translation (Xie et al., 2020; Chang et al., 2018). Although providing highly coherent top- ics, a recreation of word spaces is required when new text corpora are introduced. In our scenario, these limitations are not present. Regarding the application of topic modeling, vari- ous social media corpora are studied by domain ex- perts (Tsur et al., 2015; Ko et al., 2018). They covered different domains, such as politics, market- ing, and public health. Regarding media agenda set- ting, (Field et al., 2018) studied on how much degree a Russian newspaper related to economic downturn. They also ”introduced embedding-based methods for cross-lingually projecting English frame to Russian” based on Media Frames Corpus. In contrast, we pro- pose a straightforward topic modeling method with- out fine-tuning but only clustering necessary on a so- cial media corpus. This enables further investigation on media agenda setting cross-lingually and cross- culturally. CONCLUSION This case study shows that our technically simple ap- proach successfully generates an high proportion of relevant and coherent topics for our domain, i.e., or- ganic food products and related consumption behav- ior based on English and German social media texts. Moreover, the topics display the text contents cor- rectly and support a domain expert in the content analysis of social media texts written in multiple lan- guages. However, the presented paper did not provide quantitative measurements of topic coherences and comparisons with the state-of-the-art. For mono- language topic modeling, it would be LDA (Blei et al., 2003); for advanced cross-lingual topic modeling, it could be attention-based aspect extraction (He et al., 2017) utilizing aligned multi-lingual word vectors (Conneau et al., 2017). Several multi-lingual datasets would need to be included for a representative com- parison. Since pre-trained models trained on exter- nal data are used for the proposed method, it might be relevant for coherence score calculation to include intrinsic coherence scoring methods based on train test splits, such as, UMass coherence score (Mimno et al., 2011), and explore extrinsic methods calculated on external validation corpora, e.g., Wikipedia (Ro ̈der et al., 2015). Regarding multi-lingual sentiment analysis, the difference in the sentiment analysis frameworks for different languages must be considered. For example, since two independent but similar sentiment analysis models are applied for English and German,thesenti- ment distribution could be affected. Therefore, future studies on developing and evaluating comparable sentiment model should be conducted.",1
"Keywords: Opinion Mining, Topic Modeling, Sentiment Analysis, Cross-Lingual, Multi-Lingual, Market Research. Abstract: User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics. on English and German newspaper and social media texts in Section 4. We aim at inspiring pragmatic ideas to explore the potential for comparative, inter- cultural market research and agenda setting studies. Unsolved problems and future potential are given in Section 5. RELATED WORK Topic modeling is meant to learn thematic structure from text corpora. With probabilistic topic model- ing methods, such as latent semantic indexing (LSI) (Deerwester et al., 1990) or latent Dirichlet allocation (LDA) (Blei et al., 2003), researchers try to extend the capabilities of topic modeling for application from a single language to multiple languages. Using multi- lingual dictionaries and translated corpora is an intu- itive way to tackle cross-lingual topic modeling prob- lems (Zhang et al., 2010; Vulic et al., 2013). Further examples exist for topic modeling with either dictio- naries or translation text collections (Gutie rrez et al.,2016; Boyd-Graber and Blei, 2012; Jagarlamudi and Daume, 2010). However, this puts dependence on the availability of dictionary or good quality of transla- tions. Significant manual labor and verification are required to prevent deteriorating noise. Recently, methods converting words to vectors according to their semantics are widely adopted (Mikolov et al., 2013). Several studies showed text embeddings improve topic coherence (Bianchi et al., 2020; Srivastava and Sutton, 2017). Regarding multi- linguality, embeddings in word level and sentence level enable text in different languages to be projected to the same vector space (Cer et al., 2018) such that semantically similar texts are clustered together in- dependently of their languages. This favors studies on multi-lingual topic modeling without relying on dictionaries and translation (Xie et al., 2020; Chang et al., 2018). Although providing highly coherent top- ics, a recreation of word spaces is required when new text corpora are introduced. In our scenario, these limitations are not present. Regarding the application of topic modeling, vari- ous social media corpora are studied by domain ex- perts (Tsur et al., 2015; Ko et al., 2018). They covered different domains, such as politics, market- ing, and public health. Regarding media agenda set- ting, (Fieldal., 2018) studied on how much degree a Russian newspaper related to economic downturn. They also introduced embedding-based methods for cross-lingually projecting English frame to Russian based on Media Frames Corpus. In contrast, we pro- pose a straightforward topic modeling method with- out fine-tuning but only clustering necessary on a so- cial medium, i.e., a newspaper. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages sim- tions. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics.
Keywords: Opinion Mining, Topic Modeling, Sentiment Analysis, Cross-Lingual, Multi-Lingual, Market Research. Abstract: User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics. on English and German newspaper and social media texts in Section 4. We aim at inspiring pragmatic ideas to explore the potential for comparative, inter- cultural market research and agenda setting studies. Unsolved problems and future potential are given in Section 5. RELATED WORK Topic modeling is meant to learn thematic structure from text corpora. With probabilistic topic model- ing methods, such as latent semantic indexing (LSI) (Deerwester et al., 1990) or latent Dirichlet allocation (LDA) (Blei et al., 2003), researchers try to extend the capabilities of topic modeling for application from a single language to multiple languages. Using multi- lingual dictionaries and translated corpora is an intu- itive way to tackle cross-lingual topic modeling prob- lems (Zhang et al., 2010; Vulic et al., 2013). Further examples exist for topic modeling with either dictio- naries or translation text collections (Gutie rrez et al.,2016; Boyd-Graber and Blei, 2012; Jagarlamudi and Daume, 2010). However, this puts dependence on the availability of dictionary or good quality of transla- tions. Significant manual labor and verification are required to prevent deteriorating noise. Recently, methods converting words to vectors according to their semantics are widely adopted (Mikolov et al., 2013). Several studies showed text embeddings improve topic coherence (Bianchi et al., 2020; Srivastava and Sutton, 2017). Regarding multi- linguality, embeddings in word level and sentence level enable text in different languages to be projected to the same vector space (Cer et al., 2018) such that semantically similar texts are clustered together in- dependently of their languages. This favors studies on multi-lingual topic modeling without relying on dictionaries and translation (Xie et al., 2020; Chang et al., 2018). Although providing highly coherent top- ics, a recreation of word spaces is required when new text corpora are introduced. In our scenario, these limitations are not present. Regarding the application of topic modeling, vari- ous social media corpora are studied by domain ex- perts (Tsur et al., 2015; Ko et al., 2018). They covered different domains, such as politics, market- ing, and public health. Regarding media agenda set- ting, (Fieldal., 2018) studied on how much degree a Russian newspaper related to economic downturn. They also introduced embedding-based methods for cross-lingually projecting English frame to Russian based on Media Frames Corpus. In contrast, we pro- pose a straightforward topic modeling method with- out fine-tuning but only clustering necessary on a so- cial medium, i.e., a newspaper. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages sim- tions. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. INTRODUCTION Topic modeling on social media texts is difficult, since lack of data as well as spelling and grammati- cal errors can make the approach unfeasible. Dealing with multiple languages at the same time adds more complexity to the problem which oftentimes makes the approach unusable for domain experts. Thus, we propose a cross-lingually pre-trained deep neural network as a black box with very little textual pre- processing necessary before embedding the texts and forming their clustering and topic distributions. For our method, we leverage current research regarding multi-lingual topic modeling, see Section 2. We provide an extensive description of a simple method to support domain experts from specific so- cial media domains in its application in Section 3. We qualitatively demonstrate our topic model, its fea- sibility, and its cross-lingual semantic characteristics.
",0
"Abstract Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art meth- ods. However, resolving annotator bias pre- cisely and reliably is the key to understand annotators’ labeling behavior and to success- fully resolve corresponding individual miscon- ceptions and wrongdoings regarding the anno- tation task. Our contribution is an explana- tion and improvement for precise neural end- to-end bias modeling and ground truth esti- mation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has po- tential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators. Introduction Modeling annotator bias in conditions where each data point is annotated by multiple annotators, be- low referred to as multi-labeled crowdsourcing, has been investigated thoroughly. However, bias mod- eling when every data point is annotated by only one person, hereafter called singly labeled crowd- sourcing, poses a rather specific and difficult chal- lenge. It is in particular relevant for sentiment anal- ysis, where singly labeled crowdsourced datasets are prevalent. This is due to data from the social web which is annotated by the data creators them- selves, e.g., rating reviewers or categorizing image uploaders. This might further include multi-media contents such as audio, video, images, and other forms of texts. While the outlook for such forms of data is promising, end-to-end approaches have not yet been fully explored on these types of crowd- sourcing applications. With these benefits in mind, we propose a neural network model tailored for such data with singly labeled crowdsourced annotations. It computes a latent truth for each sample and the correct bias of every annotator while also considering input feature distribution during training. We modify the loss function such that the annotator bias con- verges towards the actual confusion matrix of the regarding annotator and thus models the annotator biases correctly. This is novel, as previous meth- ods either require a multi-labeled crowdsourcing setting (Dawid and Skene, 1979; Hovy et al., 2013) or do not produce a correct annotator bias during training which would equal the confusion matrix, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). A correct annotator- or annotator-group bias, however, is necessary to de- rive correct conclusions about the respective an- notator behavior. This is especially important for highly unreliable annotators who label a high num- ber of samples randomly – a setting, in which our proposed approach maintains its correctness, too. Our contributions are as follows. We describe the corresponding state-of-the-art for crowdsourc- ing algorithms and tasks in section 2. Our neural network model method for end-to-end crowdsourc- ing modeling is explained in section 3, which in- cludes a mathematical explanation that our linear bias modeling approach yields the actual confusion matrices. The experiments in section 4 underline our proof, show that the model handles annotator bias correctly as opposed to previous models, and demonstrate how the approach impacts classification. Related Work 2.1 Crowdsourcing Algorithms Problemdefinition. The need of data in the Growing research areas of machine learning has given rise to the generalized use of crowdsourcing. This method of data collection increases the amount of data, saves time and money but comes at the poten- tial cost of data quality. One of the key metrics of data quality is annotator reliability, which can be affected by various factors. For instance, the lack of rater accountability can entail spamming. Spam- mers are annotators that assign labels randomly and significantly reduce the quality of the data. Raykar and Yu (2012) and Hovy et al. (2013) addressed this issue by detecting spammers based on rater trustworthiness and the SpEM algorithm. How- ever, spammers are not the only source of label inconsistencies. The varied personal backgrounds of crowd workers often lead to annotator biases that affect the overall accuracy of the models. Sev- eral works have previously ranked crowd workers (Hovy et al., 2013; Whitehill et al., 2009; Yan et al., 2010), clustered annotators (Peldszus and Stede, 2013), captured sources of bias (Wauthier and Jor- dan, 2011) or modeled the varying difficulty of the annotation tasks (Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010) allowing for the elimi- nation of unreliable labels and the improvement of the model predictions. Ground truth estimation. One common chal- lenge in crowdsourced datasets is the ground truth estimation. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the raters’ abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al.,  2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data. Snow et al. (2008) proved that many non-expert annotators give a better performance than a few expert annotators and are cheaper in comparison. Singly Labeled Crowdsourced Datasets. Singly labeled datasets are an option given a fixed budget and unlimited data. Khetan et al. (2017) showed that it is possible to model worker quality with single labels even when the annotations are made by non-experts. Thus, multiple annotations can not only be redundant but come at the expense of fewer labeled samples. For singly labeled data, it can be distinguished between reviewer annotators and external annotators. Reviewer annotators rate samples they created themselves. It is common in forums for product and opinion reviews where a review is accompanied by a rating. As an example of this, we utilized the TripAdvisor dataset (Thel- wall, 2018). Further candidates are the Amazon review dataset (Ni et al., 2019), the Large Movie Review Dataset (Maas et al., 2011), and many more comprising sentiment. External annotators anno- tate samples they have not created. Experts are needed for complex annotation tasks requiring do- main knowledge. These are not crowdsourced, since the number of annotators is small and fixed. More common are external non-experts. Snow et al. (2008) showed that multi-labeled datasets anno- tated by non-expert improve performance. Khetan et al. (2017) showed that it also performs well in the singly labeled case. Thus, datasets made of singly labeled non-expert annotations can be cheaper, faster, and obtain performances compara- ble to those comprised of different types of annota- tions. Our organic dataset is annotated accordingly, see section 4.3. Conclusion We showed the efficacy of LTNet for modeling crowdsourced data and the inherent bias accurately and robustly. The bias matrices produced by our modified LTNet improve such that they are more similar to the actual bias between the latent truth and ground truth. Moreover, the produced bias shows high robustness under very noisy condi- tions making the approach potentially usable out- side of lab conditions. The latent truth, which is a hidden layer below all annotator biases, can be used for ground truth estimation in our sin- gle label crowdsourcing scenario, providing al- most identical ground truth estimates as pseudo labeling. Classification on three crowdsourced datasets show that LTNet approaches outperfom naive approaches not considering each annotator separately. The proposed log removal from the loss function showed better results on singly labeled crowdsourced datasets, but this observation needs further experiments to be substantiated. Further- more, there might be many use cases to explore the approach on other tasks than sentiment analysis.",1
"Abstract Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art meth- ods. However, resolving annotator bias pre- cisely and reliably is the key to understand annotators' labeling behavior and to success- fully resolve corresponding individual miscon- ceptions and wrongdoings regarding the anno- tation task. Our contribution is an explana- tion and improvement for precise neural end- to-end bias modeling and ground truth esti- mation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has po- tential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators. Introduction Modeling annotator bias in conditions where each data point is annotated by multiple annotators, be- low referred to as multi-labeled crowdsourcing, has been investigated thoroughly. However, bias mod- eling when every data point is annotated by only one person, hereafter called singly labeled crowd- sourcing, poses a rather specific and difficult chal- lenge. It is in particular relevant for sentiment anal- ysis, where singly labeled crowdsourced datasets are prevalent. This is due to data from the social web which is annotated by the data creators them- selves, e.g., rating reviewers or categorizing image uploaders. This might further include multi-media contents such as audio, video, images, and other forms of texts. While the outlook for such forms of data is promising, end-to-end approaches have not yet been fully explored on these types of crowd- sourcing applications. With these benefits in mind, we propose a neural network model tailored for such data with singly labeled crowdsourced annotations. It computes a latent truth for each sample and the correct bias of every annotator while also considering input feature distribution during training. We modify the loss function such that the annotator bias con- verges towards the actual confusion matrix of the regarding annotator and thus models the annotator biases correctly. This is novel, as previous meth- ods either require a multi-labeled crowdsourcing setting (Dawid and Skene, 1979; Hovy et al., 2013) or do not produce a correct annotator bias during training which would equal the confusion matrix, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). A correct annotator- or annotator-group bias, however, is necessary to de- rive correct conclusions about the respective an- notator behavior. This is especially important for highly unreliable annotators who label a high num- ber of samples randomly a setting, in which our proposed approach maintains its correctness, too. Our contributions are as follows. We describe the corresponding state-of-the-art for crowdsourc- ing algorithms and tasks in section 2. Our neural network model method for end-to-end crowdsourc- ing modeling is explained in section 3, which in- cludes a mathematical explanation that our linear bias modeling approach yields the actual confusion matrices. The experiments in section 4 underline our proof, show that the model handles annotator bias correctly as opposed to previous models, and demonstrate how the approach impacts classification. Related Work 2.1 Crowdsourcing Algorithms Problemdefinition. The need of data in the Growing research areas of machine learning has given rise to the generalized use of crowdsourcing. This method of data collection increases the amount of data, saves time and money but comes at the poten- tial cost of data quality. One of the key metrics of data quality is annotator reliability, which can be affected by various factors. For instance, the lack of rater accountability can entail spamming. Spam- mers are annotators that assign labels randomly and significantly reduce the quality of the data. Raykar and Yu (2012) and Hovy et al. (2013) addressed this issue by detecting spammers based on rater trustworthiness and the SpEM algorithm. How- ever, spammers are not the only source of label inconsistencies. The varied personal backgrounds of crowd workers often lead to annotator biases thatthe overall accuracy of the models. Sev- eral works have previously ranked crowd workers (Hovy et al., 2013; Whitehill et al., 2009; Yan et al., 2010), clustered annotators (Peldszus and Stede, 2013), captured sources of bias (Wauthier and Jor- dan, 2011) or modeled the varying difficulty of the annotation tasks (Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010) allowing for the elimi- nation of unreliable labels and the improvement of the model predictions. Ground truth estimation. One common chal- lenge in crowdsourced datasets is the ground truth estimation. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the raters abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al., 2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data. Snow et al. (2008) proved that many non-expert annotators give a better performance than a few expert annotators and are cheaper in comparison. Singly Labeled Crowdsourced Datasets. Singly labeled datasets are an option given a fixed budget and unlimited data.et al. (2017) showed that it is possible to model worker quality with single labels even when the annotations are made by non-experts. Thus, multiple annotations can not only be redundant but come at the expense of fewer labeled samples. For singly labeled data, it can be distinguished between reviewer annotators and external annotators. Reviewer annotators rate samples they created themselves. It is common in forums for product and opinion reviews where a review is accompanied by a rating. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the raters abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al., 2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018).",0
"Abstract This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be claimed that this corpus is currently the largest Persian corpus that has been prepared independently for the Persian language. This corpus is presented in both raw and preprocessed forms, and based on the preprocessed corpus some word embedding models are produced. By the provided models, the hmBlogs is compared with some of the most important corpora available in Persian, and the results show the superiority of the hmBlogs corpus over the others. These evaluations also present the importance and effects of corpora, evaluation datasets, model production methods, different hyperparameters and even the evaluation methods. In addition to evaluating the corpus and its produced language models, this research also presents a semantic analogy dataset. Keywords Persian Corpus, Blog, Word Embedding, Analogy Test Set Introduction Language corpora are basic resources in natural language processing (NLP). These corpora range from small to very big (including billions of words). They can include just raw text or have some meta-data such as tags and annotations. As a general rule, it can be said that a larger corpus leads to a more useful corpus. This is especially the case when the main reliance is on statistical methods. Persian (Farsi) has an especially wide range of speakers across the world, specifically in Iran, Tajikistan and Afghanistan and is used as either a native or second language. It is, therefore, especially important to develop the necessary NLP tools and resources for this language. A major downfall of Persian as a low resource language is its lack of sufficiently large, covered and up-to-date corpora. In recent years, the use of language models based on word embeddings has become a common phenomenon. These models are not only crucial and practical on their own, but also have deep neural networks applications. These models are produced by the corpora and their quality strongly depends to the used corpora. A basic requirement for a corpus is its ability to represent the language adequately and incorporate the necessary genres and domains. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora. Related work Among the various corpora (with different dimensions and areas) prepared for the Persian language, some are private and publicly unavailable and others are either publicly available or accessible to researchers. It can be claimed that the volume and variety of corpora available for the Persian language is much less than that available for other languages such as English, which is a laboratory language. Corpora can be obtained from different news websites, newspapers and magazines, books, blogs and tweets, and etc. Due to a lack of Persian content on the Internet in the past years, older sources, such as the Hamshahri corpus (AleAhmad et al., 2009), rely less on Internet-created data. Newer corpora, such as the MirasText (Sabeti et al., 2018), on the other hand, are made only from the Persian content found on the Internet; and, even the presence of snippets is due to the publication of those snippets on the Internet. The Hamshahri corpus (AleAhmad et al., 2009) is one of the oldest Persian corpora and was founded on the compilation of news and articles of the Hamshahri newspaper, one of the most famous newspapers in Iran. It contains more than 166 thousand news and more than 63 million words. Its content is dated between the years 1996 and 2002. Articles in this corpus are labeled in 12 main thematic categories. One problem with this corpus is its lack of up-to- date content, which means that it fails to include certain important language and content changes such as changes in language and linguistic framing, semantic drifts and appearing new names and events. Due to its news nature, it is also devoid of the different types of language uses common among Persian speakers. An advantage of this corpus, however, is its accuracy and lack of typographical errors of the compiled text. Peykareh (Bijankhan et al., 2011) is another early popular Persian corpus for NLP applications. This corpus contains 100 million words from a collection of news, web sites, and written sources. Peykareh texts range from the year 1978 to 2003. An important feature of this corpus from an NLP stance is its POS tagging, which is done semi-automatically. Of course, only about one tenth of this corpus has been labeled. Based on the word embedding training and the fact that its latest texts were compiled before 2004, Peykareh is a corpus of old texts. Furthermore, its size may not be large enough to make appropriate word embedding models. Another important resource used in NLP field is Wikipedia. The Persian Wikipedia (FaWiki) was launched in December 2003 and contained more than 775,000 articles by March 2021 (“Persian Wikipedia,” 2021). Persian is currently the 19th language on Wikipedia (List of Wikipedias, n.d.). Like the Hamshahri corpus, the FaWiki has an official language that cannot cover all language types. Furthermore, due to its encyclopedic nature, a large number of Wikipedia articles are too specialized for Persian speakers and everyday usage. For example, the vast majority of Persian speakers do not know the capital, or currency of many countries. Or many people are unfamiliar with the names of drugs, programming languages or specialized topics in astronomy or anatomy. The existence of such articles and the preparation of word embedding based on such can cause incorrect bias and a disconnect between the obtained model and the real language of the people. Of course, an important advantage of Wikipedia compared to others is its constant updates and continuous expansion. IrBlogs (AleAhmad et al., 2016) is a corpus that has been created by crawling Persian blogs. Blogs are one of the most important resources for making corpora and are a valuable and rich source of a variety of texts due to a diversity of authors and topics. IrBlogs is the result of the crawl of more than 564 thousand blogs and includes nearly 5 million posts. It contains posts collected from 2002 to 2013 to study the Persian blogging space. This corpus also includes posts that are not in Persian. The existence of posts with other languages impairs the preparation of a suitable language model. Also, the newest texts of irBlogs are about 7 years old and do not include the most recent changes such as new names and events. Another text corpus prepared for the Persian language is the MirasText (Sabeti et al., 2018). When published in 2018, the MirasText was considered to be the largest corpus available in Persian. It contains about 2.8 million documents and about 1.4 billion words collected from around 250 websites, of which at least 150 sites of them are news websites. In addition to plain text, this corpus provides information, such as titles, descriptions, and keywords for its documents. The Persian Raw Text (PRT) (Persiannlp/Persian-Raw-Text, 2020/2021) is a corpus provided by putting together a variety of Persian corpora (including MirasText and FaWiki) and is not produced independently. Also, no special processing has been performed on it to detect any duplication. The main part of PRT is obtained from the Persian section of the Common Crawl project (Common Crawl, n.d.). Common Crawl is a web crawl project that crawls and collects resources available on the web in any language, including some in Persian. PRT is larger than hmBlogs in size, but as shown in the evaluation section, it scores lower than hmBlogs. Conclusion This paper aimed to introduce a large, open and general corpus for the Persian language, as a low resource language. The introduced corpus is, by far, the largest of all the few independent and publicly available Persian corpora (with the exception of PRT, which is not an independent corpus but a collection of several corpora). The hmBlogs corpus is a rather large corpus even when compared to the English corpora (for example see (English Corpora, n.d.)). HmBlogs has had the latest style of Persian writing in recent years and has preserved texts from the last 15 years in the Persian blog space. HmBlogs showed overall better performance than other corpora such as irBlogs, PRT and FaWiki in word embedding models. A new analogy dataset, (FATS), was also presented as a side product of this research along with the obtained models. Also all factors including the corpora, evaluation data, evaluation methods, model construction methods and hyperparameters were found to have notable effects on the evaluation output. One of the challenges is keeping such corpora up to date. To solve this problem, it may be necessary to design processes that can keep corpora with the latest changes and adding new content. Designing such a process and its related systems could be a topic for future studies. The hmBlogs can also be considered for further development with new resources, such as crawling and adding post comments, in the future. Another possibility could be combining the corpus with other sources, such as news sources, the Persian wiki, and social media data. Tokenization poses another challenge to the Persian language because many words in Persian are multi-token. Even in some cases, particularly in some Persian verbs, tokens of a word are not consecutive and many words can fall between the gaps. Analyzing this phenomenon was beyond the scope of this research and should be addressed independently, but further research in this area can help to increase the quality of language models produced from corpora. A further task can also be the labeling of the present corpus. Topic modeling, official/informal/colloquial language labeling, tagging the polarity of posts, and annotating highly similar or nearly duplicated contents are all topics for further studies.",1
"Abstract This paper introduces the hmBlogs corpus for Persian, as a low resource language. This corpus has been prepared based on a collection of nearly 20 million blog posts over a period of about 15 years from a space of Persian blogs and includes more than 6.8 billion tokens. It can be claimed that this corpus is currently the largest Persian corpus that has been prepared independently for the Persian language. This corpus is presented in both raw and preprocessed forms, and based on the preprocessed corpus some word embedding models are produced. By the provided models, the hmBlogs is compared with some of the most important corpora available in Persian, and the results show the superiority of the hmBlogs corpus over the others. These evaluations also present the importance and effects of corpora, evaluation datasets, model production methods, different hyperparameters and even the evaluation methods. In addition to evaluating the corpus and its produced language models, this research also presents a semantic analogy dataset. Keywords Persian Corpus, Blog, Word Embedding, Analogy Test Set Introduction Language corpora are basic resources in natural language processing (NLP). These corpora range from small to very big (including billions of words). They can include just raw text or have some meta-data such as tags and annotations. As a general rule, it can be said that a larger corpus leads to a more useful corpus. This is especially the case when the main reliance is on statistical methods. Persian (Farsi) has an especially wide range of speakers across the world, specifically in Iran, Tajikistan and Afghanistan and is used as either a native or second language. It is, therefore, especially important to develop the necessary NLP tools and resources for this language. A major downfall of Persian as a low resource language is its lack of sufficiently large, covered and up-to-date corpora. In recent years, the use of language models based on word embeddings has become a common phenomenon. These models are not only crucial and practical on their own, but also have deep neural networks applications. These models are produced by the corpora and their quality strongly depends to the used corpora. A basic requirement for a corpus is its ability to represent the language adequately and incorporate the necessary genres and domains. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora. Related work Among the various corpora (with different dimensions and areas) prepared for the Persian language, some are private and publicly unavailable and others are either publicly available or accessible to researchers. It can be claimed that the volume and variety of corpora available for the Persian language is much less than that available for other languages such as English, which is a laboratory language. Corpora can be obtained from different news websites, newspapers and magazines, books, blogs and tweets, and etc. Due to a lack of Persian content on the Internet in the past years, older sources, such as the Hamshahri corpus (AleAhmad et al., 2009), rely less on Internet-created data. Newer corpora, such as the MirasText (Sabeti et al., 2018), on the other hand, are made only from the Persian content found on the Internet; and, even the presence of snippets is due to the publication of those snippets on the Internet. The Hamshahri corpus (AleAhmad et al., 2009) is one of the oldest Persian corpora and was founded on the compilation of news and articles of the Hamshahri newspaper, one of the most famous newspapers in Iran. It contains more than 166 thousand news and more than 63 million words. Its content is dated between the years 1996 and 2002. Articles in this corpus are labeled in 12 main thematic categories. One problem with this corpus is its lack of up-to- date content, which means that it fails to include certain important language and content changes such as changes in language and linguistic framing, semantic drifts and appearing new names and events. Due to its news nature, it is also devoid of the different types of language uses common among Persian speakers. An advantage of this corpus, however, is its accuracy and lack of typographical errors of the compiled text. Peykareh (Bijankhanal., 2011) is another early popular Persian corpus for NLP applications. This corpus contains 100 million words from a collection of news, web sites, and written sources. Peykareh texts range from the year 1978 to 2003. An important feature of this corpus from an NLP stance is its POS tagging, which is done semi-automatically. Of course, only about one tenth of this corpus has been labeled. Based on the word embedding training and the fact that its latest texts were compiled before 2004, Peykareh is a corpus of old texts. Furthermore, its size may not be large enough to make appropriate word embedding models. Another important resource used in NLP field is Wikipedia. The Persian Wikipedia (FaWiki) was launched in December 2003 and contained more than 775,000 articles by March 2021 (Persian Wikipedia, 2021). Persian is currently the 19th language on Wikipedia (List of Wikipedias, n.d.). Like the Hamshahri corpus, the FaWiki has an official language that cannot cover all language types. Furthermore, due to its encyclopedic nature, a large number of Wikipedia articles are too specialized for Persian speakers and everyday usage. For example, the vast majority of Persian speakers do not know the capital, or currency of many countries. Or many people are unfamiliar with the names of drugs, programming languages or specialized topics in astronomy or anatomy. The existence of such articles and the preparation of word embedding based on such can cause incorrect bias and a disconnect between the obtained model and the real language of the people. Of course, an important advantage of Wikipedia compared to others is its constant updates and continuous expansion. IrBlogs (AleAhmad et al., 2016) is a corpus that has been created by crawling Persian blogs. Blogs are one of the most important resources for making corpora and are a valuable and rich source of a variety of texts due to a diversity of authors and topics. IrBlogs is the result of the crawl of more than 564 thousand blogs and includes nearly 5 million posts. It contains posts collected from 2002 to 2013 to study the Persian blogging space. This corpus also includes posts that are not in Persian. The existence of posts with other languages impairs the preparation of a suitable language model. Also, the newest texts of irBlogs are about 7 years old and do not include the most recent changes such as new names and events. Another text corpus prepared for the Persian language is the MirasText (Sabeti et al., 2018). When published in 2018, the MirasText was considered to be the largest corpus available in Persian. It contains about 2.8 million documents and about 1.4 billion words collected from around 250 websites, of which at least 150 sites of them are news websites. In addition to plain text, this corpus provides information, such as titles, descriptions, and keywords for its documents. The Persian Raw Text (PRT) (Persiannlp/Persian-Raw-Text, 2020/2021) is a corpus provided by putting together a variety of Persian corpora (including MirasText and FaWiki) and is not produced independently. Also, no special processing has been performed on it to detect any duplication. The main part of PRT is obtained from the Persian section of the Common Crawl project (Common Crawl, n.d.). Common Crawl is a web crawl project that crawls and collects resources available on the web in any language, including some in Persian. PRT is larger than hmBlogs in size, but as shown in the evaluation section, it scores lower than hmBlogs. Conclusion This paper aimed to introduce a large, open and general corpus for the Persian language, as a low resource language. The introduced corpus is, by far, the largest of all the few independent and publicly available Persian corpora (with the exception of PRT, which is not an independent corpus but a collection of several corpora). The hmBlogs corpus is a rather large corpus even when compared to the English corpora (for example see (English Corpora, n.d.)). HmBlogs has had the latest style of Persian writing in recent years and has preserved texts from the last 15 years in the Persian blog space. HmBlogs showed overall better performance than other corpora such as irBlogs, PRT and FaWiki in word embedding models. A new analogy dataset, (FATS), was also presented as a side product of this research along with the obtained models. Also all factors including the corpora, evaluation data, evaluation methods, model construction methods and hyperparameters were found to have notable effects on the evaluationOne of the challenges is keeping such corpora up to date. To solve this problem, it may be necessary to design processes that can keep corpora with the latest changes and adding new content. Designing such a process and its related systems could be a topic for future studies. The use of language models based on word embeddings is another recent phenomenon. These models are not only crucial and practical on their own, but also have deep neural networks applications. These models are produced by the corpora and their quality strongly depends to the used corpora. A basic requirement for a corpus is its ability to represent the language adequately and incorporate the necessary genres and domains. The Persian language, however, lacks a widely available corpus with such features. The hmBlogs corpus is presented in this research as a large, accessible, public and diverse corpus of various language genres, and some of the word embedding models prepared by the hmBlogs corpus are also included. A set of analogy dataset prepared to evaluate this corpus is provided alongside, and the hmBlogs corpus is compared and evaluated based on the present dataset and two other datasets with some of the most important available Persian corpora.",0
"Abstract Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. Introduction Tremendous effort has gone into the research and development of task-oriented dialogue agents for English and a few other major languages in recent years. A methodology that can transfer the effort to other languages automatically will greatly benefit the large population of speakers of the many other languages in the world. Underlying an effective TOD agent is dialogue state tracking, the task of predicting a formal representation of the conversation sufficient for the dialogue agent to reply, in the form of slots and values. However, DST currently remains restricted to a few popular languages (Razumovskaia et al., 2021). Traditional DST agents require large handannotated Wizard-of-Oz (Kelley, 1984) datasets for training, which are prohibitively labor-intensive to produce in most languages (Gunasekara et al., 2020). Large, multi-domain WOZ datasets are only available in English and Chinese (Quan et al., 2020; Ye et al., 2021a). The contributions of this paper are as follows: 1. We propose an automatic technique to build high-quality multilingual data sets using machine translation. Machine translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too. 2. We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 3. We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA on RiSAWOZ (Quan et al., 2020) and CrossWOZ (Zhu et al., 2020), two large-scale multi-domain WoZ dialogue datasets, by 10.7% and 17% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective on translated data as evident by bigger performance improvement: on RiSAWOZ-EN-auto and RiSAWOZ-DE-auto datasets, automatically translated versions of RiSAWOZ, BART-CSP improves SOTA by 32.4% and 52.5%. Related Work 2.1 Dialogue State Tracking Dialogue state tracking (DST) refers to the task of predicting a formal state of a dialogue at its current turn, as a set of slot-value pairs at every turn. State-of-the-art approaches apply large transformer networks (Peng et al., 2020; Hosseini-Asl et al., 2020) to encode the full dialogue history in order to predict slot values. Other approaches include question-answering models (Gao et al., 2019), ontology matching in the finite case (Lee et al., 2019), or pointer-generator networks (Wu et al., 2019). Both zero-shot cross-lingual DST transfer (Ponti et al., 2018; Chen et al., 2018) and multilingual knowledge distillation (Hinton et al., 2015; Tan et al., 2019) have been investigated; however, training with translated data is the dominant approach, outperforming zero-shot and few-shot methods. 2.2 Contextual Semantic Parsing Alternatively to encoding the full dialogue history, previous work has proposed including the state as context (Lei et al., 2018; Heck et al., 2020; Ye et al., 2021b) together with the last agent and user utterance. Recently, Cheng et al. (2020b) proposed replacing the agent utterance with a formal representation as well. Existing models rely on custom encoder architectures and loss functions for the state (Heck et al., 2020). Our formulation of CSP is different since we encode the formal dialogue state directly as text, which simplifies the architecture and makes better use of the pretrained model’s understanding of natural text. Previous work also applied rule-based state trackers that compute the state based on the agent and user dialogue acts (Schulz et al., 2017; Zhong et al., 2018; Zhu et al., 2020). Such techniques cannot handle state changes outside of a state machine defined ahead of time and do not achieve state-of-the-art accuracy on WOZ dialogues. 2.3 Multilingual Dialogues Several multilingual dialogue benchmarks have been created over the past few years. Dialogue State Tracking Challenge (DSTC) has released several datasets (Kim et al., 2016; Hori et al., 2019; Gunasekara et al., 2020), covering only a few domains and languages. CrossWOZ (Zhu et al., 2020) and RiSAWOZ (Quan et al., 2020) are Chinese datasets collected through crowdsourcing which is expensive and human errors degrade quality. The creation of affordable high-quality datasets for other languages still remains a challenge (Razumovskaia et al., 2021). Conclusion Given a dialogue dataset in one language, this paper shows how to build contextual semantic parsers for a new language using automatically machinetranslated data. We propose an improved alignment approach for dialogues to ensure faithful translation of slot values. This removes the need for costly human-post editing used in all previous benchmarks. We show that the compounding effects of translation noise across turns can be mitigated with a CSP model for dialogue state tracking. By leveraging pretrained seq2seq models such as BART, training with CSP can outperform state-of-the-art results on RiSAWOZ, CrossWOZ, and MultiWOZ-ZH while remaining competitive on MultiWOZ despite not encoding any previous conversation turns or having access to a predefined ontology. Using our methodology, we create RiSAWOZ English and German, the first automatically created high-quality translated datasets for dialogue state tracking with no human-in-the-loop. We have implemented our methodology as a toolkit which developers can use to create a new multilingual dialogue datasets and contextual semantic parsers for it. Our new datasets and software will be released open-source upon publication.",1
"Abstract Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. Introduction Tremendous effort has gone into the research and development of task-oriented dialogue agents for English and a few other major languages in recent years. A methodology that can transfer the effort to other languages automatically will greatly benefit the large population of speakers of the many other languages in the world. Underlying an effective TOD agent is dialogue state tracking, the task of predicting a formal representation of the conversation sufficient for the dialogue agent to reply, in the form of slots and values. However, DST currently remains restricted to a few popular languages (Razumovskaia et al., 2021). Traditional DST agents require large handannotated Wizard-of-Oz (Kelley, 1984) datasets for training, which are prohibitively labor-intensive to produce in most languages (Gunasekara et al., 2020). Large, multi-domain WOZ datasets are only available in English and Chinese (Quan et al., 2020; Ye et al., 2021a). The contributions of this paper are as follows: 1. We propose an automatic technique to build high-quality multilingual data sets using machine translation. Machine translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too. 2. We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 3. We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA on RiSAWOZ (Quan et al., 2020) and CrossWOZ (Zhu et al., 2020), two large-scale multi-domain WoZ dialogue datasets, by 10.7% and 17% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective on translated data as evident by bigger performance improvement: onand RiSAWOZ-DE-auto datasets, automatically translated versions of RiSAWOZ, BART-CSP improves SOTA by 32.4% and 52.5%. Related Work 2.1 Dialogue State Tracking Dialogue state tracking (DST) refers to the task of predicting a formal state of a dialogue at its current turn, as a set of slot-value pairs at every turn. State-of-the-art approaches apply large transformer networks (Peng et al., 2020; Hosseini-Asl et al., 2020) to encode the full dialogue history in order to predict slot values. Other approaches include question-answering models (Gao et al., 2019), ontology matching in the finite case (Lee et al., 2019), or pointer-generator networks (Wu et al., 2019). Both zero-shot cross-lingual DST transfer (Ponti et al., 2018; Chen et al., 2018) and multilingual knowledge distillation (Hinton et al., 2015; Tan et al., 2019) have been investigated; however, training with translated data is the dominant approach, outperforming zero-shot and few-shot methods. 2.2 We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 2.3 We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA by 10.6% and 17.5% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective in translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in all previous benchmarks. 2.4 We present a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ-EN-auto and RiSAWOZ-DE-auto datasets, we improve our translation quality on both English and German by 11.6% and 17.5%. We also present a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterance. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ-EN-auto and RiSAWOZ-DE-auto datasets, we improve our translation quality on both English and German by 11.6% and 17.5%. We also present a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterance. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ-EN-auto and RiSAWOZ-DE-auto datasets, we improve our translation quality on both English and German by 11.6% and 17.5%. We also present a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterance. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks.",0
"Abstract Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on largescale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality. Introduction Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have achieved remarkable success in various natural language processing tasks (Sun et al., 2019). As such, there is a growing trend of using pre-trained language models for conversation modeling (Budzianowski and Vulic´, 2019; Zhang et al., 2019; Feng et al., 2021). For example, Zhang et al. (2019) proposed DialoGPT, a dialogue generation model that trains an extended GPT-2 (Radford et al., 2019) on large dialogue corpus. Feng et al. (2021) further explore the usage of DialoGPT for dialogue summarization. These pre-trained dialogue models are often pre-trained on large text corpora and fine-tuned on smaller dialogue datasets (Zhang et al., 2019). One limitation of PLM-based dialogue modeling, and even for other PLM tasks, is the trade-off between pre-training and fine-tuning (Ben-David et al., 2021). That is, the task-specific data used for fine-tuning is usually scarce and costly. As such, the reusability of prior knowledge learned in the pre-training phase can be limited during finetuning, hence some dialogue models are simply trained from scratch on the limited task-specific data. Consequently, recent works have resorted to prompt learning, a lightweight alternative to finetuning. Prompt learning keeps the PLM parameters frozen but optimizes only a small portion of task-specific prompts or related modules (Liu et al., 2021a; Shin et al., 2020; Liu et al., 2021b; Li and Liang, 2021). For example, Liu et al. (2021b) propose p-tuning, which preprends trainable prompt tokens to the input of a PLM. The trainable prompt embeddings are optimized while the PLM parameters are kept frozen. Prompt learning allows fewshot or nearly zero-shot learning for pre-trained models in new tasks with little or unlabeled data and it has been demonstrated to be substantially effective over fine-tuning in many tasks (Liu et al., 2021a; Qin and Eisner, 2021). However, applying prompt learning directly to conversation modeling is challenging. The general prompt-learning models assign universal prompt tokens to all inputs in the same task (Liu et al., 2021b). For example, prompts used for sentiment analysis share the same embeddings that are inferred from the training data (Liu et al., 2021b). In contrast, conversations are context-sensitive. Dialogue responses are affected by contextual information, such as the topic of discussion, pre-dialogue context, and participant personalities. “Blanket” prompts can restrict the expressiveness of prompt learning due to the lack of context-awareness, leading to sub-optimal performance in response generation. In this work, we present DialogPrompt, a novel prompt-based paradigm for response generation on top of large pre-trained language models. DialogPrompt prepends a sequence of prompt tokens to each dialogue context for eliciting response from large pre-trained language models. In order to construct context-aware prompts, we propose a conditional prompt encoder on top of the Transformer (Vaswani et al., 2017). The prompt tokens are initially encoded conditionally on the dialogue context. The resulting prompt encoding is then taken as the initial hidden state of the large PLM to generate responses. Compared to fine-tuning, DialogPrompt is encouraged to search proper prompts which controls the large PLMs into producing higher-quality responses directly. We evaluate DialogPrompt on popular multi-turn conversation datasets such as DailyDialog and MultiWOZ. Results show that DialogPrompt outperforms fine-tuning counterparts and other prompt tuning methods in terms of automated evaluation measures and the average length of generated responses. Human evaluation supports the superiority of our approach in generating informative and knowledgeable responses. Our contributions are summarized as follows: • To our knowledge, we are the first to propose prompt-based dialogue response generation. Our approach can better reuse knowledge from existing large-scale PLMs and produce more knowledgeable responses. • We design a novel conditional prompt encoder for encouraging context-aware prompt learning. • We extensively evaluated our approach on popular multi-turn conversation datasets and demonstrated the superiority of our approach in terms of quantitative automatic evaluations and qualitative human evaluations. Related Work This work is closely related to (1) pre-trained models for conversations, and (2) prompt learning for pre-trained language models. Pre-trained Models for Dialogue Generation. Recently, an emerging trend in dialogue generation explores the adaptation of large pre-trained language models on dialogue corpora (Golovanov et al., 2019; Zhang et al., 2019). For example, Golovanov et al. (2019) studied how pre-trained architectures can be adapted for natural language generation, comparing a number of architectural and training schemes. The state-of-the-art DialoGPT (Zhang et al., 2019) pre-trains a GPT-2 model on largescale conversation datasets and achieves a giant leap in performance against traditional conversation models. Another line of work related to exploiting the use of pre-trained models for task-oriented dialogues. For example, Budzianowski and Vulic´ (2019) proposed a task-oriented dialogue model that operates solely on text input. Their model is built on top of the TransferTransfo framework (Golovanov et al., 2019) that effectively bypasses explicit policy and language generation modules. TOD-BERT proposed by Wu et al. (2020) bridges the difference of general text and task-oriented dialogue by unifying nine human-human and multi-turn task-oriented dialogue datasets for language modeling. The model also incorporates user and system tokens into the masked language modeling and proposes a contrastive objective function to simulate the response selection task. Compared to these related works which directly fine-tune the dialogue model based on a pre-trained model, DialogPrompt is a novel paradigm for pretrained dialogue models which elicits knowledge from PLMs directly through minimal optimizing of prompt tokens. Prompt Learning for Pre-trained Language Models. There is a growing trend of automatically finding prompts to adapt pre-trained language models to downstream tasks (Shin et al., 2020; Li and Liang, 2021; Liu et al., 2021b). For example, Shin et al. (2020) proposed AutoPrompt which automatically optimizes prompts using a gradient signal. Unlike our method, AutoPrompt searches for hard prompts, thus it may be less versatile than the continuous methods. Instead, Liu et al. (2021b) proposed a continuous prompt tuning model named p-tuning. p-tuning optimizes fill-inthe-blank prompts in a continuous space, tested on GPT-2 and BERT models. A similar idea was proposed by Li and Liang (Li and Liang, 2021) who considered the tuning of prompts using a textual prefix. Specifically, they prepended a few taskspecific “soft tokens” (prefix) to the source text and tuned the hidden states of only these tokens (at all Transformer layers). Similarly, Lester et al. (2021) prepended a sequence of prompt tokens to the source text, but only the word embeddings of these tokens are optimized. Qin and Eisner (2021) proposed prompt-based learning on relation extraction tasks using data-dependent mixtures of prompt templates and parameters. Our method differs from existing prompt-based tuning methods in that we propose a novel contextaware prompt tuning mechanism that can optimize prompt encodings conditioned on dialogue contexts. Conclusion In this paper, we propose DialogPrompt, a novel prompt based response generation model. DialogPrompt prepends a prompt utterance to the dialogue context and only optimizes the prompt encoder. In order to adapt to different contexts, we propose a conditional prompt encoder that updates prompt activation based on the hidden states of context before response generation. Results on two popular conversation datasets, namely, DailyDialog and MultiWOZ show that DialogPrompt signifi- cantly outperforms fine-tuning counterparts and other prompt based models on both automatic and human evaluations. In the future, we will investigate prompt-based dialogue modeling based on more pre-trained language models.",1
"Abstract Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on largescale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality. Introduction Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have achieved remarkable success in various natural language processing tasks (Sun et al., 2019). As such, there is a growing trend of using pre-trained language models for conversation modeling (Budzianowski and Vulic, 2019; Zhang et al., 2019; Feng et al., 2021). For example, Zhang et al. (2019) proposed DialoGPT, a dialogue generation model that trains an extended GPT-2 (Radford et al., 2019) on large dialogue corpus. Feng et al. (2021) further explore the usage of DialoGPT for dialogue summarization. These pre-trained dialogue models are often pre-trained on large text corpora and fine-tuned on smaller dialogue datasets (Zhang et al., 2019). One limitation of PLM-based dialogue modeling, and even for other PLM tasks, is the trade-off between pre-training and fine-tuning (Ben-David et al., 2021). That is, the task-specific data used for fine-tuning is usually scarce and costly. As such, the reusability of prior knowledge learned in the pre-training phase can be limited during finetuning, hence some dialogue models are simply trained from scratch on the limited task-specific data. Consequently, recent works have resorted to prompt learning, a lightweight alternative to finetuning. Prompt learning keeps the PLM parameters frozen but optimizes only a small portion of task-specific prompts or related modules (Liu et al., 2021a; Shin et al., 2020; Liu et al., 2021b; Li and Liang, 2021). For example, Liu et al. (2021b) propose p-tuning, which preprends trainable prompt tokens to the input of a PLM. The trainable prompt embeddings are optimized while the PLM parameters are kept frozen. Prompt learning allows fewshot or nearly zero-shot learning for pre-trained models in new tasks with little or unlabeled data and it has been demonstrated to be substantially effective over fine-tuning in many tasks (Liu et al., 2021a; Qin and Eisner, 2021). However, applying prompt learning directly to conversation modeling is challenging. The general prompt-learning models assign universal prompt tokens to all inputs in the same task (Liu et al., 2021b). For example, prompts used for sentiment analysis share the same embeddings that are inferred from the training data (Liu et al., 2021b). In contrast, conversations are context-sensitive. Dialogue responses are affected by contextual information, such as the topic of discussion, pre-dialogue context, and participant personalities. ""Blanket"" prompts can restrict the expressiveness of prompt learning due to the lack of context-awareness, leading to sub-optimal performance in response generation. In this work, we present DialogPrompt, a novel prompt-based paradigm for response generation on top of large pre-trained language models. DialogPrompt prepends a sequence of prompt tokens to each dialogue context for eliciting response from large pre-trained language models. In order to construct context-aware prompts, we propose a conditional prompt encoder on top of the Transformer (Vaswani et al., 2017). The prompt tokens are initially encoded conditionally on the dialogue context. The resulting prompt encoding istaken as the initial hidden state of the large PLM to generate responses. Compared to fine-tuning, DialogPrompt is encouraged to search proper prompts which controls the large PLMs into producing higher-quality responses directly. We evaluate DialogPrompt on popular multi-turn conversation datasets such as DailyDialog and MultiWOZ. Results show that DialogPrompt outperforms fine-tuning counterparts and other prompt tuning methods in terms of automated evaluation measures and the average length of generated responses. Human evaluation supports the superiority of our approach in generating informative and knowledgeable responses. Our contributions are summarized as follows: To our knowledge, we are the first to propose prompt-based dialogue response generation. Our approach can better reuse knowledge from existing large-scale PLMs and produce more knowledgeable responses. We design a novel conditional prompt encoder for encouraging context-aware prompt learning. We extensively evaluated our approach on popular multi-turn conversation datasets and demonstrated the superiority of our approach in terms of quantitative automatic evaluations and qualitative human evaluations. Related Work This work is closely related to (1) pre-trained models for conversations, and (2) prompt learning for pre-trained language models. Pre-trained Models for Dialogue Generation. Recently, an emerging trend in dialogue generation explores the adaptation of large pre-trained language models on dialogue corpora (Golovanov et al., 2019; Zhang et al., 2019). For example, Golovanov et al. (2019) studied how pre-trained architectures can be adapted for natural language generation, comparing a number of architectural and training schemes. The state-of-the-art DialoGPT (Zhang et al., 2019) pre-trains a GPT-2 model on largescale conversation datasets and achieves a giant leap in performance against traditional conversation models. Another line of work related to exploiting the use of pre-trained models for task-oriented dialogues. For example, Budzianowski and Vulic (2019) proposed a task-oriented dialogue model that operates solely on text input. Their model is built on top of the TransferTransfo framework (Golovanov et al., 2019) that effectively bypasses explicit policy and language generation modules. TOD-BERT proposed by Wu et al. (2020) bridges the difference of general text and task-oriented dialogue by unifying nine human-human and multi-turn task-oriented dialogue datasets for language modeling. The model also incorporates user and system tokens into the masked language modeling and proposes a contrastive objective function to simulate the response selection task. Compared to these related works which directly fine-tune the dialogue model based on a pre-trained model, DialogPrompt is a novel paradigm for pretrained dialogue models which elicits knowledge from PLMs directly through minimal optimizing of prompt tokens. Prompt Learning for Pre-trained Language Models. There is a growing trend of automatically finding prompts to adapt pre-trained language models to downstream tasks (Shin et al., 2020; Li and Liang, 2021; Liu et al., 2021b). For example, Shin et al. (2020) proposed AutoPrompt which automatically optimizes prompts using a gradient signal. Unlike our method, AutoPrompt searches for hard prompts, thus it may be less versatile than the continuous methods. Instead, Liu et al. (2021b) proposed a continuous prompt tuning model named p-tuning. p-tuning optimizes fill-inthe-blank prompts in a continuous space, tested on GPT-2 and BERT models. A similar idea was proposed by Li and Liang (Li and Liang, 2021) who considered the tuning of prompts using a textual prefix. Specifically, they prepended a few taskspecific soft tokens (prefix) to the source text and tuned the hidden states of only these tokens (at all Transformer layers). Similarly, Lester et al. (2021) prepended a sequence of prompt tokens to the source text, but only the word embeddings of these tokens are optimized. Qin and Eisner (2021) proposed prompt-based learning on relation extraction tasks using data-dependent mixtures of prompt templates and parameters. Our method differs from existing prompt-based tuning methods in that we propose a novel contextaware prompt tuning mechanism that can optimize prompt encodings conditioned on dialogue contexts. Conclusion In this paper, we propose DialogPrompt, a novel prompt based response generation model. DialogPrompt prepends a prompt utterance to the dialogue context and only optimizes the prompt encoder. In order to adapt to different contexts, we propose a conditional prompt encoder that updates the prompt activation based onhidden states of context before response generation. Results on two popular conversation datasets, namely, DailyDialog and MultiWOZ show that DialogPrompt signifi- cantly outperforms fine-tuning counterparts and other prompt based models on both automatic and human evaluations. In the future, we will investigate prompt-based dialogue modeling based on more pre-trained language models. ",0
"Abstract Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models’ performance in terms of Accuracy in LAMBADA and Children’s Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. To make this architecture more efficient, a plethora of approaches have been introduced (Kitaev et al., 2020; Beltagy et al., 2020; Child et al., 2019, inter alia), aiming to lower the computational complexity without sacrificing performance. Tay et al. (2020) summarizes these approaches and categorizes them based on type of attention mechanism in their survey. Yet, even with linear complexity, physical resources will always limit the amount of contextual information that can be handled simultaneously. In terms of language, entities represent a natural way to tie words together through large pieces of discource. In NLP, Coreference Resolution is the task that aims to identify and group these entity mentions together, when they refer to the same real world entity (Stylianou and Vlahavas, 2021). Therefore, Coreference Resolution presents a natural way to link the context that we are currently handling with distant information, far outside the capabilities model architectures and current hardware resources. However, while this information is present in the text, it is usually not annotated and when annotated are very sparse and in small quantities making it extremely difficult to train large LMs (Kunz and Hardmeier, 2019). In this paper we present a framework to effectively use coreference annotations to further finetune large PLMs, increasing their performance far more than just by fine-tuning on the same data. By using large PLMs we take advantage of existing resources that are expensive to reproduce and come with a big environmental cost (Strubell et al., 2019). What is more, fine-tuning takes advantage of the massive amount of data that essentially initialize the model, making it possible to introduce new capabilities to models with small amounts of annotated data. In our approach, we use GPT2 as our base model and extend its architecture with the addition of a new Entity-Gating layer that handles entity annotations along with a gating mechanism that handles information flow between the base model and the Entity-Gating layer. As such, our approach uses entity representations when they are available through both training and inference, without imposing any constraints to the model’s functionality. For our experiments, we compare the performance of GPT2, post and pre fine-tuning, with and without our changes in a series of relative tasks. Furthermore, since most coreference annotated datasets are either very small or hard to acquire, we use GUMBY (Gessler et al., 2020) as our fine-tuning dataset which is a model annotated corpus. In addition, by using noisy annotations we aim to show the resilience of our approach to noise and the universality of our framework. Our results highlight the effects of our framework in language modeling, modeling long-range dependencies, and in specific word types where our fine-tuned model achieves better performance than GPT2. Conclusions and Future Work In this paper we presented CoreLM, a modular Fine-Tuning framework for PLMs to exploit modelcreated coreference annotations in order to create better mention representations and an overall better LM. In our experiments we showcased a performance increase when evaluating in a zero-shot setting, compared to the similarly fine-tuned model, even when the fine-tuning corpus did not generalize well to the end tasks. Our analysis shows that coreference annotations play a significant role in both Fine-Tuning and in downstream task performance, with correct annotations leading to better performance when used. In addition, our work helps in adding a new frontier to Coreference Resolution through the effective use of coreference annotations in Language Modeling. In this paper we showcased the effects of coreference annotation even when the information is within the context window of the model. Using coreference annotations can further lead to the decrease of the required context window and boost approaches like Shortformer (Press et al., 2020), leading to better and more efficient LMs. In the future we aim to create a more efficient approach to LM through the use of both intralinguistic (Coreference) and extra-linguistic (KG) features. Undeniably, KGs provide a means for structured, high quality information that cannot be found in a single text. We believe that an information fusion from coreference annotation and graph nodes, along with short context window will not be computationally prohibitive and lead in better, information rich, LMs.",1
"Abstract Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models performance in terms of Accuracy in LAMBADA and Children's Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. To make this architecture more efficient, a plethora of approaches have been introduced (Kitaev et al., 2020; Beltagy et al., 2020; Child et al., 2019, inter alia), aiming to lower the computational complexity without sacrificing performance. Tay et al. (2020) summarizes these approaches and categorizes them based on type of attention mechanism in their survey. Yet, even with linear complexity, physical resources will always limit the amount of contextual information that can be handled simultaneously. In terms of language, entities represent a natural way to tie words together through large pieces of discource. In NLP, Coreference Resolution is the task that aims to identify and group these entity mentions together, when they refer to the same real world entity (Stylianou and Vlahavas, 2021). Therefore, Coreference Resolution presents a natural way to link the context that we are currently handling with distant information, far outside the capabilities model architectures and current hardware resources. However, while this information is present in the text, it is usually not annotated and when annotated are very sparse and in small quantities making it extremely difficult to train large LMs (Kunz and Hardmeier, 2019). In this paper we present a framework to effectively use coreference annotations to further finetune large PLMs, increasing their performance far more than just by fine-tuning on the same data. By using large PLMs we take advantage of existing resources that are expensive to reproduce and come with a big environmental cost (Strubell et al., 2019). What is more, fine-tuning takes advantage of the massive amount of data that essentially initialize the model, making it possible to introduce new capabilities to models with small amounts of annotated data. In our approach, we use GPT2 as our base model and extend its architecture with the addition of a new Entity-Gating layer that handles entity annotations along with a gating mechanism that handles information flow between the base model and the Entity-Gating layer. As such, our approach uses entity representations when they are available through both training and inference, without imposing any constraints to the model's functionality. For our experiments, we compare the performance of GPT2, post and pre fine-tuning, with and without our changes in a series of relative tasks. Furthermore, since most coreference annotated datasets are either very small or hard to acquire, we use GUMBY (Gessler et al., 2020) as our fine-tuning dataset which is a model annotated corpus. In addition, by using noisy annotations we aim to show the resilience of our approach to noise anduniversality of our framework. Our results highlight the effects of our framework in language modeling, modeling long-range dependencies, and in specific word types where our fine-tuned model achieves better performance than GPT2. Conclusions and Future Work In this paper we presented CoreLM, a modular Fine-Tuning framework for PLMs to exploit the Transformers architecture and make available information outside the contextual space of the model. By using large datasets, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models' performance in terms of Accuracy in LAMBADA and Children's Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance.",0
"Abstract: In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients. Keywords: MeQA, AEMPS, Question Answering.  Introduction The leaflets for medicinal products for human use include their complete composition and instructions for their administration, use, and storage. Adverse effects, their interactions and contraindications are also specified. In addition, the text is written clearly, and they have to pass a readability test 4 so that the number of possible lexical, syntactic, or semantic errors is very low. These characteristics make leaflets a relatively easy resource to process using Natural Language Processing (NLP) techniques. This article presents the description of a project of the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ¿El ibuprofeno está contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ¿el Ibuprofeno acelera la pérdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called “Doctor Answers”, in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals. In section 2 the state of the art in QA is reviewed, in section 3 the main features of MeQA are described, section 4 describes the evaluation process, and finally section 5 contains the conclusions and future work. State of the Art QA systems can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, they suffer from misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (Fan et al., 2019), where questions require a long answer; community QA, ComQA (Abujabal et al., 2019), which makes use of data sets of pairs of questions and answers created by a certain community such as Quora or Stack Overflow. In addition, this task is also present in other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (Gordon et al., 2017), or Visual Question Answering, VQA (Antol et al. ., 2015), in which the objective is to answer questions about certain elements present in photographs, such as the color of certain elements, objects, etc. It is such an active field of AI that a new task called Embodied Question Answering (Das et al., 2018) has recently been created, which consists of generating an agent at a random location in a 3D environment and asking it a question (such as ¿What color is the car?). To answer, the agent must first intelligently navigate to explore the environment, collect the necessary visual information through first-person vision, and then answer the question (e.g. orange). This task combines different fields of AI such as language comprehension (LU), visual recognition, active perception, goal-based navigation, common sense reasoning, long-term memory, and conversion of language into actions. Within the biomedical domain, BioASQ (Tsatsaronis et al., 2015) has organized several QA tasks from structured data and free text, as well as the Medical Question Answering tasks organized in the TREC (Ben Abacha et al., 2017). The proposed system, MeQA, has been developed following the paradigm of QA systems based on information retrieval and, as we will see, it combines machine learning and deep learning techniques. It has the advantage of being easily expandable to other languages, as it does not need to annotate large amounts of question-answer pairs, something that QA systems based on complete neural architectures such as encoder-decoders (also known as seq2seq) do need (Sutskever, Vinyals and Quoc, 2014), or those who perform fine-tuning on pre-trained models such as BERT (Devlin et al., 2019). Conclusions and Future Work This paper describes a system developed at the AEMPS, called MeQA, which allows answering questions about medicines through the leaflet. Its architecture has been shown and explained in a general way, and also the modules that compose it. MeQA can be very useful for users, since there are a large number of web pages offering a service called ""Doctor Answers"", in which most of the questions deal with medicines for human use that can be answered through the leaflet. MeQA has been evaluated both automatically and manually. The automatic evaluation has been carried out in a general way as well as of each of the described modules, obtaining, in general, an F1 performance of 87%. MeQA combines machine learning and deep learning methods, and although it is not a semi-supervised system, it is a low-supervision system. We believe that this is precisely the great advantage of MeQA over other approaches based entirely on deep learning. MeQA hardly needs annotated data to work, only the module that predicts the sections in which the answer is likely to be found uses annotated data, the rest of the modules are unsupervised. As mentioned, the annotation of this information is very simple, and very fast, but not the complete annotation of the answer, which would be needed to develop systems based entirely on deep learning. In the future, improvements are expected to increase the performance of the system. In particular, because the leaflets are divided into sections, MeQA is able to go directly to the predicted sections. However, it would be useful to have a module that allows to go through the entire leaflet (without splitting) and obtain those sections. As explained above, MeQA performs worst on very complex questions. A possible solution to assess would consist of dividing these questions into fragments, analyzing and answering each one of them, and combining the answers into one.",1
"Abstract: In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients. Keywords: MeQA, AEMPS, Question Answering. Introduction The leaflets for medicinal products for human use include their complete composition and instructions for their administration, use, and storage. Adverse effects, their interactions and contraindications are also specified. In addition, the text is written clearly, and they have to pass a readability test 4 so that the number of possible lexical, syntactic, or semantic errors is very low. These characteristics make leaflets a relatively easy resource to process using Natural Language Processing (NLP) techniques. This article presents the description of a project of the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ¿El ibuprofeno está contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ¿el Ibuprofeno acelera la p©rdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called ""Doctor Answers"", in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals. In section 2 the state of the art in QA is reviewed, in section 3 the main features of MeQA are described, section 4 describes the evaluation process, and finally section 5 contains the conclusions and future work. State of the Art QA systems can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, theyfrom misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (Doctor Who), where questions require a long answer; community QA, ComQA (Twice as Many As MeQA), which makes use of data sets of pairs of questions and answers created by a certain community such as Quora or Stack Overflow. In addition, this task is also present in other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (Doctor Who), or Visual Question Answering, VQA (GoT)). This task is also present in other fields of machine learning, called Image Question Answering, IQA (Intelligence), or Visual Question Answering, VQA (Doctor Who). Conclusions and Future Work This paper describes the state of the art in QA, and the main features so far are described in a general way. QA is based on a system called MedicAAD, which allows users to query structured databases. Its architecture has been shown and explained in a great deal, and its modules can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, they suffer from misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (Doctor Who), where questions require a long answer; community QA, ComQA (Twice as Many As MeQA), (which makes use of data sets of pairs of questions and answers created by a certain community such as Quora or Stack Overflow). Other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (Doctor Who), or Visual Question Answering, VQA (Doctor Who), make use of the term ""MedicAAD"" more than any other field of AI. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5 called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, they suffer from misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. A domain-specific QA is a project that belongs to the AEMPS, many of these QA systems have been developed over the years (e.g. Savia, Vinyals and Quoc, 2014), and some have even been used in the NLP community (e.g. Salgado-Torres, Raffel and Shazeer, 2020). The AEMPS is a project that belongs to the AEMPS, many of these QA systems have been developed over the years (e.g. Savia, Vinyals and Quoc, 2014), and some have even been used in the NLP community (e.g. Salgado-Torres, Raffel and Shazeer, 2020). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, theretrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages.",0
"ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformerbased models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model · Transformer · Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a novel bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed method’s loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. This methodology has been well adapted for use in natural language processing domain for developing language models for word embedding. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, which usually cause curse of dimensionality during testing because of unseen words in training set. Distributed representation is proposed to overcome this limitation like this, for word sequence containing word not seen in training set but similar to words in a seen sentence, the target word sequence can still obtain high probability because of similar word vectors with that of sentence in the training set[Bengio et al., 2003]. Moreover, Mikolov et al[Mikolov et al., 2013] produced many works to distributed representation such as Skip-gram, negative sampling, all of which make word embedding in NLP an essential methodology and corner stone in many tasks including machine translation speech recognition. However, the above-mentioned word level representations cannot capture information arising from various polysemy of word use across linguistic context. To address this issue, Peters et al[Peters et al., 2018] trained bidirectional LSTM with a coupled language model objective for producing word vectors, which led to that the derived representations improved the state of the art in diverse language understanding problems. Radford et al[Radford et al., 2018] proposed a framework with transformer as base architecture for achieving long-range dependency, the ablation study shows that apparent score drop without using transformers. All of the results show that contextualized representation are beneficial in language modelling. Furthermore, Devlin et al[Devlin et al., 2018] devised bidirectional encoder representation transformers which show significant progress in eleven natural language processing tasks. Although deep contextualized language model achieved excellent performance, huge amount of parameters of these models incur extremely high cost of computing hardware and computation time. Another line of research tries to address this problem. Lan et al[Lan et al., 2019] proposed A Lite BERT architecture which has 18 times fewer parameters and 1.7 times faster tranning time than a traditional BERT architecture. While Sanh et al[Sanh et al., 2019] presented a method to pretrain a smaller model that can be finetuned for the downstream task, and achieved a 1.4 times fewer parameter with 1.6 times faster inference. However, none of these studies tried to investigate the effect of attention layer’s information leakage, for the purpose of information recombination, on the language model’s training speed and language representation efficacy for the purpose of decreasing computational resources utilization and training time. Conclusion In this work, we propose a attention based bidirectional language model named text denoised autoencoder, in order to save the training time for bidirectional language models as well as reduce the computation time of context language representations for unsupervised applications. We conduct both reranking test and the semantic textual similarity tasks to validate the proposed method in downstream applications, the result of which demonstrate that the proposed text denoised autoencoder is apparently faster than the conventional BERT based method in terms of producing contextualized representation. Moreover, the proposed method yields context representations which have more beneficial effect for downstream applications, demonstrating its improved encoding ability when compared with that of BERT and GPT.",1
"ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformerbased models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model· Transformer · Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a novel bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed method's loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. This methodology has been well adapted for use in natural language processing domain for developing language models for word embedding. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, which usually cause curse of dimensionality during testing because of unseen words in training set. Distributed representation is proposed to overcome this limitation like this, for word sequence containing word not seen in training set but similar to words in a seen sentence, the target word sequence can still obtain high probability because of similar word vectors with that of sentence in the training set[Bengio et al., 2003]. Moreover, Mikolov et al[Mikolov et al., 2013] produced many works to distributed representation such as Skip-gram, negative sampling, all of which makeembedding in NLP an essential methodology and corner stone in many tasks including machine translation speech recognition. However, the above-mentioned word level representations cannot capture information arising from various polysemy of word use across linguistic context. To address this issue, Peters et al[Peters et al., 2018] trained bidirectional LSTM with a coupled language model objective, which has revealed significant improvements in learning natural language representation. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformerbased models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model· Transformer· Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation.",0
"Abstract Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine transla- tion systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of gram- matical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to uti- lize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmen- tation method, our model achieves strong per- formances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results. Introduction Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sen- tences. Due to the growing number of language learners of English, there has been increasing atten- tion to the English GEC in the past few years. Considering the outstanding performance of neu- ral network models in machine translation tasks, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhao et al., 2019; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by Velicˇkovic ́ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model. Exper- iments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Among models without using the large pre-trained models, our model achieves the best F-score on all benchmarks. Comparing with models which incorporate the large pre-trained models, our model achieves very competitive per- formance as well. In general, our model achieves strong performance without using any large pre- trained models. Our contributions are summarized as follows: 1. To the best of our knowledge, we introduce syntactic knowledge into neural GEC model for the first time, by applying graph attention mechanism to utilize the dependency tree. 2. We propose a dependency tree correction task to deal with the problem that the dependency trees of grammatically incorrect source sentences might provide incorrect syntactic knowledge. 3. Without using any large pre-trained model, our SG-GEC model achieves strong performances on public GEC benchmarks. Related Work Early published works in GEC developed models based on manually designed grammar rules (Mu- rata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). After Han et al. (2006) pointed out the limitation of rule-based method, some researchers turned their attention to the statistical machine learning method (Knight and Chander, 1994; Min- nen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed various neural network models to solve GEC task. Some regarded the GEC task as a translation problem and applied cutting-edge neural machine translation model to deal with it (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018). Many recent works (Junczys-Dowmunt et al., 2018; Zhao et al., 2019) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Considering the tremendous performance of pre-trained methods, pre-trained language model, such as BERT(Devlin et al., 2019),RoBERTa(Liu et al., 2019) and XLNet(Yang et al., 2019), have been adopted in GEC models(Kaneko et al., 2020; Omelianchuk et al., 2020a). A challenge in applying neural machine transla- tion models to GEC task is the requirement of the large training data. Given that, many works incor- porated data augmentation methods to address this problem. Many works adopted pre-defined rules to generate synthetic samples with grammatical er- rors. (Grundkiewicz et al., 2019; Lichtarge et al., 2018; Choe et al., 2019). Kiyono et al. (2019) fur- ther studied the data augmentation methods and showed the efficacy of back-translation procedure. Recently, dependency parsing has been further developed with neural network(Dozat and Man- ning, 2016; Li et al., 2018). Benefiting from it, models could receive syntactic knowledge with higher accuracy. Many works showed the po- tential of using syntactic knowledge in various tasks(Zhang et al., 2020; Wang et al., 2020; Jin et al., 2020). Inspired by previous works, we pro- posed the SG-GEC model to utilize the syntactic knowledge within dependency trees. Conclusion In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to utilize the syntactic knowledge of dependency tree. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmentation method, our model achieves strong performances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results.",1
"Abstract Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine transla- tion systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of gram- matical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to uti- lize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmen- tation method, our model achieves strong per- formances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results. Introduction Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sen- tences. Due to the growing number of language learners of English, there has been increasing atten- tion to the English GEC in the past few years. Considering the outstanding performance of neu- ral network models in machine translation tasks, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhao et al., 2019; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by Velic kovic et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model. Exper- iments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Among models without using the large pre-trained models, our model achieves the best F-score on all benchmarks. Comparing with models which incorporate the large pre-trained models, our model achieves very competitive per- formance as well. In general, our model achieves strong performance without using any large pre- trained models. Our contributions are summarized as follows: 1. To the best of our knowledge, we introduce syntactic knowledge into neural GEC model for the first time, by applying graph attention mechanism to utilize the dependency tree. 2. We propose a dependency tree correction task to deal with the problem that the dependency trees of grammatically incorrect source sentences might provide incorrect syntactic knowledge. 3. Without using any large pre-trained model, our SG-GEC model achieves strong performances on public GEC benchmarks. Related Work Early published works in GEC developed models based on manually designed grammar rules (Mu- rata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). After Han et al. (2006) pointed out the limitation of rule-based method, some researchers turned their attention to the statistical machine learning method (Knight and Chander, 1994; Min- nen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed various neural network models to solve GEC task. Some regarded the GEC task as aproblem and applied cutting-edge neural machine translation model to deal with it (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018). Many recent works (Junczys-Dowmunt et al., 2018; Zhao et al., 2019) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Considering the tremendous performance of pre-trained models, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhang et al., 2020; Junczys-Dowmunt et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by Velic kovic et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model.",0
"ABSTRACT State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This rises the question whether different, possibly time-specific, representations of literal meanings may impact on the metaphor detection task. To the best of our knowledge, this is the first study which examines the metaphor detection task with a detailed explorative analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned to different state-of-the-art approaches. The results suggest that different word embeddings do impact on the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. However, results also suggest that temporal word embeddings may provide representations of words’ core meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented. Keywords Metaphor Detection · Temporal Word Embeddings · Word Embeddings Introduction Accounting for figurative language is one of the key challenges in Natural Language Processing (NLP). Many recent methods have proposed solutions to this problem based on machine learning methods [Recupero et al., 2019, Shutova, 2015, Leong et al., 2020]. Figurative language often contains metaphorical expressions which map one concept from a source domain to another concept in a target domain. For instance, in the sentence “The wheels of Stalin’s regime were well-oiled and already turning"", a political system (target concept) is viewed in terms of a mechanism (source concept) which can function, break, have wheels, etc. This association allows us to transfer knowledge from the domain of mechanical engineering to that of politics. Therefore, political systems are thought about in terms of mechanisms, leading to multiple metaphorical expressions. The phenomenon of source-target domain mapping was first introduced by George Lakoff known as Conceptual Metaphor Theory [Lakoff and Johnson, 1980]. Due to previously defined characteristics, the presence of metaphorical expression in text causes misinterpretation in the algorithms such as machine translation or sentiment analysis [Saif Mohammad, 2016]. For example, in machine translation, the text may be translated literally choosing inappropriate words, instead of those ones that capture the metaphorical meaning. Similarly, in the case of sentiment analysis, the polarity of the sentences can be misinterpreted and even inverted due to the presence of metaphors. Therefore, several studies have addressed the problem of detecting metaphors in text. The studies addressing the metaphor detection problem in the last decade [Shutova, 2015] usually exploit word embeddings to encode word meaning. Word embeddings are distributed representations, i.e., vectors, derived from the usage of words in large text corpora: words occurring in similar contexts have similar meanings and are eventually close to each other in a vector space [Almeida and Xexéo, 2019]. In metaphor detection, these vector-based representations can be used as signals to classify sentences or individual words as being used with a metaphorical meaning or not. The key intuition is to recognize that words are used in a context that is different from their usual context, i.e., that their usage in a specific context significantly differs from their usage in their most frequent contexts, which qualify their literal meanings. In the previous example, “wheels"" is collocated close to “Stalin"" and “regime"", which define a context different from the contexts in which it usually appears, i.e., in the domain of mechanical engineering. In other words, the classifier, usually implemented as a neural network, is expected to compare literal word meanings with word meanings in specific contexts to detect if in that context a metaphor is used. Most of the recent approaches have therefore combined non-contextual and contextual word embeddings to provide signals for this comparison (e.g., exploiting the concatenation of non-contextualized and contextualized word vectors) [Mao et al., 2019, Swarnkar and Singh, 2018]. For example, Mao et al. [2019], Gulordava and Baroni [2011], Tomas Mikolov and Dean [2013a] combine non-contextual GloVe embeddings [Jeffrey Pennington, 2014] with contextual ELMo embeddings [Peters M, 2018] within a BiLSTM neural network for sequence labelling. Glove embeddings account for literal word meanings, while ELMo embeddings account for contextual word meanings. The network tries to learn when the comparison between the two vectors indicates a metaphorical word usage. Word embedding techniques such as GloVe or Word2Vec [Tomas Mikolov and Dean, 2013a] are static embeddings that associate each word with one, context independent, representation. Contextual word embeddings, such as ELMo and BERT [Devlin et al., 2018], associate each word with one representation per sentence. For example, in the following two sentences: “Apple sells phones” and “I eat an apple”, contextual embeddings will represent “apple” differently in each sentence, while static embedding can not distinguish the semantic difference between the two references of “apple”. Usually, the latter represents a word with its core meaning, i.e., with the meaning in those contexts that are more frequent in the corpus used to train the embeddings (when more meanings compete as core meanings, words may be associated with vectors that mediate across contexts). An important linguistic phenomenon that is not accounted in static and contextual word embeddings is language evolution. While contextual word embeddings are expected to be sentence-specific and thus de-contextualized with respect to time, static word embeddings are expected to account for core meanings - and literal meanings in metaphor detection. Core meanings of course change over time and several approaches have been proposed to capture this changes [William L. Hamilton, 2018]. The trait of evolution of the meaning over time is also shared by metaphorical expressions which can be due to various reasons such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings derive this evolution to some extent [Smith and Höfler, 2015a, Aitchison, 2003]. This leads to the question whether different, possibly time-specific, representations of literal meanings impact the task of metaphor detection. In conclusion, if metaphor detection approaches tend to compare a sentence-specific and a literal meaning, we must be aware that literal meaning as accounted in static word embeddings 1) depends on the corpus and method used to train the embeddings and 2) evolve over time. To this end, the current empirical study focuses on analyzing the impact of different embeddings accounting for literal word meaning on the task of metaphor detection. In particular, we want to study the interactions between metaphor detection approaches and different word embeddings used to account for the literal meaning of words in these approaches. We want to dedicate a special attention to possible interactions between metaphor detection and time-specific (non–contextual) word representations used to account for literal meanings at different times. To the best of our knowledge, this is the first study seeking for interactions between time-dependent word representations and metaphor detection approaches. The empirical study discussed in this paper aims to make a first step into addressing the co-evolution of metaphors and language evolution that is known to be an important factor for language evolution itself [Smith and Höfler, 2015a, Aitchison, 2003]. The methodology adopted in our study consists in following protocol. First, we select a state-of-the-art Recurrent Neural Networks (RNN)-based model [Ge Gao and Zettlemoyer, 2018] for metaphor detection which uses static word embeddings to account for literal word meaning; the model performs metaphor detection as a sequence classification task where each word occurrence is labeled as either a metaphor usage or a literal usage. Second, we select three benchmark data sets widely used to evaluate the performance of metaphor detection approaches. Third, we feed the RNN-based model with literal meaning vectors obtained from different (non contextual) word embeddings spaces; these spaces differs for the corpora used to train them and, especially include temporal word embeddings computed for different decades and aligned with state-of-the-art alignment methods, such as Procrustes [Edouard Grave, 2018] and the Compass method (first referred to as Temporal Word Embeddings with a Compass [Di Carlo et al., 2019] - TWEC and later as Compass-aligned Distributional Embeddings [Bianchi et al., 2020]. The experimental results indicate that different word embeddings impact the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. These quantitative results are then explained with the help of a qualitative analysis of the predictions made by the models. During this analysis, some interesting recurring patterns were observed. Some patterns concern the interactions between literal meanings and domains of discourse. For example, in sentences containing correctly identified metaphors, the topics related to economics, politics, and emotions are the most recurring ones. Besides, verbs having a literal meaning characterized by physical connotations, often assume figurative meanings when used in the sentences related to the previously listed contexts. Some patterns concern indeed the interaction between time and language. Studying the predictions obtained with the embeddings of one specific time period, we noticed that none of the sentences belonging to the “news"" domain of the VUA dataset were correctly predicted. This could indicate that for that specific time period (1990 decade), words’ representations of that domain are biased towards their metaphorical meaning, and this would prevent the neural networks from correctly identifying the metaphors. Furthermore, if temporal word embeddings provided words’ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting them would correctly identify metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding space, e.g., obtained with GloVe [Jeffrey Pennington, 2014] and in a decade-specific temporal space, e.g., obtained from the CoHa1 corpus with Procrustes [Edouard Grave, 2018].This pattern is highlighted by the following sentence example: “The virus attacked Argonne National Laboratory outside Chicago starting at 11.54 pm EST Wednesday and throughout the night"". If we investigate the ten nearest neighbors of “virus"", in the temporal embedding we find words such as “infection, respiratory and organism"", while in the atemporal one there are for example “malware and spyware"", that diverge from the core literal meaning and are related to a modern connotation of the word. When exploiting the temporal word embedding, the model is able to understand that the “virus"" in this sentence is a computer one, and therefore that it is used in a metaphorical way along with the verb “attacked"". The paper is organized as follows: Section 2 discusses the related work about metaphor detection as well as temporal word embeddings. Section 3 discusses the methodology followed while Section 4 shows the experimental results of the paper. Finally, Section 5 concludes the paper. Discussion and Conclusion In the previous sections, we analyzed the results of our work. As an additional step to evaluate the impact of temporal embeddings on the evolution of language and the meaning of the words, we also performed our experiments using the words with the biggest semantic shift across time. The words’ list was retrieved from the SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection Competition2.We tried building a custom dataset by searching for metaphorical and literal sentences that would contain all these words so that it would be possible to perform metaphor detection tasks exploiting different temporal embeddings. Unfortunately, this final experiment was not feasible, due to the fact that there were not enough metaphorical statements in the competition dataset or in the state-of-the-art ones with the aforementioned words being used in a proper way. We performed metaphor detection as a sequence classification task in order to identify words with figurative meanings inside sentences. This approach allowed us to take advantage of different types of word representations, especially temporal ones, and evaluate their impact on metaphor detection. Looking at the numerous and diversified results, we can affirm that temporal word embeddings do generally improve the performance of the task of metaphor detection, however, their overall impact is rather limited. Besides, independently from the absolute performance, the interaction effect between the specificity of the embeddings (especially their temporal specificity) and metaphor detection is found in the experiments conducted in this study. In fact, these experiments verify that if the core meaning of the words of interest in a sentence is too similar to their figurative one in the word embedding, a metaphorical sentence could get misclassified as literal. Moreover, when temporal word embeddings provide words’ representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting end up correctly identifying metaphors more easily. Furthermore, embeddings of words belonging to some language domains in specific time periods can be biased towards their metaphorical meaning, leading to words being used in metaphorical contexts way more than in literal ones. This would prevent neural models from correctly identifying the words as metaphors. We could verify these hypotheses by examining the examples analyzed in section 1. We explored the nearest neighbors of the target metaphorical words of sentences extracted from state of the art datasets, both in a static and atemporal word embedding such as GloVe [Jeffrey Pennington, 2014] and CoHa SGNS respectively. When exploiting the temporal word embedding, the model could correctly understand that the words (in our examples: “apple, virus, attack, hearts, and glow"") were used in a figurative way, thus correctly classifying them as metaphors. Furthermore, the entire sentences were correctly classified as metaphorical too, since the words’ core meanings were closer to their literal core ones. This also explains the fluctuations across slices, because corpora are never fully representative, and some contexts may be represented more than others in one specific decade. Finally, we performed qualitative analyses on the predictions made by sequence classification neural networks-based approaches and fine-tuned models for metaphor detection. The results suggest that topics related to economics, politics, and emotions are the most recurring ones in sentences containing correctly identified metaphors and that verbs having a literal meaning characterized by physical connotations, often assume a metaphorical meaning when used in sentences related to the aforementioned contexts.",1
"ABSTRACT State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using sequential metaphor classifiers based on neural networks. The signal that represents the literal meaning is often represented by (non-contextual) word embeddings. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This rises the question whether different, possibly time-specific, representations of literal meanings may impact on the metaphor detection task. To the best of our knowledge, this is the first study which examines the metaphor detection task with a detailed explorative analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned to different state-of-the-art approaches. The results suggest that different word embeddings do impact on the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. However, results also suggest that temporal word embeddings may provide representations of words' core meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented. Keywords Metaphor Detection· Temporal Word Embeddings· Word Embeddings Introduction Accounting for figurative language is one of the key challenges in Natural Language Processing (NLP). Many recent methods have proposed solutions to this problem based on machine learning methods [Recupero et al., 2019, Shutova, 2015, Leong et al., 2020]. Figurative language often contains metaphorical expressions which map one concept from a source domain to another concept in a target domain. For instance, in the sentence ""The wheels of Stalin"" regime were well-oiled and already turning"", a political system (target concept) is viewed in terms of a mechanism (source concept) which can function, break, have wheels, etc. This association allows us to transfer knowledge from the domain of mechanical engineering to that of politics. Therefore, political systems are thought about in terms of mechanisms, leading to multiple metaphorical expressions. The phenomenon of source-target domain mapping was first introduced by George Lakoff known as Conceptual Metaphor Theory [Lakoff and Johnson, 1980]. Due to previously defined characteristics, the presence of metaphorical expression in text causes misinterpretation in the algorithms such as machine translation or sentiment analysis [Saif Mohammad, 2016]. For example, in machine translation, the text may be translated literally choosing inappropriate words, instead of those ones that capture the metaphorical meaning. Similarly, in the case of sentiment analysis, the polarity of the sentences can be misinterpreted and even inverted due to the presence of metaphors. Therefore, several studies have addressed the problem of detecting metaphors in text. The studies addressing the metaphor detection problem in the last decade [Shutova, 2015] usually exploit word embeddings to encode word meaning. Word embeddings are distributed representations, i.e., vectors, derived from the usage of words in large text corpora: words occurring in similar contexts have similar meanings and are eventually close to each other in a vector space [Almeida and Xexeo, 2019]. In metaphor detection, these vector-based representations can be used as signals to classify sentences or individual words as being used with a metaphorical meaning or not. The key intuition is to recognize that words are used in a context that is different from their usual context, i.e., that their usage in a specific context significantly differs from their usage in their most frequent contexts, which qualify their literal meanings. In the previous example, ""wheels"" is collocated close to ""Stalin"" and ""regime"", which define a context different from the contexts in which it usually appears, i.e., in the domain of mechanical engineering. In other words, the classifier, usually implemented as a neural network, is expected to compare literal word meanings with word meanings in specific contexts to detect if in that context a metaphor is used. Most of the recent approachestherefore combined non-contextual and contextual word embeddings to provide signals for this comparison (e.g., exploiting the concatenation of non-contextualized and contextualized word vectors) [Mao et al., 2019, Swarnkar and Singh, 2018]. For example, Mao et al. [2019], Gulordava and Baroni [2011], Tomas Mikolov and Dean [2013a] combine non-contextual GloVe embeddings [Jeffrey Pennington, 2014] with contextual ELMo embeddings [Peters M, 2018] within a BiLSTM neural network for sequence labelling. Glove embeddings account for literal word meanings, while ELMo embeddings account for contextual word meanings. The network tries to learn when the comparison between the two vectors indicates a metaphorical word usage. Word embedding techniques such as GloVe or Word2Vec [Tomas Mikolov and Dean, 2013a] are static embeddings that associate each word with one, context independent, representation. Contextual word embeddings, such as ELMo and BERT [Devlin et al., 2018], associate each word with one representation per sentence. For example, in the following two sentences: ""Apple sells phones"" and ""I eat an apple"", contextual embeddings will represent ""apple"" differently in each sentence, while static embedding can not distinguish the semantic difference between the two references of ""apple"". Usually, the latter represents a word with its core meaning, i.e., with the meaning in those contexts that are more frequent in the corpus used to train the embeddings (when more meanings compete as core meanings, words may be associated with vectors that mediate across contexts). An important linguistic phenomenon that is not accounted in static and contextual word embeddings is language evolution. While contextual word embeddings are expected to be sentence-specific and thus de-contextualized with respect to time, static word embeddings are expected to account for core meanings - and literal meanings in metaphor detection. Core meanings of course change over time and several approaches have been proposed to capture this changes [William L. Hamilton, 2018]. The trait of evolution of the meaning over time is also shared by metaphorical expressions which can be due to various reasons such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings derive this evolution to some extent [Smith and Hofler, 2015a, Aitchison, 2003]. This leads to the question whether different, possibly time-specific, representations of literal meanings impact the task of metaphor detection. In conclusion, if metaphor detection approaches tend to compare a sentence-specific and a literal meaning, we must be aware that literal meaning as accounted in static word embeddings 1) depends on the corpus and method used to train the embeddings and 2) evolve over time. To this end, the current empirical study focuses on analyzing the impact of different embeddings accounting for literal word meaning on the task of metaphor detection. In particular, we want to study the interactions between metaphor detection approaches and different word embeddings used to account for the literal meaning of words in these approaches. We want to dedicate a special attention to possible interactions between metaphor detection and time-specific (non contextual) word representations used to account for literal meanings at different times. To the best of our knowledge, this is the first study seeking for interactions between time-dependent word representations and metaphor detection approaches. The empirical study discussed in this paper aims to make a first step into addressing the co-evolution of metaphors and language evolution that is known to be an important factor for language evolution itself [Smith and Hofler, 2015a, Aitchison, 2003]. The methodology adopted in our study consists in following protocol. First, we select a state-of-the-art Recurrent Neural Networks (RNN)-based model [Ge Gao and Zettlemoyer, 2018] for metaphor detection which uses static word embeddings to account for literal word meaning; the model performs metaphor detection as a sequence classification task where each word occurrence is labeled as either a metaphor usage or a literal usage. Second, we select three benchmark data sets widely used to evaluate the performance of metaphor detection approaches. Third, we feed the RNN-based model with literal meaning vectors obtained from different (non contextual) word embeddings spaces; these spaces differs for the corpora used to train them and, especially includeword embeddings computed for different decades and aligned with state-of-the-art alignment methods, such as Procrustes [Edouard Grave, 2018] and the Compass method (first referred to as Temporal Word Embeddings with a Compass [Di Carlo et al., 2019] - TWEC and later as Compass-aligned Distributional Embeddings [Bianchi et al., 2020]. The experimental results indicate that different word embeddings impact the metaphor detection task and some temporal word embeddings slightly outperform static methods on some performance measures. These quantitative results are then explained with the help of a qualitative analysis of the predictions made by the models. During this analysis, some interesting recurring patterns were observed. Some patterns concern the interactions between literal meanings and domains of discourse. For example, in sentences containing correctly identified metaphors, the topics related to economics, politics, and emotions are the most recurring ones. Besides, verbs having a literal meaning characterized by physical connotations, often assume figurative meanings when used in the sentences related to the previously listed contexts. Some patterns concern indeed the interaction between time and language. Studying the predictions obtained with the embeddings of one specific time period, we noticed that none of the sentences belonging to the ""news"" domain of the VUA dataset were correctly predicted. This could indicate that for that specific time period (1990 decade), words' representations of that domain are biased towards their metaphorical meaning, and this would prevent the neural networks from correctly identifying the metaphors. Furthermore, if temporal word embeddings provided words' representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting them would correctly identify metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding space, e.g., obtained with GloVe [Jeffrey Pennington, 2014] and in a decade-specific temporal space, e.g., obtained from the CoHa1 corpus with Procrustes [Edouard Grave, 2018].This pattern is highlighted by the following sentence example: ""The virus attacked Argonne National Laboratory outside Chicago starting at 11.54 pm EST Wednesday and throughout the night"". If we investigate the ten nearest neighbors of ""virus"", in the temporal embedding we find words such as 
infection, respiratory and organism"", while in the atemporal one there are for example malware and spyware"", that diverge from the core literal meaning and are related to a modern connotation of the word. When exploiting the temporal word embedding, the model is able to understand that the virus"" in this sentence is a computer one, and therefore that it is used in a metaphorical way along with the verb ""attacked"". The paper is organized as follows: Section 2 discusses the related work about metaphor detection as well as temporal word embeddings. Section 3 discusses the methodology followed while Section 4 shows the experimental results of the paper. Finally, Section 5 concludes the paper. Discussion and Conclusion In the previous sections, we analyzed the results of our work. As an additional step to evaluate the impact of temporal embeddings on the evolution of language and the meaning of the words, we also performed our experiments using the words with the biggest semantic shift across time. The words' list was retrieved from the SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection Competition2.We tried building a custom dataset by searching for metaphorical and literal sentences that would contain all these words so that it would be possible to perform metaphor detection tasks exploiting different temporal embeddings. Unfortunately, this final experiment was not feasible, due to the fact that there were not enough metaphorical statements in the competition dataset or in the state-of-the-art ones with the aforementioned words being used in a proper way. We performed metaphor detection as a sequence classification task in order to identify words with figurative meanings inside sentences. This approach allowed us to take advantage of different types of word representations, especially temporal ones, and evaluate their impact on metaphor detection. Looking at the numerous and diversified results, we can affirm that temporal word embeddings do generally improve the performance of the task of metaphor detection, however, their overall impact is rather limited. Besides, independently from the absolute performance, the interaction effect between the specificity of the embeddings (especially their temporal specificity) and metaphor detection is found in the experiments conducted in this study. In fact, these experiments verify that if the core meaning of the words ofin a sentence is too similar to their figurative one in the word embedding, a metaphorical sentence could get misclassified as literal. Moreover, when temporal word embeddings provide words' representations that are more inclined towards their literal core meaning (and not the metaphorical one), models exploiting end up correctly identifying metaphors more easily. One way to investigate these hypotheses further is to explore the nearest neighbors of a word in the word embeddings used in a figurative way inside a sentence, both in a static (atemporal) word embedding, i.e., a decade, time period, and/or a temporal (osfringes) word embedding, i.e., a decade, time period. To the best of our knowledge, this is the first study seeking for interactions between time-dependent word representations and metaphor detection approaches. The empirical study discussed in this paper aims to make a first step into addressing the co-evolution of metaphors and language evolution that is known to be an important factor for language evolution itself [Smith and Hofler, 2015a, Aitchison, 2003]. The methodology adopted in our study consists in following protocol. First, we select a state-of-the-art Recurrent Neural Networks (RNN)-based model [Ge Gao and Zettlemoyer, 2018] for metaphor detection which uses static word embeddings to account for literal word meaning; the model performs metaphor detection as a sequence classification task where each word occurrence is labeled as either a metaphor usage or a literal usage. Second, we select three benchmark data sets widely used to evaluate the performance of metaphor detection approaches.",0
"Abstract— Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media. Keywords— Sexism classification, social media, natural language processing, neural networks, machine learning, BERT, transfer learning. INTRODUCTION Social media has completely altered the way communities are formed and utilised, which provides incredible advantages while also having severe repercussions. Due to the ‘online disinhibition effect’ (Suler, 2004, p. 1), when users are provided with an anonymised and accessible platform, they engage in behaviours they would not partake in when interacting face-to-face (Wright et al., 2019). A significant example of this is the hate speech produced and propagated through social media platforms. Hate speech is defined as language which is ‘insulting, degrading, defaming, negatively stereotyping, or inciting hatred, discrimination or violence against people in virtue of their race, ethnicity, nationality, religion, sexual orientation, disability, gender identity’ (Brown, 2017, p. 1). The prevalence of hate speech in everyday life has increased in correlation with social media usage, particularly during the COVID-19 pandemic, with internet usage levels having increased between 50% to 70% as of early April 2020 (UN Women, 2020). Online hate speech, especially targeted discrimination, has been associated with an increase in hate crimes offline (Hatzipanagos, 2018; Laub, 2019; Relia et al., 2019); therefore, the ability to successfully tackle this issue within the virtual space itself is vital. Sexism refers to a sub-classification of hate speech where the targeted people are typically female. Women are more likely to report having experienced sexual harassment online (16% vs. 5%) or being cyber-stalked (13% vs. 9%) compared to men (Vogels, 2021); with 1 in 10 women reporting having experienced cyber harassment since the age of 15 in the European Union (UN Women, 2020). Women and girls are specifically seen to face a digital gender divide1, especially with the COVID-19 pandemic being the first major one in the age of social media2. While social media platforms like Twitter do ban hate speech3, these policies are enforced primarily through manual methods which cannot scale up to counteract the data being produced (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (Rodríguez-Sánchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the model’s performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest of this paper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; Rodríguez-Sánchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018). An alternative architecture was proposed by Zhang and Luo (2019) with two deep neural network models to tackle hate speech classification on Twitter datasets. These models consist of CNN and Gated Recurrent Unit (GRU) architectures with the results outperforming the best methods at the time. More recently, the introduction of BERT has led to new state-of-the-art performances across a range of natural language processing tasks, including text classification (Devlin et al., 2018). BERT is a multi-layer bidirectional transformer encoder which notably uses bidirectional self- attention to learn contextual information between words and sub-words within a text (Alammar, 2018). BERT has been pre-trained using BooksCorpus (800M words) and English Wikipedia (2500M words) on masked language modelling and next sentence prediction (Devlin et al., 2018). This causes the embeddings taken from the model to contain useful contextual information that can be fine-tuned for specific tasks. Rodríguez-Sánchez et al. (2020) show BERT being used to give the best performance on the task of identifying sexist content through fine tuning pre-trained mBERT-Base parameters with a fully connected layer. Multi-label sexism classification was first seen in a paper by Parikh et al. (2020) where a BERT based neural architecture was used along with distributional and word level embeddings. Samghabadi et al. (2020) also show BERT being used without fine-tuning to identify aggression and misogyny in English, Hindi, and Bengali tweets with positive results from the model. Limited research has been conducted on the automatic classification of subtle expressions of sexism encompassing a broad range of categories, compared to the sole use of profanities or explicit hatred against women. Rodríguez- Sánchez et al. (2020) collected instances of various types of sexism, ranging from subtle inequality to explicit violence to create a dataset to then be used in an automatic classification task. The range of expressions collated is similar to the dataset used in this paper. An important point to note is that hate speech and sexism is defined differently across these papers, with offensive language often considered to be equivalent (Davidson et al., 2017). Another challenge for automatic sexism classification is the lack of an established benchmark dataset. Detecting sexist expressions is a challenge for human coders as well, with racist or homophobic tweets often considered to be hate speech while sexist or derogatory terms are found to be offensive as opposed to hateful (Waseem and Hovy, 2016; Davidson et al., 2017). To tackle these issues of subtlety and context, the DistilBERT, which is a lightweight version of BERT with 40% fewer parameters (Sanh et al., 2019), and BERT models used in this paper are fine-tuned with additional layers to allow them to learn contextual embeddings and perform effectively on the sexism classification tasks. CONCLUSION AND FUTURE WORK The increased use of social media has enabled hate speech, including sexist speech, to easily propagate and affect people globally. With online hate speech linked to offline violence, it is essential to successfully classify speech as hateful through automatic methods. This paper presented a variety of deep neural networks using BERT and DistilBERT to differentiate sexist tweets and gabs from non-sexist ones, as well as further classify sexist text into types of sexism using the EXIST dataset. The best model for the binary classification task used BERT along with a CNN architecture using filter sizes of 4, 6, and 8 to achieve an accuracy of 76.2% while the best accuracy from the competition was 77%. The same model along with data augmentation achieved the best performance on the multi-class classification task with an F1 score of 51.9% which was lower than the best F1 score (56%) from the competition. Due to the subjectivity involved in annotating sexist text as well as the complexity of natural language in tweets and gabs, this task proved to be challenging for the models to achieve ideal results on. Categories which contain explicit hatred like ‘sexual-violence’ were seen to be labelled more accurately than more subtle instances of sexism such as those seen under ‘ideological-inequality’. Profanities are often used on social media platforms without necessarily insinuating sexist speech, such as in casual conversation or song lyrics and hence were not very helpful features for classification. Similarly, speech can be sexist without using any explicit and specific words to indicate this. Due to the varying perceptions of sexism by humans, some labels within the dataset were found to have been potentially mislabelled, which creates further challenges for using the dataset to train models. The type of texts (tweets versus gabs) was not seen to affect the BERT + MultiCNN model’s performance significantly for either task. This could be attributed to the use of the convolutional layer with different filter sizes with max pooling. The model was seen to perform better for text of length 500+ characters and <100 characters, although the impact of length would need to be further examined to confirm this by creating more balanced datasets for this purpose. Another avenue for further exploration could be using additional features such as the gender or ethnicity of authors as stated by Waseem and Hovy (2016), although this information may be challenging to obtain. The models trained on the EXIST dataset could also be tested on a different sexism dataset to observe the generalisability of the models across different annotated data. Finally, a benchmark annotated dataset for sexism would also allow for better development and comparison of models.",1
"Abstract Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media. Keywords' Sexism classification, social media, natural language processing, neural networks, machine learning, BERT, transfer learning. INTRODUCTION Social media has completely altered the way communities are formed and utilised, which provides incredible advantages while also having severe repercussions. Due to the ""online disinhibition effect"" (Suler, 2004, p. 1), when users are provided with an anonymised and accessible platform, they engage in behaviours they would not partake in when interacting face-to-face (Wright et al., 2019). A significant example of this is the hate speech produced and propagated through social media platforms. Hate speech is defined as language which is insulting, degrading, defaming, negatively stereotyping, or inciting hatred, discrimination or violence against people in virtue of their race, ethnicity, nationality, religion, sexual orientation, disability, gender identity (Brown, 2017, p. 1). The prevalence of hate speech in everyday life has increased in correlation with social media usage, particularly during the COVID-19 pandemic, with internet usage levels having increased between 50% to 70% as of early April 2020 (UN Women, 2020). Online hate speech, especially targeted discrimination, has been associated with an increase in hate crimes offline (Hatzipanagos, 2018; Laub, 2019; Relia et al., 2019); therefore, the ability to successfully tackle this issue within the virtual space itself is vital. Sexism refers to a sub-classification of hate speech where the targeted people are typically female. Women are more likely to report having experienced sexual harassment online (16% vs. 5%) or being cyber-stalked (13% vs. 9%) compared to men (Vogels, 2021); with 1 in 10 women reporting having experienced cyber harassment since the age of 15 in the European Union (UN Women, 2020). Women and girls are specifically seen to face a digital gender divide1, especially with the COVID-19 pandemic being the first major one in the age of social media2. While social media platforms like Twitter do ban hate speech3, these policies are enforced primarily through manual methods which cannot scale up to counteract the data being produced (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (Rodriguez-Sanchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the model's performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest ofpaper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; Rodriguez-Sanchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018). An alternative architecture was proposed by Zhang and Luo (2019) with two deep neural network models to tackle hate speech classification on Twitter datasets. These models consist of CNN and Gated Recurrent Unit (GRU) architectures with the results outperforming the best methods at the time. More recently, the introduction of BERT has led to new state-of-the-art performances across a range of natural language processing tasks, including text classification (Devlin et al., 2018). BERT is a multi-layer bidirectional transformer encoder which notably uses bidirectional self- attention to learn contextual information between words and sub-words within a text (Alammar, 2018). BERT has been pre-trained using BooksCorpus (800M words) and English Wikipedia (2500M words) on masked language modelling and next sentence prediction (Devlin et al., 2018). This causes the embeddings taken from the model to contain useful contextual information that can be fine-tuned for specific tasks. Rodriguez-Sanchez et al. (2020) show BERT being used to give the best performance on the task of identifying sexist content through fine tuning pre-trained mBERT-Base parameters with a fully connected layer. Multi-label sexism classification was first seen in a paper by Parikh et al. (2020) where a BERT based neural architecture was used along with distributional and word level embeddings. Samghabadi et al. (2020) also show BERT being used without fine-tuning to identify aggression and misogyny in English, Hindi, and Bengali tweets with positive results from the model. Limited research has been conducted on the automatic classification of subtle expressions of sexism encompassing a broad range of categories, compared to the sole use of profanities or explicit hatred against women. Rodriguez- Sanchez et al. (2020) collected instances of various types of sexism, ranging from subtle inequality to explicit violence to create a dataset to then be used in an automatic classification task. The range of expressions collated is similar to the dataset used in this paper. An important point to note is that hate speech and sexism is defined differently across these papers, with offensive language often considered to be equivalent (Davidson et al., 2017). Another challenge forsexism classification is the lack of an established benchmark dataset. Detecting sexist expressions is a challenge for human coders as well, with racist or homophobic tweets often considered to be hate speech while sexist or derogatory terms are found to be offensive as opposed to hateful (Waseem and Hovy, 2016; Zhang and Luo, 2019). To tackle these issues of hate speech and sexism classification, the DistilBERT, which is a lightweight version of the DistilBERT used in the early days of social media, has also been missing due to the subjectivity of the labels and the complexity of natural language used in social media. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (Rodriguez-Sanchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the model's performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest of this paper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; Rodriguez-Sanchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. ",0
"Abstract Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based model with a Fusion of Adapter layers which leverages knowledge from more simple sentiment analysis tasks to improve the emotion detection task on large scale dataset, such as CMU-MOSEI, using the textual modality only. Results show that our proposed method is competitive with other approaches. We obtained state-of-the-art results for emotion recognition on CMU-MOSEI even while using only the textual modality. Introduction Sentiment analysis is a subject that has long interested multiple researchers in the domain of natural language understanding. It is a task which aims to identify sentiment polarity for a given signal, which can be of the audio, visual or textual modality. Emotion recognition is a related task which consists of assigning more fine-grained labels, such as anger, joy, sadness, etc. This work focuses on analyzing textual inputs. The ability to recognize the sentiment or emotion behind a given sentence or paragraph can lead to multiple applications, such as empathetic dialogue agents and tools to assess the mental state of a patient. While sentiment analysis in the form of assigning polarities (positive, negative, and sometimes neutral) to text data is a task that is often studied and for which adequate results have already been obtained for multiple datasets, identifying finer-grained labels such as specific emotions is still a challenge. In addition to the task complexity, in most datasets available for this task, some emotions are much less represented than others, making the training data unbalanced. To address this issue, the model proposed in this work combines knowledge from less complex tasks and is trained using methods to counteract class imbalance. It is based on a Transformer-based model with a Fusion of Adapter layers to leverage knowledge from the more simple sentiment analysis task. The results obtained are competitive with state-of-the-art multi-modal models on the CMU-MOSEI dataset (Bagher Zadeh et al., 2018), while only utilizing the textual modality. Our main contribution can be formulated as: • We designed a method that capitalizes on both pretrained Transformer language models and knowledge from complementary tasks to improve on the emotion recognition task, whilst using Adapter layers that require less training parameters than the conventional fine-tuning approach and taking into account class imbalance. Prior Works and Background There are multiple approaches that have been used to solve text-based sentiment analysis and emotion detection tasks, namely rule-based and machine learning approaches. Rule-based approaches consist of creating grammatical and logical rules to assign emotions and use lexicons to assign emotions or polarities to words. Previous works using this approach include the ones of Udochukwu and He (2015), Tan et al. (2015) and Seal et al. (2019). These methods are limited by the size and contents of the lexicon used and by the ambiguity of some keywords. Most recent methods are based on the machine learning approach were the network is trained to learn the relationships between words and emotions. Methods such as those proposed by Abdul- Mageed and Ungar (2017), Tang et al. (2015) and Ma et al. (2019) use recurrent neural networks to solve sentiment analysis tasks to break down sentences and understand the relationship between the succession of words and sentiments or emotions. Since the release of pretrained models, recent works (Park et al., 2019; Acheampong et al., 2021) have been focused on fine-tuning transformer models, which have consistently outperformed previous methods thanks to the multi-head attention applied on words. To improve previous textual emotion recognition methods, we believe that in addition to transfer learning, multi-task learning and class imbalance should be considered. 2.1 Transfer Learning Transfer learning is a method where the weights of a model trained on a task are used as starting point to train a model for another task. The use of transfer learning with pretrained models has been, for the past few years, the way to obtain state-of-the-art results for multiple natural language understanding (NLU) tasks. Transformer-based pretrained models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), etc. have been dominating the field over previously used methods. 2.2 Multi-Task Learning Multi-task learning is used to train one model to solve multiple tasks instead of fine-tuning separate models. Multiple approaches have been used to solve multi-task learning problems. Liu et al. (2019a) proposed a Multi-Task Deep Neural Network (MT-DNN) with a shared transformer encoder and task-specific heads. Clark et al. (2019) and Liu et al. (2019b) presented a new training procedure based on knowledge distillation to improve the performances of the MT-DNN. These approaches allow the model to learn a shared representation between all tasks. Houlsby et al. (2019) introduced a new model architecture using task-specific adapter layers and keeping the weights of the pretrained encoder frozen. This method, while preventing task interference and catastrophic forgetting, does not allow to transfer knowledge between tasks. To counter this weakness, Pfeiffer et al. (2020a) proposed AdapterFusion, a way to combine knowledge from multiple adapters. 2.3 Class Imbalance Class imbalance is a challenge in resolving many artificial intelligence tasks. It occurs when one or multiple classes make up significantly less samples of the data than the majority class or classes, often leading to a poor predictive performance for those minority classes. Classic approaches to this problem include re-sampling minority class samples or weighting the loss function according to class frequency. In the field of computer vision, Lin et al. (2018) proposed a modified version of the cross-entropy loss called the focal loss to handle imbalance. Conclusion The model presented in this work surpasses state-of-the-art results for emotion recognition on CMU- MOSEI even while using only the textual modality. There is still improvement needed for the rarer emotions in the dataset, but at of the time of producing this article, the results presented are substantially stronger than other contributions in terms of F1-scores. Due to the lack of large-scale datasets for emotion detection in text, testing the model on purely textual data will have to be done in further studies as the data will become available.",1
"Abstract Identifying and understanding underlying sentiment or emotions in text is a key component of multiple natural language processing applications. While simple polarity sentiment analysis is a well-studied subject, fewer advances have been made in identifying more complex, finer-grained emotions using only textual data. In this paper, we present a Transformer-based model with a Fusion of Adapter layers which leverages knowledge from more simple sentiment analysis tasks to improve the emotion detection task on large scale dataset, such as CMU-MOSEI, using the textual modality only. Results show that our proposed method is competitive with other approaches. We obtained state-of-the-art results for emotion recognition on CMU-MOSEI even while using only the textual modality. Introduction Sentiment analysis is a subject that has long interested multiple researchers in the domain of natural language understanding. It is a task which aims to identify sentiment polarity for a given signal, which can be of the audio, visual or textual modality. Emotion recognition is a related task which consists of assigning more fine-grained labels, such as anger, joy, sadness, etc. This work focuses on analyzing textual inputs. The ability to recognize the sentiment or emotion behind a given sentence or paragraph can lead to multiple applications, such as empathetic dialogue agents and tools to assess the mental state of a patient. While sentiment analysis in the form of assigning polarities (positive, negative, and sometimes neutral) to text data is a task that is often studied and for which adequate results have already been obtained for multiple datasets, identifying finer-grained labels such as specific emotions is still a challenge. In addition to the task complexity, in most datasets available for this task, some emotions are much less represented than others, making the training data unbalanced. To address this issue, the model proposed in this work combines knowledge from less complex tasks and is trained using methods to counteract class imbalance. It is based on a Transformer-based model with a Fusion of Adapter layers to leverage knowledge from the more simple sentiment analysis task. The results obtained are competitive with state-of-the-art multi-modal models on the CMU-MOSEI dataset (Bagher Zadeh et al., 2018), while only utilizing the textual modality. Our main contribution can be formulated as: We designed a method that capitalizes on both pretrained Transformer language models and knowledge from complementary tasks to improve on the emotion recognition task, whilst using Adapter layers that require less training parameters than the conventional fine-tuning approach and taking into account class imbalance. Prior Works and Background There are multiple approaches that have been used to solve text-based sentiment analysis and emotion detection tasks, namely rule-based and machine learning approaches. Rule-based approaches consist of creating grammatical and logical rules to assign emotions and use lexicons to assign emotions or polarities to words. Previous works using this approach include the ones of Udochukwu and He (2015), Tan et al. (2015) and Seal et al. (2019). These methods are limited by the size and contents of the lexicon used and by the ambiguity of some keywords. Most recent methods are based on the machine learning approach were the network is trained to learn the relationships between words and emotions. Methods such as those proposed by Abdul- Mageed and Ungar (2017), Tang et al. (2015) and Ma et al. (2019) use recurrent neural networks to solve sentiment analysis tasks to break down sentences and understand the relationship between the succession of words and sentiments or emotions. Since the release of pretrained models, recent works (Park et al., 2019; Acheampong et al., 2021) have been focused on fine-tuning transformer models, which have consistently outperformed previous methods thanks to the multi-head attention applied on words. To improve previous textual emotion recognition methods, we believe that in addition to transfer learning, multi-task learning and class imbalance should be considered. 2.1 Transfer Learning Transfer learning is a method where the weights of a model trained on a task are used as starting point to train a model for another task. The use of transfer learning with pretrained models has been, for the past few years, the way to obtain state-of-the-art results for multiple natural language understanding (NLU) tasks. Transformer-based pretrained models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), etc. have been dominating the field over previously used methods. 2.2 Multi-Task Learning Multi-taskis used to train one model to solve multiple tasks instead of fine-tuning separate models. Multiple approaches have been used to solve multi-task learning problems. Liu et al. (2019a) proposed a Multi-Task Deep Neural Network (MT-DNN) with a shared transformer encoder and task-specific heads. Clark et al. (2019) and Liu et al. (2019b) presented a new training procedure based on knowledge distillation to improve the performances of the MT-DNN. These approaches allow the model to learn the relationships between words and emotions. The results obtained are competitive with state-of-the-art multi-task designs on the CMU-MOSEI dataset (Bagher Zadeh et al., 2018), while only utilizing the textual modality. Introduction Sentiment analysis is a subject that has long interested multiple researchers in the domain of natural language understanding. It is a task which aims to identify sentiment polarity for a given signal, which can be of the audio, visual or textual modality. Emotion recognition is a related task which consists of assigning more fine-grained labels, such as anger, joy, sadness, etc. This work focuses on analyzing textual inputs. The ability to recognize the sentiment or emotion behind a given sentence or paragraph can lead to multiple applications, such as empathetic dialogue agents and tools to assess the mental state of a patient. While sentiment analysis in the form of assigning polarities (positive, negative, and sometimes neutral) to text data is a task that is often studied and for which adequate results have already been obtained for multiple datasets, identifying finer-grained labels such as specific emotions is still a challenge. In addition to the task complexity, in most datasets available for this task, some emotions are much less represented than others, making the training data unbalanced. To address this issue, the model proposed in this work combines knowledge from less complex tasks and is trained using methods to counteract class imbalance.",0
"Abstract Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically de- tect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is re- ceived by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo. Introduction We present an approach for identifying the dialect of a speaker automatically solely based on text and on audio and text together. We compare the uni- modal approach to the bimodal one. There are no previous dialect identification approaches for Finnish. There are several situations were a dialect identification method can be of use. For example, if we have ASR models fine tuned for specific di- alects, the dialect identification from audio could be used as a preprocessing step. The model could also be used to label recorded materials automatically in order to create archival metadata. In order to make our contribution useful for others, we have released our code, models and processed data openly on GitHub1 and Zenodo2. Finnish is a large Uralic language that is one of the official languages of Finland, and is used essen- tially at all levels of the modern society. There are approximately five million Finnish speakers. The language belongs to the Finnic branch of the Uralic language family, and is very closely related to Kare- lian, Meänkieli and Kveeni, and is also closely re- lated to the Estonian language. It is more distantly related to numerous Uralic languages spoken in Russia. The history of written Finnish starts in the 16th century. Current orthography is connected to this written tradition, which developed into the current form in the late 19th century with a conscious plan- ning and systematic development of the lexicon. After this, the changes have been minor (Häkkinen, 1994, 16), and also impacted lexicon, especially what it comes to the development of the vocabu- lary of the modern society and traditional agrarian terminology becoming less known. The Finnish spoken language, however, is still largely based on Finnish dialects. In the 20th cen- tury some of the strongest dialectal features have been disappearing, but there are still clearly dis- tinguishable spoken vernacular varieties that are regionally marked. It has been shown that instead of clear disappearance of dialects there are vari- ous features that are spreading, but not at uniform rate, and reportedly younger speakers use the are- ally marked features less than the older speakers (Lappalainen, 2001, 92). Finnish vernaculars also represent historically rather different Finnic vari- eties, with major split between Eastern and Western dialects. There are, however, also dialect continu- ums and traditionally found gradual differentiation from region to region. Many of the changes have been lexical due to technical innovations and modernization of the society: orthographic spelling conventions have largely remained the same. Spoken Finnish, on the other hand, traditionally represents an areally di- vided dialect continuum, with several sharp bound- aries, and many regions of gradual differentiation from one municipality to another municipality. As mentioned, in the later parts of the 20th cen- tury relatively strong dialect leveling has been tak- ing place. Some of the Finnish dialects may already be concerned endangered, although the complex re- lationship between contemporary vernaculars and the most traditional dialectal forms makes this hard to ascertain. Dialect leveling in itself is a process known from many parts of Europe (Auer, 2018). However, in the case of Finnish the written stan- dard has remained relatively far from the spoken Finnish, besides individual narrow domains such as news broadcasts were the written form is used also in speech. Additionally there have been distinct text col- lections that include materials from this dialect archive. These include dialect books specific regions and municipalities, such as Oulun mur- rekirja [Dialect Book of Oulu] (Pääkkönen, 1994) or Savonlinnan seudun murrekirja [Dialect book of Savonlinna region] (Palander, 1986). There have also been more recent larger collections that contains excerpts from essentially all dialects (Lyytikäinen et al., 2013). Especially in the later parts of 21th century the spoken varieties have been leveling away from very specific local dialects, and although regional vari- eties still exist, most of the local varieties have certainly became endangered. Similar processes of dialect convergence have been reported from dif- ferent regions in Europe, although with substantial variation (Auer, 2018). In the case of Finnish this has not, however, resulted in merging of the written and spoken standards, but the spoken Finnish has remained, to our day, very distinct from the written standard. In a late 1950s, a program was set up to document extant spoken dialects, with the goal of recording 30 hours of speech from each municipal- ity. This work resulted in very large collections of dialectal recordings (Lyytikäinen, 1984, 448-449). Many of these have been published, and some por- tion has also been manually normalized. Dataset used is described in more detail in Section 3 Data. In Finnish linguistics the dialect identification has primarily been studied in the context of folk linguistics. In this line of research the perceptions of native speakers are investigated (Niedzielski and Preston, 2000). This type of studies have been done for Finnish, for example, by Mielikäinen and Palander (2014), Räsänen and Palander (2015) and Palander (2011). It has been possible to suggest for individual dialects which features are the most stable and will remain as local regional markers, and which seem to be in retention (Räsänen and Palander, 2015, 25). In this study we conduct just individual experiments and report their results, but in the further research we hope the results could be analyzed in more detail in connection with the earlier dialect perception studies, as we believe the differences in perceived dialect differences could be compared to the difficulties and successes the model has to differentiate individual varieties. Related work The current approaches to Finnish dialect have fo- cused on the textual modality only. Previously, bi- directional LSTM (long short-term memory) based models have been used to normalize Finnish di- alects to standard Finnish (Partanen et al., 2019) and to adapt standard Finnish text into different dialectal forms (Hämäläinen et al., 2020). Similar approach has also been used to normalize historical Finnish (Hämäläinen et al., 2021; Partanen et al., 2021). The closest research to our paper conducted for Finnish has been detection of foreign accents from audio. Behravan et al. (2013) have detected for- eign accents from audio only by using i-vectors. However, foreign accent detection is a very differ- ent task to native speaker dialect detection. Many foreign accents have clear cues through phonemes that are not part of the Finnish phonotactic system, where as with dialects, all phonemes are part of Finnish. There have been several recent approaches for Arabic to detect dialect from text (Balaji et al., 2020; Talafha et al., 2020; Alrifai et al., 2021). Textual dialect detection has been done also for German (Jauhiainen et al., 2018), Romanian (Za- haria et al., 2021) and Low Saxon (Siewert et al., 2020). The methods used range from traditional machine learning with features such as n-grams to neural models with pretrained embeddings, as it is typically the case in NLP research. None of these approaches use audio, as they rely on text only. At the same time, North Sami dialects have been identified from audio by training several models, kNNs, SVMs, RFs, CRFs, and LSTM, based on ex- tracted features (Kakouros et al., 2020). Kethireddy et al. (2020) use Mel-weighted SFF spectrogram to detect spoken Arabic dialects. Mel spectograms are also used by Draghici et al. (2020). All these approaches are mono-modal and use only audio. Based on our literature review, the existing ap- proaches use either text or audio for dialect detec- tion. We, however, use both modalities and apply them on a language with no existing dialect detec- tion models. Conclusions We have presented the first model for Finnish di- alect classification for a relatively large number of different dialects, 23 in total. Based on our ex- periments, a text only model is not as effective in dialect classification as a model with text and audio. It is clear that the amount of data alone is not the only variable that constitutes a high performance of the model for a given dialect, but also how dis- tinctive a given dialect is from other dialects. Since the speakers in the test set were not present in the training, we are confident that the dialect is the feature that the model has learned to predict. Using the audio materials offers in itself new interesting possibilities for dialect clustering and comparison. Traditional dialect atlases have also been used in automatic comparison and grouping of different Finnish dialects (Syrjänen et al., 2016). In further research we believe also this kind of infor- mation could be connected to the analysis to show how the dialect identification exactly interacts with the dialectal variation and differences at close mu- nicipality level. At the same time the identifiability of a dialect must be connected to the degree of di- alect leveling, linguistic distances and differences between them, so applying the model into newer recordings could also yield information about these processes. We have made all the data, code and models openly available on Github11 and Zenodo12. We believe that this is the only way to ensure this line of research continues for the Finnish language in the future as well.",1
"Abstract Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically de- tect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is re- ceived by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo. Introduction We present an approach for identifying the dialect of a speaker automatically solely based on text and on audio and text together. We compare the uni- modal approach to the bimodal one. There are no previous dialect identification approaches for Finnish. There are several situations were a dialect identification method can be of use. For example, if we have ASR models fine tuned for specific di- alects, the dialect identification from audio could be used as a preprocessing step. The model could also be used to label recorded materials automatically in order to create archival metadata. In order to make our contribution useful for others, we have released our code, models and processed data openly on GitHub1 and Zenodo2. Finnish is a large Uralic language that is one of the official languages of Finland, and is used essen- tially at all levels of the modern society. There are approximately five million Finnish speakers. The language belongs to the Finnic branch of the Uralic language family, and is very closely related to Kare- lian, Meankieli and Kveeni, and is also closely re- lated to the Estonian language. It is more distantly related to numerous Uralic languages spoken in Russia. The history of written Finnish starts in the 16th century. Current orthography is connected to this written tradition, which developed into the current form in the late 19th century with a conscious plan- ning and systematic development of the lexicon. After this, the changes have been minor (Hakkinen, 1994, 16), and also impacted lexicon, especially what it comes to the development of the vocabu- lary of the modern society and traditional agrarian terminology becoming less known. The Finnish spoken language, however, is still largely based on Finnish dialects. In the 20th cen- tury some of the strongest dialectal features have been disappearing, but there are still clearly dis- tinguishable spoken vernacular varieties that are regionally marked. It has been shown that instead of clear disappearance of dialects there are vari- ous features that are spreading, but not at uniform rate, and reportedly younger speakers use the are- ally marked features less than the older speakers (Lappalainen, 2001, 92). Finnish vernaculars also represent historically rather different Finnic vari- eties, with major split between Eastern and Western dialects. There are, however, also dialect continu- ums and traditionally found gradual differentiation from region to region. Many of the changes have been lexical due to technical innovations and modernization of the society: orthographic spelling conventions have largely remained the same. Spoken Finnish, on the other hand, traditionally represents an areally di- vided dialect continuum, with several sharp bound- aries, and many regions of gradual differentiation from one municipality to another municipality. As mentioned, in the later parts of the 20th cen- tury relatively strong dialect leveling has been tak- ing place. Some of the Finnish dialects may already be concerned endangered, although the complex re- lationship between contemporary vernaculars and the most traditional dialectal forms makes this hard to ascertain. Dialect leveling in itself is a process known from many parts of Europe (Auer, 2018). However, in the case of Finnish the written stan- dard has remained relatively far from the spoken Finnish, besides individual narrow domains such as news broadcasts were the written form is used also in speech. Additionally there have been distinct text col- lections that include materials from this dialect archive. These include dialect books specific regions and municipalities, such as Oulun mur- rekirja [Dialect Book of Oulu] (Paakkonen, 1994) or Savonlinnan seudun murrekirja [Dialect book of Savonlinna region] (Palander, 1986). There have also been more recent larger collections that contains excerpts from essentiallydialects (Lyytikainen et al., 2013). Especially in the later parts of 21th century the spoken varieties have been leveling away from very specific local dialects, and although regional vari- eties still exist, most of the local varieties have certainly became endangered. Similar processes of dialect convergence have been reported from dif- ferent regions in Europe, although with substantial variation (Auer, 2018). In the case of Finnish this has not, however, resulted in merging of the written and spoken standards, but the spoken Finnish has remained, to our day, very distinct from the written standard. In a late 1950s, a program was set up to document extant spoken dialects, with the goal of recording 30 hours of speech from each municipal- ity. This work resulted in very large collections of dialectal recordings (Lyytikainen, 1984, 448-449). Many of these have been published, and some por- tion has also been manually normalized. Dataset used is described in more detail in Section 3 Data. In Finnish linguistics the dialect identification has primarily been studied in the context of folk linguistics. In this line of research the perceptions of native speakers are investigated (Niedzielski and Preston, 2000). This type of studies have been done for Finnish, for example, by Mielikainen and Palander (2014), Rasanen and Palander (2015) and Palander (2011). It has been possible to suggest for individual dialects which features are the most stable and will remain as local regional markers, and which seem to be in retention (Rasanen and Palander, 2015, 25). In this study we conduct just individual experiments and report their results, but in the further research we hope the results could be analyzed in more detail in connection with the earlier dialect perception studies, as we believe the differences in perceived dialect differences could be compared to the difficulties and successes the model has to differentiate individual varieties. Related work The current approaches to Finnish dialect have fo- cused on the textual modality only. Previously, bi- directional LSTM (long short-term memory) based models have been used to normalize Finnish di- alects to standard Finnish (Partanen et al., 2019) and to adapt standard Finnish text into different dialectal forms (Hamalainen et al., 2020). Similar approach has also been used to normalize historical Finnish (Hamalainen et al., 2021; Partanen et al., 2021). The closest research to our paper conducted for Finnish has been detection of foreign accents from audio. Behravan et al. (2013) have detected for- eign accents from audio only by using i-vectors. However, foreign accent detection is a very differ- ent task to native speaker dialect detection. Many foreign accents have clear cues through phonemes that are not part of the Finnish phonotactic system, where as with dialects, all phonemes are part of Finnish. There have been several recent approaches for Arabic to detect dialect from text (Balaji et al., 2020; Talafha et al., 2020; Alrifai et al., 2021). Textual dialect detection has been done also for German (Jauhiainen et al., 2018), Romanian (Za- haria et al., 2021) and Low Saxon (Siewert et al., 2020). The methods used range from traditional machine learning with features such as n-grams to neural models with pretrained embeddings, as it is typically the case in NLP research. None of these approaches use audio, as they rely on text only. At the same time, North Sami dialects have been identified from audio by training several models, kNNs, SVMs, RFs, CRFs, and LSTM, based on ex- tracted features (Kakouros et al., 2020). Kethireddy et al. (2020) use Mel-weighted SFF spectrogram to detect spoken Arabic dialects. Mel spectograms are also used by Draghici et al. (2020). All these approaches are mono-modal and use only audio. Based on our literature review, the existing ap- proaches use either text or audio for dialect detec- tion. We, however, use both modalities and apply them on a language with no existing dialect detec- tion models. Conclusions We have presented the first model for Finnish di- alect classification for a relatively large number of different dialects, 23 in total. Based on our ex- periments,text only model is not as effective in dialect classification as a model with text and audio. It is clear that the amount of data alone is not the only variable that constitutes a high performance of the model for a given dialect, but also how dis- tinctive a given dialect is from other dialects. As mentioned, in the later parts of the 20th cen- tury relatively strong dialect leveling has been tak- ing place. Some of the Finnish dialects may already be concerned endangered, although the complex re- lationship between contemporary vernaculars and the most traditional dialectal forms makes this hard to ascertain. Dialect leveling in itself is a process known from many parts of Europe (Auer, 2018). However, in the case of Finnish the written stan- dard has remained relatively far from the spoken Finnish, besides individual narrow domains such as news broadcasts were the written form is used also in speech. Additionally there have been distinct text col- lections that include materials from this dialect archive. These include dialect books specific regions and municipalities, such as Oulun mur- rekirja [Dialect Book of Oulu] (Paakkonen, 1994) or Savonlinnan seudun murrekirja [Dialect book of Savonlinna region] (Palander, 1986). There have also been more recent larger collections that contains excerpts from essentially all dialects (Lyytikainen et al., 2013).",0
"ABSTRACT Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for the NER tasks to minimize the annotation cost without sacrificing model performance. However, heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose AL sentence query evaluation functions which pay more attention to possible positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize too long or too short sentences. Our experiments on three datasets from different domains reveal that the proposed approaches reduce the number of annotated tokens while achieving better or comparable prediction performance with conventional methods. Keywords Active learning, Named entity recognition, Annotation cost, Semi-supervised clustering Introduction Name entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes (e.g., person names, organizations, locations). NER is one of the fundamental natural language processing (NLP) tasks and is used in other NLP tasks such as entity linking, event extraction, and question answering. Although deep learning-based pre-trained language models [Devlin et al., 2019, Yang et al., 2019, Liu et al., 2019, Raffel et al., 2020] have advanced the state-of-the-art performance in NER [Torfi et al., 2020, Peters et al., 2019], a sufficient amount of labeled data is still necessary for achieving satisfactory prediction performance in most domains [Tikhomirov et al., 2020, Wang et al., 2020]. Since acquiring labeled data is both time and budget-consuming, efficient label acquisition for NER remains a challenge. A general framework for tackling the labeled data acquisition problem is active learning, in which the learner strategically chooses the most valuable instances as opposed to selecting a random sample for labeling [Thompson et al., 1999]. In the pool-based active learning setup, the active learner selects the most useful examples from an unlabeled pool of samples, queries them to an annotator for labeling; upon receiving the labels for the queried examples, the model is retrained with the augmented labeled set. These query selection, annotation, and retraining steps are iterated multiple times until either the desired performance is achieved or the labeling budget is exhausted [Settles, 2011]. The goal is to reduce the annotation cost by creating a smaller labeled set while still achieving good predictive performance. Active learning (AL) has demonstrated success in various sequence annotation tasks such as part-of-speech tagging [Ringger et al., 2007], dependency parsing [Li et al., 2016b] and semantic parsing [Thompson et al., 1999]. AL has been used to tackle the label acquisition problem in the NER task as well. Shen et al. [2017] demonstrated that AL combined with deep learning achieves nearly the same performance on standard datasets with just 25% of the original training data. Chen et al. [2015] developed and evaluated both existing and new AL methods for a clinical NER task. Their results showed that AL methods, particularly uncertainty-sampling approaches, provide significant savings in the annotation cost. In active learning, the most critical step is selecting the useful query examples for manual annotation. This step becomes more challenging for sequence labeling tasks, especially for named entity recognition, for two reasons. The first challenge of applying active learning to NER arises due to imbalanced data distribution. In NER annotation, a token is either labeled with its corresponding named entity class if it is part of a named entity or with the “other” class if it is not part of a named entity. The other class is generally referred as negative annotation or negative token, and all other labels -named entity labels- are referred as positive annotations or positive tokens [Marcheggiani and Artières, 2014]. In NER datasets, negative tokens are usually over-abundant compared to positive tokens. The second challenge in applying active learning to NER is related to the varying length of sentences. In NER, tokens are annotated one by one, but the context, hence the corresponding sentence, is still required for accurate token annotation. Therefore, at each active learning iteration, sentences are queried instead of tokens. Active learners that select the informative sentences for querying by directly aggregating over all the tokens are biased towards longer sentences. In order to prevent this bias towards sentences with more terms, the aggregated sentence scores are normalized by the number of tokens in the sentence [Engelson and Dagan, 1996, Haertel et al., 2008, Settles and Craven, 2008]. This commonly used approach solves the problem only partially, since this time, the active learner starts to query ""too"" short sentences in the early, and intermediate rounds [Tomanek, 2010]. In this paper, we moderate these two extreme cases and propose a normalization which exploits the corresponding datasets’ token count distribution. The varying length of sentences also affects the cost evaluation of the active learning framework. Some studies [Settles and Craven, 2008, Yao et al., 2009, Kim et al., 2006, Liu et al., 2020a] treat all sentences equally and compare active learning methods directly with respect to the number of sentences queried. However, this is not realistic since the cost is not fixed across sentences as sentences differ in the number of tokens and the number of named entities they contain [Arora et al., 2009, Haertel et al., 2008]. Therefore, the number of annotated tokens should be incorporated into the active learning cost. In this regard, many studies in the literature [Shen et al., 2004, Settles and Craven, 2008, Reichart et al., 2008, Shen et al., 2017] measure the cost of the annotation by the number of tokens annotated even though they query the sentences. Using only the token count is also an imperfect strategy as the cost of annotating the same number of tokens distributed over multiple sentences is not equivalent to annotating these tokens within a single sentence [Settles et al., 2008, Tomanek and Hahn, 2010]. This is mainly because there is a cost factor associated with each new sentence that is independent of its content and length. Even though we do not propose a new cost calculation method that encompasses all these different aspects, we consider these two cost evaluation set-ups to analyze the existing and proposed approaches in detail. In this study, we propose an extension to the subset of the existing uncertainty sampling methods to handle the challenges associated with the over-abundance of the negative tokens. In our proposed approach, the query evaluation metrics are designed to pay less attention to the tokens that are predicted to have negative annotations. We identify potentially negative tokens through clustering of pre-trained BERT representations after a semi-supervised dimensionality reduction. To the best of our knowledge, the use of the BERT embeddings directly in the active learning querying step for NER has never been attempted before this paper. Last but not least, this paper proposes a better normalization strategy for aggregating token scores to attain a good sentence query metric. For a fair comparison, we evaluate different active learning query methods both under the assumption of fixed annotation cost per sentence and fixed annotation cost per token. Our experiments on three datasets from different domains illustrate that our proposed approach reduces the number of annotated tokens while maintaining the slightly better or same level of prediction performance with the compared methods. We also present an extensive investigation about the effects of different pre-trained language embeddings on the performance of our NER model. The rest of the paper is organized as follows: Section 2 presents the NER data collections used in the experiments together with additional details to motivate the reader for the proposed method, Section 3 summarizes the general active learning setup and the commonly used active learning strategies for the NER task, and Section 4 describes the proposed approach. We describe our experimental setting in Section 5 and detail our results in Section 6. Section 7 concludes with a summary of our findings. Conclusion In this work, we focus on active learning for NER. One challenge of applying NER in active learning is the abundance of negative tokens. Uncertainty-based sentence query functions aggregate the scores of the tokens in a sentence, and since the negative tokens’ uncertainty scores dominate the overall score, they shadow the informative, positive tokens. In this work, we propose strategies to overcome this by focusing on the possible positive tokens. To identify positive tokens, we use a semi-supervised clustering strategy of tokens’ BERT embeddings. We experiment with several strategies where the sentence uncertainty score focuses on positive tokens and show empirically on multiple datasets that this is useful. A second challenge of querying sentences with NER is related to the length of the sentences. Longer sentences that contain more tokens can bring more information at once; however, the annotation cost is higher. Normalizing sentences with the tokens they contain, on the other hand, yields to querying too short sentences. We propose to normalize the scores such that sentences with the typical length for the dataset are queried more often. We evaluate the suggested methods based on both sentence and token-based cost analysis. Overall, we believe the work presented here can support active learning efforts in the NER task.",1
"ABSTRACT Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes. Even though deep learning-based pre-trained language models achieve good predictive performances, many domain-specific NER tasks still require a sufficient amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for the NER tasks to minimize the annotation cost without sacrificing model performance. However, heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose AL sentence query evaluation functions which pay more attention to possible positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize too long or too short sentences. Our experiments on three datasets from different domains reveal that the proposed approaches reduce the number of annotated tokens while achieving better or comparable prediction performance with conventional methods. Keywords Active learning, Named entity recognition, Annotation cost, Semi-supervised clustering Introduction Name entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into the predefined named entity classes (e.g., person names, organizations, locations). NER is one of the fundamental natural language processing (NLP) tasks and is used in other NLP tasks such as entity linking, event extraction, and question answering. Although deep learning-based pre-trained language models [Devlin et al., 2019, Yang et al., 2019, Liu et al., 2019, Raffel et al., 2020] have advanced the state-of-the-art performance in NER [Torfi et al., 2020, Peters et al., 2019], a sufficient amount of labeled data is still necessary for achieving satisfactory prediction performance in most domains [Tikhomirov et al., 2020, Wang et al., 2020]. Since acquiring labeled data is both time and budget-consuming, efficient label acquisition for NER remains a challenge. A general framework for tackling the labeled data acquisition problem is active learning, in which the learner strategically chooses the most valuable instances as opposed to selecting a random sample for labeling [Thompson et al., 1999]. In the pool-based active learning setup, the active learner selects the most useful examples from an unlabeled pool of samples, queries them to an annotator for labeling; upon receiving the labels for the queried examples, the model is retrained with the augmented labeled set. These query selection, annotation, and retraining steps are iterated multiple times until either the desired performance is achieved or the labeling budget is exhausted [Settles, 2011]. The goal is to reduce the annotation cost by creating a smaller labeled set while still achieving good predictive performance. Active learning (AL) has demonstrated success in various sequence annotation tasks such as part-of-speech tagging [Ringger et al., 2007], dependency parsing [Li et al., 2016b] and semantic parsing [Thompson et al., 1999]. AL has been used to tackle the label acquisition problem in the NER task as well. Shen et al. [2017] demonstrated that AL combined with deep learning achieves nearly the same performance on standard datasets with just 25% of the original training data. Chen et al. [2015] developed and evaluated both existing and new AL methods for a clinical NER task. Their results showed that AL methods, particularly uncertainty-sampling approaches, provide significant savings in the annotation cost. In active learning, the most critical step is selecting the useful query examples for manual annotation. This step becomes more challenging for sequence labeling tasks, especially for named entity recognition, for two reasons. The first challenge of applying active learning to NER arises due to imbalanced data distribution. In NER annotation, a token is either labeled with its corresponding named entity class if it is part of a named entity or with the ""other"" class if it is not part of a named entity. The other class is generally referred as negative annotation or negative token, and all other labels -named entity labels- are referred as positive annotations or positive tokens [Marcheggiani and Artieres, 2014]. In NER datasets, negative tokens are usually over-abundant compared to positive tokens. The second challenge in applying active learning to NER is related to the varying length of sentences. In NER, tokens are annotated one by one, but the context, hence the corresponding sentence, is still required for accurate token annotation. Therefore, at each active learning iteration, sentences are queried instead of tokens. Active learners that select thesentences for querying by directly aggregating over all the tokens are biased towards longer sentences. In order to prevent this bias towards sentences with more terms, the aggregated sentence scores are normalized by the number of tokens in the sentence [Engelson and Dagan, 1996, Haertel et al., 2008, Settles and Craven, 2008]. This commonly used approach solves the problem only partially, since this time, the active learner starts to query ""too"" short sentences in the early, and intermediate rounds [Tomanek, 2010]. In this paper, we moderate these two extreme cases and propose a normalization which exploits the corresponding datasets' token count distribution. The varying length of sentences also affects the cost evaluation of the active learning framework. Some studies [Settles and Craven, 2008, Yao et al., 2009, Kim et al., 2006, Liu et al., 2020a] treat all sentences equally and compare active learning methods directly with respect to the number of sentences queried. However, this is not realistic since the cost is not fixed across sentences as sentences differ in the number of tokens and the number of named entities they contain [Arora et al., 2009, Haertel et al., 2008]. Therefore, the number of annotated tokens should be incorporated into the active learning cost. In this regard, many studies in the literature [Shen et al., 2004, Settles and Craven, 2008, Reichart et al., 2008, Shen et al., 2017] measure the cost of the annotation by the number of tokens annotated even though they query the sentences. Using only the token count is also an imperfect strategy as the cost of annotating the same number of tokens distributed over multiple sentences is not equivalent to annotating these tokens within a single sentence [Settles et al., 2008, Tomanek and Hahn, 2010]. This is mainly because there is a cost factor associated with each new sentence that is independent of its content and length. Even though we do not propose a new cost calculation method that encompasses all these different aspects, we consider these two cost evaluation set-ups to analyze the existing and proposed approaches in detail. In this study, we propose an extension to the subset of the existing uncertainty sampling methods to handle the challenges associated with the over-abundance of the negative tokens. In our proposed approach, the query evaluation metrics are designed to pay less attention to the tokens that are predicted to have negative annotations. We identify potentially negative tokens through clustering of pre-trained BERT representations after a semi-supervised dimensionality reduction. To the best of our knowledge, the use of the BERT embeddings directly in the active learning querying step for NER has never been attempted before this paper. Last but not least, this paper proposes a better normalization strategy for aggregating token scores to attain a good sentence query metric. For a fair comparison, we evaluate different active learning query methods both under the assumption of fixed annotation cost per sentence and fixed annotation cost per token. Our experiments on three datasets from different domains illustrate that our proposed approach reduces the number of annotated tokens while maintaining the slightly better or same level of prediction performance with the compared methods. We also present an extensive investigation about the effects of different pre-trained language embeddings on the performance of our NER model. The rest of the paper is organized as follows: Section 2 presents the NER data collections used in the experiments together with additional details to motivate the reader for the proposed method, Section 3 summarizes the general active learning setup and the commonly used active learning strategies for the NER task, and Section 4 describes the proposed approach. We describe our experimental setting in Section 5 and detail our results in Section 6. Section 7 concludes with a summary of our findings. Conclusion In this work, we focus on active learning for NER. One challenge of applying NER in active learning is the abundance of negative tokens. Uncertainty-based sentence query functions aggregate the scores of the tokens in a sentence, and since the negative tokens' uncertainty scores dominate the overall score, they shadow the informative, positive tokens. In this work, we propose strategies to overcome this by focusing on the possible positive tokens. To identify positive tokens, we use a semi-supervised clustering strategy of tokens' BERT embeddings. We experiment with several strategies where the sentence uncertainty score focuses on positive tokens and show empirically on multiple datasets that this is useful. A second challenge of querying sentences with NER is related to the length of the sentences. Longer sentences that contain more tokens can bring more information at once; however, the annotation cost is higher. Normalizing sentences with the tokens they contain, on the otheryields to querying too short sentences. We propose to normalize the scores such that sentences with the typical length for the dataset are queried more often. We evaluate the suggested methods based on both sentence and token-based cost analysis. Overall, we believe the work presented here can support active learning efforts in the NER task as well.",0
"Abstract In this work we consider the col- lection of deceptive April Fools’ Day (AFD) news articles as a useful addition in existing datasets for deception detec- tion tasks. Such collections have an es- tablished ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that in- cludes diachronic AFD and normal ar- ticles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current re- search thread, we also discuss the individ- ualism/collectivism dimension in decep- tion with respect to these two datasets. Lastly, we build classifiers by testing vari- ous monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observa- tions of other deception detection works. Introduction April Fools’ Day (for short AFD) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide. Specifically, the contributions of this work are: • A new corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites2, adding one more AFD collection to the currently unique one in En- glish (Dearden and Baron, 2019). • A study and discussion of the linguistic cues of deception that prevail in the Greek and En- glish collection, along with their similarities. • A discussion on whether the consideration of the individualism/collectivism cultural di- mension in the context of AFD aligns with the results of our previous work. • An examination of the performance of vari- ous classifiers in identifying AFD articles, in- cluding multilanguage setups. Related Work The creation of reliable and realistic ground truth datasets for the deception detection task is a chal- lenging task (Fitzpatrick and Bachenko, 2012). Crowdsourcing, in the form of online campaigns in which people express themselves in truth- ful and/or deceitful manner for a small pay- ment are a well established way to collect de- ceptive data (Ott et al., 2011). Real-life situations such as trials (Soldner et al., 2019) or the use of data from board games have also been employed (Peskov et al., 2020). Also a popular approach is the reuse of content from sites that debunk ar- ticles like fake news and hoaxes (Wang, 2017; Kochkina et al., 2018). Lastly, satire news are another way to collect deceptive texts, but with some particularities due to humorous deception (Skalicky et al., 2020). The only work that explores AFD articles is that of Dearden et al. (2019). They collected 519 AFD and 519 truthful stories and articles in English for a period of 14 years. A large set of features was exploited to identify deception cues in AFD sto- ries. Structural complexity and level of detail were among the most valuable features while the ex- ploitation of the same feature set to a fake news dataset resulted in similar observations. To the best of our knowledge, the only decep- tion related dataset for the Greek language is that of Karidi et al. (2019). This work proposed an automatic process for the creation of a fake news and hoaxes articles corpus, but unfortunately the created corpus over Greek websites is not avail- able. If we also consider that the creation of a Greek dataset for deception through crowdsourc- ing is a cumbersome and expensive task, that is further hindered by the exceptionally limited num- ber of native Greek crowd workers, it is easy to understand why there is a lack of datasets. Regarding the individualism/collectivism cul- tural dimension, it constitutes a well-known divi- sion of cultures that concerns the degree in which members of a culture value more individual over group goals and vice versa. In individualism, ties between individuals are loose and individuals are expected to take care of only themselves and their immediate families, whereas in collectivism ties in society are stronger. In Papantoniou et al. (2021) there is an preliminary effort driven by prior work in psychology discipline (Taylor et al., 2017) to examine if deception cues are altered across cul- tures and if this can be attributed to this divide. Among the conclusions were that people from in- dividualistic cultures employ more third and less first person pronouns to distance themselves from the deceit when they are deceptive, whereas in the collectivism group this trend is milder, signalling the effort of the deceiver to distance the group from the deceit. In addition, in individualistic cul- tures positive sentiment is employed in deceptive language, whereas in collectivists there is a re- straint of expression of sentiment both in truthful and deceptive texts. To this end, this work explores the deception- related characteristics of a new Greek cor- pus based on AFD articles from a variety of sources, and compares them with the En- glish ones3 . Further, since related stud- ies (Triandis and Vassiliou, 1972; Hofstede, 1980; Koutsantoni, 2005) describe Greece as a culture with more collectivistic characteristics (by using country as proxy from culture), we also discuss differences in deception cues along this cultural dimension. Conclusion and Future work We introduced a new dataset with AFD news articles in Greek and analyzed and compared its de- ception cues with another English one. The results showcased the use of emotional language, es- pecially of positive sentiment, for deceptive arti- cles which is even more prevalent in the individ- ualistic English dataset. Further, deceptive arti- cles use less concrete language, as manifested by the increased use of adverbs, hedges, and boosters and less usage of named entities, spatial related words and conjunctions compared to the truth- ful ones. The future and past tenses were corre- lated with deceptive and truthful articles respec- tively. All the above, mainly align with previ- ous work (Papantoniou et al., 2021), except from some differences in the usage of pronouns for the Greek dataset, which is attributed to the idiosyn- crasies of the news domain. The accuracy of the deployed classifiers offered adequate perfor- mance, with no statistically significant differences between the best logistic regression and the BERT models. In the future we aim at creating even more crosslingual datasets for deception detection tasks through crowdsourcing and by employing the Chattack platform (Smyrnakis et al., 2021).",1
"Abstract In this work we consider the col- lection of deceptive April Fools' Day (AFD) news articles as a useful addition in existing datasets for deception detec- tion tasks. Such collections have an es- tablished ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that in- cludes diachronic AFD and normal ar- ticles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current re- search thread, we also discuss the individ- ualism/collectivism dimension in decep- tion with respect to these two datasets. Lastly, we build classifiers by testing vari- ous monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observa- tions of other deception detection works. Introduction April Fools' Day (for short AFD) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide. Specifically, the contributions of this work are:  A new corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites2, adding one more AFD collection to the currently unique one in En- glish (Dearden and Baron, 2019). A study and discussion of the linguistic cues of deception that prevail in the Greek and En- glish collection, along with their similarities. A discussion on whether the consideration of the individualism/collectivism cultural di- mension in the context of AFD aligns with the results of our previous work. An examination of the performance of vari- ous classifiers in identifying AFD articles, in- cluding multilanguage setups. Related Work The creation of reliable and realistic ground truth datasets for the deception detection task is a chal- lenging task (Fitzpatrick and Bachenko, 2012). Crowdsourcing, in the form of online campaigns in which people express themselves in truth- ful and/or deceitful manner for a small pay- ment are a well established way to collect de- ceptive data (Ott et al., 2011). Real-life situations such as trials (Soldner et al., 2019) or the use of data from board games have also been employed (Peskov et al., 2020). Also a popular approach is the reuse of content from sites that debunk ar- ticles like fake news and hoaxes (Wang, 2017; Kochkina et al., 2018). Lastly, satire news are another way to collect deceptive texts, but with some particularities due to humorous deception (Skalicky et al., 2020). The only work that explores AFD articles is that of Dearden et al. (2019). They collected 519 AFD and 519 truthful stories and articles in English for a period ofyears. A large set of features was exploited to identify deception cues in AFD sto- ries. Structural complexity and level of detail were among the most valuable features while the ex- ploitation of the same feature set to a fake news dataset resulted in similar observations. To the best of our knowledge, the only decep- tion related dataset for the Greek language is that of Dearden and Baron. This work proposed an automatic process for the creation of a fake news dataset, but unfortunately the created datasets for Greek and En- glish websites are not avail- able. If we also consider that the creation of a Greek dataset for deception through our in- struments with news websites is a cumbersome and expensive task, that is further hindered by the exceptionally limited num- ber of native Greek crowd workers, it is easy to understand why there is a lack of datasets. Regarding the individualism/collectivism cul- tural dimension, it constitutes a well-known divi- sion of cultures that concerns the degree in which members of a culture value more individual over group goals and vice versa. In individualism, ties between individuals are loose and individuals are expected to take care of only themselves and their immediate families, whereas in collectivism ties in society are stronger. In Papantoniou et al. (2005). reflecting on the Greek and En- glish collection, they consider the cul- tural dimension to be in doubt. They build a rich linguistic feature set, and analyze and compare its deception cues with the only other dataset, which is in English. Following a current re- search thread, we also discuss the individ- ualism/collectivism dimension in decep- tion with respect to the Greek and En- glish datasets. Lastly, we build classifiers by testing vari- ous monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection stud- ies, and are in alignment with the observa- tions of other deception detection works. Introduction April Fools' Day (for short AFD) is a long stand- ing custom, mostly in Western societies. It is the only day of the year when practical jokes and de- ception are expected. This is the case for all social interactions, including journalism, which is gener- ally considered to aim at the presentation of truth. Every year on this day, newspapers and news web- sites take part in an unofficial competition to in- vent the most believable, but untrue story. In this respect, AFD news articles fall into the deception spectrum, as they satisfy widely acceptable defini- tions of deception as in Masip et al. (2005). The massive participation of news media in this custom establishes a rich corpus of decep- tive articles from a diversity of sources. Although AFD articles may exploit common linguistic in- struments with satire news, like exaggeration, hu- mour, irony and paralogism, they are usually con- sidered a distinct category. This is mainly due to the fact that they also employ other mecha- nisms which characterize deception in general, like sophisms, and changes in cognitive load and emotions (Hauch et al., 2015) to deceive their au- dience. AFD articles are often believable, and there exist cases where sophisticated AFD articles have been reproduced by major international news agencies worldwide1. This motivated us to extend our previous work on linguistic cues of deception and their relation to the cultural dimension of individualism and col- lectivism (Papantoniou et al., 2021), in the context of the AFD. That work examines if differences in the usage of linguistic cues of deception (e.g., pronouns) across cultures can be identified and at- tributed to the individualism/collectivism divide.",0
"Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a ‘subcontinent’. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ̄ra ̄o, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally, would pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: • We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of Indic language data. This data has been curated in a short time frame from public sources which have a permissive license. It indicates that the feasibility of collecting large amount of pretraining data and further efforts can be made to significantly expand this collection. • Starting from the wav2vec 2.0 model, we perform extensive ablation studies on architecture choices, pretraining and fine-tuning strategies, language models and choice of lexicon to arrive at a training and decoding regimen that works well for Indian languages. • Our ASR models achieve SOTA performance on 9 Indian languages on 3 publicly available bench- marks with small fine-tuning datasets. These re- sults indicate that end-to-end ASR systems based on multilingual pretraining with the wav2vec model hold promise for Indic languages. • Our ablation studies reveal that the accuracy of the ASR system on Indic languages sensitively de- pends on the size of the pretraining corpus, amount of labelled data for fine-tuning, and access to task- specific lexicon. In summary, we establish that the recent ad- vances of pretraining wav2vec models transfer to Indic ASR and achieve SOTA results against mod- els proposed over multiple years. However, unlike in the reported results of English ASR, we observe that the WER reported for Indic ASR is signifi- cantly higher and sensitively depends on availability of resources: pretraining corpus, fine-tuning data, and task-specific language information. This suggests that the ASR task on Indic languages re- mains far from being solved and requires model innovation and continued efforts on curating re- sources. We publicly release all the artifacts of our work to spur further work in the area of Indic ASR. This includes: (a) sources of pretraining data along with scripts for their collection and pre-processing, (b) pretraining, fine-tuning and decoding scripts, (c) language models, and (d) our best ASR models. Conclusion We report results of applying two recent and suc- cessful ideas from English ASR to Indic ASR: use of wav2vec like model architecture and use of un- labelled data to pretrain the model. We implement this with a curated dataset on Indic languages and a range of ablation studies on architecture, pretrain- ing, fine-tuning, and decoding choices. Through this, we obtain state-of-the-art results on 9 Indic languages across 3 datasets. While advancing ASR systems for the next billion users from the sub- continent, our results highlight the need for larger resources and benchmarks across more languages. All the models developed as a part of this work, viz., the pretrained model, the language-specific fine-tuned models and the language models along with the Fairseq and KenLM scripts and configu- ration files used for building them will be publicly released. We hope that these models will help in advancing the state of the art for Indian Speech Technology.",1
"Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a ""subcontinent"". It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba rao, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally,pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of Indo-Aryan and Dravidian languages is identified. This data has been curated in a short time frame from public sources which have a permissive license. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a ""subcontinent"". It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subbarao, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective.",0
"Abstract Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expen- sive to train. We propose a simple and effi- cient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classifi- cation datasets in four domains, TLM achieves results better than or similar to pretrained lan- guage models (e.g., RoBERTa-Large) while re- ducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development. Introduction Pretrained language models (PLMs) have drawn much attention from the natural language process- ing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are finetuned on a small amount of labeled data for specific downstream tasks. This pretraining-finetuning framework has significantly improved the performance of many NLP tasks. However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36×1021 FLOPs3. Larger PLMs such as GPT- 3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretrain- ing loss functions. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Even though there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019). In this work, we explore alternatives to the stan- dard pretraining-finetuning paradigm, aiming at more drastic efficiency improvement without per- formance drop. We propose a simple, efficient, pretraining-free framework, Task-driven LanguageModeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, hu- mans master a task by using only a small portion of world knowledge (e.g., students only need to re- view a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a spe- cific task. Second, training on supervised labeled data is much more data efficient for downstream performance than optimizing the language model- ing objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modeling objective using both the retrieved data and the task data. We evaluate TLM on eight different tasks cov- ering the domains of news, review, computer sci- ence, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude. Related work Pretrained Language Models Pretrained lan- guage models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those mod- els are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB En- glish corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger cor- pus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% ∼ 70% reduc- tion in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled represen- tation of content and position, which brings about 50% ∼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efficiency by reducing training data redundancy. Our approach also re- sults in more drastic improvements. Efficient Inference of Pretrained Models An- other line of work aims at improving inference efficiency of PLMs. Some works improve in- ference efficiency by distilling large PLMs into small-sized models and using the distilled mod- els for inference, such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), Mobile- BERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with low- precision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). An- other type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distil- lation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our ap- proach doesn’t rely on large-scale pre-training and achieves better or at least comparable performance. Domain and Task Adaptation for Pretrained Models Domain-adaptive finetuning is a method that finetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adapta- tion (Gururangan et al., 2020; Zhang et al., 2019; Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive fine- tuning and TLM. First, TLM is a general method to improve training efficiency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In com- parison, domain-adaptive finetuning uses domain data to improve domain adaptation. Second, while previous works on domain-adaptive finetuning are built upon a model pretrained on the general corpus, TLM learns from scratch without large-scale pretraining to substantially save computation costs. Conclusions In this paper, we have proposed a simple, efficient, pretraining-free framework, TLM. The core idea is to only use a tiny, task-relevant subset of the general corpus for language model training. Our experiments show that TLM achieves results simi- lar to or even better than PLMs, with a reduction of training FLOPs by two orders of magnitude. TLM opens the possibility of reducing the heavy reliance on large-scale PLMs and training a model from scratch in an efficient manner, while not hurt- ing the overall performance. We hope TLM will contribute to democratizing NLP and expediting its development by allowing most researchers to freely explore the architectures, loss functions, al- gorithms, and other design choices in the neighbor- hood of a state-of-the-art solution. As discussed in Section 3.2, there are several potential directions for future work. It will be in- teresting to study how to use TLM to match the performance even larger-scale PLMs. Moreover, further extending and improving TLM for few-shot and zero-shot learning is a crucial problem.",1
"Abstract Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expen- sive to train. We propose a simple and effi- cient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classifi- cation datasets in four domains, TLM achieves results better than or similar to pretrained lan- guage models (e.g., RoBERTa-Large) while re- ducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development. Introduction Pretrained language models (PLMs) have drawn much attention from the natural language process- ing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are finetuned on a small amount of labeled data for specific downstream tasks. This pretraining-finetuning framework has significantly improved the performance of many NLP tasks. However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36 1021 FLOPs3. Larger PLMs such as GPT- 3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretrain- ing loss functions. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Even though there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019). In this work, we explore alternatives to the stan- dard pretraining-finetuning paradigm, aiming at more drastic efficiency improvement without per- formance drop. We propose a simple, efficient, pretraining-free framework, Task-driven LanguageModeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, hu- mans master a task by using only a small portion of world knowledge (e.g., students only need to re- view a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a spe- cific task. Second, training on supervised labeled data is much more data efficient for downstream performance than optimizing the language model- ing objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modelingusing both the retrieved data and the task data. We evaluate TLM on eight different tasks cov- ering the domains of news, review, computer sci- ence, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude. Related work Pretrained Language Models Pretrained lan- guage models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those mod- els are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB En- glish corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger cor- pus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% ¼ 70% reduc- tion in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled represen- tation of content and position, which brings about 50% ¼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efficiency by reducing training data redundancy. Our approach also re- sults in more drastic improvements. Efficient Inference of Pretrained Models An- other line of work aims at improving inference efficiency of PLMs. Some works improve in- ference efficiency by distilling large PLMs into small-sized models and using the distilled mod- els for inference, such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), Mobile- BERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with low- precision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). An- other type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distil- lation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our ap- proach doesn't rely on large-scale pre-training and achieves better or at least comparable performance. Domain and Task Adaptation for Pretrained Models Domain-adaptive finetuning is a method that finetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adapta- tion (Gururangan et al., 2020; Zhang et al.,Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive fine- tuning and TLM. First, TLM is a general method to improve training efficiency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In BERT, all data is pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT is also best suited for (i) trained neural networks with limited computational costs (e.g., RoBERTa-Large) and (ii) using task data as queries to retrieve a tiny subset of the general corpus. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% ¼ 70% reduc- tion in computational costs.",0
"Abstract Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation of the administering agency and the COVID-19 vaccines themselves. This paper proposes a semi-supervised ma- chine learning pipeline to perform topic modeling, sentiment analysis, and an analysis of vaccine brand reputation to obtain an in-depth understanding of national public opinion of Filipinos on Facebook. The methodology makes use of a multilingual version of Bidirectional Encoder Representations from Transformers or BERT for topic modeling, hierarchical clustering, five dif- ferent classifiers for sentiment analysis, and cosine similarity of BERT topic embeddings for vaccine brand reputation analysis. Results suggest that any type of COVID-19 misinformation is an emergent property of COVID-19 public opinion, and that the detection of COVID-19 misinformation can be an unsupervised task. Sentiment analysis aided by hierarchical clustering reveal that 21 of the 25 topics extrapolated by topic modeling are negative topics. Such negative comments spike in count whenever the Department of Health in the Philippines posts about the COVID-19 situation in other countries. Additionally, the high numbers of laugh reactions on the Face- book posts by the same agency—without any humorous content—suggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase neg- ative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4. Sputnik V, AstraZeneca, and Sinovac suffer from a negative public reputation 5. Ministries of health and stakeholders in vaccination campaigns are rec- ommended to employ interventions that correct misinformation, engage people and use local narratives of success Introduction The COVID-19 pandemic has drastically affected the overall wellness and health of the entire world. On January 30, 2020, the first case of COVID- 19, in the Philippines, has been reported by the country’s Department of Health (DOH). COVID-19 is a respiratory disease caused by the SARS-CoV- 2 virus—first identified in the province of Wuhan, located in China (Paules et al., 2020). Over a year later, on March 1, 2021, the Philippines started with its COVID-19 vaccination program. Vaccine hesitancy among Filipinos is an on- going phenomenon that the national vaccine campaign efforts are struggling with (Alfonso et al., 2021). According to a recent study in the Philippines (Caple et al., 2021), only 62.5% of their 7,193 respondents are willing to be vaccinated against COVID-19. A majority of the same respondents are only willing to be inoculated after many others have received the vaccine or after political figures have done so (Caple et al., 2021). Additionally, the partici- pants’ preferences of vaccine brand are also studied; 59.7% of the participants are confident in a USA-made or European-made COVID-19 vaccine (Caple et al., 2021). Sentiment analysis is a common natural language processing (NLP) task that has been done on a number of studies regarding COVID-19 public opin- ion (Melton et al., 2021; Garcia and Berton, 2021). It computationally classi- fies the polarity of text data—neutral, positive, or negative sentiment. This is primarily done since gauging the sentiment of the public, especially on critical topics such as a pandemic like COVID-19, help determine possible policies and interventions that could shape the actions that society takes. Furthermore, topic modeling is proposed as part of the NLP pipeline of the same studies mentioned earlier. Topic modeling is an unsupervised technique for obtaining the relevant ideas that public opinion holds, more of which is discussed in a later section of this article. This paper proposes a pipeline for understanding the public opinion in the Philippines regarding COVID-19 and the country’s vaccination efforts. The semi-supervised pipeline is named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions (VER- TEBRATE). It is comprised of three main modules: a topic extraction or topic modeling of opinions, an analysis of vaccine brand reputation, and a sentiment analysis of the same set of opinions. Topic modeling is per- formed through the use of contextual embeddings provided by a multilingual transformer architecture—discussed further in Section 2.3. The topics ob- tained are compared and associated with the vaccine brands available in the Philippines through cosine similarity. Next, a hierarchical clustering of the topics extrapolated through topic modeling is used to assign the labels used for sentiment analysis. Sentiment analysis on the data is performed by five different classification algorithms—assessing which architecture models the sentiments of the text data most effectively. The classification algorithms proposed are XGBoost, LightGBM, K-nearest neighbors, Naive Bayes’ al- gorithm, and support vector machine. The public opinions considered for this study are comments made on 50 different Facebook posts by the official page of the Department of Health Philippines. The posts are part of the RESBAKUNA campaign of the same government department. Conclusion The COVID-19 pandemic has radically changed the world. Over a year after the start of the outbreak, worldwide vaccine efforts against COVID-19 have begun. On the first day of March 2021, the Philippines started its vac- cination program under the RESBAKUNA campaign. A recent study shows that vaccine hesitancy is a prevalent issue in the Philippines. The same concerns and complaints over the COVID-19 vaccines can also be found on social media, like Facebook. To understand and gauge national public opinion via the Facebook platform, the top comments from 50 Facebook posts of the official Facebook page of the Department of Health Philippines were scraped via Selenium and Python. The 50 Facebook posts are part of the “#RESBAKUNA” Facebook campaign by the agency. Next, a semi- supervised pipeline named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions or VERTE- BRATE is proposed. It is a pipeline that leverages topic modeling via multi- lingual BERT, analysis of vaccine brand reputation through cosine similarity of the BERT topic embeddings, hierarchical clustering of the BERT topic embeddings, and sentiment classification of the Facebook comments that are labeled automatically by the clades produced by the hierarchical clustering model. The BERT topic model extracted 25 topics—2 of which are indeter- minable, 2 of which are positive, and 21 topics are negative comments. These topics are precisely clustered through Ward’s linkage hierarchical clustering. The model suggests that COVID-19 misinformation and COVID-19 vaccine misinformation are emergent properties of COVID-19 public opinion. Using supervised techniques to model such misinformation is unnecessary. In terms of the temporal distribution of these topics, the analysis suggests that neg- ative comments drastically increase when the Department of Health Philip- pines posts about other countries. It can also be noted that the huge numbers of laugh reactions throughout the Facebook posts—without the presence of any humorous content—entail that the Department of Health Philippines has a public image issue. In this instance, people tend to react the way they do, not because of the posts but because of who posted them. As for the brand reputation of the different COVID-19 vaccines, Pfizer and Moderna are doing relatively well—especially Pfizer’s vaccine. Mean- while, Sputnik V, AstraZeneca, and Sinovac suffer from negative associations. Additionally, clade-assisted sentiment classification effectively models public sentiment. The best-performing classifier, LightGBM, that is proposed in the study has managed to perform with 92.4% accuracy. It also has a strong level of agreement in terms of Cohen’s kappa statistic, with a value of 0.847. Our present study highlights the persisting prevalence of COVID-19 vac- cine misinformation in social media. Conspiracy beliefs and other forms of misinformation had been noted as a significant predictor of complete vac- cine hesitancy (Al-Sanafi and Sallam, 2021). We noticed that DOH and its entities offered no responses to the comments posted by netizens under these infographics. Meta-analytic evidence suggests the importance of iden- tifying misinformation most susceptible to correction, and engage experts in responding to misinformation (Walter et al., 2020). We recommend for DOH to form a social media team composed of health care professionals and in- terdisciplinary communication practitioners whose mandate is to respond to misinformation found in the comments of their posts. This engaging way of correcting false vaccine information will not only help quell vaccine doubts, but also hopefully improve the image of DOH among citizens. Also, our present study suggests that instead of drawing aspirational sen- timents from people, posting about the success of wealthier countries in terms of vaccination only intensifies Filipinos’ xenocentric tendencies to rationalize the poor COVID outcomes of the country. We recommend for DOH and other stakeholders involved in vaccine promotion to use narratives that are closer to home, like the diseases curbed by the expanded program on immu- nization in the Philippines. Using relatable narratives and storytelling had been indicated as effective means to combat anti-vaccine conspiracies (Lazi ́c and Zˇeˇzelj, 2021). The VERTEBRATE pipeline effectively highlights the contents to avoid when posting on social media about COVID-19. Future work could include the implementation of an automatic labeling procedure for the topic model to further reduce manual effort.",1
"Abstract Vaccine hesitancy and other COVID-19-related concerns and complaints in the Philippines are evident on social media. It is important to identify these different topics and sentiments in order to gauge public opinion, use the insights to develop policies, and make necessary adjustments or actions to improve public image and reputation of the administering agency and the COVID-19 vaccines themselves. This paper proposes a semi-supervised ma- chine learning pipeline to perform topic modeling, sentiment analysis, and an analysis of vaccine brand reputation to obtain an in-depth understanding of national public opinion of Filipinos on Facebook. The methodology makes use of a multilingual version of Bidirectional Encoder Representations from Transformers or BERT for topic modeling, hierarchical clustering, five dif- ferent classifiers for sentiment analysis, and cosine similarity of BERT topic embeddings for vaccine brand reputation analysis. Results suggest that any type of COVID-19 misinformation is an emergent property of COVID-19 public opinion, and that the detection of COVID-19 misinformation can be an unsupervised task. Sentiment analysis aided by hierarchical clustering reveal that 21 of the 25 topics extrapolated by topic modeling are negative topics. Such negative comments spike in count whenever the Department of Health in the Philippines posts about the COVID-19 situation in other countries. Additionally, the high numbers of laugh reactions on the Face- book posts by the same agency without any humorous content suggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase neg- ative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4. Sputnik V, AstraZeneca, and Sinovac suffer from a negative public reputation 5. Ministries of health and stakeholders in vaccination campaigns are rec- ommended to employ interventions that correct misinformation, engage people and use local narratives of success Introduction The COVID-19 pandemic has drastically affected the overall wellness and health of the entire world. On January 30, 2020, the first case of COVID- 19, in the Philippines, has been reported by the country's Department of Health (DOH). COVID-19 is a respiratory disease caused by the SARS-CoV- 2 virus first identified in the province of Wuhan, located in China (Paules et al., 2020). Over a year later, on March 1, 2021, the Philippines started with its COVID-19 vaccination program. Vaccine hesitancy among Filipinos is an on- going phenomenon that the national vaccine campaign efforts are struggling with (Alfonso et al., 2021). According to a recent study in the Philippines (Caple et al., 2021), only 62.5% of their 7,193 respondents are willing to be vaccinated against COVID-19. A majority of the same respondents are only willing to be inoculated after many others have received the vaccine or after political figures have done so (Caple et al., 2021). Additionally, the partici- pants' preferences of vaccine brand are also studied; 59.7% of the participants are confident in a USA-made or European-made COVID-19 vaccine (Caple et al., 2021). Sentiment analysis is a common natural language processing (NLP) task that has been done on a number of studies regarding COVID-19 public opin- ion (Melton et al., 2021; Garcia and Berton, 2021). It computationally classi- fies the polarity of text data neutral, positive, or negative sentiment. This is primarily done since gauging the sentiment of the public, especially on critical topics such as a pandemic like COVID-19, help determine possible policies and interventions that could shape the actions that society takes. Furthermore, topic modeling is proposed as part of the NLP pipeline of the same studies mentioned earlier. Topic modeling is an unsupervised technique for obtaining the relevant ideas that public opinion holds, more of which is discussed in a later section of this article. This paper proposes a pipeline for understanding the public opinion in the Philippines regarding COVID-19 and the country's vaccination efforts. The semi-supervised pipeline is named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, andEmotions (VER- TEBRATE). It is comprised of three main modules: a topic extraction or topic modeling of opinions, an analysis of vaccine brand reputation, and a sentiment analysis of the same set of opinions. Topic modeling is per- formed through the use of contextual embeddings provided by a multilingual transformer architecture discussed further in Section 2.3. The topics ob- tained are compared and associated with the vaccine brands available in the Philippines through cosine similarity. Next, a hierarchical clustering of the topics extrapolated through topic modeling is used to assign the labels used for sentiment analysis. Sentiment analysis on the data is performed by five different classification algorithms assessing which architecture models the sentiments of the text data most effectively. The classification algorithms proposed are XGBoost, LightGBM, K-nearest neighbors, Naive Bayes al- gorithm, and support vector machine. The public opinions considered for this study are comments made on 50 different Facebook posts by the official page of the Department of Health Philippines. The posts are part of the RESBAKUNA campaign of the same government department. Conclusion The COVID-19 pandemic has radically changed the world. Over a year after the start of the outbreak, worldwide vaccine efforts against COVID-19 have begun. On the first day of March 2021, the Philippines started its vac- cination program under the RESBAKUNA campaign. A recent study shows that vaccine hesitancy is a prevalent issue in the Philippines. The same concerns and complaints over the COVID-19 vaccines can also be found on social media, like Facebook. To understand and gauge national public opinion via the Facebook platform, the top comments from 50 Facebook posts of the official Facebook page of the Department of Health Philippines were scraped via Selenium and Python. The 50 Facebook posts are part of the ""#RESBAKUNA"" Facebook campaign by the agency. Next, a semi- supervised pipeline named Vaccine-related Exploratory Research via Topic Extraction, Brand Reputation Analysis, and Topic Emotions or VERTE- BRATE is proposed. It is a pipeline that leverages topic modeling via multi- lingual BERT, analysis of vaccine brand reputation through cosine similarity of the BERT topic embeddings, hierarchical clustering of the BERT topic embeddings, and sentiment classification of the Facebook comments that are labeled automatically by the clades produced by the hierarchical clustering model. The BERT topic model extracted 25 topics' of which are indeter- minable, 2 of which are positive, and 21 topics are negative comments. These topics are precisely clustered through Ward's linkage hierarchical clustering. The model suggests that COVID-19 misinformation and COVID-19 vaccine misinformation are emergent properties of COVID-19 public opinion. Using supervised techniques to model such misinformation is unnecessary. In terms of the temporal distribution of these topics, the analysis suggests that neg- ative comments drastically increase when the Department of Health Philip- pines posts about other countries. It can also be noted that the huge numbers of laugh reactions throughout the Facebook posts without the presence of any humorous contententail that the Department of Health Philippines has a public image issue. In this instance, people tend to react the way they do, not because of the posts but because of who posted them. As for the brand reputation of the different COVID-19 vaccines, Pfizer and Moderna are doing relatively well especially Pfizer's vaccine. Mean- while, Sputnik V, AstraZeneca, and Sinovac suffer from negative associations. Additionally, clade-assisted sentiment classification effectively models public sentiment. The best-performing classifier, LightGBM, that is proposed in the study has managed to perform with 92.4% accuracy. It also has a strong level of agreement in terms of Cohen's kappa statistic, with a value of 0.847. Our present study highlights the persisting prevalence of COVID-19 vac- cine misinformation in social media. Conspiracy beliefs and other forms of misinformation had been noted as a significant predictor of complete vac- cine hesitancy (Al-Sanafi and Sallam, 2021). We noticed that DOH and its entities offered no responses to the comments posted by netizens under these infographics. Meta-analytic evidence suggests the importance of iden- tifying misinformation most susceptible to correction, and engage experts in responding to misinformation (Walter et al., 2020). We recommend for DOH to form a social media team composed of health care professionals and in- terdisciplinary communication practitioners whose mandate is to respond to misinformation found in theof their posts. This engaging way of correcting false vaccine information will not only help quell vaccine doubts, but also hopefully improve the image of DOH among citizens. Also, our present study suggests that instead of drawing aspirational sen- timents from people, posting about the success of wealthier countries in terms of vaccination only intensifies Filipinos' confidence in the country's vaccines. The high numbers of laugh reactions on the Face- book posts by the same agency without any humorous content suggest that the reactors of these posts tend to react the way they do, not because of what the posts are about but because of who posted them. Keywords: COVID-19, vaccine brand reputation, vaccine hesitancy, sentiment analysis, topic modeling Highlights 1. COVID-19 vaccine hesitancy in the Philippines is primarily due to mis- information 2. Posting about the COVID-19 situation in other countries increase neg- ative comments 3. Laugh reactions on humorless posts suggest they are targeted at the source of posts 4.",0
"Abstract. Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue sys- tems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user’s input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation. Keywords: natural language understanding, ontology, Ontolex, data virtualization. Introduction The corporate automated system users expect them to be “intellectual” enough to give precise answers to their questions. It implies that a system must give meaningful answers in the dialogue with a user and ask clarifying questions as a human would do. To achieve that, the system must deal with the structured representation of each ques- tion and answer of a dialogue, as well as with a structured data required to give an answer. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets – for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1)  Conceptual model construction for representing the users’ domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2)  Disparate corporate data sets into a virtual Knowledge Graph. 3)  The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of user’s natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a user’s question into a query to the graph. The func- tional requirements to this system are: 1)  It must find the class or property which instances would be an answer to the user’s question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2)  The system’s answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the user’s query, includ- ing visualization of the relations chain that led to these objects. 3)  If there is no unambiguous answer to the user’s query, the system should ask clarifying questions. 4)  The system should answer user’s clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with the big graphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a KG, which also contains data on the telemetry sensors and the parameters they measure. We have created a compact ontology for this domain, which offers diversity of the re- lations between objects and the playground for making queries involving 3-4 related graph vertices. In the real use case such a system will include a much extensive set of facts on the various aspects of the enterprise activity. The facts will be gathered into KG from the variety of data sources, such as corporate applications’ databases, the or- ganizational and administrative documents, and will be consolidated by a data virtual- ization platform. The resulting graph should be available with SPARQL interface. Related works We have used some well-known technologies in our NL processing pipeline. Named entities recognition techniques developed over long time [Shen, 2015]. POS tagging is considered in [Mikic, 2009; Wu, 2020; Huang, 2015; Le, 2018; Piccinno, 2014]. Since our ideas are based on moving from syntactical relations to semantical relations (see: [Melchuk, 1999; Gerd, 2005; Kolshansky, 1980; Banarescu, 2013; Fensel, 2003]), we need a morphological and syntactical analysis of sentences and coreference clusters (chains) finding. Morpho-syntactical properties of tokens, analysis of syntactic struc- ture of a phrase, syntactic relationships discovery between words are considered in [Ju- rafsky, 2008]. Co-reference resolution techniques review is given in [Zheng, 2011]. Pre-trained language models such as BERT [Kuratov, 2019] can be used to deter- mine the context-depending word meaning. Transformer models allow vectorization of the word sequences, which can be used to interpret their meanings [Kalyan, 2021]. The ability to retrieve an entity from a Knowledge Base given a textual input is a key com- ponent for several applications (see: [Ferrucci, 2012; Slawski, 2015; Yang, 2018]). We have used Ontolex1 ontology, developed by W3C Ontology-Lexica Community Group2, to formalize lexical model. This ontology was published in 20163 and it is well documented. Its key features are described in its developer’s publications, for example [Cimiano, 2011], [McCrae, 2017]. Ontolex is often used in the computational linguis- tics tasks, for examples see: [Declerck, 2019, Abgaz, 2020]. It is also used in the inter- disciplinary projects, particularly it was used in the European Commission PMKI pro- ject (Public Multilingual Knowledge Infrastructure) in 20184. Conclusions In the quest of the most effective combination of the machine learning and KG tools we have developed the architecture of the natural language understanding pipeline. The algorithms of establishing links between tokens of the recognized text and the domain ontology play the key role in it. The lexical ontology layer, describing words usage to denote domain concepts, is necessary to make them work. It allows semantic ambigui- ties resolution considering semantic fields in which the words are included. The practical value of the developed architecture significantly increases if the KG queries it generates to answer user questions are processed by the data virtualization platform which can access the huge arrays of the data disparate in the corporate stor- ages. It opens the way to create a dialogue system which allows user to discover previ- ously hidden, implicit, or virtually unavailable information from these storages, and involves it in the business processes including decision making support. This way of KG usage can be described as the valuable component of the true Knowledge Manage- ment System of an enterprise. The further work on our NL solution improvement includes quantitative question answering implementation (object counting, searching for maximal/minimal values, summation, and other aggregation methods), work with the date and numeric intervals, temporal relations recognition. These functions are valuable in the corporate data pro- cessing.",1
"Abstract. Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue sys- tems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user's input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation. Keywords: natural language understanding, ontology, Ontolex, data virtualization. Introduction The corporate automated system users expect them to be ""intellectual"" enough to give precise answers to their questions. It implies that a system must give meaningful answers in the dialogue with a user and ask clarifying questions as a human would do. To achieve that, the system must deal with the structured representation of each ques- tion and answer of a dialogue, as well as with a structured data required to give an answer. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1) Conceptual model construction for representing the users' domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2) Disparate corporate data sets into a virtual Knowledge Graph. 3) The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of user's natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a user's question into a query to the graph. The func- tional requirements to this system are: 1) It must find the class or property which instances would be an answer to the user's question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2) The system's answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the user's query, includ- ing visualization of the relations chain that led to these objects. 3) If there is no unambiguous answer to the user's query, the system should ask clarifying questions. 4) The system should answer user's clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with thegraphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a graph by the graph-based data virtualization platform. The graph is restricted by a restricted vocabulary describing the system's relations chain that led to these objects and objects are the answers to the user's query. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1) Conceptual model construction for representing the users' domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2) Disparate corporate data sets into a virtual Knowledge Graph. 3) The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of user's natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a user's question into a query to the graph. The func- tional requirements to this system are: 1) It must find the class or property which instances would be an answer to the user's question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2) The system's answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the user's query, includ- ing visualization of the relations chain that led to these objects. 3) If there is no unambiguous answer to the user's query, the system should ask clarifying questions. 4) The system should answer user's clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with the big graphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites.",0